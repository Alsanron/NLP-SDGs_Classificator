Computers in Human Behavior 61 (2016) 198e204

Contents lists available at ScienceDirect

Computers in Human Behavior
journal homepage: www.elsevier.com/locate/comphumbeh

Full length article

Is there a gender difference in interacting with intelligent tutoring
system? Can Bayesian Knowledge Tracing and Learning Curve Analysis
Models answer this question?
Leyla Zhuhadar a, Scarlett Marklin b, Evelyn Thrasher a, Miltiadis D. Lytras c, *
a
b
c

Western Kentucky University, Gordon Ford College of Business, Bowling Green, KY 42101, USA
Tallahassee, Florida Area, Florida State University, USA
6 Gravias Street GR-153 42, Aghia Paraskevi, Athens, The American College of Greece, Greece

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 27 October 2015
Received in revised form
17 February 2016
Accepted 18 February 2016
Available online 17 March 2016

Multiple studies have been conducted on Project LISTEN, an intelligent tutoring system (ITS) used to
analyze educational learning through case analysis of students' interactions with ITS. Studies have
dened the phenomenon by exploring what happens when/if questions and analyzing these in the
context of the specied phenomenon occurrence. While ITS often focus on student decisions regarding
when and how to use the system's resources, we suggest further analysis and monitoring are needed to
get the best results from these systems. In this study, we argue that boys interact differently with ITS
than girls. This nding is evident in results from both the Bayesian Knowledge Tracing and Learning Curve
Analysis models.
 2016 Elsevier Ltd. All rights reserved.

Keywords:
Intelligent tutoring systems
Bayesian knowledge tracing
Learning curve analysis

1. Introduction and related works
For almost a decade, faculty members from Western Kentucky
University (WKU1) have been designing a variety of intelligent
systems, such as CaseGrader (Crews & Murphy, 2007) and HyperManyMedia (Zhuhadar & Nasraoui, 2008). In CaseGrader, Crews &
Murphy (2007) used intelligent methods to provide personalized
automated scoring to students based on their performance in
solving mathematical or business problems within Microsoft Excel.
On the other hand, the HyperManyMedia2 platform provided recommendations to students based on their previous browsing activities; these recommendations were based on articial
intelligence algorithms where ontology is dened and semantic
web is utilized to provide the most accurate recommendations to
students based on their level within the course. Prior research
describes this process in more detail (Zhuhadar, 2015; Zhuhadar,

* Corresponding author.
E-mail addresses: leyla.zhuhadar@wku.edu (L. Zhuhadar), sm14as@my.fsu.edu
(S.
Marklin),
evelyn.thrasher@wku.edu
(E.
Thrasher),
mlytras@acg.edu
(M.D. Lytras).
URL: http://www.acg.edu
1
http://www.wku.edu.
2
http://hmm.wku.edu/.
http://dx.doi.org/10.1016/j.chb.2016.02.073
0747-5632/ 2016 Elsevier Ltd. All rights reserved.

Carson, Daday, & Nasraoui, 2015; Zhuhadar & Nasraoui, ;
Zhuhadar, Nasraoui, & Wyatt, 2007; Zhuhadar, Nasraoui, & Wyatt,
2009a, b; Zhuhadar, Nasraoui, Wyatt, & Romero, 2009; Zhuhadar,
Nasraoui, Wyatt, & Yang, 2010; Zhuhadar & Yang, 2012).
However, in this research we utilize a platform (Project LISTEN3)
designed and developed by researchers at Carnegie Mellon University.4 Many studies have been conducted on Project LISTEN
(Huang & Mostow, 2015a, b; Mostow & Prieditis, 2014; Yuan,
Chang, Taylor, & Mostow, 2014), an intelligent tutoring system
(ITS) used to analyze educational learning through case analysis of
students' interactions. Many of these studies required specic details such as student choices, timing intervals, student outcomes
(predictions), classiers, and types of help given. The complexity of
these details can make them laborious to browse and gather. (Beck,
Chang, Mostow, & Corbett, 2008) address three simple techniques
to make data mining easier to interpret, stating researchers can
directly store interactions and index them into a database, which
allows ease of access without the need to browse log les.
Beck et al. (2008) also addressed a method to identify a tutorial
event by linking the student, computer, and time interval together,
as well as, restraining time intervals to dene tutorial interactions

3
4

https://www.cs.cmu.edu/~listen/.
http://www.cmu.edu/.

L. Zhuhadar et al. / Computers in Human Behavior 61 (2016) 198e204

within a hierarchical structure. The efciency, generality, usability,
and utility of a session browser aids in the successful facilitation of
data mining efforts and will continue to be used in future research.
Cen, Koedinger, & Junker (2006) looked at improving the analysis of intelligent learning systems by focusing on the cognitive
model, a set of predened rules that inuence the process of student problem solving tailored to provide helpful feedback and hints
while increasing difculty to improve student learning and
knowledge. In their research, three questions were asked regarding
ITS: 1) how can researchers describe learning behavior in existing
cognitive models; 2) can a learning rate be established for the
student; and 3) how can the cognitive model be improved inexpensively by dening measures of complexity to improve the curriculum for various learning styles (Cen et al., 2006). The
researchers proposed the Learning Factor Analysis, a semiautomated method used in Java that combines statistics, human
experience, and a combinatorial search (heuristic guidance) to add
to tutor development, giving better insight into the analysis of data
and log les through a knowledge-tracing algorithm (Cen et al.,
2006).
While ITS often focus on learner control, the power of use is
given to the student. Therefore, the student decides when and how
to use the system's resources, essentially self-monitoring and
judging when/if they can benet from the help provided. (Aleven &
Koedinger, 2000), using the PACT geometry tutor, suggested that
students often times do not have the required cognitive skills to
take advantage of the resources available through the tutor. They
argue for a meta-cognitive help-seeking model that can monitor
the student's strategies in using resources provided from the tutor
to lend the most support for on-request help or glossary access
(Aleven & Koedinger, 2000). Results indicated that students used
the tutor's intelligent help facilities (hints) more frequently than
the non-intelligent resources (glossary).
Aleven & Koedinger (2000) argue that the meta-cognitive helpseeking model could be implemented as a production rule model
and could be used for model tracing, taking into account the student model information and whether a student might be overusing or under-using resources. With the meta-cognitive strategy,
the tutor would make greater use of the glossary, a low cost
resource, to nd relevant information and apply it to the current
problem. The tutor would also initiate intelligent help after two
errors, thereby reducing the overall number of errors and the time
required. They state that in order for an intelligent tutor system to
be adaptive, meta-cognitive skills must be taken into consideration to produce better learners (Aleven & Koedinger, 2000).
Regarding response intervals, Joseph (2005) further addresses
the issue of time as an indicator and predictor of how much a
student learns. Previous research conducted by Aleven and
Koedinger (2000) indicated that students do not always try their
best in solving problems; therefore, Joseph (2005) proposed an
engagement tracing model that would better model student
engagement by primarily focusing on disengagement. This
approach would analyze the response times and correctness of
responses to model overall engagement while using an intelligent
tutor. The method is inexpensive and sensitive enough to detect
temporal changes during the student's interactions with the tutor
(Joseph, 2005). By modeling a student's engagement, research can
predict how much an individual will benet from using intelligent
tutors, while allowing for modications that adapt to student interactions, for greater learning efciency.
, Mostow, Luengo, & Guin (2013) noted the challenge in
Lalle
evaluating student models by their impact on the success of an
intelligent tutor's decision about which type of help to offer students. Individualized help can have a strong impact on learning;
therefore, the better the tutor can adapt its help to the student and

199

situation, the more likely the student will learn from it. Using logs
of randomized tutorial decisions and ensuing student performance,
 et al. (2013) trained a classier to predict tutor decision outLalle
comes (success or failure) based on situational features, such as
student and task. Using historical data to simulate a policy by
extrapolating its effects from the subset of randomized decisions
that happened to follow the policy, the authors tested the method
on data logged by Project LISTEN's reading tutor, which randomly
 et al., 2013). They also
chooses what type of help to give (Lalle
compared the impact of student models (knowledge-tracing
model, constraint based model, and control based approach) on the
expected success of tutorial decisions (greatest probability) to offer
help. Using the learner policies to pick which type of help yields the
greatest probability of success, taking into consideration the types
of help available, student features, domain features and the student
model, the measure has greater utility for measuring student
 et al., 2013) found that all learned policies tested
learning. (Lalle
improved the reading tutor's expected success compared to its
original randomized decisions. Yet, this only applies to tutors that
make decisions based on multiple types of available help.
Furthermore, (Beck & Mostow, 2008) assessed learning decomposition to examine how much students learn from instruction.
Learning decomposition determines the relative adequacy of
different types of learning opportunities using a generalization of
the learning curve analysis with non-linear regression. The authors
suggested that students learn words better when they read a wide
selection of stories rather than reading the same story multiple
times (Beck & Mostow, 2008). Reading new stories, thereby
expanding the exposure to words, is good for long-term learning.
Beck and Mostow's model further indicated that when students
reread words, the effectiveness of learning that word decreased,
supporting the argument that students benet less from mass
practice (2008). Individuals who beneted from mass practice and
repeated reading were older, less procient readers who were
tagged as requiring learning support. As (Beck & Mostow, 2008)
indicated, learning factor analysis, as noted earlier by Cen et al.
(2006), is used to create better tting learning curves; and
learning decomposition (focused on determining impact of practice) is concerned with greater understanding of student learning
potential. Gonz
alez-Brenes & Mostow (2011) assessed the prediction value of models by using a regularized logistic regression,
arguing that conventional classier learners require large amounts
of data to avoid over-tting and do not generalize well to unseen
examples (predictions). Using regularized logistic regression makes
it feasible to classify dialogues in a high dimensional space and to
demonstrate on real data from Project LISTEN's Reading Tutor
(Gonz
alez-Brenes & Mostow, 2011). One classier predicts task
completion to characterize differences in the behavior of children
when they choose the story they read (71% accuracy), while
another classier (73.6% accuracy) infers who chose the story based
on dialogue. Their approach solved two problems in classifying
children's oral reading dialogue, predicting which stories they
would nish and characterizing the student behavior according to
who chose the stories. They achieved a 71.1% and a 73% cross
validated classication accuracy on a balanced set of data from
unseen students, indicating that regularized logistic regression is
lezthe best for assessing these problems in prediction (Gonza
Brenes & Mostow, 2011).
As previous research has shown, several models have been
tested to assess how and when students learn and whether or not
tutor help is effective in increasing student learning. In assessing
the ndings, some researchers have called for a unied framework
that simultaneously allows both the skills and impact of practice to
vary (Beck & Mostow, 2008). Some researchers have addressed the
cost of analyzing ITS data (Yuan et al., 2014), suggesting an

200

L. Zhuhadar et al. / Computers in Human Behavior 61 (2016) 198e204

2. Comparative study

Fig. 1. Bayesian knowledge tracing.

inexpensive EEG model that can assess information about
comprehension, but noting that future work needs to increase
prediction accuracy by assessing other dimensions of knowledge
and applying those assessments to improve learning outcomes.
Other research has examined modeling dialogue, arguing that high
dimensional space opens the door from small manually generated
sets of features to richer, automatically generated sets of features,
thus improving the ability to assess student learning outcomes
lez-Brenes & Mostow, 2011).
with regard to ITS (Gonza
 et al. (2013) compared the impact of student models
Lalle
(knowledge tracing model, constraint based model, and control
based approach) on the expected success of tutorial decisions to
offer help through learner policies. These policies are vulnerable to
under-covering and over-tting; therefore, more accurate student
models such as LR-DBN and stronger classiers such as Support
Vector Machines (SVM) or Random Forests should be used to
improve the prediction accuracy of successful help in ITS. While
there is still debate on what method works best, there seems to be
consensus on the Bayesian Evaluation and Assessment approach
(Beck et al., 2008), which assesses both student and tutorial interventions, allowing students to transfer knowledge gained to
later problems as a model for predicting learner outcomes and
learner factor analysis (Cen et al., 2006). Furthermore, Beck et al.'s
research on the two effects of help, scaffolding immediate performance and boosting actual learning, argues that evaluations of
synthetic data are the most promising (2008).
During The 10th Annual LearnLab Summer Research School,5 at
Pittsburg Science of Learning Center, we acquired a large dataset
from Project LISTEN.6 This dataset consisted of students' knowledge
tracing while interacting with the system. This data provided information about 90,000 student-word encounters. Each word is
considered a skill that can be known, but that knowledge is hidden
from us. We used Bayes Net Toolbox for Student Modeling (BNTSM7) to facilitate the use of dynamic Bayes nets in the student
modeling community. In this study, we argue that boys interact
with Project LISTEN's Intelligent Tutoring System differently than
girls. This nding was not only evident by using Bayesian Knowledge
Tracing but also by using Learning Curve Analysis. In the following
paragraphs, we discuss our ndings in detail.

5
6
7

http://www.learnlab.org/opportunities/summer/.
http://www.cs.cmu.edu/~listen/pubs.html.
http://www.cs.cmu.edu/~listen/BNT-SM/.

A challenge to evaluating ITS is to evaluate student models by
their impact on the success of help given. Which type of help do
students really need? Is the help better addressed on an individualized or mass practice basis? Beck & Mostow (2008) looked at
three approaches for evaluating this issue, including experimental
trials, learning decomposition, and Bayesian Evaluation and
Assessment (using Bayesian networks). The model controlled for
student knowledge while estimating the intervention's effectiveness. This is similar to item response theory, which enables better
comparisons of students across groups by estimating student prociency and question difculty. Findings indicated that the only
method that helped students with long-term learning was the
Bayesian Evaluation and Assessment approach, which assesses
both student and tutorial interventions, allowing students to
transfer knowledge gained to later problems (Beck et al., 2008).
Interestingly, the other two actually hurt student learning e a
negative consequence. Furthermore, the authors indicated that if a
student does not know the skill, they are more likely to generate a
correct response with help than without. This supports the idea
that tutor help has a scaffolding effect on assisting immediate
performance; but the teaching effect is more benecial in the long
run than the scaffolding effect, further showing that student
knowledge and student performance are indeed affected by tutor
help.
Prior research was conducted on Project LISTEN (Yuan et al.,
2014), showing that reading comprehension assessment can be
costly and obtrusive. To address these issues, the research analyzed
the efcacy of EEG devices for assessing reading comprehension
(the correctness of responses) in the classroom, taking into
consideration that these devices are unobtrusive and low cost. Yuan
et al. (2014) used EEG signals to produce above-chance predictors
of student performance. Their ndings indicated that the classier
achieved signicantly above-chance accuracy trained on only the
reading portion; but there were no above-chance predictions made
by the model. Therefore, no conclusions could be drawn about
different binary distinctions (Yuan et al., 2014). While the EEG
model could not successfully make above-chance predictions, it
does suggest that some information regarding student comprehension can be teased out by using EEG devices (Yuan et al., 2014).
lezIn our study, we use the same methods proposed by Gonza
Brenes & Mostow (2011) to assess student learning outcomes with
regard to ITS. We also look at three approaches for evaluating this
issue, including experimental trials, learning decomposition, and
Bayesian Evaluation and Assessment; however, our goal is to
examine if there is a signicant difference in the way students
interact with the tutoring system. More specically, is there a
gender difference in these interactions? If so, does this support the
idea of gender-specic tutoring system designs?
The sections that follow provide details of our framework, our
research questions, our ndings, and our future research.

3. Proposed research framework and data analysis
Project LISTEN8 logs its interactions with children directly into a
database. These interactions are archived at multiple grain sizes
ranging from sessions to stories, from sentences to utterances, from
individual words to mouse clicks and key presses. The Reading
Tutor administers within-subject randomized controlled trials by
selecting randomly among alternative tutorial actions; the

8

http://www.cs.cmu.edu/~listen/pubs.html.

L. Zhuhadar et al. / Computers in Human Behavior 61 (2016) 198e204

201

Table 1
Excerpt from Evidence.xls.
Id

Utterance start time

Utterance_sms

Target_word_number

Skill

fBS7-7-1990-02-03

fDL7-5-1993-11-28
Help
1

2

5/12/00 17:52

640

8

WORLD

10/13/04 13:56
Knowledge
1

421
asr_accept
2

13
Condence score
.0668763

WORLD
asr_condence
1

2

2

.0430581

1

Table 2
Excerpt from Param_table.xls.

Table 4
Effect size between genders.

Skill

Num of users

Num of cases

ll

L1

skill_HELLO

skill_WORLD
Skill
skill_HELLO

skill_WORLD

14

23

3.609

.744

46
Guess
.721

218
Slip
.000005

90.177
t
.982517

.695
Forget
.000001

.634

.113612

.256071

.000001

resulting experiments have had as many as 180,000 trials. In
addition, the logged data includes text and speech, with some gaze
and EEG as well.
We used the Session Browser to explore individual interactions
and MySQL queries to aggregate them. Finally, we used tools such
as Matlab and SAS to run statistical analysis and machine learning

Knew
Learn
Guess
Slip

Male

Female

Effect size (d)

P_value

.41
.19
.70
.07

.44
.20
.71
.06

.580
.300
.507
.032

<.0001
<.0001
<.0001
<.0001

(.03)
(.03)
(.04)
(.03)

(.04)
(.04)
(.04)
(.03)

algorithms on the resulting data. We used various approaches to
predict whether a child would nish a story or not. For instance, we
analyzed the relative value of different types of reading practice for
oral reading uency; and we compared the efcacy of different
types of help being used by each child on hard words.
Dynamic Bayes Nets (DBNs) provide a powerful way to represent and reason uncertainty in time series data and are, therefore,
well-suited to model a student's changing knowledge state during
skill acquisition (Murphy, 2001).

Table 3
Excerpt from inference_result.xls.

fCA8

fCA7
Help
1

1

Utterance start time

Utterance_sms

Target_word_number

Skill

2004-11-09/11:25:24

218

1

HELLO

2005-01-24/11:33:12
Knowledge
.801846

468
asr_accept
2

1
Condence score
.124751

HELLO
asr_condence
1

.997498

2

.0785152

1

Fig. 2. Proposed research framework.

202

L. Zhuhadar et al. / Computers in Human Behavior 61 (2016) 198e204

Table 5
Wilcoxon Scores.

Table 6
Comparison between models.
Model

Student N

Word N

Prior knowledge

Transfer

Guess

Slip

p (Know)

p (Cn)

Female specic
Male specic
Aggregated female
Aggregated male
Aggregated

33
30
33
30
63

4811
3322
5199
4395
5829

.41
.40
.41
.41
.41

.19
.18
.19
.19
.19

.69
.68
.69
.69
.69

.06
.06
.06
.06
.06

.61
.50
.61
.60
.60

.85
.81
.85
.85
.85

(.03)
(.02)
(.03)
(.03)
(.05)

Table 7
Weighted average per word encounter.

Male
Female

Accepted

Fluency

Help seeking

.88
.88

.8
.8

.03
.03

Accordingly, we used Knowledge Tracing (KT) for our student

9

https://code.google.com/p/bnt/.

(.01)
(.01)
(.02)
(.02)
(.05)

(.03)
(.02)
(.03)
(.03)
(.22)

(.01)
(.01)
(.01)
(.02)
(.03)

(.23)
(.21)
(.23)
(.22)
(.22)

(.07)
(.06)
(.07)
(.06)
(.06)

modeling toolbox.9 The goal was to assess the student's knowledge
from his or her observed actions at each successive opportunity to
apply a skill by using the updated Knowledge Tracing estimated
probability that a student knows the skill. These updates are
associated with skill-specic learning, performance parameters,
and the observed student performance (evidence).
Fig. 1 shows a diagram of DBN-SM workow. Three parameters
are used to estimate the probability that a student knows the
encountered word, represented in the diagram as P(Know/Learn).
These parameters are: P(already Knew), P(Guess), and P(Slip). In the
next sections, we present BNT-SM outputs and our research
question.

L. Zhuhadar et al. / Computers in Human Behavior 61 (2016) 198e204

ERROR RATE (PERCENT)

Male Learning Curve
for the Word "Different"

203

4. Proposed research framework

0.4

Fig. 2 illustrates our research framework using two models:
Model A (boys) and Model B (girls). Our null hypothesis states that
there is no difference between these two models. Therefore, if we
compare the knowledge tracing parameters, P(Know/Learn),
P(already Knew), P(Guess), and P(Slip), between theses two models,
we should not nd a signicant difference.

0.2

4.1. Research question?

1
0.8
0.6

0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
PROBLEM SEQUENCE
Fig. 3. Male learning curve.

 Is there a difference between girls (female) and boys (male) in their
ways of interacting with the intelligent tutoring system?
5. Experiments
5.1. Data set

ERROR RATE (PERCENT)

Female Learning Curve
for the Word "Different"
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

We queried a dataset of student-word encounters (a balanced
dataset of easy words and hard words). For complete results, refer
to these links (male results,10 female results11).
5.2. Building models

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
PROBLEM SEQUENCE
Fig. 4. Female learning curve.

3.1. Generated output
Evidence.xls consists of Bayes Net data for each skill from all
children. Evidence data are comprehensive. Hidden variables and
missing observations are marked with NULL. For discrete variables,
the values cannot be 0 because Matlab uses array subscripting
(starting with 1). Therefore, we often increment the discrete variables by 1. In the case of a binary variable, 1 is used for 0 or false
values, whereas 2 is used for 1 or true values, as shown in Table 1.
Param_table.xls consists of BNT-SM estimates for skill-specic
models where a Bayes Net is trained for each skill in the training
dataset, as shown in Table 2.
Inference_result.xls has a format identical to that of evidence.xls, except that BNT-SM performs inferences on the hidden
variables and estimates their values as follows:
 For binary hidden variable X, the estimated value will be the
probability of X  1 (represented X   2 in Matlab).
 For discrete hidden variables Y with values greater than 2, the
probability of all values will be output in the form of [p1; p2; 
pn;]
 By default, BNT-SM infers posterior probability (after observing
the evidence)
 If instead, you want to infer prior probability (before observing
the evidence, e.g. classic Knowledge Tracing), you can make a
switch in RunBnet.m when calling inference_bnet.m, as shown
in Table 3.

We used BNT-SM to generate these four training parameters for
each student-word encounter: P(Know/Learn), P(already Knew),
P(Guess), and P(Slip).
Table 4 shows how our population holds a signicant effect size
between genders for these four training parameters (please refer to
the P_value associated with each parameter).
We used the Wilcoxon signed-rank test instead of t-test since
our observations are not independent. The Wilcoxon signed-rank
test is a nonparametric test used to compare two sets of scores
that come from the same participants. This can occur when we wish
to investigate any change in scores from one point in time to
another, or when individuals are subjected to more than one condition. For more information, refer to the resource below.12 The
Wilcoxon Scores of P(Know/Learn), P(already Knew), P(Guess), and
P(Slip) are reported in Tables 5 and 6.
Table 6 shows the number of encountered student-words for
each model. In addition, we listed the average value of each
parameter. For more information about this dataset, refer to the
resource below.13 In addition, Table 7 provides the weighted
average per word encounters for boys (male) and girls (female).
6. Findings
6.1. Bayesian Knowledge Tracing
Despite the observation of females encountering more words,
we found that the means and standard deviations do not appear to
differ between the gender-specic model and the aggregated
model. However, a difference is seen across genders within the
gender-specic model, as shown in Table 6. When comparing the
percentage of a correctly spoken word (accepted), the rate of familiarity the student has with a word (uency), and the percentage

10
https://www.dropbox.com/home/learnlab2014/bnt-sm/results/comparison_
results/male_model.
11
https://www.dropbox.com/home/learnlab2014/bnt-sm/results/comparison_
results/female_model.
12
https://statistics.laerd.com/spss-tutorials/wilcoxon-signed-rank-test-usingspss-statistics.php.
13
https://www.dropbox.com/sh/0hc030ct4u2p08j/AAB6WiLh1AOE7NFYeNblZ5za?dl0.

204

L. Zhuhadar et al. / Computers in Human Behavior 61 (2016) 198e204

of help seeking for a word, it appears there are no differences in the
weighted averages between genders, as shown in Table 7.
On the other hand, by looking at Wilcoxon Scores, seen in
Table 5, we see that our population holds a signicant effect size
between gender for these four training parameters (Z-score14).
More specically, we can report the following ndings:
1. The P_already Knew, P_Guess, P_Know/Learn for girls (female) are
signicantly higher than boys (male), and;
2. The P_Slip for boys (male) is signicantly higher than girls (female).

6.2. Learning curve analysis
In addition to investigating Bayesian Knowledge Tracing, we
compared the Learning Curve Analysis for a specic word
Different between male and female, as shown in Figs. 3 and 4.
Comparing these gures, we see a signicant difference between
boys and girls in language acquisition. They learn the same words in
a different way. Boys tend to have a high error rate percent. In
addition, boys tend to need more time (more problems to solve) to
excel. Of course, this analysis needs further research, such as
observing these patterns on different types of words (easy vs. hard)
and bigger sets of words. In addition, we should also consider the
context in which these words appear. In general, we conclude that a
difference exists between genders in the way children interact with
the tutoring systems. Therefore, we suggest a gender specic
tutoring system where, for example, the reading materials are
correlated with gender type.
We would assume that a boy would be more interested in
reading a story about car races, whereas a girl would be more
interested in reading princess stories. Knowing there is a difference
does not mean that girls are better than boys regarding reading
comprehension. Rather, our ndings suggest that there is a difference in their way of learning; and we should embrace this difference to effectively promote greater reading comprehension by
providing reading materials that would be of interest to them.
7. Conclusion and future works
The LearnLab Summer Research School15 at the Pittsburgh Science
of Learning Center provides researchers with opportunities to access
large datasets from various projects dealing with, but limited to:
cognitive psychology or educational psychology, computersupported collaborative learning, development of technologyenhanced course content, and analysis of student data or educational data mining. In this research, we used a large dataset
generated by students using Project LISTEN.16 This dataset consisted
of students' knowledge tracing while interacting with the system.
This data provided information about 90,000 student-word encounters. We found that boys (males) interact with Project LISTEN's
ITS differently than girls (females). This nding was not only
evident by using the Bayesian Knowledge Tracing but also by using
Learning Curve Analysis. While ITS often focus on the student decisions regarding when and how to use the system's resources, we
suggest further analysis and monitoring are required to get the best
results from these systems. Considering gender differences in the
way students learn, in addition to the context of the reading materials presented, is essential, especially for boys. In this study, we

14
If you have a large number of participants, you can convert Wilcoxon into a zscore.
15
http://www.learnlab.org/opportunities/summer/.
16
http://www.cs.cmu.edu/~listen/pubs.html.

argue that boys learn vocabulary differently than girls. This nding
was evident in both Bayesian Knowledge Tracing and in Learning
Curve Analysis. Our future work will include an extension of the
learning curve analysis considering Linear Mixed Effects Models.

References
Aleven, V., & Koedinger, K. R. (2000). Limitations of student control: do students
know when they need help?. In Paper presented at the intelligent tutoring
systems.
Beck, J. E., Chang, K.-m., Mostow, J., & Corbett, A. (2008). Does help help? Introducing the Bayesian evaluation and assessment methodology. In Paper presented at the intelligent tutoring systems.
Beck, J. E., & Mostow, J. (2008). How who should practice: using learning decomposition to evaluate the efcacy of different types of practice for different types
of students. In Paper presented at the intelligent tutoring systems.
Cen, H., Koedinger, K., & Junker, B. (2006). Learning factors analysisea general
method for cognitive model evaluation and improvement. In Paper presented at
the intelligent tutoring systems.
Crews, T., & Murphy, C. (2007). CaseGrader: Microsoft ofce Excel 2007 casebook with
autograding technology. Course Technology Press.
Gonz
alez-Brenes, J. P., & Mostow, J. (2011). Classifying dialogue in high-dimensional
space. ACM Transactions on Speech and Language Processing (TSLP), 7(3), 8.
Huang, Y.-T., & Mostow, J. (2015a). Evaluating human and automated generation of
distractors for diagnostic multiple-Choice cloze questions to assess children's
reading comprehension. In Paper presented at the articial intelligence in education: 17th International Conference, AIED 2015. Madrid, Spain, June 22-26,
2015. Proceedings.
Huang, Y.-T., & Mostow, J. (2015b). Evaluating human and automated generation of
distractors for diagnostic multiple-choice cloze questions to assess children's
reading comprehension. In Paper presented at the articial intelligence in
education.
Joseph, E. (2005). Engagement tracing: Using response times to model student
disengagement. In Articial intelligence in education: Supporting learning through
intelligent and socially informed technology (Vol. 125, p. 88).
, S., Mostow, J., Luengo, V., & Guin, N. (2013). Comparing student models in
Lalle
different formalisms by predicting their impact on help success. In Paper presented at the articial intelligence in education.
Mostow, J., & Prieditis, A. E. (2014). Discovering admissible search heuristics by
abstracting and optimizing. Machine Learning Proceedings, 1989, 240.
Murphy, K. (2001). The bayes net toolbox for matlab. Computing Science and Statistics, 33(2), 1024e1034.
Yuan, Y., Chang, K.-m., Taylor, J. N., & Mostow, J. (2014). Toward unobtrusive measurement of reading comprehension using low-cost EEG. In Paper presented at
the Proceedings of the Fourth International Conference on learning analytics and
knowledge.
Zhuhadar, L. (2015). A synergistic strategy for combining thesaurus-based and
corpus-based approaches in building ontology for multilingual search engines.
Computers in Human Behavior, 51, 1107e1115.
Zhuhadar, L., Carson, B., Daday, J., & Nasraoui, O. (2015). A universal design infrastructure for multimodal presentation of materials in STEM programs: universal design. In Paper presented at the Proceedings of the 24th International
Conference on World Wide Web, Florence, Italy.
Zhuhadar, L., & Nasraoui, O.. Personalized cluster-based semantically enriched web
search for E-learning. Paper presented at the CIKM 08: ONISW the 2nd International workshop on ontologies and information systems for the semantic
web.
Zhuhadar, L., & Nasraoui, O. (2008). Semantic information retrieval for personalized
E-learning. In , 20th IEEE International Conference on ICTAI '08: Vol. 1. Tools with
articial intelligence, 2008 (pp. 364e368).
Zhuhadar, L., Nasraoui, O., & Wyatt, R. (2007). Knowledge mining for adaptive
multimedia web-based educational platform.
Zhuhadar, L., Nasraoui, O., & Wyatt, R. (2009a). Automated discovery, categorization
and retrieval of personalized semantically enriched E-learning resources. In
International Conference on semantic computing, 0 pp. 414e419).
Zhuhadar, L., Nasraoui, O., & Wyatt, R. (2009b). Dual representation of the semantic
user prole for personalized web search in an evolving domain. In Paper presented at the Proceedings of the AAAI 2009 Spring Symposium on social semantic
web, where Web 2.0 meets Web 3.0.
Zhuhadar, L., Nasraoui, O., Wyatt, R., & Romero, E. (2009). Multi-model ontologybased hybrid recommender system in e-learning domain. In Paper presented
at the Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on
web intelligence and intelligent agent technology (Vol. 03).
Zhuhadar, L., Nasraoui, O., Wyatt, R., & Yang, R. (2010). Visual knowledge representation of conceptual semantic networks. Social Network Analysis and Mining,
1e11.
Zhuhadar, L., & Yang, R. (2012). The impact of social multimedia systems on
cyberlearners. Computer in Human Behaviors.

