{"artificial_intelligence_a_study_on_governance_policies_and_regulations": {"name": "Artificial Intelligence: A Study on Governance, Policies, and Regulations", "abstract": " Artificial Intelligence (AI) is displacing jobs and creating an upheaval in the world. It will change the way we work and the way we live. What should be the AI governance, policies, and regulations? How can AI governance, policies, and regulations mitigate and alleviate the negative aspects of AI advancement? How will AI governance, policies, and regulations impact the future of work and the future of humanity? This longitudinal multiple case studies research will study the evolution and revolution of AI governance, policies, and regulations, and how governance, policies and regulations impact AI advancement and are impacted by AI advancement. The research plans to study the top five leading countries in AI research -China, US,", "keywords": "Artificial Intelligence,Machine Learning,Robots,Automation,Governance,Policy,Regulation", "introduction": "Artificial intelligence (AI) is a fairly broad concept and is ubiquitous today . The recommendation assistant from Amazon, virtual assistants such as Apple's Siri and Google Assistant, systems that detect of credit card frauds, and face recognition applications are all supported by AI technology. Self-driving cars and home assistants are well-known AI-based applications. These applications are able to carry out specific tasks very well and this type of AI technology is known as narrow AI (or weak AI). The Artificial General Intelligence (AGI, or strong AI) is able to support multitasks simultaneously and is regarded as the intelligence that can surpass or even replace human intelligence.(Siau and Wang, 2018)Experts have different opinions about how soon AGI will become a reality. For instance, AI researchers Muller and Bostrom report in a survey that AGI will have 50% chance to be developed between 2040 and 2050, and 90% by 2075 . Some AI experts believe that AGI is still centuries away. Nevertheless, the investment and research in AI cannot be ignored. AI attracted US $12 billions of investment from venture capitalists globally in 2017, double the volume in 2016 (Health, 2018). In July 2017, China unveiled a national plan in which AI will be built into a US$152.5 billion industry by 2030 (Yu, 2018). According to (Yi, 2017), 80% of enterprises have some forms of AI today, 30% plan on expanding their investment in AI, and 62% expect to hire a Chief AI Officer.Columbus (2017)AI is a double-edged sword. AI can assist us in our jobs and lives, and release us from mundane and monotonous tasks . However, AI is replacing human jobs (Siau, 2017)(Siau and Yang, 2017;. Amazon Go, a cashier-free supermarket, has demonstrated AI's ability to replace cashiers. Self-driving trucks are predicted to take over 1.7 million jobs in the next decade Siau, 2018). Another big concern is the ethical issues caused by AI. What is the role of governments in creating, adopting, and enforcing ethical and moral standards? AI and automation are expected to further widen the wealth gap. What kind of government policies and regulations can help to close the wealth gap? What should the governments do to help the workers who are replaced by AI? Should Universal Basic Income be implemented? If so, how to fund the Universal Basic Income? How can the governments verify/certify that AI applications are not contaminated with human biases (especially those biases that are unlawful)?(Health, 2018)The same situation existed in the early days of the Internet. Policymakers, at that time, were struggling with how to best deal with that shifts and how to regulate the new technology to capitalize on its potential benefits while mitigating the potential risks. The previous governance experience will be helpful in resolving today's AI issues. AI, as a new phenomenon, will likely create governance and regulatory issues that are not encountered in earlier technology evolutions. This qualitative study will collect data from literature and policymakers in different countries to analyze the governments' economic policies and regulations, and examine how these policies and regulations impact AI advancement and are impacted by AI advancement.", "body": "Artificial intelligence (AI) is a fairly broad concept and is ubiquitous today Experts have different opinions about how soon AGI will become a reality. For instance, AI researchers Muller and Bostrom report in a survey that AGI will have 50% chance to be developed between 2040 and 2050, and 90% by 2075 AI is a double-edged sword. AI can assist us in our jobs and lives, and release us from mundane and monotonous tasks The same situation existed in the early days of the Internet. Policymakers, at that time, were struggling with how to best deal with that shifts and how to regulate the new technology to capitalize on its potential benefits while mitigating the potential risks. The previous governance experience will be helpful in resolving today's AI issues. AI, as a new phenomenon, will likely create governance and regulatory issues that are not encountered in earlier technology evolutions. This qualitative study will collect data from literature and policymakers in different countries to analyze the governments' economic policies and regulations, and examine how these policies and regulations impact AI advancement and are impacted by AI advancement.LITERATURE REVIEWNature of AIAlthough AI has been widely applied to areas such as medical, finance, education, transportation, courtrooms, and homes, there is no universally accepted definition of AI. AI is regarded as an umbrella term that refers to a wide range of disciplines and techniques. Machine learning, automation, and robotics are all relevant to or belong to AI technology. AI can be broadly classified into two categories: weak AI and strong AI. Strong AI is a highly controversial topic and some regard strong AI as an existential threat to humans. Many weak AI applications exist and the adoption of weak AI has already led to policy, governance, and legal issues. For instance, safety and privacy issues, and justice and equality issues.Gasser and Almeida (2017) highlight three challenges of building a governance model of AI. First, the \"black boxes\" in AI applications (e.g., deep learning applications have very limited capability to explain their recommendations or decisions) give rise to people's limited understanding of technological and social implications of AI, causing massive information asymmetries among AI experts, AI users, and policymakers. Second, finding normative consensus among different stakeholders is not easy. Third, the undercurrents put limits on traditional approaches to policymaking in the AI age. Advanced governance models, such as active matrix theory and hybrid regulation, may be able to shed light on the governance of AI.Concept of Governance and Policy ChangeGovernance refers to the various ways in which social life is coordinated. It has become a popular buzzword in a variety of scientific fields, leading to a considerable debate about the many uses of the word. Toikka (2011, p.10) defines governance as \"self-organizing, inter-organizational networks that are charged with policy-making.\" Fukuyama (2013) writes that governance is \"a government's ability to make and enforce rules, and to deliver services.\" Policy change refers to incremental shifts in existing structures, or new and innovative policies Previous study of governance and policy change of AIThere are not many studies on AI governance, policy, and regulatory issues. However, the potential threads of AI and the challenges posed by AI must receive the attention of policymakers. More than 20,000 researchers signed a letter, warning policymakers of the dangers of autonomous weapons that may use algorithms to strike specific targets Researchers, practitioners, and policymakers are starting to pay attention to AI governance, policies, and regulatory issues. In the Spring of 2016, United States government announces the \"White House Future of Artificial Intelligence Initiative\" to explore the impacts of artificial intelligence GOVERNANCE THEORIESGood governance should be transparent, accountable, effective and equitable, ensuring that economic, social and political priorities are based on broad consensus. In order to apply governance, which offers organizing theories, the objectives of governance must be specified. According to Given that the threats of AI are indisputably real, governance and regulation are inevitable and necessary. A distinction is often made between economic and social regulation (Den Hertog, 1999). Economic regulation can be grouped in positive and normative theories. According to positive theories of regulation, regulation occurs because \"the government is interested in overcoming information asymmetries with the operator and in aligning the operator's interest with the government's interest\" (\"Theories of Regulation\", n.d.). Another theory of regulation, normative theories of regulation, conclude that regulators should minimize the costs of information asymmetries, provide price structures that \"improve economic efficiency, provide regulation under the law, and establish regulatory processes that provide independence, transparency, predictability, legitimacy, and credibility for the regulatory system\" (\"Theories of Regulation\", n.d.). From the economic perspective, the supply-side policy encourages cutting taxes to stimulate economic growth. Cutting taxes, however, can result in budget deficits and unemployment may remain high (\"Theories of Economic Policy\", n.d.).According to Lyons (2017), the responsibility of governance and policy is global. Governance theory will not be a one-sizefits-all prescription, but a coherent framework upon which each practice can vary in recognition of cultural and contextual particulars. The AI governance will ultimately cross borders and leverage international governance bodies. A global AI governance system must be \"flexible enough to accommodate cultural differences and bridge gaps across different national legal systems\" (Gasser and Almeida, 2017, p.58). RESEARCH QUESTIONS AND PROCEDUREThis longitudinal multiple case studies research will study the evolution and revolution of AI governance, policies, and regulations, and how governance, policies and regulations impact AI advancement and are impacted by AI advancement. The research plans to study the top five leading countries in AI research -China, US, Japan, UK, and Germany.We will utilize the qualitative approach to conduct this research AI: Governance, Policies, and RegulationsProceedings of the Thirteenth Midwest Association for Information Systems Conference, Saint Louis, Missouri May 17-18, 2018  4    research provides the flexibility in gathering data and manage the research process, which may be lengthy and ambiguous.Case study research is an effective methodology to investigate and understand complex, comprehensive, and in-depth issues in real-world settings. It is especially recommended to answer how, why, and what research questions CONCLUSIONS AND EXPECTED CONTRIBUTIONS", "conclusions": "Understanding and addressing governance, policy, and regulatory issues related to AI are still in an infancy stage. Nevertheless, AI is advancing rapidly, and the governance, policy, and regulatory issues are critical and need to be discussed now. This research aims to call attention to the urgent need for various policymakers and government officials to pay attention to these issues. While attempting to formulate the governance, policy, and regulatory models for AI, we will gain insights into the future development of AI technology, understand the economic, social, and political impact of AI better, and improve our understanding and application of governance, policy, and regulatory theories in the AI age. This study is expected to contribute to both the academic progress in the field, and the writing and implementation of governance, policies, and regulations that are related to AI.", "SDG": [1]}, "deep_learning_applied_to_mobile_phone_data_for_individual_income_classification": {"name": "Deep learning applied to mobile phone data for Individual income classification", "abstract": " Deep learning has in recent years brought breakthroughs in several domains, most notably voice and image recognition. In this work we extend deep learning into a new application domain -namely classification on mobile phone datasets. Classic machine learning methods have produced good results in telecom prediction tasks, but are underutilized due to resource-intensive and domain-specific feature engineering. Moreover, traditional machine learning algorithms require separate feature engineering in different countries. In this work, we show how socio-economic status in large de-identified mobile phone datasets can be accurately classified using deep learning, thus avoiding the cumbersome and manual feature engineering process. We implement a simple deep learning architecture and compare it with traditional data mining models as our benchmarks. On average our model achieves 77% AUC on test data using location traces as the sole input. In contrast, the benchmarked state-of-the-art data mining models include various feature categories such as basic phone usage, top-up pattern, handset type, social network structure and individual mobility. The traditional machine learning models achieve 72% AUC in the best-case scenario. We believe these results are encouraging since average regional household income is an important input to a wide range of economic policies. In underdeveloped countries reliable statistics of income is often lacking, not frequently updated, and is rarely fine-grained to sub-regions of the country. Making income prediction simpler and more efficient can be of great help to policy makers and charity organizations -which will ultimately benefit the poor.", "keywords": "Deep learning,Mobile phone data,poverty,household income,Big Data Analytics,Machine learning,Asia, Mobile network operator,metadata,algorithms", "introduction": "Recent advances in Deep Learning  [1] have made it possible to extract high-level features from raw sensory data, leading to breakthroughs in computer vision [2][10] [9] and speech recognition [11] [12]. It seems natural to ask whether similar techniques could also be beneficial for useful prediction tasks on mobile phone data, where classic machine learning algorithms are often under-utilized due to timeconsuming country and domain-specific feature engineering [13].[6]Our work investigates how we can separate individuals with high and low socio-economic status using mobile phone call detail records (CDR). Finding good proxies for income in mobile phone data could lead to better poverty predictionwhich could ultimately lead to more efficient policies for addressing extreme poverty in hardest hit regions.With this in mind, we perform a large-scale countryrepresentative survey in a low HDI Asian country, where household income is collected from approximately 80 000 individuals. The individual records are de-identified and coupled with raw mobile phone data that span over 3 months. This dataset allow us to build a deep learning model, as well as a benchmarking model using custom feature engineering and traditional data mining algorithms.From the household income we derive two binary classifiers (1) below or above median household income and (2) below or above upper poverty level. The income threshold for poverty level is derived from official statistics and based on average national household income and household size. Our survey classifies participants into 13 household income bins -where bin 1 and 2 correspond to below upper poverty level.The rest of this paper is organized as follows: In section 2 we describe the features and models used for benchmarking our deep learning approach. Section 3 describes the deep learning approach itself. In section 4 we compare the results of the two approaches. Finally, we draw our conclusions in section 5.", "body": "Recent advances in Deep Learning Our work investigates how we can separate individuals with high and low socio-economic status using mobile phone call detail records (CDR). Finding good proxies for income in mobile phone data could lead to better poverty predictionwhich could ultimately lead to more efficient policies for addressing extreme poverty in hardest hit regions.With this in mind, we perform a large-scale countryrepresentative survey in a low HDI Asian country, where household income is collected from approximately 80 000 individuals. The individual records are de-identified and coupled with raw mobile phone data that span over 3 months. This dataset allow us to build a deep learning model, as well as a benchmarking model using custom feature engineering and traditional data mining algorithms.From the household income we derive two binary classifiers (1) below or above median household income and (2) below or above upper poverty level. The income threshold for poverty level is derived from official statistics and based on average national household income and household size. Our survey classifies participants into 13 household income bins -where bin 1 and 2 correspond to below upper poverty level.The rest of this paper is organized as follows: In section 2 we describe the features and models used for benchmarking our deep learning approach. Section 3 describes the deep learning approach itself. In section 4 we compare the results of the two approaches. Finally, we draw our conclusions in section 5.II. BEST PRACTICE -TRADITIONAL DATA MININGThis section describes the features and the standard machine learning algorithms used as our benchmark. The data preparation phase in the data mining process is a tedious task, that requires specific knowledge about both the local market and the various data channels as potential sources for input features. Typically the data warehouse architecture and the data format vary between the operators and third-party software packages, making it hard to create generalizable feature-sets.A. FeaturesWe build a structured dataset consisting of 150 features from 7 different feature families, see Table Advanced phone usageInternet volume/count, MMS count, video count/duration, value added services duration/count etc.A. ModelsEnsemble methods have been proven as powerful algorithms when applied to large-scale telecom datasets 1) Random forest, where we build several independent classifiers and average their predictions, thus reducing the variance. We choose grid search to optimize the tree size.2) Gradient boosting machines (GBM) where the base classifiers are built sequentially. The algorithm combines the new classifier with ones from previous iterations in an attempt to reduce the overall error rate. The main motivation is to combine several weak models to produce a powerful ensemble.In our set-up, each model is trained and tested using a 75/25 split. For the response variable related to poverty level we introduce misclassfication cost. Since few people are below the poverty level (minority class), a naive model will predict everyone above poverty line.We therefore apply a misclassification cost to the minority class to achieve fewer false positives and adjust for the ratio between the classes.III. DEEP LEARNINGIn this section we describe the input data and the structure of our deep learning model.B. FeaturesClassification results of the traditional learning algorithms are inherently limited in performance by the quality of the extracted features Earlier studies have shown a good correlation between location/human mobility and socio-economic levels Using this as a motivation we build a simple vector whose length corresponds to the number of mobile towers (8100 dimensions), and the vector elements correspond to the mobile phone activity at the given tower -shown in Table TABLE 2 INPUT VECTORS TO DL ALGORITHM BEFORE NORMALIZATIONHashedA. ModelsWe use a standard multi-layer feedforward architecture where the weighted combination of the n input signals is aggregated, and an output signal f(\u03b1) is transmitted by the connected neuron. The function f used for the nonlinear  activation is rectifier f(\u03b1) \u2248 log(1+e \u03b1 ) . To minimize the loss function we apply a standard stochastic gradient descent with the gradient computed via back-propagation. We use dropout IV. RESULTSOur deep neural network is trained to separate individuals with high and low socio-economic status. We evaluate our models' performance with AUC Next, we feed the RF and GBM models with the same representation as fed into the DL algorithm -achieving RF performance of AUC 64% and GBM performance of AUC 61% . The performance of these models greatly suffers as the number of input features increase without increasing the training size. We conclude that given a fixed training sample, the traditional models are not able to learn a complicated function that represents higher level extractions, but perform better when using manual domain-specific feature engineering.A posteriori inspection of the top features selected by our traditional models leads to some interesting qualitative insights. We observe a similar pattern in the top features selected by the RF and GBM model. Figure 1) Location dynamics: Where the user spends most of his time is a good signal of his income. This indicates that our models have detected regions of low economic development status. 2) The handset brand: In the country of our study, minimal and more affordable handset brands are very popular among the lower income quantiles, while expensive smartphones are considered as a huge status symbol.3) The top-up pattern: Interestingly, the recharge amount per transaction is more predictive than the total recharge amount. We observe that individuals from the lower income quantiles usually top-up with lower amounts when they first fill up their account.V. CONCLUSION", "conclusions": "In order to predict household income based on mobile phone communication and mobility patterns, we implemented a multi-layer feedforward deep learning architecture. Our approach introduces a novel data representation for learning neural networks on real CDR data.Our approach suggests that multi-layer feedforward models are an effective tool for predicting economic indicators based on mobile communication patterns. While capturing the complex dependencies between different dimensions of the data, deep learning algorithms do not overfit the training data as seen by our test performance. Furthermore, our deep learning model, using only a single dimension of the data in its raw form, achieves a 7% better performance compared to the best traditional data mining approach based on custom engineered features from multiple data dimensions. Even though such an automated approach is time-saving, many of the classic machine learning approaches have the advantage of being interpretable. However, since a large portion of a data mining process is data preparation, there is a big demand to automate this initial step.As future work, we would like to investigate the performance implications of including temporal aspects of raw CDRs in our models and the data representation. In addition, we will work on finding a general representation of telecom data that can be used for various prediction tasks.An important application of this work is the prediction of regional and individual poverty levels in low HDI countries. Since our approach only requires de-identified customer and tower IDs, we find this method more privacy preserving compared to traditional data mining approaches where the input features may reveal sensitive information about the customers.", "SDG": [1]}, "determination_and_prediction_of_standardized_precipitation_index_(spi)_using_trmm_data_in_arid_ecosystems": {"name": "Determination and prediction of standardized precipitation index (SPI) using TRMM data in arid ecosystems", "abstract": " Drought over a period threatens the water resources, agriculture, and socioeconomic activities. Therefore, it is crucial for decision makers to have a realistic anticipation of drought events to mitigate its impacts. Hence, this research aims at using the standardized precipitation index (SPI) to predict drought through time series analysis techniques. These adopted techniques are autoregressive integrating moving average (ARIMA) and feed-forward backpropagation neural network (FBNN) with different activation functions (sigmoid, bipolar sigmoid, and hyperbolic tangent). After that, the adequacy of these two techniques in predicting the drought conditions has been examined under arid ecosystems. The monthly precipitation data used in calculating the SPI time series (SPI 3, 6, 12, and 24 timescales) have been obtained from the tropical rainfall measuring mission (TRMM). The prediction of SPI was carried out and compared over six lead times from 1 to 6 using the model performance statistics (coefficient of correlation (R), mean absolute error (MAE), and root mean square error (RMSE)). The overall results prove an excellent performance of both predicting models for anticipating the drought conditions concerning model accuracy measures. Despite this, the FBNN models remain somewhat better than ARIMA models with R \u2265 0.7865, MAE \u2264 1.0637, and RMSE \u2264 1.2466. Additionally, the FBNN based on hyperbolic tangent activation function demonstrated the best similarity between actual and predicted for SPI 24 by 98.44%. Eventually, all the activation function of FBNN models has good results respecting the SPI prediction with a small degree of variation among timescales. Therefore, any of these activation functions can be used equally even if the sigmoid and bipolar sigmoid functions are manifesting less adjusted R 2 and higher errors (MAE and RMSE). In conclusion, the FBNN can be considered a promising technique for predicting the SPI as a drought monitoring index under arid ecosystems.", "keywords": "", "introduction": "Drought is one of the most extremely complex natural disasters that have a direct influence on water resources. Therefore, it adversely affects the quality of life and economic conditions.In the context of the current climate change, the existence of drought could be more disruptive . However, there is no universal definition of drought (Nam et al. 2015). However, many scholars defined it as an extended period with inadequate water supply (Oh et al. 2015)(Qureshi and Akhtar 2004;Raudkivi 2013;. In other words, the drought is a deficiency of precipitation from its regular rates that can affect the required amount of water for ecosystems Strelcov\u00e1 et al. 2008). The occurrence of drought is not limited to the arid and hyper-arid areas and can be observed in many other ecosystems. Accordingly, many parts of the globe have been attacked by drought. This reason encouraged scientists to study the drought and develop plentiful indices for describing the drought condition. Most of these indices are discussed and mentioned in (Estrela and Vargas 2012) and Heim (2002). Among the developed indices, the standardized precipitation index (SPI) is one of the widely used and recently applied in different drought studies such as: Smakhtin and Hughes (2004), Bowden et al. (2016), Huang et al. (2015), Ionita et al. (2016) and Musuuza et al. (2016). This attention in using the SPI is due to its calculation simplicity and flexibility at different timescales because it is only based on precipitation data over time Olivares et al. (2016).(Mahfouz et al. 2016)The proper drought management depends substantially on the timely receipt of information about the onset of drought . The beginning of a drought can be captured from its appropriate forecasting that will improve the early warning systems (Hosseini-Moghari and Araghinejad 2015; (Farokhnia et al. 2011)). Several attempts have been applied to forecast the drought Morid et al. 2007(Huang et al. 2016;Le et al. 2016;Lohani et al. 1998;Ren et al. 2016;. The traditional approach of drought forecasting is based mainly on the autoregressive integrated moving average (ARIMA). The method of ARIMA is assuming that time series is formed from a linear process. In fact, this is often contrary to the real world where the time series is commonly nonlinear Rezaeianzadeh et al. 2017)(Mishra and Desai 2006;. However, it is indispensable using an alternative approach that overcomes the previous problem of ARIMA models. The ARIMA models have been applied widely in different studies related to drought forecasting. Zhang 2003) have forecasted the drought with vegetation temperature condition index using the ARIMA models.Tian et al. (2016)Recently, one of the broadly known forecasting approaches is the artificial neural networks (ANNs). The ANNs models have demonstrated satisfactory results in sectors of water resources and drought monitoring. For example,  have used the wavelet-artificial neural networks to predict the daily river flow. Shafaei and Kisi (2016) attempted to deploy the ANN for forecasting short-term reference evapotranspiration (ET o ) using public weather forecast-restricted messages. Traore et al. (2016) predicted standardized precipitation evapotranspiration index (SPEI) and SPI by using the integrated ANNs. Maca and Pech (2016) applied the ANNs for precipitation analysis to detect drought and wet year alarms. Valipour (2016) modeled a catchment flow using the neuro-wavelet (WNN) and neuro-fuzzy (ANFIS).Zakhrouf et al. (2016)The lack of historical precipitation data, as a result of the scattered distribution of precipitation gauges, can lead to significant errors in estimating the SPI's statistical parameters. Therefore, the tropical rainfall measuring mission (TRMM) is a potent and advantageous tool for precipitation monitoring that can solve the previous problem in different regions. The TRMM is a joint mission between the National Aeronautics and Space Administration (NASA) and the Japan Aerospace Exploratory Agency (JAXA). This satellite mission is launched in November 1997 with a primary goal of studying and monitoring the tropical rainfall . The TRMM can furnish the precipitation data for large-scale areas with high temporal (about 3 h) and spatial (0.25 \u00d7 0.25\u00b0) resolutions (Sassa et al. 2014). Accordingly, the TRMM data were extensively validated and used for many hydrological studies in different locations over the world. For instance, a study conducted in West Africa shows that the TRMM data has a good agreement with the in situ data in monthly and seasonal timescales (Brown 2008)(Adeyewa and Nakamura 2003;Dinku et al. 2007;. Nicholson et al. 2003) stated that the TRMM data could not surpass the spatially interpolated daily precipitation data. Meanwhile, in some regions with a shortage of in situ records, the TRMM data work better than the spatially interpolated values for drought monitoring. Practically, the TRMM data have been used in different hydrologic studies Rhee and Carbone (2011)(Collischonn et al. 2008;Karakoc and Patil 2016;Li et al. 2012;Meng et al. 2014;Simons et al. 2016;Su et al. 2008;). Meanwhile, it was applied to monitor and determine the drought conditions based on the SPI over different climatic ecosystems Tang et al. 2016(Li et al. 2013;Naumann et al. 2012;.Tao et al. 2016)The present work aims to determine the SPI at four timescales (SPI 3,6,12, using the data obtained from TRMM as a cost-effective precipitation data source. In addition, studying the suitability of employing the Autoregressive Integrated Moving Average (ARIMA) and Feed-Forward Backpropagation Neural Network (FBNN) models to predict the SPI in the future.and 24)", "body": "Drought is one of the most extremely complex natural disasters that have a direct influence on water resources. Therefore, it adversely affects the quality of life and economic conditions.In the context of the current climate change, the existence of drought could be more disruptive The proper drought management depends substantially on the timely receipt of information about the onset of drought Recently, one of the broadly known forecasting approaches is the artificial neural networks (ANNs). The ANNs models have demonstrated satisfactory results in sectors of water resources and drought monitoring. For example, The lack of historical precipitation data, as a result of the scattered distribution of precipitation gauges, can lead to significant errors in estimating the SPI's statistical parameters. Therefore, the tropical rainfall measuring mission (TRMM) is a potent and advantageous tool for precipitation monitoring that can solve the previous problem in different regions. The TRMM is a joint mission between the National Aeronautics and Space Administration (NASA) and the Japan Aerospace Exploratory Agency (JAXA). This satellite mission is launched in November 1997 with a primary goal of studying and monitoring the tropical rainfall The present work aims to determine the SPI at four timescales Material and methodsResearch extent and data processingThe study area is located in the Kingdom of Saudi Arabia (KSA), a country situated in the far southwest corner of Asia. The coordinates of the KSA lies between latitudes 16\u00b02 2\u2032 46\u2033 and 32\u00b014\u2032 00\u2033 N and longitudes 34\u00b029\u2032 30\u2033 and 55\u00b04 0\u2032 00\u2033 E. As shown in Fig. Drought quantification and prediction remain one of the significant challenges and crucial task for water resources decision makers. Thus, two different types of time series prediction models (ARIMA and FBNN) are compared in this work. The available SPI time series data are divided into two groups, the SPI time series data from 2000 to 2011 was dedicated to developing the prediction the models. Meanwhile, the rest of the time series data from 2012 to 2013 was directed to check the prediction accuracy. The SPI of 3, 6, 12, and 24 timescales were predicted over six lead times by using the best selected ARIMA model and FBNN models. Additionally, these FBNN models are based on three activations functions (sigmoid, bipolar sigmoid, and hyperbolic tangent).SPI time series developmentAfter creating the monthly precipitation time series, an algorithm was used to calculate the SPI and create a new time series of it. The SPI calculation procedure is based on the methodology proposed by where \u03b1 and \u03b2 are factors for shape and scale respectively (greater than 0), and they have been approximated to maximum likelihood according to Putting \u03c7= \u03b2 = t will reduce the above equation to incomplete gamma function. Since the gamma distribution function is undefined at \u03c7 = 0, the precipitation distribution may be equal to nil. Thus, the cumulative probability, H(\u03c7), with the probability of zero distribution of precipitation (q) will become as follows: Finally, an equiprobability transformation is made for the H(\u03c7) to obtain the value of SPI according to SPI time series prediction modelsCoping with water scarcity needs a comprehensive water management that bases mainly on the extreme drought event prediction. Usually, the drought prediction is the estimation of drought conditions. Numerous approaches and models have been applied to predict it. The following section briefly addresses the basic concepts of the predicting methods applied to the SPI foreseeing in this study.Stochastic modelingThe stochastic or ARIMA models have been used in this study. Initially, the development of ARIMA models uses an iterative three-step modeling approach that proposed by Artificial neural network modelingRecently, the Artificial neural networks (ANN) have made important advancement that proves their effectiveness in addressing many hydrologic issues. The ANNs show a strong ability to approximate the multivariate functions which are called universal approximation. Therefore, the ANNs can be adapted to a wide range of predicting time series data with a satisfactory performance In which y k and \u0177k are the observed and predicted values of output neuron, k. Meanwhile, the gradient descent method is selecting the connection weights based on the gradient error (\u2207w ij ), which is expressed as:where the w ij is the connective weight, \u03b7 is the learning rate, and \u2202E \u2202w ij is based on the output value of sublayer A n\u22121 i and the error signal \u03b4 n j . They can be adjusted mathematically during training as:  The activation functions determine the relationship between inputs and outputs of the network. However, the activation functions of sigmoid, bipolar sigmoid, and hyperbolic tangent are used. The mathematical equations of these activation functions are as follows:Prediction accuracy measuresThe overall performance of each developed model is criticized by comparing the predicted SPI time series with the actual SPI time series. This comparison has been carried out through two commonly accuracy measures of fits that are based on the prediction errors (mean absolute error (MAE) and root mean square error (RMSE)) and can be expressed as the following:where n is the number of prediction events, y i the actual SPI and \u0177i the predicted SPI. Similarity of prediction modelsA similarity comparison between the actual and predicted data of SPI has been conducted through the hierarchical procedure. It is, therefore, possible to know which set of actual and predicted data are similar of correlated to each other. Accordingly, the similarity between selected models is considered as a tool for choosing the related models. It was calculated by the following equation:where d y i \u0177i is is the distance between the actual and predicted SPI and d max is the maximum original distance.Results and discussionPrecipitation analysis of satellite observationsInitially, a descriptive analysis of the gridded precipitation data series, as the most drought-relevant parameter, was conducted for the study area. The statistical features of precipitation data series for each gridded point are shown Table From this table, it can be noted that there are no significant differences in the annual data averages of precipitations. At the same time, a spatial interpolation of the precipitation rates over the KSA was depicted in Fig. Spatial and temporal extent of SPIIn this work, the SPI time series of each grid point was interpolated to obtain the drought maps for all the study area. These maps (Fig. Time series predictionFour best ARIMA model combination structures are selected according to ACF and PACF for each timescale By comparing the results of ARIMA and FBNN models for predicting the SPI time series, it can be mentioned that both models are performing well at different lead times, mainly for the long-term SPI Prediction of selected FBNN modelBecause the number of models formed is enormous, only the best models have been taken to present the comparison results of the actual and predicted SPI data. However, the prediction was conducted for FBNN models at SPI 3, 6, 12, and 24 timescales and one lead time. The FBNN models were developed by taking into account the three activation functions used in the study. Table Figure As is evident from the above, the FBNN models have good results concerning prediction as well as the activation functions slightly vary in the SPI timescale. Therefore, any of these activation functions can be used equally even if the sigmoid and bipolar sigmoid functions are manifesting less adjusted R 2 and higher errors (MAE and RMSE).Conclusions", "conclusions": "Notably, one of the major problematic issues in water resources management is quantifying and anticipating the drought events, especially with the lack of in situ weather data. Therefore, the current research attempts to use the TRMM data, as a good source of precipitation data, in calculating and predicting the SPI. Therefore, the SPI time series was estimated at various timescales (SPI 3,6,12, over the KSA during 2000-2014. Subsequently, the created SPI time series were used in developing the ARIMA and FBNN models. Hence, these two models were evaluated and applied for drought forecasting using the SPI under arid ecosystems. The SPI 3, 6, 12, and 24 timescales were modeled and predicted over one to six lead times using the approaches mentioned above. The developed models were evaluated quantitatively using the R, MAE, and RMSE. The predicting results of these models are demonstrating two main trends concerning the SPI predicting performance measures. Firstly, the prediction accuracy is decreased by diminishing the lead times. Secondly, the prediction is improved by increasing the SPI timescale (i.e., the worst forecasting models were in the short-term timescale (SPI 3), while the best models were in the long-term timescale). As well as it can be drawn from the comparison between the ARIMA and FBNN models: (1) The FBNN models with different activations functions perform better than ARIMA models with higher values of R and lower values of errors (MAE and RMSE). (2) The FBNN that based on hyperbolic tangent activation function is the best model for predicting the SPI under KSA. Based on the similarity of the FBNN activation functions, there are no evident differences between FBNN models developed by these activation functions. Thus, they can be adopted for predicting the SPI although some models showed a slightly greater linking over others concerning similarity and errors. and 24)", "SDG": [1, 2]}, "economic_growth_and_automation_risks_in": {"name": "Economic Growth and Automation Risks in Developing Countries Due to the Transition Toward Digital Modernity", "abstract": " This paper is aimed at making recommendations to establish a stewardship to embody the responsibilities and morality that will prevent the automation risks faced by Least Development Countries (LDCs) due to transition toward digital modernity. Recent studies reveal that law qualified individuals in developed countries will face the risks of jobs that are highly subject to automation. This indicates that corresponding risks will be seen in LDCs. The rise of digitalization may override developing country's opportunities to elevate itself from a low-income country to a middle or high-income country. In the aftermath of World War \u2161, western society tried to support LDCs in maintain the principles of a free world system and contribute to reducing poverty through market oriented development. Since the end of Cold War, defense of the free world has lost its significance but economy is getting more important political instrument. Advancement of automation technologies might aggravate opportunities for developing country's economic growth and international society needs the alternative principles.", "keywords": "\u2022 Social and professional topics \u2192 Computing and business,employment issues, automation, economic impact \u2022 Applied computing \u2192 Law, Social, and Behavioral Science,Economics, Sociology development economy, automation risks, stewardship, transition studies, global governance, digital modernity", "introduction": "The transition toward digital modernity with the fourth industrial revolution will provide huge benefits to our society. These benefits can potentially change industry, business, people's lifestyles, the education system, and so on. However, the negative factors of the revolution, such as unemployment risks and gender problems, have also been reported [1][2][3][4]. Solutions such as the introduction of a basic income, short-term labor, a robotics tax, educational improvements, etc. are being actively debated on various media such as the Internet, newspaper, SNS, and in academia. However, these are mainly the preserve of advanced countries, which have the ability to deal with the risks of digitalization and the automation. Although developing countries will be subject to the damages, effective countermeasures are not apparent yet. In addition, the fourth industrial revolution will have an impact on the economic development typically the East and Southeast Asian economic model. This is also a critical issue for the field of international aid because East and Southeast Asian economic growth have contributed remarkably to the reduction in poverty. The transition toward the fourth industrial revolution may have meant that the LDCs have lost the opportunities to break out of the vicious cycle of poverty. It is necessary for the international community to take responsibility for the issue. The aim of this paper is to make recommendations to establish a stewardship to embody the responsibilities and morality that will prevent the automation risks faced by LDCs.[5]This study begins by briefly summarizing previous research focusing on the risks in the era of the fourth industrial revolution. In the third section, it describes the characteristics of the regional exports and the fourth section examines how economic growth contributed to poverty reduction in East and Southeast Asian economies and addresses the background factors of their economic growth and political relationship to Western foreign policy and ends with a brief discussion of the responsibilities of the international community for developing countries regarding the risks of digitalization and automationrelated issues.", "body": "The transition toward digital modernity with the fourth industrial revolution will provide huge benefits to our society. These benefits can potentially change industry, business, people's lifestyles, the education system, and so on. However, the negative factors of the revolution, such as unemployment risks and gender problems, have also been reported This study begins by briefly summarizing previous research focusing on the risks in the era of the fourth industrial revolution. In the third section, it describes the characteristics of the regional exports and the fourth section examines how economic growth contributed to poverty reduction in East and Southeast Asian economies and addresses the background factors of their economic growth and political relationship to Western foreign policy and ends with a brief discussion of the responsibilities of the international community for developing countries regarding the risks of digitalization and automationrelated issues.RISKS IN THE ERA OF THE FOURTH INDUSTRIAL REVOLUTIONHuman society has experienced three industrial revolutions.The first industrial revolution occurred in England from the 1750s to the 1870s. The mechanization of water and the steam engine promoted efficiencies of production and contributed to the trilateral trade under colonization. The second industrial revolution occurred from 1945 to the mid-1970s. At this time, world hegemony shifted from the European countries to the United States. A new production system called \"Taylorism\" was invented. It also symbolized mass production and intensive labor. The other driving forces during this era were internal combustion engines, telephones, home appliances, and electric energy. They dramatically changed people's life styles. During the 1970s to 1990s, the so-called \"computation age\" began. Electronics and IT were critical in manufacturing systems. This phase also witnessed the end of the Cold War, which significantly affected the political and economic landscape globally. It is obvious that the history of industrial revolutions has been accompanied by the beginning of world trade and the rise of globalization from the very start. Moreover, from the first industrial revolution to the third industrial revolution, the economy periodically underwent repeated convergence and divergence and was not able to sustain stable economic growth. The fourth industrial revolution is the ongoing technological revolution. Table However, there are both good and bad sides to these development, especially with technological innovations in manufacturing and the service sector. Frey and Osborne (2013) investigated the automation of job risks based on the occupation-based approach Concerns regarding the automation and associated job risks have been seen repeatedly when a society faced with emerging technologies. In the first industrial revolution, the Luddite movements undertook destructive behavior against capitalists and engineers. REGIONAL SOPHISTICATION OF EXPORTSLall et al., (2005) developed a method of classifying exports based on their level of technology and level of sophistication. The technology level is defined as the Research and Development intensity of the core industrial process and the sophistication level is based on the average income of the exporter of a product The results show that all developing regions have lower sophistication scores, compared with the two highly industrialized regions, North America and Western Europe. Moreover, the results also indicate that the automation risks in developing regions are higher than those of the developed countries because of their less sophisticated levels of industrialization. South Asia excluding India faces the highest automation risks. In the next section, the Asian economic status quo is described in more detail.TRAJECTORIES OF THE EAST AND SOUTHEAST ASIAN ECONOMYTrajectory of the economic developmentSince the 1960s, four waves of economic growth have been seen in Eastern Asia As East and Southeast Asian economy grows, extreme poverty rates have improved remarkably. According to the Millennium Development Goals report of 2015, the extreme poverty rate in Eastern Asia was reduced from 61% in 1990 to only 4% in 2015 Economic Growth and Automation Risks in Developing Countries Due to the Transition Toward Digital ModernityICEGOV'18, April 2018, Galway, Ireland Among them, Bangladesh is one of the countries improved the living standards. Bangladesh recognized as an LDC since Bangladesh became independent from Pakistan in 1971 and received international aid for a long time. The World Bank reported Bangladesh, Kenya, Myanmar, and Tajikistan have become lower-middle income countries Background factors for the success of the East and Southeast Asian economiesThere is a question about how the East and Southeast Asian economy have been successful. To elucidate the mechanism of East Asian economic growth attracted attention as a research theme of many researchers in various fields. The World Bank (1993) reported that East Asian economic growth relied on foreign investment as a springboard for economic development and promoted expanding export opportunities On the other hand, MOVE TOWARD DIGITAL MODERNITY AND ESTABLISHMENT OF NEW INTERNATIONAL PRINCIPLESTransition from Industrial Modernity to Digital ModernityWhat did digital technologies bring to our sight so far? For instance, they are able to duplicate products without loss of quality. We have already seen this through streaming services for music, movies, and books. IoT is shape-shifting the products and nature of electronic appliances. Its influences range from commodities (smart phone, smart watch, autonomous vehicle, etc.) to built environment (smart houses and smart buildings) and infrastructure (smart cities). AI has already become an influential player in our society and it is no longer merely science fiction. We use AI in everyday life, such as the Apple Siri, Amazon Echo, Google Home, etc. These products allow us to feel the future and promote AI as a pleasant neo-futuristic technology. Moreover, digital technologies will step into a new realm. For example, the characteristics of block chain are credibility, transparency, and openness. It is therefore used to transfer Bit coins, but another face of block chain is it has abilities to wipe out the inter-mediators in either the public or private sectors. If block chain technology is applied to the private sector, organizational work will be more pared down and efficient. This will impact on the structural reform. As just described, the digital technologies are developing rapidly and pervading every aspect of our lives in unexpected ways. Moreover, the promotions of IoT, AI, block chain, etc. have huge potentials to change human work roles. These imply that our society is entering a transition period between industrial modernity and digital modernity. The Dilemma for GlobalizationThe trajectory of East and Southeast Asian development was deeply related to Western diplomatic policies, especially during the Cold War period. In the aftermath of World War \u2161, the United States led the way to an extraordinary achievement in the world order based on a sense of responsibility, strategies to promote state power, and moral universalism for freedom and democracy Coexistence and A New framework for International PrinciplesIt is assumed that all goods and services that both developed country and developing countries have produced will be replaced by automation and will no longer function like current manufacturing; this might make it difficult to sustain the current trade system.Since the solutions for developed countries have been discussed widely, this study especially focuses on the problems for developing countries; one is to return to an economic system that imposes high tariffs and restores protectionism as in the past; the second is to introduce a global level taxation system such as global consumer tax, global robotics tax, global corporate tax, and so on, collected from the technological advanced countries to distribute to developing countries. It is hard to introduce the former solution because perfect closed system will be difficult realized and history taught us prewar protectionism created the vicious economic circle and became one of the factors in causing World War. The latter one is applicable but it must obtain all parties agreement. They are only hypothetical solutions given the current situation. Even so it is critical for our society to provide alternative principles by which to co-operate on the North-South problem. Also, the principles have to be endowed with the sharing value between developing and developed countries International society needs to establish a form of a stewardship and principles to embody the responsibilities and morality that can ensure poverty reduction as well as preventing automation risks for developing countries. This study also suggests that the next principle is not just speculation on the part of the West bloc alone; it is better to begin with a first step that allows both developed and developing countries to decide the future through meaningful discussion about principles. Moreover, it is necessity for our society to establish new international rules for mitigating negative effects of the digitalization in order to reduce economic disparity.CONCLUSIONS", "conclusions": "Our society is facing a transition period from industrial modernity to digital modernity. The digital technology is changing our society and occurring rapidly in unforeseen ways. During this period, the changes may be inevitable and unexpected side effects may occur in the near future. A review of the literature suggests that in developing countries, law qualified individuals in developed countries will face the risks of jobs that may be highly subject to automation. This might be a side effect of the transition toward digitalization. In addition, corresponding risks will be seen in developing countries because of their less sophisticated levels of industrialization. The rise of automation and digitalization might take over the developing country's opportunities to elevate from low-income country to middle or high-income country. To take the case of Bangladesh as an example, it took a long time to become a lower-middle income country. There were several factors that contributed to Bangladesh's economic success such as increasing self-sufficiency in food, international aid, educational attainment, and microcredit, but the factor that contributed the most was manufacturing. Other East and South East Asian countries are the same as Bangladesh. Also, manufacturing will no longer provide the same dual benefits of productivity gains and job creation for the low skilled labor . Moreover, the fourth industrial revolution will lead to reinforcing of the economic disparity between nations.[32]In the aftermath of World War \u2161, western society tried to support developing countries in maintaining the principles of a free world system based on the political and economic intention. East and Southeast Asia is one of them. Previous study showed that the success of East and Southeast Asia's economy related to various factors. Among them, foreign investment, program of tariff cuts, and import liberalization are specifically supported by the principle of maintaining free world and played critical roles for the economic development. East and Southeast Asian countries would not have received such sufficient aids unless it was a Cold War period. However, defense of the free world has lost its significance since the end of the Cold War. On the other hand, economy is becoming more important political instrument for enhancing interdependency. Although our society enjoys the benefits brought on by globalization, reflecting on advancing globalization shows a contradiction, as it led to economic disparity and caused serious social damages in the developing world. We need to introduce alternative principles for developing countries to solve the digitalization problems. It is necessary to avoid the situation where human beings work below the cost in order to maintain robots. Multilateral economic relationships have become stronger than before, and have also been imbued with politically and socially important meaning. Moreover, both developing and developed countries need to share their values through meaningful discussion and change their rules and principles to be more adaptive in the transition to digital modernity. This study suggests that international society establish a stewardship that will embody the responsibilities and morality necessary to prevent the risks of automation faced by developing countries.", "SDG": [1]}, "big_data_in_smart_farming_a_review": {"name": "Big Data in Smart Farming -A review", "abstract": " Smart Farming is a development that emphasizes the use of information and communication technology in the cyber-physical farm management cycle. New technologies such as the Internet of Things and Cloud Computing are expected to leverage this development and introduce more robots and artificial intelligence in farming. This is encompassed by the phenomenon of Big Data, massive volumes of data with a wide variety that can be captured, analysed and used for decision-making. This review aims to gain insight into the state-of-the-art of Big Data applications in Smart Farming and identify the related socio-economic challenges to be addressed. Following a structured approach, a conceptual framework for analysis was developed that can also be used for future studies on this topic. The review shows that the scope of Big Data applications in Smart Farming goes beyond primary production; it is influencing the entire food supply chain. Big data are being used to provide predictive insights in farming operations, drive real-time operational decisions, and redesign business processes for game-changing business models. Several authors therefore suggest that Big Data will cause major shifts in roles and power relations among different players in current food supply chain networks. The landscape of stakeholders exhibits an interesting game between powerful tech companies, venture capitalists and often small startups and new entrants. At the same time there are several public institutions that publish open data, under the condition that the privacy of persons must be guaranteed. The future of Smart Farming may unravel in a continuum of two extreme scenarios: 1) closed, proprietary systems in which the farmer is part of a highly integrated food supply chain or 2) open, collaborative systems in which the farmer and every other stakeholder in the chain network is flexible in choosing business partners as well for the technology as for the food production side. The further development of data and application infrastructures (platforms and standards) and their institutional embedment will play a crucial role in the battle between these scenarios. From a socio-economic perspective, the authors propose to give research priority to organizational issues concerning governance issues and suitable business models for data sharing in different supply chain scenarios.", "keywords": "Agriculture,Data,Information,and,communication,technology,Data,infrastructure,Governance,Business,modelling", "introduction": "As smart machines and sensors crop up on farms and farm data grow in quantity and scope, farming processes will become increasingly datadriven and data-enabled. Rapid developments in the Internet of Things and Cloud Computing are propelling the phenomenon of what is called Smart Farming . While Precision Agriculture is just taking in-field variability into account, Smart Farming goes beyond that by basing management tasks not only on location but also on data, enhanced by context-and situation awareness, triggered by real-time events (Sundmaeker et al., 2016). Real-time assisting reconfiguration features are required to carry out agile actions, especially in cases of suddenly changed operational conditions or other circumstances (e.g. weather or disease alert). These features typically include intelligent assistance in implementation, maintenance and use of the technology. Fig. (Wolfert et al., 2014) summarizes the concept of Smart Farming along the management cycle as a cyber-physical system, which means that smart devices -connected to the Internet -are controlling the farm system. Smart devices extend conventional tools (e.g. rain gauge, tractor, notebook) by adding autonomous context-awareness by all kind of sensors, built-in intelligence, capable to execute autonomous actions or doing this remotely. In this picture it is already suggested that robots can play an important role in control, but it can be expected that the role of humans in analysis and planning is increasingly assisted by machines so that the cyberphysical cycle becomes almost autonomous. Humans will always be involved in the whole process but increasingly at a much higher intelligence level, leaving most operational activities to machines.1Big Data technologies are playing an essential, reciprocal role in this development: machines are equipped with all kind of sensors that measure data in their environment that is used for the machines' behaviour. This varies from relatively simple feedback mechanisms (e.g. a thermostat regulating temperature) to deep learning algorithms (e.g. to implement the right crop protection strategy). This is leveraged by combining with other, external Big Data sources such as weather or market data or benchmarks with other farms. Due to rapid developments in this area, a unifying definition of Big Data is difficult to give, but generally it is a term for data sets that are so large or complex that traditional data processing applications are inadequate . Big data requires a set of techniques and technologies with new forms of integration to reveal insights from datasets that are diverse, complex, and of a massive scale (Wikipedia, 2016). Big Data represents the information assets characterized by such a high volume, velocity and variety to require specific technology and analytical methods for its transformation into value (Hashem et al., 2015). The Data FAIRport initiative emphasizes the more operational dimension of Big Data by providing the FAIR principle meaning that data should be Findable, Accessible, Interoperable and Reusable (De Mauro et al., 2016). This also implies the importance of metadata i.e. 'data about the data' (e.g. time, location, standards used, etc.).(Data FAIRport, 2014)Both Big Data and Smart Farming are relatively new concepts, so it is expected that knowledge about their applications and their implications for research and development is not widely spread. Some authors refer to the advent of Big Data and related technology as another technology hype that may fail to materialize, others consider Big Data applications may have passed the 'peak of inflated expectations' in Gartner's Hype Cycle (Fenn and LeHong, 2011;. This review aims to provide insight into the state-of-the-art of Big Data applications in relation to Smart Farming and to identify the most important research and development challenges to be addressed in the future. In reviewing the literature, attention is paid to both technical and socio-economic aspects. However, technology is changing rapidly in this area and a state-of-the-art of that will probably be outdated soon after this paper is published. Therefore the analysis primarily focuses on the socio-economic impact Big Data will have on farm management and the whole network around it because it is expected that this will have a longerlasting effect. From that perspective the research questions to be addressed in this review are:Needle, 2015)1. What role does Big Data play in Smart Farming? 2. What stakeholders are involved and how are they organized? 3. What are the expected changes that are caused by Big Data developments? 4. What challenges need to be addressed in relation to the previous questions?The latter question can be considered as a research agenda for the future.To answer these questions and to structure the review process, a conceptual framework for analysis has been developed, which is expected to be useful also for future analyses of developments in Big Data and Smart Farming. In the remainder of this paper the methodology for reviewing the literature (Section 2) and the framework will be described (Section 3). Then the main results from the analysis will be presented in Section 4. Section 5 concludes the review and provides recommendations for further research and actions.", "body": "As smart machines and sensors crop up on farms and farm data grow in quantity and scope, farming processes will become increasingly datadriven and data-enabled. Rapid developments in the Internet of Things and Cloud Computing are propelling the phenomenon of what is called Smart Farming Big Data technologies are playing an essential, reciprocal role in this development: machines are equipped with all kind of sensors that measure data in their environment that is used for the machines' behaviour. This varies from relatively simple feedback mechanisms (e.g. a thermostat regulating temperature) to deep learning algorithms (e.g. to implement the right crop protection strategy). This is leveraged by combining with other, external Big Data sources such as weather or market data or benchmarks with other farms. Due to rapid developments in this area, a unifying definition of Big Data is difficult to give, but generally it is a term for data sets that are so large or complex that traditional data processing applications are inadequate Both Big Data and Smart Farming are relatively new concepts, so it is expected that knowledge about their applications and their implications for research and development is not widely spread. Some authors refer to the advent of Big Data and related technology as another technology hype that may fail to materialize, others consider Big Data applications may have passed the 'peak of inflated expectations' in Gartner's Hype Cycle 1. What role does Big Data play in Smart Farming? 2. What stakeholders are involved and how are they organized? 3. What are the expected changes that are caused by Big Data developments? 4. What challenges need to be addressed in relation to the previous questions?The latter question can be considered as a research agenda for the future.To answer these questions and to structure the review process, a conceptual framework for analysis has been developed, which is expected to be useful also for future analyses of developments in Big Data and Smart Farming. In the remainder of this paper the methodology for reviewing the literature (Section 2) and the framework will be described (Section 3). Then the main results from the analysis will be presented in Section 4. Section 5 concludes the review and provides recommendations for further research and actions.MethodologyTo address the research questions as outlined in the Introduction, we surveyed literature between January 2010 and March 2015. The choice of the review period was a practical one and took into consideration the fact that Big Data is a rather recent phenomenon; it was not expected that there would be any reference before 2010. Beside the period of publication, we used two inclusion criteria for the literature search: 1) full article publication; 2) relevance to the research question. Two exclusion criteria were used: 1) articles published in languages other than English or Chinese; 2) articles focussing solely on technological design. The literature survey followed a systematic approach. This was done in three steps. In the first step we searched two major bibliographical databases, Web of Science and Scopus, using all combinations of two groups of keywords of which the first group addresses Big Data (i.e. Big Data, data-driven innovation, data-driven value creation, internet of things, IoT) and the second group refers to farming (i.e. agriculture, farming, food, agri-food, precision agriculture). The two databases were chosen because of their wide coverage of relevant literature and advanced bibliometric features such as suggesting related literature or citations. From these two databases 613 peer-reviewed articles were retrieved. These were scanned for relevance by identifying passages that were addressing the research questions. In screening the literature, we first used the search function to locate the paragraphs containing the key words and then read the text to see whether they can be related to the research questions. The screening was done by four researchers, with each of them judging about 150 articles and sharing their findings with the others through the reference management software EndNote X7. As a result, 20 were considered most relevant and 94 relevant. The remaining articles were considered not relevant as they only tangentially touch upon Big Data or agriculture and therefore excluded from further reading and analysis. We found the number of relevant peerreviewed literature not very high which can be explained because Big Data and Smart Farming are relatively new concepts. Especially the applications are rapidly evolving and expected not to be taken into account in peer-reviewed articles which are usually lagging behind. Therefore we decided to also include grey literature into our review.For that purpose we have used Google Scholar and the search engine LexisNexis for reports, magazines, blogs, and other web-items in English. This has resulted in 3 reports, 225 magazine articles, 319 blogs and 19 items on twitter. Each of the 319 blogs was evaluated on relevance based on its title and sentences containing the search terms. Also possible duplications were removed. The result was a short list containing 29 blogs that were evaluated by further reading. As a result, 9 blogs have been considered as presenting relevant information for our framework. Each of the 225 magazine articles was similarly evaluated on their relevance based on its title and sentences containing the search terms. After removing duplicates, the result is a short list of 25 articles. We then read these 25 articles through for further evaluation. Consequently 9 articles have been considered as containing relevant information for further analysis.In the second step, we read the selected literature in detail to extract the information relevant to our research questions. Additional literature that had not been identified in the first step was retrieved in this step as well if they were referred to by the 'most relevant' literature. This 'snow-ball' approach has resulted in 11 additional articles and web-items from which relevant information was extracted as well. In the third step, the extracted information was analysed and synthesized following the conceptual framework as described in Section 3.Conceptual frameworkFor this review a conceptual framework was developed to provide a systematic classification of issues and concepts for the analysis of Big Data applications in Smart Farming from a socio-economic perspective. A major complexity of such applications is that they require collaboration between many different stakeholders having different roles in the data value chain. For this reason, the framework draws upon literature on chain network management and data-driven strategies. Chain networks are considered to be composed of the actors which vertically and horizontally work together to add value to customers The often-cited conceptual framework of For our purpose the framework was tailored to networks for Big Data applications in Smart Farming as presented in Fig. In this framework, the business processes (lower layer) focus on the generation and use of Big Data in the management of farming processes. For this reason, we subdivided this part into the data chain, the farm management and the farm processes. The data chain interacts with farm processes and farm management processes through various decision making processes in which information plays an important role. The stakeholder network (middle layer) comprises all stakeholders that are involved in these processes, not only users of Big Data but also companies that are specialized in data management and regulatory and policy actors. Finally, the network management layer typifies the organizational and technological structures in the network that facilitate coordination and management of the processes that are performed by the actors in the stakeholder network layer. The technology component of network management (upper layer) focuses on the information infrastructure that supports the data chain. The organizational component focuses on the governance and business model of the data chain. Finally, several factors can be identified as key drivers for the development of Big Data in Smart Farming and as a result challenges can be derived from this development.The next subsections provide a more detailed description of each subcomponent of the business processes layer and network management layer of the framework.Farm processesA business process is a set of logically related tasks performed to achieve a defined business outcome Farm managementManagement or control processes ensure that the business process objectives are achieved, even if disturbances occur. The basic idea of control is the introduction of a controller that measures system behaviour and corrects if measurements are not compliant with system objectives. Basically, this implies that they must have a feedback loop in which a norm, sensor, discriminator, decision maker, and effector are present \u2022 Sensing and monitoring: measurement of the actual performance of the farm processes. This can be done manually by a human observer or automated by using sensing technologies such as sensors or satellites. In addition, external data can be acquired to complement direct observations. \u2022 Analysis and decision making: compares measurements with the norms that specify the desired performance (system objectives concerning e.g. quantity, quality and lead time aspects), signals deviations and decides on the appropriate intervention to remove the signalled disturbances. \u2022 Intervention: plans and implements the chosen intervention to correct the farm processes' performance.Data chainThe data chain refers to the sequence of activities from data capture to decision making and data marketing Being an integral part of business processes, the data chain consists necessarily of a technical layer that captures raw data and converts it into information and a business layer that makes decisions and derives value from provided data services and business intelligence. The two layers can be interwoven in each stage and together they form the basis of what has come to be known as the 'data value chain' Network management organizationThe network management organization deals with the behaviour of the stakeholders and how it can be influenced to accomplish the business process objectives. For the uptake and further development of Big Data applications, two interdependent aspects are considered relevant: governance and business model. Governance involves the formal and informal arrangements that govern cooperation within the stakeholder network. Important arrangements for the management of big data include agreements on data availability, data quality, access to data, security, responsibility, liability, data ownership, privacy and distribution of costs. Three basic forms of network governance can be distinguished Despite agreement on the importance of business model to an organization's success, the concept is still fuzzy and vague, and there is little consensus regarding its compositional facets. Network management technologyThe network management technology includes all computers, networks, peripherals, systems software, application packages (application software), procedures, technical, information and communication standards (reference information models and coding and message standards) etc., that are used and necessary for adequate data management in the inter-organizational control of farming processes \u2022 Data resources stored in shared databases and a shared understanding of its content (shared data model of the database). \u2022 Information systems and services that allow us to use and maintain these databases. An information system is used to process information necessary to perform useful activities using activities, facilities, methods and procedures. \u2022 The whole set of formalised coding and message standards (both technically and content-wise) with associated procedures for use, connected to shared databases, which are necessary to allow seamless and error-free automated communication between business partners in a food supply chain network. \u2022 The necessary technical infrastructure. None of the above can work if we don't have the connected set of computers (workstations of individual associates or people employed by or interested in the network and the database, communication and application servers and all associated peripherals) that will allow for its usage.In conclusion, this framework now provides a coherent set of elements to describe and analyse the developments of Big Data in Smart Farming. The results are provided in the next section.ResultsDrivers for Big Data in Smart FarmingThere has been a significant trend to consider the application of Big Data techniques and methods to agriculture as a major opportunity for application of the technology stack, for investment and for the realisation of additional value within the agri-food sector As with many technological innovations changes by Big Data applications in Smart Farming are driven by push-pull mechanisms. Pull, because there is a need for new technology to achieve certain goals. Push, because new technology enables people or organizations to achieve higher or new goals. This will be elaborated in the next subsections.Pull factorsFrom a business perspective, farmers are seeking ways to improve profitability and efficiency by on the one hand looking for ways to reduce their costs and on the other hand obtaining better prices for their product. Therefore they need to take better, more optimal decisions and improve management control. While in the past advisory services were based on general knowledge that once was derived from research experiments, there is an increasing need for information and knowledge that is generated on-farm in its local-specific context. It is expected that Big Data technologies help to achieve these goals in a better way From a public perspective global food security is often mentioned as a main driver for further technological advancements Push factorsA general future development is the Internet of Things (IoT) in which all kinds of devicessmart objectsare connected and interact with each other through local and global, often wireless network infrastructures Wireless data transfer technology also permits farmers to access their individual data from anywherewhether they are at the farmhouse or meeting with buyers in Chicagoenabling them to make Information share and data integration Data-driven services informed decisions about crop yield, harvesting, and how best to get their product to market Table Business processesFarm processesAgricultural Big Data are known to be highly heterogeneous PM data, or the traditional business data, result from agricultural processes that record and monitor business events of interest, such as purchasing inputs, feeding, seeding, applying fertilizer, taking an order, etc. PM data are usually highly structured and include transactions, reference tables and relationships, as well as the metadata that define their context. Traditional business data are the vast majority of what IT managed and processed, in both operational and business information systems, usually structured and stored in relational database systems.MG data are derived from the vast increasing number of sensors and smart machines used to measure and record farming processes; this development is currently boosted by what is called the Internet of Things (IoT). MG data range from simple sensor records to complex computer logs and are typically well-structured. As sensors proliferate and data volumes grow, it is becoming an increasingly important component of the farming information stored and processed. Its well-structured nature is suitable for computer processing, but its size and speed is beyond traditional approaches. For Smart Farming, the potential of unmanned aerial vehicles (UAVs) has been well-recognized HM data is the record of human experiences, previously recorded in books and works of art, and later in photographs, audio and video. Human-sourced information is now almost entirely digitized and stored everywhere from personal computers to social networks. HM data are usually loosely structured and often ungoverned. In the context of Big Data and Smart Farming, human-sourced data have rarely been discussed except in relation to the marketing aspects Table From the business perspective, the main data products along the Big Data value chain are (predictive) analytics that provide decision support to business processes at various levels. The use or analysis of sensor data or similar data must somehow fit into existing or reinvented business processes. Integration of data from a variety of sources, both traditional and new, with multiple tools, is the first prerequisite.Farm managementAs Big Data observers point out: big or small, Big Data is still data Since the advent of large-scale data collections or warehouses, the so-called data rich, information poor (DRIP) problems have been pervasive. The DRIP conundrum has been mitigated by the Big Data approach which has unleashed information in a manner that can support informed -yet, not necessarily defensible or valid -decisions or choices. Thus, by somewhat overcoming data quality issues with data quantity, data access restrictions with on-demand cloud computing, causative analysis with correlative data analytics, and model-driven with evidence-driven applications Big data on its own can offer 'a-ha' insights, but it can only reliably deliver long-term business advantage when fully integrated with traditional data management and governance processes Data chainAs often discussed in the literature, a wide range of issues need to be addressed for big data applications. Both technical and governance issues can arise in different stages of the data chain, where governance challenges become increasingly dominant at the later stages of the data chain. Table Stakeholder networkIn view of the technical changes brought forth by Big Data and Smart Farming, we seek to understand the stakeholder network around the farm. The literature suggests major shifts in roles and power relations among different players in existing agri-food chains. We observed the changing roles of old and new software suppliers in relation to Big Data and farming and emerging landscape of data-driven initiatives with prominent role of big tech and data companies like Google and IBM. In Fig. The stakeholder networks exhibits a high degree of dynamics with new players taking over the roles played by other players and the incumbents assuming new roles in relation to agricultural Big Data. As opportunities for Big Data have surfaced in the agribusiness sector, big agriculture companies such as Monsanto and John Deere have spent hundreds of millions of dollars on technologies that use detailed data on soil type, seed variety, and weather to help farmers cut costs and increase yields Monsanto has been pushing big-data analytics across all its business lines, from climate prediction to genetic engineering. It is trying to persuade more farmers to adopt its cloud services. Monsanto says farmers benefit most when they allow the company to analyse their data -along with that of other farmers -to help them find the best solutions for each patch of land While corporates are very much engaged with Big Data and agriculture, start-ups are at the heart of action, providing solutions across the value chain, from infrastructure and sensors all the way down to software that manages the many streams of data from across the farm. As the ag-tech space heats up, an increasing number of small tech startups are launching products giving their bigger counterparts a run for their money. In the USA, start-ups like FarmLogs Venture capital firms are now keen on investing in agriculture technology companies such as Blue River Technology, a business focusing on Table Cycle of Smart FarmingArable Livestock Horticulture FisherySmart sensing and monitoring Robotics and sensors Smart analysis and planningSeeding, Planting, Soil typing, Crop health, yield modelling Smart controlPrecision farming Machine learning algorithms, normalize, visualize, anonymize The new players to Smart Farming are tech companies that were traditionally not active in agriculture. For example, Japanese technology firms such as Fujitsu are helping farmers with their cloud based farming systems Beside business players such as corporates and start-ups, there are many public institutions (e.g., universities, USDA, the American Farm Bureau Federation, GODAN) that are actively influencing Big Data applications in farming through their advocacy on open data and data-driven innovation or their emphasis on governance issues concerning data ownership and privacy issues. Well-known examples are the Big Data Coalition, Open Agriculture Data Alliance (OADA) and AgGateway. Public institutions like the USDA, for example, want to harness the power of agricultural data points created by connected farming equipment, drones, and even satellites to enable precision agriculture for policy objectives like food security and sustainability. Precision farming is considered to be the \"holy grail\" because it is the means by which the food supply and demand imbalance will be solved. To achieve that precision, farmers need a lot of data to inform their planting strategies. That is why USDA is investing in big, open data projects. It is expected that open data and Big Data will be combined together to provide farmers and consumers just the right kind of information to make the best decisions Network managementOrganizationData ownership is an important issue in discussions on the governance of agricultural Big Data generated by smart machinery such as tractors from John Deere There is also a growing number of initiatives to address or ease privacy and security concerns. For example, the Big Data Coalition, a coalition of major farm organizations and agricultural technology providers in the USA, has set principles on data ownership, data collection, notice, third-party access and use, transparency and consistency, choice, portability, data availability, market speculation, liability and security safeguards The 'Ownership Principle' of the Big Data Coalition states that \"We believe farmers own information generated on their farming operations. However, it is the responsibility of the farmer to agree upon data use and sharing with the other stakeholders (...).\" While having concerns about data ownership, farmers also see how much companies are investing in Big Data. In 2013, Monsanto paid nearly 1 billion US dollars to acquire The Climate Corporation, and more industry consolidation is expected. Farmers want to make sure they reap the profits from Big Data, too. Such change of thinking may lead to new business models that allow shared harvesting of value from data.Big data applications in Smart Farming will potentially raise many power-related issues TechnologyTo make Big Data applications for Smart Farming work, an appropriate technological infrastructure is essential. Although we could not find much information about used infrastructures in literature it can be expected that the applications from the AgTech and AgBusiness companies in Fig. In North America, several initiatives are undertaken to open up data transfer between several platforms and devices. The ISOBlue project facilitates data acquisition through the development of and open-source hardware platform and software libraries to forward ISOBUS messages to the cloud and develop applications for Android smartphones In Europe, much work to realize an open infrastructure for data exchange and collaboration was done within the Future Internet programme. The focus of this programme was to realize a set of Generic Enablers (GEs) for e.g. cloud hosting, data and context management services, IoT services, security and Big Data Analysis which are common to all Future Internet applications for all kind of different sectors, called FIWARE FIspace uses FIWARE Generic Enablers (GEs) but has two particular extensions for business collaboration: the App Store and the Real-Time B2B collaboration core. These key components are connected with several other modules to enable system integration (e.g. with IoT), to ensure Security, Privacy and Trust in business collaboration and an Operating Environment and Software Development Kit to support an 'ecosystem' in which Apps for the FIspace store can be developed. The FIspace platform will be approachable through various type of front-ends (e.g. web or smartphone), but also direct M2M communication is possible.Because all mentioned open platforms are result from recent projects, their challenge is still how they could be broadly adopted. For the FIspace platform, a first attempt was made in the FIWARE accelerator programme 1 in which several hundreds of start-ups were funded to develop apps and services and also received business support. Some of them were already successful in receiving further funding from private investors, but it is too early to determine the final success rate of this programme.ChallengesThe challenges for Big Data and Smart Farming found in literature can be broadly classified into technical and organizational ones of which the latter category is considered the most important One of the biggest challenges of Big Data governance is probably how to ensure privacy and security All aforementioned challenges make that the current amounts of farm data is currently underutilized Conclusions and recommendations", "conclusions": "In this paper a literature review on Big Data applications in Smart Farming was conducted. In Section 2 it was concluded that currently there are not many references in peer-reviewed scientific journals. Therefore, a reliable, quantitative analysis was not possible. Furthermore, findings from grey literature may lack scientific rigor as can be expected from peer-reviewed journal articles. However, as articles from grey literature are publicly available, they can be seen as being subject to public scrutiny and therefore reasonably reliable. As such, we consider that the knowledge base was enriched by articles from grey literature. Besides, much effort was put into developing a framework for analysis that can be used for future reviews with a more quantitative approach.Based on the findings in this paper several conclusions can be drawn on the state-of-the-art of Big Data applications in Smart Farming. First of all, Big Data in Smart Farming is still in an early development stage. This is based on the fact there are only limited scientific publications available on this topic and much information had to be derived from 'grey literature'. The applications discussed are mainly from Europe and Northern America, with a growing number of applications expected from other countries as well. Considering the scope of the review, no geographic analysis was performed in this study. Further conclusions, drawn as answers to the research questions we formulated in the Introduction, are elaborated below.What role does Big Data play in Smart Farming? Big Data is changing the scope and organization of farming through a pull-push mechanism. Global issues such as food security and safety, sustainability and as a result efficiency improvement are tried to be addressed by Big Data applications. These issues make that the scope of Big Data applications extends far beyond farming alone, but covers the entire supply chain. The Internet of Things development, wirelessly connecting all kind of objects and devices in farming and the supply chain, is producing many new data that are real-time accessible. This applies to all stages in the cyber-physical management cycle (Fig. ). Operations and transactions are most important sources of process-mediated data. Sensors and robots producing also non-traditional data such as images and videos provide many machine-generated data. Social media is an important source for human-sourced data. These big amounts of data provide access to explicit information and decisionmaking capabilities at a level that was not possible before. Analytics is key success factor to create value out of these data. Many new and innovative start-up companies are eager to sell and deploy all kind of applications to farmers of which the most important ones are related to sensor deployment, benchmarking, predictive modelling and risk management.1What stakeholders are involved and how are they organized? Referring to Fig. , there are first of all the traditional players in agriculture such as input suppliers and technology suppliers for which there is a clear move towards Big Data as their most important business model. Most of them are pushing their own platforms and solutions to farmers, which are often proprietary and rather closed environments although a tendency towards more openness is observed. This is stimulated by farmers -organized in cooperatives or coalitions -that are concerned about data privacy and security and also want to create value with their own data or at least want to benefit from Big Data solutions. Beside the traditional players we see that Big Data is also attracting many new entrants which are often start-ups supported by either large private investors or large ICT or non-agricultural tech companies. Also public institutions aim to open up public data that can be combined with private data.5These developments raise issues around data ownership, value of data and privacy and security. The architecture and infrastructure of Big Data solutions are also significantly determining how stakeholder networks are organized. On the one hand there is a tendency towards closed, proprietary systems and on the other hand towards more open systems based on open source, standards and interfaces. Further development of Big Data applications may therefore likely result in two extremes of supply chain scenarios: one with further integration of the supply chain in which farmers become franchisers; another in which farmers are empowered by Big Data and open collaboration and can easily switch between suppliers, share data with government and participate in short supply chains rather than integrated long supply chains. In reality, the situation will be a continuum between these two extremes differentiated by crop, commodity, market structure, etc.What are the expected changes that are caused by Big Data developments?From this review it can be concluded that Big Data will cause major changes in scope and organization of Smart Farming. Business analytics at a scale and speed that was never seen before will be a real game changer, continuously reinventing new business models. Referring to Fig. , it can be expected that farm management and operations will drastically change by access to real-time data, real-time forecasting and tracking of physical items and in combination with IoT developments in further automation and autonomous operation of the farm. Taking also the previous research question into account, it is already visible that Big Data will also cause major shifts in power relationships between the different players in the Big Data farming stakeholder network. The current development stage does however not reveal yet towards which main scenario Smart Farming will be developed.1What challenges need to be addressed in relation to the previous questions?A long list of key issues was already provided in Table , but the most important ones are4\u2022 Data-ownership and related privacy and security issuesthese issues have to be properly addressed, but when this is applied too strictly it can also slow down innovations; \u2022 Data qualitywhich has always been a key issue in farm management information systems, but is more challenging with big, real-time data; \u2022 Intelligent processing and analyticsfor Big Data this is also more challenging because of the large amount of often unstructured, heterogeneous data which requires a smart interplay between skilled data scientists and domain experts; \u2022 Sustainable integration of Big Data sourcesintegration of many different data sources is challenging but because this is crucial for your business model this has to be done in a sustainable manner; \u2022 Business models that are attractive enough for solution providers but that also enable a fair share between the different stakeholders; \u2022 Openness of platforms that will accelerate solution development and innovation in general but also empower farmers in their position in supply chains.The framework for analysis was developed from a chain network perspective with specific attention to network management between the stakeholders that are involved. In future research it could also be valuable to look at this subject from a wider innovation perspective (Busse et al., 2015;Klerkx et al., 2010;. The same holds for ethical aspects of an innovation such as Big Data that could be addressed in future research Lamprinopoulou et al., 2014)(Carolan, 2016;Driessen and Heutinck, 2015;.Wigboldus et al., 2016)The promise of Big Data in agriculture is alluring, but the challenges above have to be addressed for increased uptake of Big Data applications. Although there are certainly technical issues to be resolved we recommend to focus first on the governance issues that were identified and design suitable business models because these are currently the most inhibiting factors.", "SDG": [2]}, "counting_apples_and_oranges_with_deep_learning_a_data_driven_approach": {"name": "Counting Apples and Oranges with Deep Learning: A Data Driven Approach", "abstract": " This paper describes a fruit counting pipeline based on deep learning that accurately counts fruit in unstructured environments. Obtaining reliable fruit counts is challenging because of variations in appearance due to illumination changes and occlusions from foliage and neighboring fruits. We propose a novel approach that uses deep learning to map from input images to total fruit counts. The pipeline utilizes a custom crowd-sourcing platform to quickly label large data sets. A blob detector based on a fully convolutional network extracts candidate regions in the images. A counting algorithm based on a second convolutional network then estimates the number of fruit in each region. Finally, a linear regression model maps that fruit count estimate to a final fruit count. We analyze the performance of the pipeline on two distinct data sets of oranges in daylight, and green apples at night, utilizing human generated labels as ground truth. We also show that the pipeline has a short training time and performs well with a limited data set size. Our method generalizes across both data sets and is able to perform well even on highly occluded fruits that are challenging for human labelers to annotate.", "keywords": "Agricultural Automation,Object detection, segmentation, categorization,Visual Learning", "introduction": "F RUIT counting is an important task for growers to estimate yield and manage orchards. An accurate automated fruit detection and counting algorithm gives agricultural enterprises the ability to optimize and streamline their harvest process. Through a better understanding of the variability of yield across their farmlands, growers can make more informed and cost-effective decisions for labor allotment, storage, packaging, and transportation. Smart sensor suites as well as autonomous robots such as unmanned aerial vehicles (UAVs) will benefit from data-driven fruit counting algorithms that enable growers to estimate yield at scale. For example, Figure  shows a UAV with onboard cameras that can be used for rapid estimation of yield by flying between rows of trees 1.[1]Estimation of fruit count from images is a challenging task for a number of reasons including appearance variability due to illumination, and occlusion due to surrounding foliage and fruits. Previous fruit counting algorithms relied on traditional computer vision methods involving hand-crafted features that exploited the shape, color, texture or spatial orientation of various fruit . While these methods work well under specific conditions, they are usually fruit specific, require careful control of the environment, and cannot handle heavily occluded fruits.[2]Fig. : UAVs can fly between rows of trees autonomously and produce fruit counts at scale, from onboard camera imagery.1An additional challenge to fruit counting that is not present in fruit detection is distinguishing multiple overlapping fruits. Most algorithms either ignore this problem, or use simple heuristics based on shape and size, which do not generalize to natural settings that feature heavy occlusion and high variability of depth in fruit location .[3]Deep learning is a natural choice to deal with the unstructured environments that traditional computer vision methods have difficulty with . This paper presents a novel pipeline that accurately estimates counts across different fruit types, illumination, and occlusion levels. The broad steps of the pipeline are:[4]\u2022 collect human-generated labels from a set of fruit images;\u2022 train a blob detection fully convolutional network to perform image segmentation; \u2022 train a count convolutional network to take the segmented image and output an intermediate estimate of the fruit count; and \u2022 train a linear regression to map intermediate fruit count estimates to final counts using human-generated labels as ground truth.Figure  displays the output of the blob detection network and Figure 2 displays the output of the count neural network.3Our goal is to provide a data-driven fruit counting methodology that an agricultural enterprise can easily adapt and apply in unstructured farm settings. As a result, the proposed method emphasizes labeling and training speed, generalizability, and accuracy.The main contributions of this paper are:\u2022 a labeling platform that easily scales to large amounts of data; \u2022 a novel and efficient representation of human-generated labels utilizing scalable vector graphics (SVG) to simultaneously store both the location and the number of fruits; \u2022 the application of a fully convolutional network to accurately detect blobs of fruits; and \u2022 the application of a convolutional network to accurately count the number of fruit within each segmented blob. The rest of the paper is organized as follows: 1) a summary of related work in fruit detection; 2) a formulation of the problem; 3) description of the proposed approach; 4) results and analysis; and 5) conclusion.", "body": "F RUIT counting is an important task for growers to estimate yield and manage orchards. An accurate automated fruit detection and counting algorithm gives agricultural enterprises the ability to optimize and streamline their harvest process. Through a better understanding of the variability of yield across their farmlands, growers can make more informed and cost-effective decisions for labor allotment, storage, packaging, and transportation. Smart sensor suites as well as autonomous robots such as unmanned aerial vehicles (UAVs) will benefit from data-driven fruit counting algorithms that enable growers to estimate yield at scale. For example, Figure Estimation of fruit count from images is a challenging task for a number of reasons including appearance variability due to illumination, and occlusion due to surrounding foliage and fruits. Previous fruit counting algorithms relied on traditional computer vision methods involving hand-crafted features that exploited the shape, color, texture or spatial orientation of various fruit Fig. An additional challenge to fruit counting that is not present in fruit detection is distinguishing multiple overlapping fruits. Most algorithms either ignore this problem, or use simple heuristics based on shape and size, which do not generalize to natural settings that feature heavy occlusion and high variability of depth in fruit location Deep learning is a natural choice to deal with the unstructured environments that traditional computer vision methods have difficulty with \u2022 collect human-generated labels from a set of fruit images;\u2022 train a blob detection fully convolutional network to perform image segmentation; \u2022 train a count convolutional network to take the segmented image and output an intermediate estimate of the fruit count; and \u2022 train a linear regression to map intermediate fruit count estimates to final counts using human-generated labels as ground truth.Figure Our goal is to provide a data-driven fruit counting methodology that an agricultural enterprise can easily adapt and apply in unstructured farm settings. As a result, the proposed method emphasizes labeling and training speed, generalizability, and accuracy.The main contributions of this paper are:\u2022 a labeling platform that easily scales to large amounts of data; \u2022 a novel and efficient representation of human-generated labels utilizing scalable vector graphics (SVG) to simultaneously store both the location and the number of fruits; \u2022 the application of a fully convolutional network to accurately detect blobs of fruits; and \u2022 the application of a convolutional network to accurately count the number of fruit within each segmented blob. The rest of the paper is organized as follows: 1) a summary of related work in fruit detection; 2) a formulation of the problem; 3) description of the proposed approach; 4) results and analysis; and 5) conclusion.II. RELATED WORKJimenez et al. provide a comprehensive survey of computer vision methods for fruit detection Nuske et al. present a method of grape cluster counting using a radial symmetry transform to identify candidate berry locations followed by a k-Nearest Neighbor learning algorithm for final grape detection Recent work applying deep learning to agriculture involved the classification of crops from weeds and soil using a fully convolutional network (FCN) III. PROBLEM FORMULATIONConsider a set of images x i for i = 1 . . . n. Each image x i has an unobservable state z i \u2208 N representing the actual number of fruit in the image x i . Let zi \u2208 R + 0 be the humangenerated ground truth count estimate of z i .Problem (Fruit Counting). Choose a function f (x i ) \u2208 R + 0 representing the algorithm-generated count estimate of zi which minimizes the l 2 error:Notice that the l 2 error depends on the human-generated ground truth count, not the actual count. IV. PROPOSED APPROACHThe proposed approach utilizes a pipeline of deep learning algorithms to detect and count fruit in unstructured environments (Figure A. Part 0: Obtaining Ground TruthWe have developed a special purpose web-based labeling framework called label.ag to quickly collect and store ground truth labels from a team of human labelers. The system is designed to collect these labels in the vector based Scalable Vector Graphics (SVG) format. Instead of providing a pixelbased labeling of the image, the users generate SVG data by drawing circles around fruit in the image. The locations and radii of these circles are then stored for analysis and training.Accuracy of labels: Presenting an entire 1280 \u00d7 960 image to a user containing over 100 fruit to label is a daunting task. To reduce the cognitive load on the users, each orange image is subdivided into 16 320 \u00d7 240 windows (apple images are subdivided into 16 480 \u00d7 300 windows) and these tiles are presented instead. These tiles are automatically padded to provide sufficient context for fruit at the edges. Each window is presented to 3 different users to increase the diversity of the labels. We had 22 users label approximately 5000 image tiles over the span of two weeks. Our final data set is 71 1280\u00d7960 orange images and 21 1920 \u00d7 1200 apple images.Efficiency of labels: Traditionally, image labels are stored as a matrix where each pixel entry is assigned a class label. This format ignores information about total count and the distinction between the fruits. While this approach is sufficient for image segmentation, it is a problem for counting which depends on the information thrown away. The proposed approach thus increases the efficiency of the labels by storing them in a SVG format that preserves location, size, and number of label circles, allowing us to distinguish each individual marking. These SVG labels can then be converted into the desired training images at each part of the proposed pipeline. The blob detection component uses the data to train pixel-level classifiers while the counting component uses the number of user markings to discriminate overlapping fruit.Preparing labels for blob detector: Converting SVG labels to create a label matrix is relatively straightforward. Each pixel is treated independently and averaged across all window tiles and labelers to determine how often that pixel is labeled as fruit. Pixels that have been labeled as fruit more than 50% of the time are classified as fruit.Preparing labels for counting The count neural network needs to give bounding box coordinates and receive a ground truth fruit count in that bounding box. Obtaining these ground truth counts is not straightforward. First, the labels are averaged over fruit, and unlike a pixel, a fruit has no clear correspondence between labelers since the specific location of the SVG label will vary. Second, since we presented smaller overlapped tiles of the full-sized images in order to improve label accuracy, we have to establish a correspondence between tiles.The proposed approach uses Alg. 1 to construct ground truth counts. The intuition of the algorithm is as follows: Consider a bounding box around a blob returned by our blob detector. The true unobservable count z i is that there is 1 fruit in the blob, and we need to get the human generated ground truth zi from our SVG string. This full-sized image has been labeled by 3 different labelers, thus out of the total 48 presented window tiles, every center inside the bounding box has appeared in 3 tiles. To complicate things, since the fruit is difficult to see, only 2 out of 3 labelers labeled it. The algorithm proceeds as follows. It finds 2 centers in the SVG string that lie inside the bounding box. For each center, it finds that that center has appeared in 3 windows. As a result it adds up 1  3 + 1 3 to get a total ground truth estimate of 2  3 fruit inside the bounding box. This algorithm works for the various potential configurations between multiple labelers, window tiles and fruit number. The blob detector is a fully convolutional network that takes in an image of size h \u00d7 w \u00d7 3, and outputs a score tensor of size h \u00d7 w \u00d7 n where n is the number of object classes. Each element x ijk represents the score of pixel at spatial location (i, j) for class k. The probability of a pixel being in class k is obtained by feeding the scores of that pixel for each class through a softmax function.The blob detection network has the same architecture as the original fully convolutional network in C. Part 2: Count Neural NetworkThe presence of overlapped fruit in a cluttered environment means that each detected blob can contain multiple fruit. This overlap may not be a problem in minimizing the pixelwise error, but it is a problem in obtaining accurate fruit counts. The proposed approach solves this problem by using a convolutional neural network that takes in a bounding box around each blob and outputs the estimate of the count of fruit inside the box.The count network has the same architecture as the blob detection network, except that it does not include any of the deconvolutional layers and the output is a single number as opposed to a vector. The loss function is an l 2 loss. This similar architecture allows the count network to be initialized from the features of the blob detection network using the same The second step is to create the associated training ground truth counts for the count network. Using the bounding boxes obtained from the previous step, the proposed approach calls Alg. 1 to obtain the ground truth count for each bounding box. This step has thus associated each window in the count training set with the ground truth estimate of the actual count. The automatically generated data set is then used to train the count network.Even though the count network is initialized from the blob network, it is able to improve over the blob network because it is optimizing a completely different loss function. The task of image classification, which our fruit counting task is similar to, is much simpler than the task of image segmentation since the network is not required to output locations. Approaching the count problem of large fruit images using only one network is extremely complex and difficult due to the large variations in appearance and count. The application of two neural networks is thus a way to simplify the problem into two modular subproblems.D. Part 3: Final Count Linear RegressionThe function between the the human-generated ground truth count and the intermediate output obtained by applying the blob network and count network is:where zi is the human-generated ground truth count, \u1e91i is the intermediate count estimate output, b i is the number of blobs in image i output by the blob network, and c j is the estimated count of blob j output by the count network.Using \u1e91i as an estimate of zi therefore assumes that the function g is the identity function. This assumption is not optimal because both the blob network and the count network never minimized a loss function that directly depended on the human-generated ground truth estimate zi .The final step is thus a linear regression minimizing the following loss function:Note that applying the linear regression on the counts adjusted by b i adds an additional non-linearity since the function mapping the intermediate fruit count estimates to the number of blobs is non-linear.V. RESULTS AND ANALYSISThis section presents the results of the proposed approach on the orange and apple data sets. We chose these data sets in order to demonstrate the proposed approach's ability to generalize since the two data sets vary greatly in terms of lighting, occlusion, and overall environment. We use Caffe We present the results of the proposed algorithm in both traditional pixel-wise accuracy measures and the more challenging count-based measures. We then present an empirical analysis of the effect of training time and training data set size to gauge how quickly and easily a fruit grower can apply the algorithm in the real world. Finally we present a few instances when the algorithm has outperformed the human labelers.Description of Dataset:We purposefully analyze the performance of our pipeline on two datasets that differ in lighting condition, occlusion levels, resolution and camera type in order to demonstrate that our method generalizes well across different conditions. We view this generalizability as one of the main strengths of this approach. We collected the orange fruit data set during the day with no artificial lighting at Booth Ranch LLC in California. The orange trees were in a nontrellis arrangement. We acquire images of size 1280 \u00d7 960 using a Bluefox USB 2 camera at 10Hz. We collected the apple fruit data set at night using an external flash setup at Washington State. The apple trees were in a trellis arrangement. We acquired images of size 1920 \u00d7 1200 using a PointGrey USB 3 camera at 6Hz. The orange images were collected with our sensor package mounted on a steady cam and carried by human operator at walking speed. The apple images were collected using a utility vehicle driving down the row at around 1 m/s.The orange data set is a challenging data set due to its non-trellis arrangement and daytime image. As a result there are higher levels of occlusion, more clusters of fruit, more variation in depth, and uncontrolled illumination -all characteristics of fruit in nature. The apple data set is challenging due to color similarity between the apples and the foliage, however it features less occlusion, depth variation, and has controlled illumination.Description of Metrics: We evaluate pixel-wise accuracy using metrics standard to common semantic segmentation tasks such as mean Intersection over Union (IU) as well as true positive and false positive rates to generate Receiver Operating Characteristic (ROC) curves. Figure We evaluate count accuracy using an l 2 error, mean error in the form of ratio of total fruit counted, and standard deviation of the errors. Recall from the problem formulation that the main metric is the l 2 norm. While a simple accuracy, or mean error, is appealing because it is easy to interpret, it is misleading because a model can have high accuracy over the entire data set, but low accuracy per image since the errors will wash each other out. In addition, accuracy will get better as the data set size increases, which is not a desirable property. Standard deviation will capture this variation in error, but it is also lacking since it does not capture accuracy. Using the l 2 norm takes into account both the mean and standard deviation of error, so it is the metric of choice.A. ResultsThe batch size for the blob detection neural network is 1 image because each pixel is treated as a separate training example. The batch size for the count neural network is 24 128 \u00d7 128 images. The results of a texture-based fruit detection algorithm Oranges: There are a total of 7, 200 oranges over 71 images giving on average 102 oranges per image. We sequentially partition the data set into a training set of 36 full-sized images and a testing set of 35 full-sized images in order to prevent the We evaluate pixel-wise metrics of the blob detection neural network trained for 50, 000 iterations. The blob detection network outputs a probability map, and we vary the cutoff threshold to create a binary map. The threshold value for the best ROC is 0.03, yielding a true positive rate of 0.957 and false positive rate of 0.051. The best mean IU is 0.813 at the 0.38 threshold value.We next evaluate count-based metrics of the entire pipeline using a blob detection network trained for 50, 000 iterations and a count network trained for 25, 000 iterations. The threshold value for the blob detection network was set at 0.03. This model achieves an l 2 error of 13.8 corresponding to a ratio of 0.968 total oranges counted with a standard deviation of 13.5 oranges across the entire test set.In order to quantify the effect of each component of the pipeline, we also ran the algorithm using just the blob detection output where each blob represents 1 fruit, a blob detection with only a count network, and a blob detection with only linear regression. Figure Apples: There are a total of 1, 749 apples over 21 images giving on average 83 apples per image. We sequentially partition the data set into a training set of 11 full sized images and a testing set of 10 full sized images. The same data augmentation strategy as in the orange case is applied here.The pixel-wise metrics for the blob detection network is evaluated at 50, 000 iterations, yielding a best threshold value of 0.02. The true positive rate with this threshold is 0.961 and the false positive rate is 0.033. The mean IU is 0.838 at the 0.37 threshold. We ran the count-based metrics for the apple data using a blob detection neural network trained for 50,000 iterations and a count neural network trained for 25,000 iterations. The threshold value for the blob detection network was set at 0.02. Figure The blob detector is too generous in assigning pixels as fruit, and this over-counting can be mitigated using a higher threshold value for the blob detector at the expense of potentially missing possible fruit regions. A better method, however, is to use the count neural network to accurately count the fruit in each candidate region. Like the orange count network, the apple count network reduces the standard deviation, but has a negative bias. The linear regression corrects for this bias and reduces the overall l 2 error.Our deep learning pipeline is similar to the texture-based method in that both approaches first identity candidate regions or keypoints, and then apply a learning algorithm to count or classify. The texture-based analogue to the blob detector network is the Angular Invariant Maximal (AIM) detector which exploits specular reflection to detect keypoints. The analogue to the count neural network is the random forest classifier using Radial Histogram of Oriented Gradients (Rad-HOG) and Radial Pairwise Intensity Comparisons (RadPIC) as features. While this random forest classifier only outputs a binary classification, our count neural network instead outputs a numerical count estimate. By exploiting the flexibility of deep learning, our pipeline not only achieves higher accuracy, lower standard deviation, and lower overall l 2 error in our nighttime apple data set, but it is also more generalizable to other data sets such as our orange data set because it does not rely on controlled illumination.Data Set Size: We analyze the effect of data set size on the blob network performance by training with various training Training Time:We then analyze the performance of the blob detector as training time increases. Figure We finally analyze the performance of the count network as training time increases. Performance plateaus around 20, 000 iterations, which corresponds to approximately 3 hours on a NVIDIA Titan X GPU. This analysis shows that through the use of freely available pre-trained networks, these algorithms can be trained quickly and easily.Beyond Ground Truth Labels: We observed interesting cases during our training process where the blob detector network outperforms the human generated ground truth labels. Figure VI. CONCLUSION", "conclusions": "We have presented a novel data-driven end-to-end fruit counting pipeline based on deep learning that generalizes across various unstructured environments. In order to demonstrate this generalization, we chose data sets that are challenging in unique ways: the orange data set features high level of occlusions, depth variation, and uncontrolled illumination, and the apple data set features high color similarity between fruit and foliage.We first introduced label.ag, a crowd sourced label collection platform, along with our novel usage of Support Vector Graphics (SVG) for the purpose of storing and propagating label information. We then presented the blob detector neural network, which has high pixel-wise accuracy, achieving a mean IU of 0.813 on the oranges and 0.838 on the apples. We next presented the count neural network and a linear regression model, and demonstrated the ability of the pipeline to accurately count the number of apples in the images. We achieved a best l 2 error of 13.8 on the oranges, and 10.5 on the apples. We quantitatively and qualitatively compared our deep learning pipeline against a state-of-the-art texture-based method on our apple data set and demonstrated higher accuracy, lower standard deviation, and lower l 2 error. Additionally, our method suggests accurate counting from a limited labeled data set with a short training time. A current limitation in our approach is that we are susceptible to errors in the human-generated labels. While potentially unavoidable, we can minimize these errors by calibrating human-generated labels with groundtruth per-tree fruit counts obtained after harvest. Due to the generalizability of our pipeline, we envision our methodology being applied beyond precision agriculture to applications such as plant phenotyping (identifying as well as counting plants), phytopathology (identifying and monitoring visual disease symptoms), and even microbiology cell counting.", "SDG": [2]}, "deep_learning_for_smart_agriculture_concepts_tools_applications_and_opportunities": {"name": "Deep learning for smart agriculture: Concepts, tools, applications, and opportunities", "abstract": " In recent years, Deep Learning (DL), such as the algorithms of Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN) and Generative Adversarial Networks (GAN), has been widely studied and applied in various fields including agriculture. Researchers in the fields of agriculture often use software frameworks without sufficiently examining the ideas and mechanisms of a technique. This article provides a concise summary of major DL algorithms, including concepts, limitations, implementation, training processes, and example codes, to help researchers in agriculture to gain a holistic picture of major DL techniques quickly. Research on DL applications in agriculture is summarized and analyzed, and future opportunities are discussed in this paper, which is expected to help researchers in agriculture to better understand DL algorithms and learn major DL techniques quickly, and further to facilitate data analysis, enhance related research in agriculture, and thus promote DL applications effectively.", "keywords": "deep learning,smart agriculture,neural network,convolutional neural networks,recurrent neural networks,generative adversarial networks,artificial intelligence,image processing,pattern recognition", "introduction": "A standard artificial neural network (ANN) model consists of many neurons (connected processors), each producing a sequence of real-valued activations  .[1]When sensors perceive environment changes, input neurons will be activated and other neurons will then get activated through weighted connections from previously active neurons. Depending on the specific problem and the neuron topology, these behaviors may require long chains of computational stages, where each of the stage transforms the aggregate activation of the network. DL is about how to accurately assign credit across many such stages  . Deep learning allows computational models that are composed of multiple processing layers to represent data with multiple levels of abstraction. Great improvements of the method can be found in many research domains.[2]The concept of BP (Back Propagation) Neural Network is the basis for many DL algorithms.With massive enthusiasm pouring into the DL field, great improvements have been achieved in recent years. DL has drawn a lot of attention in agriculture. One of its applications in agriculture is image recognition, which has conquered a lot of obstacles that limit fast development in robotic and mechanized agro-industry and agriculture  . These improvements can be seen in many aspects of agriculture, such as plant disease detection, weed control, and plant counting. Researchers in agriculture may not be experienced programmers. They often directly use publicly available software frameworks for deep learning without carefully examining the learning mechanisms used. An understanding of DL algorithms can facilitate data analysis and thus enhance research in agriculture. Although various commercial software frameworks are available, there is a lack of a systematic summary of major DL algorithms [3] , including concepts, application limitations, flow charts, and example codes, which can help researchers in agriculture to learn major DL techniques quickly and use them effectively.[3]In order to provide a holistic picture of DL to researchers in agriculture fields and enhance modern smart agriculture development, this work summarizes BP and common DL algorithms (Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and Generative Adversarial Networks (GAN)) and their applications in agriculture, with a focus on applications published in the last three years. Example codes for BP, CNN, RNN, and GAN in Python are also provided.", "body": "A standard artificial neural network (ANN) model consists of many neurons (connected processors), each producing a sequence of real-valued activations When sensors perceive environment changes, input neurons will be activated and other neurons will then get activated through weighted connections from previously active neurons. Depending on the specific problem and the neuron topology, these behaviors may require long chains of computational stages, where each of the stage transforms the aggregate activation of the network. DL is about how to accurately assign credit across many such stages The concept of BP (Back Propagation) Neural Network is the basis for many DL algorithms.With massive enthusiasm pouring into the DL field, great improvements have been achieved in recent years. DL has drawn a lot of attention in agriculture. One of its applications in agriculture is image recognition, which has conquered a lot of obstacles that limit fast development in robotic and mechanized agro-industry and agriculture In order to provide a holistic picture of DL to researchers in agriculture fields and enhance modern smart agriculture development, this work summarizes BP and common DL algorithms (Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and Generative Adversarial Networks (GAN)) and their applications in agriculture, with a focus on applications published in the last three years. Example codes for BP, CNN, RNN, and GAN in Python are also provided.Common deep learning algorithmsCNN, RNN, and GAN are the most commonly used DL algorithms. There are many other sub-category DL algorithms, such as VGGNet Feedforward Neural Network and Backpropagation (BP)An example structure of feedforward neural network based on backpropagation or BP neural network is shown in Figure 1Hidden layer output:Output layer input:Output layer output:Cost function:For neural networks, the cost function plays an important role in optimizing the model parameters (weights and thresholds). A cost function is a measure of the error between the predicted output and the actual output for the training samples. The training process gradually reduces the value of the cost function by the gradient descent. There are two types of cost function expressions. One is a quadratic loss function Backpropagation is based on the gradient descend of the cost function and the chain rule of differentiation Weight and threshold adjustment (local gradient) from the hidden layer to the output layer:Weight and threshold adjustment (local gradient) from the input layer to the hidden layer:In the process of backpropagation, the weights and thresholds are adjusted by reversing the value of error Figure Convolutional Neural Networks (CNN)CNN is a deep learning algorithm composed of multiple convolutional layers, pooling layers, and fully connected layers, which has resulted in many breakthroughs in speech recognition, face recognition, natural language processing and so on Convolutional Layer:Assuming that an RGB image is taken as input X and there are six convolution kernels W [19] . The image size is an H\u00d7W\u00d73 three-dimensional matrix. The convolution kernel size associated with each convolutional layer is h\u00d7w\u00d73. The threshold of convolutional layer is b. The size of new image feature X after convolution calculation is H_new\u00d7W_new, whose convolutional layer computation process is shown in Figure The major formulations are listed as Equations ( Figure Pooling Layer:The function of the pooling layer is to compress the input tensor and reduce the size of the input data, which means changing each n\u00d7n submatrix of the input image into one specific element value. There are two common pooling methods: maximum pooling and mean pooling.Maximum pooling takes the maximum of the corresponding n\u00d7n area as the pooled element value and mean pooling takes the average value of the corresponding n\u00d7n area as the pooled element value. The input volume for the pooling layer is W\u00d7H\u00d7D, the output volume for the pooling layer is W_pooling\u00d7H_pooling\u00d7D_pooling [20] .The pooling layer computation process is shown in Figure _ pooling DD \uf03dA flowchart of the CNN algorithm is shown in Figure Recurrent Neural Networks (RNN)RNN can mine temporal information and semantic information and has achieved breakthroughs in time series analysis, speech recognition and language modeling. RNN is a variant of ANN, which means that the current input of the network is related to the output of the previous moment. The specific manifestation is that the network will remember the previous information and apply it to the current network output calculation; in other words, RNN can be treated as BP neural network of which the output will be used as the input of the next network As Figure The specific formulations are listed as Equations ( Hidden state value at time t:The predicted output of recurrent neural network at time t: o t = f(v\u00d7h t + b 2 ) (17) Recurrent neural network error at time t: l = o ty t (18) A flowchart for RNN programming is shown in Figure Generative Adversarial Networks (GAN)The ingenuity of GAN lies in its design. Technically, it is a combination of existing algorithms of BP. GAN is a way that a set of noise is used to learn the distribution of the real data and to generate new data. The structure of this network consists of two models, a generation model which is to capture the distribution of the real data, and a discrimination model which is similar to a binary classifier Assuming that the generation model is a deep neural network which generates a new vector called fake data G(z) and the discrimination model is a fully connected network which obtains a probability value D(z) reflecting real data set. x is a real data set Discrimination model loss functionGeneration model loss function E g :(1 )log(1Total network loss function E:After the loss functions are obtained, the network is optimized by us V(D,G),the optimization function:The expressions above can be divided into two functions: discrimination model optimization function and generation model optimization function.Discrimination model optimization function:According to the above, it can be observed that the optimization of the discrimination model and the optimization of the generation model are independent of each other.Figure 9 Generative adversarial networks structureThe power of generative adversarial networks (GAN) lies in the ability to automatically learn the distribution of real sample data. Deep convolutional generative adversarial networks (DCGAN) resulting from the rise of convolutional neural network is widely used in image processing such as image restoration from split image, dynamic scene generation In addition, DCGAN plays an important role in face detection and recognition A flowchart for GAN programming is shown in Figure Codes of BP, CNN, RNN, and GAN in Python are provided in the supplemental files. Figure Input layer Output layer Hidden layerData fitting Pattern recognition Classification CNN LeCun Input layer Convolution layerPooling layer Full connected layerImage processingSpeech signal Natural Language Processing RNN Mikolov Time series analysisEmotion analysis Natural Language Processing GAN Goodfellow Image generationVideo generationDeep learning frameworksTensorFlow and Caffe are two commonly-used DL frameworks, which allow users to use DL without significant programming.The most popular TensorFlow and Caffe frameworks are introduced below briefly.TensorFlowTensorFlow is an open source computing framework of Google that supports deep learning algorithms, including CNN, RNN, GAN and other variants, which can be used on Linux, Windows, and Mac platforms. TensorFlow has some advantages including high flexibility, true portability, multi-language support, rich algorithm library, and excellent documentation. TensorFlow provides a very rich set of deep learning application programming interfaces (API) including basic vector matrix calculations, optimization algorithms, convolutional neural networks, recurrent neural networks, and visual aids. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate state. It maps the nodes of a dataflow graph across multiple computational devices, such as multi-core CPUS, general-purpose GPUS, and custom-designed ASICs, which are known as Tensor Processing Units (TPUS) A deep learning model, typically a multi-layer neural network, is composed of several computational layers that process data in a hierarchical fashion. Each layer takes an input and produces an output, often computed as a non-linear function or a weighted linear function of a weighted linear combination of the input values. It is particularly popular that convolutional layers apply a local function or filter to all subsets of the layer's input, such as portions of an image CaffeConvolution Architecture For Feature Extraction (Caffe) is the first deep learning framework that has been widely used in industry as an open source. The Caffe model and the corresponding optimizing methods are given as texts instead of codes. Caffe gives the definition of the model, optimal settings, and pre-training weights. Caffe handles massive data with high speed. Besides, Caffe can be modular which easily extended to new tasks. Users can define their own models using the types of neuron layers provided by Caffe.Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep learning models efficiently on varied architectures A comparison between TensorFlow and Caffe can be found in Table Table 2 Comparison between Tensorflow and CaffeRecent applications of DL in smart agricultureRecent applications of CNN, RNN, and GAN in smart agriculture are summarized in this section.CNN applications in smart agricultureCNN has strong capability in image processing, which makes it widely used in agriculture research. Generally speaking, most applications of DL in agriculture can be categorized as plant or crop classification, which is vital for pest control, robotic harvesting, yield prediction, disaster monitoring etc.Plant disease detection is time-consuming when it is done manually.Fortunately, with the development of artificial intelligence, plant disease detection can be accomplished through image processing. Plant disease recognition models are mostly based on leaf image classification and pattern recognition As many countries across the world have been developing initiatives to build national agriculture monitoring network systems, plant classification and weed identification are particularly important because of the implications for automating agriculture. Since image recognition can be applied to detect plenty of features of plants, CNN has been extensively used to detect weeds or classify plants One of the pre-trained CNN architecture that is widely used for plant classification is AlexNet. Experimental results based on AlexNet from the Istanbul Technical University in 2017 suggest that the CNN architecture outperforms machine learning algorithms that are based on hand-crafted features for the discrimination of phenological stages In another study, self-organizing Kohonen maps (SOMs) were used for optical images segmentation and subsequent restoration of missing data in a time-series of satellite imagery.Supervised classification with CNN was performed. This method added a post-processing step that included several filtering algorithms based on the available information and geospatial analysis. An accuracy of 85% was achieved for classification of major crops (wheat, maize, sunflower, soybean, and sugar beet) Fruit counting is important for yield prediction and robotic harvesting. The traditional manual counting or mobile camera counting cannot provide satisfactory results and are time-consuming.Because of changes in occlusion and illumination, pre-processing such images is challenging. Regular DL methods have difficulty in solving these problems. A blob detection method has proven to be useful Land classification usually involves classification of large areas of land and is important to such purposes as land use and land cover (LULC), disaster risk assessment, agriculture, and food security Item detection is a fast-growing domain in DL. In agriculture fields, obstacle detection is also important for farmers, especially when highly autonomous machines have been increasingly used. In order to operate these machines safely without supervision, they must perform automatic real-time risk detection with high reliability Information from satellite is very precious and important for making sustainable land use planning for minimizing CO 2 emission, maximizing economic returns, and minimizing land degradation. The challenge with using information is to interpret the images collected. Translating satellite images by using convolutional neural networks (CNN) and genetic algorithms has become a useful strategy for decision making, especially for precision agriculture and agroindustry. The data can be used to classify plant types in a land area using CNN. Land types and other data can be added to the grid form. A grid form model was evaluated to assess objectives and a genetic algorithm was used produce an optimal solution CNN can also be used in weather forecasting RNN applications in smart agricultureRNN is very useful to process time series data and has been used in many agricultural areas, such as land cover classification, phenotype recognition, crop yield estimation, leaf area index estimation, weather prediction, soil moisture estimation, animal research, and event date estimation.Land cover classification (LCC) is considered as a vital and challenging task in agriculture, and the key point is to recognize what class a typical piece of land is in. In the past, a lot of applications are based on mono-temporal observations and ignoring time-series effects in some problems. For instance, vegetation changes its spatial appearance periodically, which can confuse the mono-temporal approaches.Meanwhile, mono-temporal approaches might be influenced by some biases, like weather. Therefore, deep sequence models have been applied, and a widely-used variant RNN model is the LSTM. In an experiment led by Ru\u00dfwurm et al. Plant phenotyping has become a hot topic, yet a challenging one, because of the increasing need of precision agriculture. Briefly, plant phenotyping means to recognize the kind of a plant through its appearance or traits.Most machine learning approaches have relied on individual static observations, which cause incorrect recognition of similar plants in typical periods. A new deep learning structure for plant phenotype recognition was created by Namin et al. Beyond CNN, RNN has also been used for crop yield estimation, which uses time series data to reduce biasing. Minh et al. The leaf area index (LAI) is a key attribute of many agricultural models. Accurate LAI and its dynamics are widely used for estimations of environment, vegetation status and carbon cycle etc. Traditional LAI estimation methods fall into two categories, empirical methods and physical methods. The LAI results may suffer from spatial or temporal discontinuities, which limit their applications in climate simulation and weaken their robustness. To solve the problem, an RNN-based model named NARX (Nonlinear Autoregressive model process with eXogenous input) was applied. The inputs of the NARX model were previous prediction values and the current and previous values of an exogenous input signal. The inputs were fed to an RNN and the prediction Y were a part of inputs in the next time step. This model took not only independent inputs into consideration but also the output of the model in the past, making it more powerful and. Chai et al. RNN is useful for time series and thus has been used in weather prediction. In Biswas et al. Soil moisture (SM) is a vital hydrological attribute for precision agriculture, meteorology, and climate change. However, SM in farmlands is a function of many factors and can vary extremely with time and space, causing difficulty in precise estimation. Neural networks are applied to this task naturally because they can estimate complex functions and time-series input. Lu et al. RNN has also been used in studying animals in both the macroscopical and the microcosmic scales. With the development of deep learning, RNN-based models have proven to be very competitive. In South Africa, the movement of elephant herds hurt the endangered species of vegetation. Palangpour et al. RNN can also be used for event date estimation GAN applications in smart agricultureGAN is a new kind of neural network but has been considered a very useful method in many fields, especially in image processing. GAN has often been used to enrich datasets. It has not been applied to agriculture widely.Ledig et al. In another study, Barth et al. A Meta-Analysis of DL applications in smart agricultureMany pure DL research articles are published in the ACM proceedings. In contrast, applications of DL in agriculture are more often published in research journals. Among the published papers, the well-known databases of Science Citation Index (SCI) and Social Science Citation Index (SSCI) collect the most representative and qualified papers. Thus, a meta-data analysis was performed using the bibliographic datasets, which contain the metadata information, like journals, authors, publication year, citations, and institutions. The metadata analysis, also called as bibliometric analysis, belongs to the field of scientometrics. The analysis has the advantages of data-driven characteristics, and being objective, complete and repeatable The analysis involved the following steps: collecting datasets from the bibliographic databases with explicit search strategies; analysis of yearly output, topics, and related disciplines; and displaying of an overall picture of DL-agriculture research.The search strategy is as follows:(TS=(deep learning) AND SU=agriculture) AND language: (English) AND type: (Article) Time span: 1985-2018 Index: SCI-EXPANDED, SSCI where, TS stands for \"topic\", SU stands for \"research area\".Forty-seven records were obtained by applying the search term in the core collection of Web of Science database. \"SU=agriculture\" was used to confine the research areas in the field of \"agriculture\". The analysis results were displayed as follows.Publication Yearly OutputThe publication counts, to some degree, reveal the research intensity in a field. The yearly output of publications is shown in Figure To have a close look at the research topics, the following co-word analysis was conducted, which was based on the keyword co-occurrence relations. The illustration was generated by the bibliometric software, CiteSpace The revealed topics provide a landscape of the domain expert knowledge at a micro level. How the subject of agriculture in a macro-level is affected by the research fever is still not clear. Journals can reveal the macro status of research trends to some level. Relevant journals were investigated and the results are listed in Table The number provides a direct and simple indicator for the journal impacts on the DL-agriculture research. However, the distribution on how the research combines different disciplines is still not clear.The following overlay visualization based on dual-mapping was thus performed, as shown in Figure The dual-mapping consists of two mappings based on datasets of large-scale journals from all disciplines. The whole science mappings consist of the citing journal cluster on the left side and the cited journal cluster on the right. The citing cluster contain 10 330 all-discipline journals and the cited cluster contain 10 253 all-discipline journals Either from historical research statistics or the recent research results, it can be shown that DL can be well applied in agriculture to solve various problems that have concerned farmers and scientists for a long time. With the help of novel techniques and new theories, these new approaches outperform the previous methods in many ways. Take image recognition for instance, previously, scholars may collect images and use DL to do classification by analyzing pixels in RGB images Because there are many applications in agriculture not equipped with recent new techniques, there is still a lot of room for expanding DL in agriculture research. Although some of the results gained accuracy around or higher than 95%, robustness and reliability are still challenges. The promise of the application of DL in agriculture can be foreseen. Moreover, it is very possible that future development of DL in agriculture will be based on not only a single theory or method but a combination of multiple methods. The metadata analysis shows that applications of DL in agriculture are upward from the Recs counts. However, they are highly divergent and scattered in very different research purposes. It implies that more challenges and opportunities remain in the area and more efforts are needed.Conclusions", "conclusions": "In this summary, the concepts, tools, limitations, algorithms of DL are summarized.Applications of DL in agriculture are reviewed. It can be observed that DL has been widely used in different areas of agriculture, such as plant disease detection, plant classification and weed identification, fruit counting, land classification, obstacle detection, image translation, weather forecasting, yield prediction, and animal behavior classification. While DL research is gaining momentum in general, research of DL for agriculture is very divergent according to the metadata analysis. There are many opportunities for DL applications in smart agriculture, such as:(1) Agriculture information processing. Monitoring the status of plants and animals is vital to agriculture production. Some status variables of plants and animals cannot be measured directly. Under this case, DP can be used to determine unmeasured information from measured one because different variables of plant status may have some dependent relationships. In agriculture production, plants or animals interact with environmental factors. It is difficult to build a pure mechanism-based-model structure to describe the relationship between plants (or animals) and environmental factors. As a data-driven-model structure, DP can be directly used to build the relationship of plant (or animal) factors, environmental factors and plant (or animal) growth status. This will facilitate agriculture information processing.(2) Agriculture production system optimal control. Control strategies in agriculture production system often rely on farmer experience or experts knowledge, which do not consider plant (animal) physiological status or real time demand. This unavoidably makes the strategies not optimal. An advantage of DP is to model complex systems without heavily relying on the knowledge of mechanisms. By using DP to model agriculture production systems, optimal control strategy development thus becomes possible, which makes use real time measurement of plant (animal) physiological status and historical data.(3) Smart agriculture machinery equipments. Agriculture production involves numerous kinds of tasks. These tasks are often labor consuming and the working environment is very challenging. Using DP to mimic human behavior and driven agriculture machinery equipments has prospective future in many areas of agriculture, such as seeding, management, harvesting, and post-harvest processing. For example, a robot that can be used to harvest apples is very useful. It must be intelligent enough to position apples and pick apples with a high efficiency. Under natural condition, background light, tree branches and leaves have strong interference to computer vision signal. The routes of robot arms reach targets should also be optimized. For all these issues, DP techniques may be very useful.(4) Agricultural economic system management. Agriculture yield itself is not enough for agriculture. There are many more factors should be considered such as the prices and the quality of agriculture products. It is very meaningful to predict agriculture product prices. However, prices are related to many variables. Under this case, DP can be used to model price changes with different variables. There are complex relationships between agriculture product quality and nutrition, human health, and economy. DP can be used to model the complex relationship and enhance agricultural economic system management.", "SDG": [2]}, "extremeweather_a_large_scale_climate_dataset_for_semi_supervised_detection_localization_and_understanding_of_extreme_weather_events": {"name": "ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events", "abstract": " Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https://github.com/eracah/hur-detect.", "keywords": "", "introduction": "Climate change is one of the most important challenges facing humanity in the 21st century, and climate simulations are one of the only viable mechanisms for understanding the future impact of various carbon emission scenarios and intervention strategies. Large climate simulations produce massive datasets: a simulation of 27 years from a 25 square km, 3 hour resolution model produces on the order of 10TB of multi-variate data. This scale of data makes post-processing and quantitative assessment challenging, and as a result, climate analysts and policy makers typically take global and annual averages of temperature or sea-level rise. While these coarse measurements are useful for public and media consumption, they ignore spatially (and temporally) resolved extreme weather events such as extra-tropical cyclones and tropical cyclones (hurricanes). Because the general public and policy makers are concerned about the local impacts of climate change, it is critical that we be able to examine how localized weather patterns (such as tropical cyclones), which can have dramatic impacts on populations and economies, will change in frequency and intensity under global warming.Deep neural networks, especially deep convolutional neural networks, have enjoyed breakthrough success in recent recent years, achieving state-of-the-art results on many benchmark datasets (Krizhevsky et al., 2012;He et al., 2015; and also compelling results on many practical tasks such as disease diagnosis Szegedy et al., 2015), facial recognition (Hosseini-Asl et al., 2016), autonomous driving (Parkhi et al., 2015), and many others. Furthermore, deep neural networks have also been very effective in the context of unsupervised and semi-supervised learning; some recent examples include variational autoencoders (Chen et al., 2015), adversarial networks (Kingma & Welling, 2013)(Goodfellow et al., 2014;Makhzani et al., 2015;Salimans et al., 2016;, ladder networks Springenberg, 2015) and stacked what-where autoencoders (Rasmus et al., 2015).(Zhao et al., 2015)There is a recent trend towards video datasets aimed at better understanding spatiotemporal relations and multimodal inputs (Kay et al., 2017;Gu et al., 2017;. The task of finding extreme weather events in climate data is similar to the task of detecting objects and activities in video -a popular application for deep learning techniques. An important difference is that in the case of climate data, the 'video' has 16 or more 'channels' of information (such as water vapour, pressure and temperature), while conventional video only has 3 (RGB). In addition, climate simulations do not share the same statistics as natural images. As a result, unlike many popular techniques for video, we hypothesize that we cannot build off successes from the computer vision community such as using pretrained weights from CNNs Goyal et al., 2017)(Simonyan & Zisserman, 2014; pretrained on ImageNet Krizhevsky et al., 2012).(Russakovsky et al., 2015)Climate data thus poses a number of interesting machine learning problems: multi-class classification with unbalanced classes; partial annotation; anomaly detection; distributional shift and bias correction; spatial, temporal, and spatiotemporal relationships at widely varying scales; relationships between variables that are not fully understood; issues of data and computational efficiency; opportunities for semi-supervised and generative models; and more. Here, we address multi-class detection and localization of four extreme weather phenomena: tropical cyclones, extra-tropical cyclones, tropical depressions, and atmospheric rivers. We implement a 3D (height, width, time) convolutional encoderdecoder, with a novel single-pass bounding-box regression loss applied at the bottleneck. To our knowledge, this is the first use of a deep autoencoding architecture for bounding-box regression. This architectural choice allows us to do semi-supervised learning in a very natural way (simply training the autoencoder with reconstruction for unlabelled data), while providing relatively interpretable features at the bottleneck. This is appealing for use in the climate community, as current engineered heuristics do not perform as well as human experts for identifying extreme weather events.Our main contributions are (1) a baseline bounding-box loss formulation; (2) our architecture, a first step away from engineered heuristics for extreme weather events, towards semi-supervised learned features; (3) the ExtremeWeather dataset, which we make available in three benchmarking splits: one small, for model exploration, one medium, and one comprising the full 27 years of climate simulation output.", "body": "Climate change is one of the most important challenges facing humanity in the 21st century, and climate simulations are one of the only viable mechanisms for understanding the future impact of various carbon emission scenarios and intervention strategies. Large climate simulations produce massive datasets: a simulation of 27 years from a 25 square km, 3 hour resolution model produces on the order of 10TB of multi-variate data. This scale of data makes post-processing and quantitative assessment challenging, and as a result, climate analysts and policy makers typically take global and annual averages of temperature or sea-level rise. While these coarse measurements are useful for public and media consumption, they ignore spatially (and temporally) resolved extreme weather events such as extra-tropical cyclones and tropical cyclones (hurricanes). Because the general public and policy makers are concerned about the local impacts of climate change, it is critical that we be able to examine how localized weather patterns (such as tropical cyclones), which can have dramatic impacts on populations and economies, will change in frequency and intensity under global warming.Deep neural networks, especially deep convolutional neural networks, have enjoyed breakthrough success in recent recent years, achieving state-of-the-art results on many benchmark datasets There is a recent trend towards video datasets aimed at better understanding spatiotemporal relations and multimodal inputs Climate data thus poses a number of interesting machine learning problems: multi-class classification with unbalanced classes; partial annotation; anomaly detection; distributional shift and bias correction; spatial, temporal, and spatiotemporal relationships at widely varying scales; relationships between variables that are not fully understood; issues of data and computational efficiency; opportunities for semi-supervised and generative models; and more. Here, we address multi-class detection and localization of four extreme weather phenomena: tropical cyclones, extra-tropical cyclones, tropical depressions, and atmospheric rivers. We implement a 3D (height, width, time) convolutional encoderdecoder, with a novel single-pass bounding-box regression loss applied at the bottleneck. To our knowledge, this is the first use of a deep autoencoding architecture for bounding-box regression. This architectural choice allows us to do semi-supervised learning in a very natural way (simply training the autoencoder with reconstruction for unlabelled data), while providing relatively interpretable features at the bottleneck. This is appealing for use in the climate community, as current engineered heuristics do not perform as well as human experts for identifying extreme weather events.Our main contributions are (1) a baseline bounding-box loss formulation; (2) our architecture, a first step away from engineered heuristics for extreme weather events, towards semi-supervised learned features; (3) the ExtremeWeather dataset, which we make available in three benchmarking splits: one small, for model exploration, one medium, and one comprising the full 27 years of climate simulation output.Related workDeep learning for climate and weather dataClimate scientists do use basic machine learning techniques, for example PCA analysis for dimensionality reduction Related methods and modelsFollowing the dramatic success of CNNs in static 2D images, a wide variety of CNN architectures have been explored for video, ex. Convolutional networks have also been combined with RNNs (recurrent neural networks) for modeling video and other sequence data, and we briefly review some relevant video models here. The most common and straightforward approach to modeling sequential images is to feed single-frame representations from a CNN at each timestep to an RNN. This approach has been examined for a number of different types of video The 3D CNNs we use here are based on 3-dimensional convolutional filters, taking the height, width, and time axes into account for each feature map, as opposed to aggregated 2D CNNs. This approach was studied in detail in Stepping back, our approach is related conceptually to The ExtremeWeather datasetThe DataThe climate science community uses three flavors of global datasets: observational products (satellite, gridded weather station); reanalysis products (obtained by assimilating disparate observational products into a climate model) and simulation products. In this study, we analyze output from the third category because we are interested in climate change projection studies. We would like to better understand how Earth's climate will change by the year 2100; and it is only possible to conduct such an analysis on simulation output. Although this dataset contains the past, the performance of deep learning methods on this dataset can still inform the effectiveness of these approaches on future simulations. We consider the CAM5 (Community Atmospheric Model v5) simulation, which is a standardized three-dimensional, physical model of the atmosphere used by the climate community to simulate the global climate The LabelsGround-truth labels are created for four extreme weather events: Tropical Depressions (TD) Tropical Cyclones (TC), Extra-Tropical Cyclones (ETC) and Atmospheric Rivers (AR) using TECA Issues with the LabelsTECA, the ground truth labeling framework, implements heuristics to assign 'ground truth' labels for the four types of extreme weather events. However, it is entirely possible there are errors in the labeling: for instance, there is little agreement in the climate community on a standard heuristic for capturing Extra-Tropical Cyclones Suggested Train/Test SplitsWe provide suggested train/test splits for the varying sizes of datasets on which we run experiments. Table The modelWe use a 3D convolutional encoder-decoder architecture, meaning that the filters of the convolutional encoder and decoder are 3 dimensional (height, width, time). The architecture is shown in Figure where L rec is the mean squared squared difference between input X and reconstruction X * :where M is the total number of pixels in an image.In order to regress bounding boxes, we split the original 768x1152 image into a 12x18 grid of 64x64 anchor boxes. We then predict a box at each grid point by transforming the representation to 12x18=216 scores (one per anchor box). Each score encodes three pieces of information: (1) how much the predicted box differs in size and location from the anchor box, (2) the confidence that an object of interest is in the predicted box (\"objectness\"), and (3) the class probability distribution for that object. Each component of the score is computed by several 3x3 convolutions applied to the 640 12x18 feature maps of the last encoder layer. Because each set of pixels in each feature map at a given x, y coordinate can be thought of as a learned representation of the climate data in a 64x64 patch of the input image, we can think of the 3x3 convolutions as having a local receptive field size of 192x192, so they use a representation of a 192x192 neighborhood from the input image as context to determine the box and object centered in the given 64x64 patch. Our approach is similar to The bounding box regression loss, L sup , is determined as follows:where N is the number of time steps in the minibatch, and L box is as:where i \u2208 [0, 216) is the index of the anchor box for the ith grid point, and where 1 obj i = 1 if an object is present at the ith grid point, 0 if not; R(z) is the smooth L1 loss as used in ) i and t is the parametrization defined in , where (x a , y a , w a , h a ) is the center coordinates and height and width of the closest anchor box, (x, y, w, h) are the predicted coordinates and (x * , y * , w * , h * ) are the ground truth coordinates.L conf is the weighted cross-entropy of the log-probability of an object being present in a grid cell:(5)Finally L cls is the cross-entropy between the one-hot encoded class distribution and the softmax predicted class distribution, evaluated only for predicted boxes at the grid points containing a ground truth box:The formulation of L sup is similar in spirit to YOLO Finally, where YOLO uses squared difference between predicted and ground truth for the coordinate parametrizations, we use smooth L1, due its lower sensitivity to outlier predictions Experiments and DiscussionFramewise ReconstructionAs a simple experiment, we first train a 2D convolutional autoencoder on the data, treating each timestep as an individual training example (everything else about the model is as described in Section 4), in order to visually assess reconstructions and ensure reasonable accuracy of detection. Figure Detection and localizationAll experiments are on ExtremeWeather-small, as described in Section 3, where 1979 is train and 1981 is validation. The model is trained with Adam The code is available at https://github.com/eracah/hur-detectDuring training, we input one day's simulation at a time (8 time steps; 16 variables). The semisupervised experiments reconstruct all 8 time steps, predicting bounding boxes for the 4 labelled timesteps, while the supervised experiments reconstruct and predict bounding boxes only for the 4 labelled timesteps. Table It is interesting to note that 3D models perform significantly better than their 2D counterparts for ETC and TC (hurricane) classes. This implies that the time evolution of these weather events is an important criteria for discriminating them. In addition, the semi-supervised model significantly improves the ETC and TC performance, which suggests unsupervised shaping of the spatio-temporal representation is important for these events. Similarly, semi-supervised data improves performance of the 3D model (for IOU=0.1), while this effect is not observed for 2D models, suggesting that 3D representations benefit more from unsupervised data. Note that hyperparameters were tuned in the supervised setting, and a more thorough hyperparameter search for \u03bb and other parameters may yield better semi-supervised results.Figure Feature explorationIn order to explore learned representations, we use t-SNE (van der Maaten & Hinton, Nov 2008) to visualize the autoencoder bottleneck (last encoder layer). Figure Conclusions and Future Work", "conclusions": "We introduce to the community the ExtremeWeather dataset in hopes of encouraging new research into unique, difficult, and socially and scientifically important datasets. We also present a baseline method for comparison on this new dataset. The baseline explores semi-supervised methods for object detection and bounding box prediction using 3D autoencoding CNNs. These architectures and approaches are motivated by finding extreme weather patterns; a meaningful and important problem for society. Thus far, the climate science community has used hand-engineered criteria to characterize patterns. Our results indicate that there is much promise in considering deep learning based approaches. Future work will investigate ways to improve bounding-box accuracy, although even rough localizations can be very useful as a data exploration tool, or initial step in a larger decision-making system. Further interpretation and visualization of learned features could lead to better heuristics, and understanding of the way different variables contribute to extreme weather events. Insights in this paper come from only a fraction of the available data, and we have not explored such challenging topics as anomaly detection, partial annotation detection and transfer learning (e.g. to satellite imagery). Moreover, learning to generate future frames using GAN's  or other deep generative models, while using performance on a detection model to measure the quality of the generated frames could be another very interesting future direction. We make the ExtremeWeather dataset available in hopes of enabling and encouraging the machine learning community to pursue these directions. The retirement of Imagenet this year (Goodfellow et al., 2014) marks the end of an era in deep learning and computer vision. We believe the era to come should be defined by data of social importance, pushing the boundaries of what we know how to model.(Russakovsky et al., 2017)possible. We would also like to thank Yunjie Liu and Michael Wehner for providing access to the climate datasets; Alex Lamb and Thorsten Kurth for helpful discussions.", "SDG": [2]}, "future_projection_with_an_extreme_learning_machine_and_support_vector_regression_of_reference_evapotranspiration_in_a_mountainous_inland_watershed_i": {"name": "Future Projection with an Extreme-Learning Machine and Support Vector Regression of Reference Evapotranspiration in a Mountainous Inland Watershed in North-West China", "abstract": " This study aims to project future variability of reference evapotranspiration (ET 0 ) using artificial intelligence methods, constructed with an extreme-learning machine (ELM) and support vector regression (SVR) in a mountainous inland watershed in north-west China. Eight global climate model (GCM) outputs retrieved from the Coupled Model Inter-comparison Project Phase 5 (CMIP5) were employed to downscale monthly ET 0 for the historical period 1960-2005 as a validation approach and for the future period 2010-2099 as a projection of ET 0 under the Representative Concentration Pathway (RCP) 4.5 and 8.5 scenarios. The following conclusions can be drawn: the ELM and SVR methods demonstrate a very good performance in estimating Food and Agriculture Organization (FAO)-56 Penman-Monteith ET 0 . Variation in future ET 0 mainly occurs in the spring and autumn seasons, while the summer and winter ET 0 changes are moderately small. Annually, the ET 0 values were shown to increase at a rate of approximately 7.5 mm, 7.5 mm, 0.0 mm (8.2 mm, 15.0 mm, 15.0 mm) decade \u22121 , respectively, for the near-term projection (2010-2039), mid-term projection (2040-2069), and long-term projection (2070-2099) under the RCP4.5 (RCP8.5) scenario. Compared to the historical period, the relative changes in ET 0 were found to be approximately 2%, 5% and 6% (2%, 7% and 13%), during the near, mid-and long-term periods, respectively, under the RCP4.5 (RCP8.5) warming scenarios. In accordance with the analyses, we aver that the opportunity to downscale monthly ET 0 with artificial intelligence is useful in practice for water-management policies.", "keywords": "reference evapotranspiration (ET 0 ),extreme-learning machine,support vector regression,ET 0 projection,climate change", "introduction": "Reference evapotranspiration (ET 0 ) is a significant parameter for agriculture, ecosystems and hydrological modeling [1,. ET 0 is one of the most important indicators of global climate change and hydrological regime changes 2]. Therefore, the estimation and projection of trends in ET 0 can be very important for water-resource management, precision agriculture, irrigation planning, and hydrological modeling studies [3][4][5]. In the last few decades, many different models, including water budget-based, mass transfer-based, temperature-based, radiation-based and combination approaches, have been used to estimate ET 0 [6][7][8]. Based on a significantly large number of existing research studies, the FAO-56[9]Water 2017, 9, 880 2 of 23 Penman-Monteith (PM) equation is considered to be the most precise and widely used approach for estimating ET 0 and for providing the validation standard for the other predictive models [3,[10][11][12]. Many studies have regarded the ET 0 values estimated by the FAO-56 PM method as reference values for the other methods [13][14,.15]Other than the utilization of traditional ET 0 estimation methods, artificial intelligence (AI) based approaches have also been tested to estimate ET 0 , as well as other real-life case studies [16][17][18][19][20]. For instance, Kumar et al. [21] first investigated the ability of AI-based models in ET 0 estimation, where artificial neural network (ANN) models were validated for this purpose. A number of other researchers have also paid considerable attention to the use of AI-based methods in estimating ET 0 where ANN, adaptive neuro-fuzzy inference system (ANFIS), support vector regression (SVR), general neuro-fuzzy models, gene-expression programming, M5 Model Tree (M5Tree), extreme-learning machines (ELM), and so on, have been applied [22][4,5,[23][24][25][26][27][28][29][30][31][32]. Among these AI-based methods, the SVR model is considered to be one of the novel models to have been widely applied in ET 0 estimation studies. Wen et al. [33] evaluated the potential utility of SVR to model the daily ET 0 with limited climatic data in an extremely arid region. The results indicated that the SVR-based ET 0 was in good agreement with the FAO-56 PM based ET 0 calculations. Furthermore, the use of SVR, ANFIS-, regression-and climate-based models for ET 0 estimation in a semi-arid highland environment were also investigated by Tabari et al. [34], whose results revealed that the SVR model was considerably better than those attained by applying the regression-and climate-based models. In another study, the results from Yin et al. [35] obtained in a semi-arid mountain area showed that the SVR model was much better than the ANN model applied for estimating the daily ET 0 data. In fact, Kisi [36] found that the least square SVR models were considerably superior to the ANN models for the estimation of ET 0 data. Given the superiority of the SVR model in estimating ET 0 , this method has been proven to possess good stability with relatively high prediction accuracy in many locations.[37]Recently, an extreme-learning machine (ELM), regarded as an AI-based fast and efficient learning technique, has been introduced and tested in many different fields of research [38][39][40][41][42][43][44][45][46]. A review of the applications and trends of studies using the ELM model has been performed by Huang et al. [47]. In respect of the use of this method for an estimation of evapotranspiration, Abdullah et al. [48] first investigated the efficiency of the ELM method for the prediction of FAO-56 PM ET 0 data for three meteorological stations in Iraq, and the results proved that the ELM model was highly efficient in ET 0 estimation. Then, Gocic et al. [5] applied the ELM model to estimate monthly ET 0 for two weather stations in Serbia using data for a 31-year period, and the ELM-based ET 0 data was compared with the results of the Hargreaves, Priestley-Taylor, and Turc equations. Evidently, the ELM model was found to be a better predictive tool than the other models considered for modeling monthly ET 0 data. Although the ELM model is a relatively new AI-based method used for ET 0 estimation, the model has been used rapidly in different locations and has proved to be an efficient and satisfactory tool for predicting ET 0 .[49]Data from the Intergovernmental Panel on Climate Change (IPCC) repositories show that global average air temperature has increased by 0.85 \u2022 C from 1880 to 2012, and would further rise by 1.5 \u2022 C by the end of the 21st century [50,. If so, this can directly or indirectly affect other climatic variables because of their links to atmospheric circulation 51]. It is notable that, as a result of climatic change, ET 0 has increased or decreased in different regions of the world [52][52][53][54][55]. Considering the uncertainty as to how the ET 0 might change and its complex role in moderating climates in different regions, the projected future trends in ET 0 under the background of climate change continues to receive significant attention.[56]Future ET 0 projections can be performed using physically-based models and statistical methods (e.g., Penman-Monteith equations) or by AI-based models (e.g., ANN, SVR and ELM models) where the output climatic variables from global climate models (GCMs) and local-scale, nested systems such as regional climate models (RCMs) are adopted. Li et al.  examined the present and future characteristics of ET 0 on the Loess Plateau of China based on historical weather data in order to Water 2017, 9, 880 3 of 23 downscale HadCM3 (Hadley Centre Coupled Model, version 3) outputs. That study showed that the ET 0 values increased significantly during the 1961-2009 period, whereas the HadCM3 projections showed a continuous increase in ET 0 values into the 21st century. The future ET 0 projections on the Loess Plateau in the study of Gao et al. [57], using CMIP5 data, also demonstrated increasing trends during the 2001-2050 period. The future ET 0 on the Loess Plateau was also investigated by the study of Peng et al. [58] where the average annual ET 0 was shown to increase by approximately 12.7-23.9% from 1961 to 1990 towards the end of the 21st century. Xing et al. [59] conducted an investigation on present and future changes (i.e., 2011-2099) in ET 0 in the Haihe River Basin of China through the outputs of climatic variables extracted from the Phase 3 of the Coupled Model Intercomparison Project (CMIP3). Concluding that the future projection of ET 0 is significant in assessing the hydrological regime change impacted by climate change, the study of Wang et al. [13] selected different approaches to investigate the differences of future ET 0 response to climate change in accordance with HadCM3 outputs for the Hanjiang River Basin. The results showed that the water surplus exhibited a likely decreasing trend in the period 2011-2099. Kundu et al. [3] estimated the future change (2011-2099) trends of ET 0 in central India by downscaling HadCM3 output data.[60]In accordance with the reviews, it is evident that future ET 0 changes has been projected for many regions of the world based on simulated outputs of GCM. However, current studies have mainly been based on conventional (i.e., statistical) methods and models (e.g., the Hargreaves equation). AI-based models that have the ability to integrate historical knowledge (i.e., changes in ET 0 ) with GCM-simulated data in order to perform modeling have seldom been used to estimate future ET 0 values. Several studies have applied downscaling techniques based on artificial neural networks (ANN), multiple linear regressions and other statistical models, owing to their ability to capture non-linear relationships between ground and GCM-based predictors in respect of the predictands, such as rainfall, winds, cloud cover, streamflow and temperature (e.g., [61][62]).[63]In this paper, we have adopted two well-established AI techniques, the support vector regression (SVR) and extreme-learning machine (ELM) algorithms, considering their popularity as robust tools applied in the area of predictive modeling. Notably, the SVR model is a statistical model based in theory that utilizes the regularization framework, presenting advancement over conventional artificial neural network models; whereas the ELM model is a fast and efficient neuro-computational approach offering an improvement in its design and universal approximation capability compared to conventional ANN models. The ELM model was shown to perform more accurately than the SVR and ANN models for drought studies  and the simulation of streamflow [46]. To the authors' best knowledge, the SVR and ELM models have not been fully explored for the future projection of ET 0 . Serious consideration should be given to the fact that most of the future ET 0 projections have been based on the outputs of single-simulation climate models (e.g., HadCM3) that have some degree of uncertainty due to the models' internal variability and fidelity. Uncertainties are likely to degrade a model's overall predictive skill [47]. It is thus desirable that climate modelers and climate policy-makers assess more quantitatively a model's fidelity with respect to observed records, addressed by means of multi-model ensemble projections, in order to reduce uncertainties in the downscaled variables.[64]Considering the aforementioned review of literature, this study employs two well-established AI-based models comprising ELM and SVR algorithms to project future changes in ET 0 in an inland mountainous watershed region of China in which data from GCM outputs of CMIP5 are utilized. In order to reduce the uncertainties of single-simulation GCM models, in this paper we have selected eight-model ensemble projections (from CMIP5) to analyze the overall future variability of ET 0 in northwest of China. The FAO-56 PM based ET 0 has been chosen as the verification standard for the downscaled data, which is meaningful for agricultural applications. The rest of the paper is structured as follows. Section 2 details the materials and methods, including a description of the study area, datasets, methodologies, model development and performance-evaluation measures; Section 3 gives the details of the results; Section 4 includes the discussion; and Section 4 lists the conclusions of this research.", "body": "Reference evapotranspiration (ET 0 ) is a significant parameter for agriculture, ecosystems and hydrological modeling Water 2017, 9, 880 2 of 23 Penman-Monteith (PM) equation is considered to be the most precise and widely used approach for estimating ET 0 and for providing the validation standard for the other predictive models Other than the utilization of traditional ET 0 estimation methods, artificial intelligence (AI) based approaches have also been tested to estimate ET 0 , as well as other real-life case studies Recently, an extreme-learning machine (ELM), regarded as an AI-based fast and efficient learning technique, has been introduced and tested in many different fields of research Data from the Intergovernmental Panel on Climate Change (IPCC) repositories show that global average air temperature has increased by 0.85 \u2022 C from 1880 to 2012, and would further rise by 1.5 \u2022 C by the end of the 21st century Future ET 0 projections can be performed using physically-based models and statistical methods (e.g., Penman-Monteith equations) or by AI-based models (e.g., ANN, SVR and ELM models) where the output climatic variables from global climate models (GCMs) and local-scale, nested systems such as regional climate models (RCMs) are adopted. Li et al. In accordance with the reviews, it is evident that future ET 0 changes has been projected for many regions of the world based on simulated outputs of GCM. However, current studies have mainly been based on conventional (i.e., statistical) methods and models (e.g., the Hargreaves equation). AI-based models that have the ability to integrate historical knowledge (i.e., changes in ET 0 ) with GCM-simulated data in order to perform modeling have seldom been used to estimate future ET 0 values. Several studies have applied downscaling techniques based on artificial neural networks (ANN), multiple linear regressions and other statistical models, owing to their ability to capture non-linear relationships between ground and GCM-based predictors in respect of the predictands, such as rainfall, winds, cloud cover, streamflow and temperature (e.g., In this paper, we have adopted two well-established AI techniques, the support vector regression (SVR) and extreme-learning machine (ELM) algorithms, considering their popularity as robust tools applied in the area of predictive modeling. Notably, the SVR model is a statistical model based in theory that utilizes the regularization framework, presenting advancement over conventional artificial neural network models; whereas the ELM model is a fast and efficient neuro-computational approach offering an improvement in its design and universal approximation capability compared to conventional ANN models. The ELM model was shown to perform more accurately than the SVR and ANN models for drought studies Considering the aforementioned review of literature, this study employs two well-established AI-based models comprising ELM and SVR algorithms to project future changes in ET 0 in an inland mountainous watershed region of China in which data from GCM outputs of CMIP5 are utilized. In order to reduce the uncertainties of single-simulation GCM models, in this paper we have selected eight-model ensemble projections (from CMIP5) to analyze the overall future variability of ET 0 in northwest of China. The FAO-56 PM based ET 0 has been chosen as the verification standard for the downscaled data, which is meaningful for agricultural applications. The rest of the paper is structured as follows. Section 2 details the materials and methods, including a description of the study area, datasets, methodologies, model development and performance-evaluation measures; Section 3 gives the details of the results; Section 4 includes the discussion; and Section 4 lists the conclusions of this research.Materials and MethodsStudy AreaThe Heihe is a famous inland river in China, located in the central part of the Hexi Corridor. In this study, we selected as the study area the headwater region (Yingluoxia Watershed) of the Heihe River (Figure Water 2017, 9, 880 4 of 23Materials and MethodsStudy AreaThe Heihe is a famous inland river in China, located in the central part of the Hexi Corridor. In this study, we selected as the study area the headwater region (Yingluoxia Watershed) of the Heihe River (Figure DatasetsIn this study, we have adopted two different datasets, including the observed historical weather data and the simulated global climate model (GCM) outputs, in order to calculate the ET0 for the YLX Watershed region for the historical period 1961-2005 and the future period 2010-2099, respectively. The historical weather data including the daily maximum, minimum and mean temperature, relative humidity (%), precipitation (mm), wind speed (m/s), atmospheric pressure DatasetsIn this study, we have adopted two different datasets, including the observed historical weather data and the simulated global climate model (GCM) outputs, in order to calculate the ET 0 for the YLX Watershed region for the historical period 1961-2005 and the future period 2010-2099, respectively. The historical weather data including the daily maximum, minimum and mean temperature, relative humidity (%), precipitation (mm), wind speed (m/s), atmospheric pressure (hPa) and sunlight duration (h) at 4 weather stations in and around the YLX Watershed were downloaded from the China Meteorological Administration (http://data.cma.cn/) for the period 1961-2005. The simulated historical daily data in the same period for a total of 8 GCMs were acquired from the Coupled Model Intercomparison Project Phase 5 (CMIP5) project. The projected future daily data (e.g., daily maximum, minimum and mean temperature, daily relative humidity, wind speed, atmospheric pressure, etc.) were acquired for the period 2010-2099 for two distinct scenarios based on the Representative Concentration Pathways (RCP4.5 and 8.5) extracted from the simulations of the 8 GCMs. Table In order to downscale ET 0 for the mountainous inland watershed region in north-west China, in this study we have employed two different AI-based techniques where the ELM and SVR algorithms were used to model ET 0 for the historical period  and the future period (2010-2099). In this regard, during the historical simulation period, the observed and simulated historical datasets were partitioned into two distinct phases; with the first 30 years' of data (i.e., ) utilized as a training set and the remaining 15 years' data (i.e., 1991-2005) utilized as a testing set. The projected future datasets were also divided into three segments of 30-year forecast horizons, which were denoted as: 2010-2039, 2040-2069, 2070-2099 to represent the climate change of the near-term, mid-term and long-term periods, respectively. where ET 0 is the reference evapotranspiration (mm day \u22121 ); \u2206 is the slope of the saturation vapor pressure-temperature curve (kPa \u2022 C \u22121 ); R n is the net radiation (MJ m \u22122 day \u22121 ); G is the soil heat flux (MJ m 2 day \u22121 ); \u03b3 is the psychrometric constant (kPa \u2022 C \u22121 ); T mean is the average daily air temperature at 2 m ( \u2022 C); U 2 is the mean daily wind speed at 2 m (m s \u22121 ); e s is the saturation vapor pressure (kPa); and e a is the actual vapor pressure (kPa). The computations from all data required the calculation of the ET 0 following the method and procedures outlined in Chapter 3 of the FAO-56 manual Extreme-Learning MachineThe extreme-learning machine (ELM) as an AI-based method was first introduced by Huang et al. For i = 1, 2, . . . , N, the SLFN with L hidden neurons is expressed as:where,T is the output weight matrix computed between the hidden and the output neurons;is the hidden neuron outputs representing the randomized hidden features of predictor X i ; and h i (x) is the ith hidden neuron, given as:The non-linear piecewise-continuous hidden layer activation function h i (x) is defined using hidden neuron parameters (a, b) and must satisfy the approximation theorem, \u03d1(a i , b i , X). The model's approximation error is minimized when solving for weights connecting the hidden and output layer (\u03b2) using a least square method: minhere, is the Frobenius norm, and H is the hidden layer output matrix, given as:. . .T is the target matrix, drawn from the training dataset, and given as:An optimal solution is then determined by solving a system of linear equations:and H + is the Moore-Penrose generalized inverse function (+).Support Vector RegressionSupport vector regression (SVR) is an effective forecasting tool developed for solving regression problems by Vapnik where \u03c9 is weight vector; b is a constant; and \u03c6(x) denotes a mapping function in the feature space.The coefficients \u03c9 and b can be estimated by minimizing:where both C and \u03b5 are prescribed parameters. The term L \u03b5 ( f (x i ), y i ) is called the \u03b5-intensive loss function. This function indicates that errors below \u03b5 are not penalized. The termis the empirical error. The term 1  2 w 2 measures the smoothness of the function. C evaluates the trade-off between the empirical risk and the smoothness of the model. A Largrange multiplier and optimality constraints are used, so a nonlinear regression function is obtained using the following expression:where \u03b1 i and \u03b1 i * are the introduced Lagrange multipliers, and k(x i , x) is kernel function.Model DevelopmentIn this study, we have employ ELM and SVR models to establish the relationship between the estimated ET 0 by the FAO-56 PM method with the historical observed meteorological variables and the GCM output variables. Figure where \u03c9 is weight vector; b is a constant; and \uf028 \uf029 x \uf066 denotes a mapping function in the feature space.The coefficients \u03c9 and b can be estimated by minimizing:where both C and \u03b5 are prescribed parameters. The termis called the \u03b5-intensive loss function. This function indicates that errors below \u03b5 are not penalized. The termis the empirical error. The term 2 2 1 w measures the smoothness of the function. C evaluates the trade-off between the empirical risk and the smoothness of the model. A Largrange multiplier and optimality constraints are used, so a nonlinear regression function is obtained using the following expression:where \u03b1i and \u03b1i * are the introduced Lagrange multipliers, and k(xi, x) is kernel function.Model DevelopmentIn this study, we have employ ELM and SVR models to establish the relationship between the estimated ET0 by the FAO-56 PM method with the historical observed meteorological variables and the GCM output variables. Figure Water 2017, 9, 880 8 of 23Model Goodness-of-Fit CriteriaTo evaluate the effectiveness of the downscaling approaches, four statistical score metrics, including the coefficient of correlation (R), mean absolute relative error (MAE), root mean square error (RMSE) and Nash-Sutcliffe efficiency (NSE) The MAE value is expressed as:The RMSE value can be calculated as:The NSE value can be calculated as:In the above equations, N is the number of input test samples; ET 0\u2212PM and ET 0\u2212AI are the FAO-56 PM ET 0 and modeled ith ET 0 value; and ET 0\u2212PM and ET 0\u2212AI are the average of the FAO-56 PM value and modeled value of the ET 0 . The best performances for the SVR and ELM models are expected to yield R = 1, MAE = 0, RMSE = 0 and NSE = 1, respectively.ResultsModel Verification and ComparisonThe FAO-56 PM based ET 0 was calculated by deriving the historically observed meteorological variables (i.e., maximum and minimum air temperature, relative humidity, air pressure, sunlight duration and wind speed at 2 m height) for the period 1961 to 2005. The eight selected GCM outputs were extracted to compute ET 0 using ELM and SVR-based downscaling approaches. Figure Boxplots describing the four performance metrics for the ET 0 modelling by the ELM and SVR methods based on eight GCM outputs with different periods are shown in Figure Evaluation of Future ET0 ProjectionsAnnual Future ET0Figure Decadal and Seasonal Future ET0 ProjectionsFurthermore, we divided the projection horizons into three sub-periods: first, a sub-period (from 2010 to 2039) named as the near term; a second sub-period (from 2040 to 2069) as the mid term; and a third (2070 to 2099) as the long term. The statistics of ET0 for the three periods have been illustrated in Figure Decadal and Seasonal Future ET 0 ProjectionsFurthermore, we divided the projection horizons into three sub-periods: first, a sub-period (from 2010 to 2039) named as the near term; a second sub-period (from 2040 to 2069) as the mid term; and a third (2070 to 2099) as the long term. The statistics of ET 0 for the three periods have been illustrated in Figure Projection of Future ET0 VariationIn order to simulate the tendency of future ET0 variation, the Sen Slope method was applied Figure Projection of Future ET 0 VariationIn order to simulate the tendency of future ET 0 variation, the Sen Slope method was applied Figure Discussion and Summary", "conclusions": "In reality, the true reference evapotranspiration (ET 0 ) can be obtained experimentally; however, in many situations there are difficulties carrying out measurements. As a result, the FAO-56 Penman-Monteith formulation for ET 0 is often accepted. This paper aimed to downscale monthly ET 0 using two less-explored learning algorithms based on ELM and SVR approaches by developing and validating the regression relationships between station-based ET 0 and large-scale atmospheric variables from a suite of eight relatively high spatial-resolution GCM outputs. Without considering the physical relationship between ET 0 and the climatic variables, the approaches presented in this paper were able to successfully downscale local ET 0 time series by building on the appropriate statistical relationships between the observed ET 0 (FAO-56 PM ET 0 ) with the surface-atmospheric features, whereby an ensemble of models was studied in terms of the large-scale and transient changes of the host GCM models. In this regard, the downscaling approaches applied in the present study, where a study of future trends in ET 0 is based on many GCM selections, simulation methods and warming scenarios and trajectories, are considered as invaluable tools for advancing the relevance of hydrological models for more meaningful local-scale applications.This study has projected the future variability of ET 0 by applying the ELM and SVR approaches in a mountainous inland watershed of north-west China. A total of eight relatively high spatial-resolution GCM-based model outputs from CMIP5 simulations were employed to downscale for the historical period  and the future period (2010-2099) ET 0 under the RCP4.5 and RCP8.5 warming scenarios.The AI-based ELM and SVR approaches revealed satisfactory performances in estimating the ET 0 data. Moreover, the performance of the SVR model for ET 0 projection is modestly better than the ELM model. The future variation in ET 0 appeared to occur mainly in the spring and autumn seasons, while in the summer and winter seasons, the ET 0 changes were very small. Annually, the rate of increase in ET 0 was found to be approximately 7.5 mm/decade for the near and middle terms, and nearly close to 0 for the long-term period under the RCP4.5 warming scenario. By contrast, for RCP8.5, the rate of increase in ET 0 for the near term was found to be approximately 8.2 mm/decade, and 15 mm/decade for the mid-term and long-term periods. Compared to the historical period in this study , the relative changes were found to be approximately 2%, 5% and 6% in the near-, mid-and long-term periods, respectively, under the RCP4.5 warming scenario, whereas they were approximately 2%, 7% and 13% for the three periods, respectively, under the RCP8.5 warming scenario.Compared to the uncertainties from the different estimation formulations and different inputs from atmospheric variables, the uncertainties derived from the different GCM outputs are a prime source of a model's fidelity . Thus, it is necessary to employ multiple GCM outputs to be used as predictor variables when a local-scale climatic property needs to be projected.[81]The AI-based methods developed in this case study appear to promise soft-computing approaches for the future variability of reference evapotranspiration. However, there were some unavoidable uncertainties; for example, we tested 8 GCM-based model outputs which might not completely represent future climate change. More GCM-based model outputs should be adopted. Moreover, future work should test more AI-based methods for investigating ET 0 projection.", "SDG": [2]}, "increased_specificity_famine_prediction_using_sate": {"name": "Increased-specificity famine prediction using satellite observation data", "abstract": " This paper examines the use of remote sensing satellite data to predict food shortages among different categories of households in famine-prone areas. Normalized Difference Vegetation Index (NDVI) and rainfall estimate data, which can be derived from multi-spectral satellite radiometer images, has long been used to predict crop yields and hence famine. This gives an overall prediction of food insecurity in an area, though in a heterogeneous population it does not directly predict which sectors of society or households are most at risk. In this work we use information on 3094 households across Uganda collected between 2004-2005. We describe a method for clustering households in such a way that the cluster decision boundaries are both relevant for improved-specificity famine prediction and are easily communicated. We then give classification results for predicting food security status at a household level given different combinations of satellite data, demographic data, and household category indices found by our clustering method. The food security classification performance of this model demonstrates the potential of this approach for making predictions of famine for specific areas and demographic groups.", "keywords": "J,2 [Computer Applications]: Physical Sciences and Engineering-Earth and atmospheric sciences; I,5 [Computing Methodologies]: Pattern Recognition", "introduction": "Having an early warning of an impending famine increases the chance that something can be done about it. Both demographic and satellite data have been used in different ways to drive systems which predict food insecurity for this purpose. In this study, we combine satellite image data with data on specific households (for example, on the number of people in the household, the land size avaiable for farming, ownership of livestock and distance to the nearest road) in order to make more specific predictions than could be made with either source of data alone.Examples of the type of satellite data we use are shown in Figure . Panel (a) depicts a Normalized Difference Vegetation Index (NDVI) map of Uganda derived from the National Oceanic and Atmospheric Administration's (NOAA) Advanced Very High Resolution Radiometer (AVHRR). The AVHRR is an example of a meteorological satellite that collects data about the earth's land cover types and conditions, cloud cover patterns, sea surface temperature etc. The AVHRR collects this data across five spectral bands of the electromagnetic spectrum, from which derivatives such as NDVI and rainfall estimates (Figure 1) can be extracted. It also has a temporal resolution of 12 hours, thus making it possible to derive information about any given location on the surface of the globe at least twice a day. In spite of the coarse resolution of 8km, it has the advantage of wide area coverage.1(b)Our goal is to be able to make predictions of famine which go beyond a blanket warning for a given region. In heterogenous populations, while some sectors of the population may be at risk of famine in a certain area, other sectors may not. We therefore try to split up the households in the country into defined categories, with a particular focus on category definitions which are easy to communicate so that the results of predictions can be easily translated into policy. Most con-ventional clustering methods such as k-means give cluster regions for which the decision boundaries may be somewhat complex, e.g. a set of inequalities based on some distance measure. Hand-chosen household categories may be easier to communicate, but may not be optimal with respect to the specificity of predictions based on them (e.g. if two handcrafted categories are highly correlated). In section 4, we introduce a methodology for finding clusters of households which are informative for famine prediction, and which are also easy to communicate. Our clusters are defined by a binary tree, which we optimise using a simulated annealing strategy.We then look at the use of supervised learning to give warnings of famine. Learning the relationship between vegetation stress and food insecurity is not straightforward, as high density of vegetation in a region does not necessarily mean there is more to eat. The relationship between the different demographic indicators such as production,household income and available labour is also not straightforward, and it is important to establish the relationship between the different variables used to detect famine .[7]Our results, in section 5, show the improvement in accuracy and specificity which can be attained by combining satellite and demographic data, and the utility of our clustering method.", "body": "Having an early warning of an impending famine increases the chance that something can be done about it. Both demographic and satellite data have been used in different ways to drive systems which predict food insecurity for this purpose. In this study, we combine satellite image data with data on specific households (for example, on the number of people in the household, the land size avaiable for farming, ownership of livestock and distance to the nearest road) in order to make more specific predictions than could be made with either source of data alone.Examples of the type of satellite data we use are shown in Figure Our goal is to be able to make predictions of famine which go beyond a blanket warning for a given region. In heterogenous populations, while some sectors of the population may be at risk of famine in a certain area, other sectors may not. We therefore try to split up the households in the country into defined categories, with a particular focus on category definitions which are easy to communicate so that the results of predictions can be easily translated into policy. Most con-ventional clustering methods such as k-means give cluster regions for which the decision boundaries may be somewhat complex, e.g. a set of inequalities based on some distance measure. Hand-chosen household categories may be easier to communicate, but may not be optimal with respect to the specificity of predictions based on them (e.g. if two handcrafted categories are highly correlated). In section 4, we introduce a methodology for finding clusters of households which are informative for famine prediction, and which are also easy to communicate. Our clusters are defined by a binary tree, which we optimise using a simulated annealing strategy.We then look at the use of supervised learning to give warnings of famine. Learning the relationship between vegetation stress and food insecurity is not straightforward, as high density of vegetation in a region does not necessarily mean there is more to eat. The relationship between the different demographic indicators such as production,household income and available labour is also not straightforward, and it is important to establish the relationship between the different variables used to detect famine Our results, in section 5, show the improvement in accuracy and specificity which can be attained by combining satellite and demographic data, and the utility of our clustering method.RELATED WORKSub-Saharan Africa is a region heavily reliant on agricuture, and monitoring trends in agricultural production has long been imperative. Application of remote sensing in famine early warning systems has been used since the mid-1980s to monitor the crop and rangelands of semi-arid sub-Saharan Africa International organizations including the United States Agency for International Development (USAID) have worked on famine early warning systems Attempts have been made to integrate other sources of data like prices of major food crops in an area to satellite remote sensing data to improve on food insecurity prediction. Brown et al [1] used satellite remote sensing data in a spatially explicit price model to assess food insecurity of communities and regions in less-developed parts of the world. This model created a leading indicator of potential price movements for early warning of food insecurity indicating the importance of integration of other source of date to satellite remote sensing data. Other work has taken a different approach rather than making warnings of crop failure directly; for example Khan et al SATELLITE AND HOUSEHOLD DATAThe NDVI and rainfall estimate data for the three study areas of interest were obtained from the Famine Early Warning System Network 1 through the Africa Data Dissemination Service to coincide with the available demographic data for two agricultural seasons July-December 2004 and January-June 2005. The rainfall estimate is a product of an algorithm developed by NOAA at the Climate Prediction Center for Rainfall Estimate known as the CPC-RFE which has been widely tested and applied in the African region where NIR is the intensity measured in the near infrared spectrum, and R is the intensity in the visible red spectrum. The formulation of NDVI makes it resilient to variations attributed to calibration, noise, and changing irradiance conditions that accompany changing sun angles, topography, clouds/shadow and atmospheric conditions. Examples of NDVI fluctuations over time can be seen in Fig. The raw data in the study had problems with consistency, for example with crop production for different households which was reported in varying units such as tins, kilogram and baskets. This was corrected as far as possible, and anomalous rows in the data were excluded.HOUSEHOLD CLUSTERING TREESOur goal is to make predictions of famine not just for all residents of a certain district, but for specific categories of households. In a population containing different types of households, at a specific point in time the fact that some households have a high risk does not mean this is true for all housholds.In order to do this we have to group households together in some way. In this section we describe a novel clustering algorithm intended to produce interpretable cluster boundaries which are informative for famine prediction.Many well-known clustering algorithms are available, such as k-means, though we are constrained in this case by the form of the decision boundary after learning clusters. A prototype-based clustering algorithm gives clusters which may be difficult to communicate, being based on a distance measure and a set of inequalities. Clusters can alternatively be represented by a binary tree, as used in classification and regression trees (CART); this produces cuboid regions which can be expressed simply as a set of ranges on each descriptor variable. This is illustrated in Figure We now describe the structure of our clustering trees. Such a tree T is defined by a set of vertices V and vertex parameters \u0398. The vertices are divided into leaf nodes and decision nodes, V = VL \u222a VD. Each vertex has a set of parameters, \u0398v = {cv, iv, \u03b8v, e \u2212 v , e + v }, which are non-null under the following conditions:Some vertex v1 is assigned to be the root node, which must have no incoming edges (that is, e \u2212 v and e + v are never equal to to v1). For the clustering tree to be consistent, the graph structure defined by the edges implicit in \u0398 must contain a unique path from v1 to every other vertex.To assign a vector x = x1, . . . , xV to a cluster index, we begin at the root vertex. If c1 \u2208 {1, . . . , k}, then we assign that cluster index. Otherwise, we test if xi 1 > \u03b81. If this condition is satisfied we move to vertex e + 1 , otherwise we move to vertex e \u2212 1 , and repeat the procedure.Clustering evaluationTo evaluate a clustering tree T with respect to N rows of input data, we need an evaluation metric O(T , x1:N ). It is common to evaluate clusters using some distance measure, for example preferring clusterings which minimise the average intra-cluster distance and maximise inter-cluster distance for a training dataset.In this application, we are interested in finding clusters which allow us to make specific predictions of famine risk. Instead of applying a distance measure, we therefore look at the levels of correlation between clusters in terms of household food production in different areas.Where each household in our training data has a district and a seasonal production, we can calculate the matrix P, where P c,d is the average production in the dth district for households in the cth cluster (as assigned by clustering tree T ). Our clustering metric is then calculated as follows:where \u2022 denotes an expectation. This metric gives us the average correlation between clusters in terms of production across different districts. Minimising O(T ) gives us clusters which we expect to be specific in terms of famine risk.Stochastic searchWe use a simulated annealing method to learn cluster descriptors with low values of O(T ). In this approach, we first initialise a clustering tree. We constrain the clusters so that each cluster must occupy a single cuboid region in the data space, which is easy to implement by having the number of leaf vertices equal to the number of clusters. For clusters, we must have |VL| = k leaf vertices and therefore (in a directed tree) |VD| = k \u2212 1 decision vertices. We initialise cv, iv, \u03b8v, e \u2212 v , e + v in the decision vertices randomly, though fulfilling the tree structure constraint described above, and also with non-conflicting threshold values (that is, if vertex v b is a descendent of va, and ia = i b , then \u03b8 b < \u03b8a if v b is in the left subtree of va, and \u03b8 b > \u03b8a otherwise).Candidate trees are generated by iteratively making modifications to the tree. The possible \"moves\" are:\u2022 Swap nodes, taking any two vertices va, v b not including the root vertex and swapping the parameters \u0398v a and \u0398v b . In this case, we have to check that the resulting graph is still a valid tree. This can be done easily, for example checking for cycles by testing that the eigenvalues of A + I are all positive where A is the adjacency matrix of the tree and I is the identity matrix;\u2022 Change threshold, where we alter the value of one of the thresholds \u03b8v, resampling according to some prior distribution and in such a way that the tree is still consistent;\u2022 Change variable being considered at a certain decision vertex, i.e. altering the value of iv for some v. Again we have to check whether the tree is consistent after making such a change.Given a neighbouring tree T * generated by one of these moves (made at random), we can evaluate the improvement  where U [\u2022] denotes a uniform distribution. The \"temperature\" parameter T starts at a high level and is gradually reduced. A common cooling scheme is to use T = c \u03b9 at iteration \u03b9, which we use here with the constant c = 1. More complicated cooling schedules are also possible. The effect of this is to prevent a solution being trapped in a sub-optimal local minima. Figure RESULTSWe used the preceeding data to make classifications of food security on a household level for the 3094 households in our dataset. For each household, we look at records of the calorific intake per person over two seasons, Q3/Q4 2004, and Q1/Q2 2005. We define food insecurity as a calorific intake of less than 1800 kcals/day, a level at which a family is vulnerable to famine.To predict food insecurity, we use four sets of covariates: (a) demographic data only, (b) satellite data (NDVI and RFE) only, (c) satellite and demographic data, and (d) satellite data and household cluster IDs. We carried out training and classification with the AdaBoost algorithm [2] using decision stumps as a base classifier. Evaluation was done with 10-fold cross validation. The Weka framework Table Figure CONCLUSION", "conclusions": "We show that adding demographic information about households to satellite observation data gives better accuracy in making predictions at a household level. We have described a clustering method for this data which gives household categories that are easy to communicate, and which can be used as the basis of an improved satellite famine warning system to give famine risk alarms with increased specificity.Promising ways to expand this work would include extracting features from the satellite data, i.e. learning measures other than NDVI and RFE from raw multi-spectral satellite images. The model could also be made explicitly spatial, if we assume that households which are geographically close together are likely to be correlated in terms of famine risk.", "SDG": [2]}, "mapping_numerically_classified_soil_taxa_in_kilombero_valley_tanzania_using_machine_learning": {"name": "Mapping numerically classified soil taxa in Kilombero Valley, Tanzania using machine learning", "abstract": " Inadequacy of spatial soil information is one of the limiting factors to making evidence-based decisions to improve food security and land management in the developing countries. Various digital soil mapping (DSM) techniques have been applied in many parts of the world to improve availability and usability of soil data, but less has been done in Africa, particularly in Tanzania and at the scale necessary to make farm management decisions. The Kilombero Valley has been identified for intensified rice production. However the valley lacks detailed and up-todate soil information for decision-making. The overall objective of this study was to develop a predictive soil map of a portion of Kilombero Valley using DSM techniques. Two widely used decision tree algorithms and three sources of Digital Elevation Models (DEMs) were evaluated for their predictive ability. Firstly, a numerical classification was performed on the collected soil profile data to arrive at soil taxa. Secondly, the derived taxa were spatially predicted and mapped following SCORPAN framework using Random Forest (RF) and J48 machine learning algorithms. Datasets to train the model were derived from legacy soil map, RapidEye satellite image and three DEMs: 1 arc SRTM, 30 m ASTER, and 12 m WorldDEM. Separate predictive models were built using each DEM source. Mapping showed that RF was less sensitive to the training set sampling intensity. Results also showed that predictions of soil taxa using 1 arc SRTM and 12 m WordDEM were identical. We suggest the use of RF algorithm and the freely available SRTM DEM combination for mapping the soils for the whole Kilombero Valley. This combination can be tested and applied in other areas which have relatively flat terrain like the Kilombero Valley.", "keywords": "Kilombero,Valley,Numerical,classification,Machine,learning,Soil,mapping,Decision,tree,analysis,DEM", "introduction": "The Kilombero Valley in Tanzania presents great potential for the expansion and intensification of rice production. This valley, covering an area of about 11,600 km 2 , has been identified by the Government of Tanzania for financial and technological investments to expand and intensify rice production (Kato, 2007). Rice is the second most important cereal crop in Tanzania after maize (TIC, 2013), and its demand has been increasing following shift in preference by local population from traditional staples to rice, and increased market demands from neighboring countries. To develop and promote sustainable rice production intensification; farmers and policy makers need to identify the most suitable areas and respective management options. However, updated and detailed soil information to this support decision-making process is currently lacking. Accurate soil information is crucial for informing management recommendations aimed to increase agricultural productivity and overall food security, especially in developing countries where the GDP is heavily dependent on the agricultural sector (Bucheyeki et al., 2011)(Cook et al., 2008;. Relatively longer time is required to gather such information through conventional soil inventory and generally, larger amount of resources are required for such exercises Msanya et al., 2002). Recent developments in remote and proximal sensing, computational methods and information technology, have provided means by which soil information can be collected, shared, communicated and updated more efficiently (McBratney et al., 2003)(Malone, 2013;McBratney et al., 2003;Scull et al., 2003;V\u00e5gen et al., 2013;V\u00e5gen et al., 2016;Winowiecki et al., 2016a. Predictive soil landscape model frameworks such as the SCORPAN approach Winowiecki et al., , 2016b)) could be used to predict continuous soil classes and soil attributes that better represent soil spatial variability. The increased availability of high resolution digital elevation models (DEMs) that provide predictive variables in digital soil mapping together with the advances in machine learning techniques add to the ease of generating spatial soil information and depicting uncertainty (McBratney et al., 2003)(Hansen et al., 2009;Haring et al., 2012;Subburayalu and Slater, 2013;.Subburayalu et al., 2014)The overall goal of this study was to develop a predictive soil map for a portion of Kilombero Valley, Tanzania to serve as a basis for quantitative land evaluation for intensified rice production. Machine learning informed by legacy soil map, new field data collection, and multiple sources for environmental correlates were combined and used for mapping of numerically derived soil classes. In this paper we report comparisons of two machine learning algorithms and three sources of terrain data.", "body": "The Kilombero Valley in Tanzania presents great potential for the expansion and intensification of rice production. This valley, covering an area of about 11,600 km 2 The overall goal of this study was to develop a predictive soil map for a portion of Kilombero Valley, Tanzania to serve as a basis for quantitative land evaluation for intensified rice production. Machine learning informed by legacy soil map, new field data collection, and multiple sources for environmental correlates were combined and used for mapping of numerically derived soil classes. In this paper we report comparisons of two machine learning algorithms and three sources of terrain data.MethodsLegacy soil mapThe base map used to guide soil sampling was a reconnaissance legacy soil map developed in the late 1950s at a scale of 1:125,000 Field data collectionFAO Guidelines for Soil Description In most cases it was not possible to properly describe the soil profiles below 80 cm depth because of the high water table. The soil profile morphological characteristics recorded for this study included genetic horizon designation, depths, and soil colour. Soil colour determination was done in the field by recording the hue, value, and chroma in the Soil Munsell Colour Charts book Laboratory analysisSoil samples were submitted to the soil analysis lab at Sokoine University of Agriculture for wet chemistry. Soil attributes that were analyzed in the laboratory included: soil pH Numerical classificationNumerical soil classification techniques have been suggested and tested by many authors to generate horizon classes, soil classes, and to define taxonomic distance between the classes for existing soil classification systems in different studies The numerical methods have a great potential for use in Africa where soil information is still in great demand Numerical classification of morphological and lab data for soil horizons and profiles was performed using a web based application -OSACA In preparation for data input Munsell colour notations recorded in the field (hue, value and chroma) were converted to red, green, and blue (RGB) numerical values (0-255) using mColorConverter where V s is the standardized value of the attribute, V o is its observed value, \u03bc is sample mean and \u03c3 is the sample standard deviation.In the OSACA options, the Euclidean distance metric was chosen for horizon clustering, while Pedological distance metric was chosen for soil profile clustering.Assembling environmental correlatesThe following environmental correlates were used for SCORPAN \u2022 S-soil classes derived from a 1959 legacy soil map of the area by \u2022 N-spatial location was recorded for each attribute used in the prediction of soil classes.Climate (C), lithology (P) and age (A) factors in the SCORPAN formulation were not used in this study.ERDAS IMAGINE software was used to derive the following vegetation based parameters from the satellite image: land use/cover classes, normalized difference vegetation index (NDVI) The three available DEMs for the study area were:i. 30 m spatial resolution ASTER (Advanced Spaceborne Thermal Emission and Reflection Radiometer) ii. 1 arc sec (approximately 30 m) spatial resolution SRTM (Space Shuttle Radar Topography Mission), and iii. 12 m spatial resolution WorldDEMThe ASTER and SRTM DEMs are freely available on the internet. On the other hand, WorldDEM is a new data, officially released in 2014, and commercially available (Airbus Defence and Space, 2014). It is currently the finest spatial resolution terrain data available for the Kilombero Valley.The following terrain derivatives were calculated using Whitebox Geospatial Analysis Tool 3.2 software Mapping of soil classesMachine learning was used to predict the soil taxa. Machine learning is a type of artificial intelligence that provides computers with the ability to automatically learn programs from multi-source data sets and make predictions In this study, machine learning was performed in WEKA (Waikato Environment for Knowledge Analysis) Table 2Soil clusters predicted using J48 learner using ASTER, SRTM and WorldDEM datasets.Note: J48_ASTER = soil clusters predicted by using J48 learner on ASTER dataset. J48_SRTM = soil clusters predicted by using J48 learner on SRTM dataset. J48_WLD = soil clusters predicted by using J48 learner on WorldDEM dataset. Results and discussionsNumerical classificationThe numerical classification generated 13 soil classes (clusters). Classes grouped between one and five soil profiles. The legacy soil map identified 10 soil classes in the study area. The increased number of new soil classes identified in this study could be due to the techniques used to derive the legacy map (e.g., air photo interpretation) and the limited field data collection (due to floods and restrictive vegetation at the time) The sampled pedons with the shortest taxonomic distance to the modal pedon of each numerically synthesized soil class was classified to the Soil Taxonomy Subgroup level Algorithm comparisonThe J48 algorithm predicted 8 out of the possible 13 classes for all three DEMs, 6 of which were common across all three (Table The RF learner predicted all 13 possible clusters. Unlike J48 which is a single tree classifier, RF is an ensemble (forest) of bagged classification trees. The classification trees in RF are independent and the classification of samples does not depend upon previous trees in the ensemble In another study by Mapping soil classesThe maps demonstrate that legacy soil class was the dominant predictor variable. There is a strong correspondence between the legacy soil map and the predicted soil map. Since most of the study area exhibits low slope gradient (1-5\u00b0), it was expected that higher resolution terrain data would be more effective in depicting relevant landscape differences in this environment, and thus terrain data would be stronger predictors. However, the RF learner produced identical prediction when used with SRTM (30 m spatial resolution) and WorldDEM (12 m spatial resolution) terrain predictors (Fig. Dissimilar outputs were obtained when SRTM and ASTER datasets were used on both RF and J48 algorithms, despite both DEMs having approximately 30 m spatial resolution. This agrees with some studies suggesting differences in quality between these two widely used and freely available DEMs in terms of vertical accuracy, and presence of artifacts and noise Conclusion", "conclusions": "This work used DSM methods to map numerically classified soil clusters of a portion of Kilombero Valley, Tanzania. In this study, terrain based predictors derived from 1 arc SRTM DEM results were similar to that of 12 m WorldDEM despite differences in resolution. It was also demonstrated that RF algorithm was less sensitive to the training set sampling intensity compared to J48.We suggest the use of RF algorithm and SRTM DEM combination for soil class mapping for the remainder of the Kilombero Valley since RF was less sensitive to sampling intensity than J48 and a significantly lower cost of the SRTM DEM. This will help to generate spatial soil information which will enable decision-makers and farmers to make informed decisions for intensification of rice production in the Kilombero Valley. The RF and SRTM combination can be tested and applied in other areas which have relatively flat terrain like the Kilombero Valley.", "SDG": [2]}, "the_\u201cleft_behind\u201d_smallholders_in_contemporary_russian_agriculture": {"name": "The \"left behind\": Smallholders in contemporary Russian agriculture", "abstract": " Smallholder production in Russia has been in decline for more than a decade. The likelihood is for continued marginalization. Smallholders confront four obstacles. First, path dependencies, which includes the fact that smallholders' production remains traditional and subsistence oriented. Second, institutions in the form of state policy restrict land and animals. Third, Russia's role in the third food regime means that smallholders are unable to help Russia's emergence as a global food superpower. Fourth, smallholders are being left behind in the ongoing technological revolution led by agroholdings. As technological advancement expands in scope, the gap between large farms and smallholders will widen.", "keywords": "household plots,post-communist agriculture,Russia,smallholders,technological transformation", "introduction": "", "body": "A second discontinuity is the high degree of stratification between adapters and non-adapters among contemporary operators of lichnoe podsobnoe khoziaistvo. Whereas most operators of lichnoe podsobnoe khoziaistvo have small plots of land that usually surround the dwelling, there is also a possibility to lease additional land away from the dwelling, although only a small percentage actually do so-those that do have significantly higher income | InstitutionsNorth defines institutions as \"formal rules, informal constraints, and their enforcement characteristics\" A second institutional impact concerns a limitation on animals. The household animal husbandry sector has been in decline for more than a decade. Currently, due to health and sanitation concerns over swine and bird flu, a growing number of regions have introduced numerical restrictions. Households are blamed for spreading of the swine flu virus by transporting pork meat from quarantined zones to neighbouring villages and selling the meat 3 Regional differences were significant-two southern federal districts accounted for 56% of all land used for household gardening.The northwest district, where land quality is poorer, had only 5% of agricultural land used for lichnoe podsobnoe khoziaistvo.In spring 2017, the State Duma, the lower house of the Parliament, began to consider a bill that would establish national norms, requiring households to have a certain number of square metres per pig A third impact directly infringes upon smallholders' market share for vegetables. Smallholders produce a high percentage of the nation's vegetables, especially tomatoes. To reduce food imports, the federal government has supported the construction of technologically advanced greenhouses-industrial greenhouses that are based on automation and robotics. An ambitious effort to expand large-scale greenhouse construction and production is underway, especially in southern regions where tomato growing by households is popular | Global food regimeThe third food regime is characterized by a concentration of capital, the production of food surplus, the rise of commodity chains, and control of international food markets by corporate hegemons since 1980 During the communist period, Russia's agricultural sector was shielded from globalization, and food producers were protected from international competition. True, the Soviet Union entered the global food market to purchase grain and other agricultural commodities Agroholdings are mega-sized, corporate farms. These farms are remarkable not only for their size-they are much bigger than traditional Soviet-era state and collective farms-but also for \"new types of management, new technologies, the commercial orientation of the business and their aggressive market behaviour\" The concentration of land and other resources leads to concentration of production. The top 15 companies export 75% of Russian grain Russia's large, powerful, corporate entities obtain financial investment from abroad and domestically. During 2012-2016, more than $3 billion of foreign investment was made in Russian agriculture The second occurrence is marked by explicit statements that Russia is striving to be a global food superpower.Statements by policymakers to this effect began to appear as early as 2009 following what was then a record harvest in 2008. Former President Dmitry Medvedev expressed Russia's ambition to become a global food supplier In October 2017, Minister Tkachev asserted \"we may become a leading agrarian power,\" referring to Russia's grain exports, its growing meat exports, and its quest to become a leading exporter of organic food Because of their limited land holdings, smallholders produce less than 1% of Russia's grain, thus making virtually no contribution to grain supply.To support their superpower aspirations, in November 2016, a federal programme called \"The Export of Products of the Agroindustrial Complex\" was approved by the upper house in the Parliament, and in December 2016, the government officially brought the programme into effect The third occurrence has been the long-term absence of integration of smallholders into external markets.Russian analysts express the necessity of integrating smallholders into the international agricultural system and specifically to export markets with members of the Eurasian Economic Union-Russia's version of the European Union that includes a single market and free trade for many products | THE TECHNOLOGY REVOLUTION IN AGRICULTURESmallholders are left behind by technological advances that are being introduced by agroholdings and other large farms. During the Soviet period, there was a mechanization gap between state and collective farms and lichnoe podsobnoe khoziaistvo: the former utilizing mechanized machinery in many phases of production and the latter based on manual labour. During the 1990s, large farms in Russia demechanized during the economic collapse, and as a consequence, the mechanization gap narrowed. Starting in the 2000s, however, with the appearance of agroholdings, a technology gap has emerged. It is on the back of agroholdings and other large farms that Russian agriculture has experienced, in the words of former Minister of Agriculture V. Semenov, a \"renaissance\" Agroholdings already hold enormous advantages in land, labour, and capital. Agroholdings receive the most state financial support. Agroholdings facilitate the export food and give Russia prestige, and for that reason, they are the preferred producer in the system. The acquisition and use of technological advances give agroholdings an even greater advantage in the capture of domestic and foreign markets.The technological revolution in Russia mirrors a similar revolution occurring in global agriculture. Globally, new technologies are emerging in sensors and automation, as well as other directions Currently, Russia's agricultural producers trail the West in the use of advanced technologies, but Russian policymakers and investors are committed to making the sector modern and globally competitive. Deputy Minister of Agriculture I. Kuzin noted that \"Russia's agricultural sector has turned into a high-tech branch of the economy.Robotization and automation show that Russian agriculture has moved to a qualitatively new level of development\" (Press-sluzhba, 2017). Kuzin's position is a bit optimistic as it is still early in Russia's technological revolution-an estimated 10% of Russia's 12,000+ large farms use advanced technology As foreign and domestic investment flows into new technology, the gap between large corporate farms and smallholders continues to widen. A pilot project using robotics in cheese production was constructed in Moscow Oblast in 2017 and began production in mid-2018, producing 100 tons a day In addition, Russia and China are working together in innovative technology to develop high yield seed and animal husbandry in the Russian Far East, cooperation that has already led to 85 patents One problem for Russia's smallholders is that the acquisition of high-tech agricultural equipment is very expensive, requiring either self-financing or access to significant credit. Because most smallholders do not have sufficient capital to buy new technology on their own, incentives are created for smallholders to pool their resources to buy farm equipment Further, the technology gap will grow because the government is committed to the transformation of the agricultural sector in order to improve Russia's standing in the global food regime. Towards this end, in August 2017, the federal government adopted a programme on the scientific-technological development of agriculture that will run 2017-2025, with R26 billion in federal funding To support the dispersion of high-tech equipment into agricultural production processes, recruitment efforts are underway to attract 90,000 IT specialists who will work for large farms. Young specialists in particular are targeted, who are believed will want to work with robots and other high-tech equipment in agriculture. These initiatives are intended for large farms and not the smallholder sector.Swinnen and Burkitbayeva. Nor do smallholders have the political clout to protect their interests, the importance of which is shown by Falkowski in his paper on Poland.Since 2004, Russia's agrarian policy has favoured the concentration of economic power in agroholdings, who have become dominant players in the agricultural system. Mamonova, for example, observes that in Russia, big capital has gained control over agricultural land, monopolizes the value chain for food products and inputs, and receives the majority of state subsidies. As a result, \"this control grabbing limits diversification within rural communities based on access to productive resources. The majority of households are unable to accumulate land and capital for commercial farming\" My central argument is that Russia's technological transformation in agriculture is determining the future role of smallholders. Although big capital modernizes, smallholder production remains based on manual labour. Smallholders lack financial means to acquire new technologies. Most smallholders are unable to afford new technologies or adapt their use to their scale of farming. The upshot is that smallholders find it difficult to compete against producers who increase output and efficiency through advanced technology. Smallholders will be relegated to self-reproduction and isolation from broader macro-transformation, a cycle that ultimately reduces their economic significance. In short, the Russian case supports the argument that as big capital grows more powerful, smallholders become increasingly disadvantaged.", "conclusions": "| CONCLUSIONI have argued that Russia's smallholders confront path dependencies and institutional arrangements in the form of regional policies that limit production potential. Further, Russia's aspirations in the global food regime marginalize smallholders. Smallholders consume their household production and are isolated from markets and supply chains, thereby rendering them unable to contribute to Russia's status in the global food regime. Set in the context of other papers in this symposium, Russian smallholders have not had the success found in Central Asian states as described by Sedik and Lerman. Russia's smallholders are not as integrated into value chains as in Eastern Europe, described by WEGREN", "SDG": [2]}, "using_deeplearning_for_image_based_plant_disease_detection": {"name": "Using Deep Learning for Image-Based Plant Disease Detection", "abstract": " Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.", "keywords": "crop diseases,machine learning,deep learning,digital epidemiology", "introduction": "Modern technologies have given human society the ability to produce enough food to meet the demand of more than 7 billion people. However, food security remains threatened by a number of factors including climate change , the decline in pollinators (Report of the Plenary of the Intergovernmental Science-PolicyPlatform on Biodiversity Ecosystem and Services on the work of its fourth session, 2016), plant diseases (Tai et al., 2014), and others. Plant diseases are not only a threat to food security at the global scale, but can also have disastrous consequences for smallholder farmers whose livelihoods depend on healthy crops. In the developing world, more than 80 percent of the agricultural production is generated by smallholder farmers (Strange and Scott, 2005), and reports of yield loss of more than 50% due to pests and diseases are common (UNEP, 2013). Furthermore, the largest fraction of hungry people (50%) live in smallholder farming households (Harvey et al., 2014), making smallholder farmers a group that's particularly vulnerable to pathogen-derived disruptions in food supply.(Sanchez and Swaminathan, 2005)Various efforts have been developed to prevent crop loss due to diseases. Historical approaches of widespread application of pesticides have in the past decade increasingly been supplemented by integrated pest management (IPM) approaches . Independent of the approach, identifying a disease correctly when it first appears is a crucial step for efficient disease management.(Ehler, 2006)Historically, disease identification has been supported by agricultural extension organizations or other institutions, such as local plant clinics. In more recent times, such efforts have additionally been supported by providing information for disease diagnosis online, leveraging the increasing Internet penetration worldwide. Even more recently, tools based on mobile phones have proliferated, taking advantage of the historically unparalleled rapid uptake of mobile phone technology in all parts of the world .(ITU, 2015)Smartphones in particular offer very novel approaches to help identify diseases because of their computing power, highresolution displays, and extensive built-in sets of accessories, such as advanced HD cameras. It is widely estimated that there will be between 5 and 6 billion smartphones on the globe by 2020. At the end of 2015, already 69% of the world's population had access to mobile broadband coverage, and mobile broadband penetration reached 47% in 2015, a 12-fold increase since 2007 . The combined factors of widespread smartphone penetration, HD cameras, and high performance processors in mobile devices lead to a situation where disease diagnosis based on automated image recognition, if technically feasible, can be made available at an unprecedented scale. Here, we demonstrate the technical feasibility using a deep learning approach utilizing 54,306 images of 14 crop species with 26 diseases (or healthy) made openly available through the project PlantVillage (ITU, 2015). An example of each crop-disease pair can be seen in Figure (Hughes and Salath\u00e9, 2015).1Computer vision, and object recognition in particular, has made tremendous advances in the past few years. The PASCAL VOC Challenge , and more recently the Large Scale Visual Recognition Challenge (ILSVRC) (Everingham et al., 2010) based on the ImageNet dataset (Russakovsky et al., 2015) have been widely used as benchmarks for numerous visualization-related problems in computer vision, including object classification. In 2012, a large, deep convolutional neural network achieved a top-5 error of 16.4% for the classification of images into 1000 possible categories (Deng et al., 2009). In the following 3 years, various advances in deep convolutional neural networks lowered the error rate to 3.57% (Krizhevsky et al., 2012)(Krizhevsky et al., 2012;Simonyan and Zisserman, 2014;Zeiler and Fergus, 2014;He et al., 2015;. While training large neural networks can be very time-consuming, the trained models can classify images very quickly, which makes them also suitable for consumer applications on smartphones.Szegedy et al., 2015)Deep neural networks have recently been successfully applied in many diverse domains as examples of end to end learning. Neural networks provide a mapping between an input-such as an image of a diseased plant-to an output-such as a crop\u223cdisease pair. The nodes in a neural network are mathematical functions that take numerical inputs from the incoming edges, and provide a numerical output as an outgoing edge. Deep neural networks are simply mapping the input layer to the output layer over a series of stacked layers of nodes. The challenge is to create a deep network in such a way that both the structure of the network as well as the functions (nodes) and edge weights correctly map the input to the output. Deep neural networks are trained by tuning the network parameters in such a way that the mapping improves during the training process. This process is computationally challenging and has in recent times been improved dramatically by a number of both conceptual and engineering breakthroughs (LeCun et al., 2015;.Schmidhuber, 2015)In order to develop accurate image classifiers for the purposes of plant disease diagnosis, we needed a large, verified dataset of images of diseased and healthy plants. Until very recently, such a dataset did not exist, and even smaller datasets were not freely available. To address this problem, the PlantVillage project has begun collecting tens of thousands of images of healthy and diseased crop plants , and has made them openly and freely available. Here, we report on the classification of 26 diseases in 14 crop species using 54,306 images with a convolutional neural network approach. We measure the performance of our models based on their ability to predict the correct crop-diseases pair, given 38 possible classes. The best performing model achieves a mean F 1 score of 0.9934 (overall accuracy of 99.35%), hence demonstrating the technical feasibility of our approach. Our results are a first step toward a smartphone-assisted plant disease diagnosis system.(Hughes and Salath\u00e9, 2015)", "body": "Modern technologies have given human society the ability to produce enough food to meet the demand of more than 7 billion people. However, food security remains threatened by a number of factors including climate change Various efforts have been developed to prevent crop loss due to diseases. Historical approaches of widespread application of pesticides have in the past decade increasingly been supplemented by integrated pest management (IPM) approaches Historically, disease identification has been supported by agricultural extension organizations or other institutions, such as local plant clinics. In more recent times, such efforts have additionally been supported by providing information for disease diagnosis online, leveraging the increasing Internet penetration worldwide. Even more recently, tools based on mobile phones have proliferated, taking advantage of the historically unparalleled rapid uptake of mobile phone technology in all parts of the world Smartphones in particular offer very novel approaches to help identify diseases because of their computing power, highresolution displays, and extensive built-in sets of accessories, such as advanced HD cameras. It is widely estimated that there will be between 5 and 6 billion smartphones on the globe by 2020. At the end of 2015, already 69% of the world's population had access to mobile broadband coverage, and mobile broadband penetration reached 47% in 2015, a 12-fold increase since 2007 Computer vision, and object recognition in particular, has made tremendous advances in the past few years. The PASCAL VOC Challenge Deep neural networks have recently been successfully applied in many diverse domains as examples of end to end learning. Neural networks provide a mapping between an input-such as an image of a diseased plant-to an output-such as a crop\u223cdisease pair. The nodes in a neural network are mathematical functions that take numerical inputs from the incoming edges, and provide a numerical output as an outgoing edge. Deep neural networks are simply mapping the input layer to the output layer over a series of stacked layers of nodes. The challenge is to create a deep network in such a way that both the structure of the network as well as the functions (nodes) and edge weights correctly map the input to the output. Deep neural networks are trained by tuning the network parameters in such a way that the mapping improves during the training process. This process is computationally challenging and has in recent times been improved dramatically by a number of both conceptual and engineering breakthroughs In order to develop accurate image classifiers for the purposes of plant disease diagnosis, we needed a large, verified dataset of images of diseased and healthy plants. Until very recently, such a dataset did not exist, and even smaller datasets were not freely available. To address this problem, the PlantVillage project has begun collecting tens of thousands of images of healthy and diseased crop plants METHODSDataset DescriptionWe analyze 54,306 images of plant leaves, which have a spread of 38 class labels assigned to them. Each class label is a cropdisease pair, and we make an attempt to predict the crop-disease pair given just the image of the plant leaf. Figure Across all our experiments, we use three different versions of the whole PlantVillage dataset. We start with the PlantVillage dataset as it is, in color; then we experiment with a gray-scaled version of the PlantVillage dataset, and finally we run all the experiments on a version of the PlantVillage dataset where the leaves were segmented, hence removing all the extra background information which might have the potential to introduce some inherent bias in the dataset due to the regularized process of data collection in case of PlantVillage dataset. Segmentation was automated by the means of a script tuned to perform well on our particular dataset. We chose a technique based on a set of masks generated by analysis of the color, lightness and saturation components of different parts of the images in several color spaces (Lab and HSB). One of the steps of that processing also allowed us to easily fix color casts, which happened to be very strong in some of the subsets of the dataset, thus removing another potential bias.This set of experiments was designed to understand if the neural network actually learns the \"notion\" of plant diseases, or if it is just learning the inherent biases in the dataset. Figure Measurement of PerformanceTo get a sense of how our approaches will perform on new unseen data, and also to keep a track of if any of our approaches are overfitting, we run all our experiments across a whole range of train-test set splits, namely 80-20 (80% of the whole dataset used for training, and 20% for testing), 60-40 (60% of the whole dataset used for training, and 40% for testing), 50-50 (50% of the whole dataset used for training, and 50% for testing), 40-60 (40% of the whole dataset used for training, and 60% for testing) and finally 20-80 (20% of the whole dataset used for training, and 80% for testing). It must be noted that in many cases, the PlantVillage dataset has multiple images of the same leaf (taken from different orientations), and we have the mappings of such cases for 41,112 images out of the 54,306 images; and during all these test-train splits, we make sure all the images of the same leaf goes either in the training set or the testing set. Further, for every experiment, we compute the mean precision, mean recall, mean F 1 score, along with the overall accuracy over the whole period of training at regular intervals (at the end of every epoch). We use the final mean F 1 score for the comparison of results across all of the different experimental configurations.ApproachWe evaluate the applicability of deep convolutional neural networks for the classification problem described above. We focus on two popular architectures, namely AlexNet The AlexNet architecture (see Figure The GoogleNet architecture on the other hand is a much deeper and wider architecture with 22 layers, while still having considerably lower number of parameters (5 million parameters) in the network than AlexNet (60 million parameters). An application of the \"network in network\" architecture We analyze the performance of both these architectures on the PlantVillage dataset by training the model from scratch in one case, and then by adapting already trained models (trained on the ImageNet dataset) using transfer learning. In case of transfer learning, we re-initialize the weights of layer fc8 in case of AlexNet, and of the loss {1,2,3}/classifier layers in case of GoogLeNet. Then, when training the model, we do not limit the learning of any of the layers, as is sometimes done for transfer learning. In other words, the key difference between these two learning approaches (transfer vs. training from scratch) is in the initial state of weights of a few layers, which lets the transfer learning approach exploit the large amount of visual knowledge already learned by the pre-trained AlexNet and GoogleNet models extracted from ImageNet To summarize, we have a total of 60 experimental configurations, which vary on the following parameters:1. Choice of deep learning architecture:Choice of training mechanism:Transfer Learning, Training from Scratch.Choice of dataset type:Color, Gray scale, Leaf Segmented.Choice of training-testing set distribution:Train: 80%, Test: 20%, Train: 60%, Test: 40%, Train: 50%, Test: 50%, Train: 40%, Test: 60%, Train: 20%, Test: 80%.Throughout this paper, we have used the notation of Architecture:TrainingMechanism:DatasetType:Train-Test-Set-Distribution to refer to particular experiments. For instance, to refer to the experiment using the GoogLeNet architecture, which was trained using transfer learning on the gray-scaled PlantVillage dataset on a train-test set distribution of 60-40, we will use the notation GoogLeNet:TransferLearning:GrayScale:60-40.Each of these 60 experiments runs for a total of 30 epochs, where one epoch is defined as the number of training iterations in which the particular neural network has completed a full pass of the whole training set. The choice of 30 epochs was made based on the empirical observation that in all of these experiments, the learning always converged well within 30 epochs (as is evident from the aggregated plots (Figure To enable a fair comparison between the results of all the experimental configurations, we also tried to standardize the hyper-parameters across all the experiments, and we used the following hyper-parameters in all of the experiments:\u2022 Solver type: Stochastic Gradient Descent, \u2022 Base learning rate: 0.005,\u2022 Learning rate policy: Step (decreases by a factor of 10 every 30/3 epochs), \u2022 Momentum: 0.9, \u2022 Weight decay: 0.0005, \u2022 Gamma: 0.1, \u2022 Batch size: 24 (in case of GoogLeNet), 100 (in case of AlexNet).All the above experiments were conducted using our own fork of Caffe RESULTSAt the outset, we note that on a dataset with 38 class labels, random guessing will only achieve an overall accuracy of 2.63% on average. Across all our experimental configurations, which include three visual representations of the image data (see Figure To address the issue of over-fitting, we vary the test set to train set ratio and observe that even in the extreme case of training on only 20% of the data and testing the trained model on the rest 80% of the data, the model achieves an overall accuracy of 98.21% (mean F 1 score of 0.9820) in the case of GoogLeNet::TransferLearning::Color::20-80. As expected, the overall performance of both AlexNet and GoogLeNet do degrade if we keep increasing the test set to train set ratio (see Figure Among the AlexNet and GoogLeNet architectures, GoogLeNet consistently performs better than AlexNet (Figure The three versions of the dataset (color, gray-scale, and segmented) show a characteristic variation in performance across all the experiments when we keep the rest of the experimental configuration constant. The models perform the best in case of the colored version of the dataset. When designing the experiments, we were concerned that the neural networks might only learn to pick up the inherent biases associated with the lighting conditions, the method and apparatus of collection of the data. We therefore experimented with the gray-scaled version of the same dataset to test the model's adaptability in the absence of color information, and its ability to learn higher level structural patterns typical to particular crops and diseases. As expected, the performance did decrease when compared to the experiments on the colored version of the dataset, but even in the case of the worst performance, the observed mean F 1 score was 0.8524 (overall accuracy of 85.53%). The segmented versions of the whole dataset was also prepared to investigate the role of the background of the images in overall performance, and as shown in Figure While these approaches yield excellent results on the PlantVillage dataset which was collected in a controlled environment, we also assessed the model's performance on images sampled from trusted online sources, such as academic agriculture extension services. Such images are not available in large numbers, and using a combination of automated download from Bing Image Search and IPM Images with a visual verification step, we obtained two small, verified datasets of 121 (dataset 1) and 119 images (dataset 2), respectively (see Supplementary Material for a detailed description of the process).Using the best model on these datasets, we obtained an overall accuracy of 31.40% in dataset 1, and 31.69% in dataset 2, in successfully predicting the correct class label (i.e., crop and disease information) from among 38 possible class labels. We note that a random classifier will obtain an average accuracy of only 2.63%. Across all images, the correct class was in the top-5 predictions in 52.89% of the cases in dataset 1, and in 65.61% of the cases in dataset 2. The best models for the two datasets were GoogLeNet:Segmented:TransferLearning:80-20 for dataset 1, and GoogLeNet:Color:TransferLearning:80-20 for dataset 2. An example image from theses datasets, along with its visualization of activations in the initial layers of an AlexNet architecture, can be seen in Figure Similarly, in the n > = 2 case, dataset 2 contains 13 classes distributed among 4 crops. Random guessing in such a dataset would achieve an accuracy of 0.314, while our model has an accuracy of 0.545. In the n > = 3 case, the dataset contains 11 classes distributed among 3 crops. Random guessing in such a dataset would achieve an accuracy of 0.288, while our model has an accuracy of 0.485.DISCUSSION", "conclusions": "The performance of convolutional neural networks in object recognition and image classification has made tremendous progress in the past few years. (Krizhevsky et al., 2012;Simonyan and Zisserman, 2014;Zeiler and Fergus, 2014;He et al., 2015;. Previously, the traditional approach for image classification tasks has been based on handengineered features, such as SIFT Szegedy et al., 2015), HoG (Lowe, 2004), SURF (Dalal and Triggs, 2005), etc., and then to use some form of learning algorithm in these feature spaces. The performance of these approaches thus depended heavily on the underlying predefined features. Feature engineering itself is a complex and tedious process which needs to be revisited every time the problem at hand or the associated dataset changes considerably. This problem occurs in all traditional attempts to detect plant diseases using computer vision as they lean heavily on hand-engineered features, image enhancement techniques, and a host of other complex and labor-intensive methodologies.(Bay et al., 2008)In addition, traditional approaches to disease classification via machine learning typically focus on a small number of classes usually within a single crop. Examples include a feature extraction and classification pipeline using thermal and stereo images in order to classify tomato powdery mildew against healthy tomato leaves ; the detection of powdery mildew in uncontrolled environments using RGB images (Raza et al., 2015); the use of RGBD images for detection of apple scab (Hern\u00e1ndez-Rabad\u00e1n et al., 2014) the use of fluorescence imaging spectroscopy for detection of citrus huanglongbing (Ch\u00e9n\u00e9 et al., 2012) the detection of citrus huanglongbing using near infrared spectral patterns (Wetterich et al., 2012) and aircraft-based sensors (Sankaran et al., 2011) the detection of tomato yellow leaf curl virus by using a set of classic feature extraction steps, followed by classification using a support vector machines pipeline (Garcia-Ruiz et al., 2013), and many others. A very recent review on the use of machine learning on plant phenotyping (Mokhtar et al., 2015) extensively discusses the work in this domain. While neural networks have been used before in plant disease identification (Singh et al., 2015) (for the classification and detection of Phalaenopsis seedling disease like bacterial soft rot, bacterial brown spot, and Phytophthora black rot), the approach required representing the images using a carefully selected list of texture features before the neural network could classify them.(Huang, 2007)Our approach is based on recent work  which showed for the first time that end-to-end supervised training using a deep convolutional neural network architecture is a practical possibility even for image classification problems with a very large number of classes, beating the traditional approaches using hand-engineered features by a substantial margin in standard benchmarks. The absence of the laborintensive phase of feature engineering and the generalizability of the solution makes them a very promising candidate for a practical and scaleable approach for computational inference of plant diseases.Krizhevsky et al. (2012)Using the deep convolutional neural network architecture, we trained a model on images of plant leaves with the goal of classifying both crop species and the presence and identity of disease on images that the model had not seen before. Within the PlantVillage data set of 54,306 images containing 38 classes of 14 crop species and 26 diseases (or absence thereof), this goal has been achieved as demonstrated by the top accuracy of 99.35%. Thus, without any feature engineering, the model correctly classifies crop and disease from 38 possible classes in 993 out of 1000 images. Importantly, while the training of the model takes a lot of time (multiple hours on a high performance GPU cluster computer), the classification itself is very fast (less than a second on a CPU), and can thus easily be implemented on a smartphone. This presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.However, there are a number of limitations at the current stage that need to be addressed in future work. First, when tested on a set of images taken under conditions different from the images used for training, the model's accuracy is reduced substantially, to just above 31%. It's important to note that this accuracy is much higher than the one based on random selection of 38 classes (2.6%), but nevertheless, a more diverse set of training data is needed to improve the accuracy. Our current results indicate that more (and more variable) data alone will be sufficient to substantially increase the accuracy, and corresponding data collection efforts are underway.The second limitation is that we are currently constrained to the classification of single leaves, facing up, on a homogeneous background. While these are straightforward conditions, a real world application should be able to classify images of a disease as it presents itself directly on the plant. Indeed, many diseases don't present themselves on the upper side of leaves only (or at all), but on many different parts of the plant. Thus, new image collection efforts should try to obtain images from many different perspectives, and ideally from settings that are as realistic as possible.At the same time, by using 38 classes that contain both crop species and disease status, we have made the challenge harder than ultimately necessary from a practical perspective, as growers are expected to know which crops they are growing. Given the very high accuracy on the PlantVillage dataset, limiting the classification challenge to the disease status won't have a measurable effect. However, on the real world datasets, we can measure noticeable improvements in accuracy. Overall, the presented approach works reasonably well with many different crop species and diseases, and is expected to improve considerably with more training data.Finally, it's worth noting that the approach presented here is not intended to replace existing solutions for disease diagnosis, but rather to supplement them. Laboratory tests are ultimately always more reliable than diagnoses based on visual symptoms alone, and oftentimes early-stage diagnosis via visual inspection alone is challenging. Nevertheless, given the expectation of more than 5 Billion smartphones in the world by 2020-of which almost a Billion in Africa (GSMA Intelligence, 2016)-we do believe that the approach represents a viable additional method to help prevent yield loss. What's more, in the future, image data from a smartphone may be supplemented with location and time information for additional improvements in accuracy. Last but not least, it would be prudent to keep in mind the stunning pace at which mobile technology has developed in the past few years, and will continue to do so. With ever improving number and quality of sensors on mobiles devices, we consider it likely that highly accurate diagnoses via the smartphone are only a question of time.", "SDG": [2]}, "an_automatic_traffic_surveillance_system_for_vehicle_tracking_and_classification": {"name": "An Automatic Traffic Surveillance System for Vehicle Tracking and Classification", "abstract": " This paper presents an automatic traffic surveillance system to estimate important traffic parameters from video sequences using only one camera. Different from traditional methods which classify vehicles into only cars and non-cars, the proposed method has a good capability to categorize cars into more specific classes with a new \"linearity\" feature. In addition, in order to reduce occlusions of vehicles, an automatic scheme of detecting lane dividing lines is proposed. With the found lane dividing lines, not only occlusions of vehicles can be reduced but also a normalization scheme can be developed for tackling the problems of feature size variations. Once all vehicle features are extracted, an optimal classifier is then designed to robustly categorize vehicles into different classes even though shadows, occlusions, and other noise exist. The designed classifier can collect different evidences from the database and the verified vehicle itself to make better decisions and thus much enhance the robustness and accuracy of classification. Experimental results show that the proposed method is much robust and powerful than other traditional methods.", "keywords": "", "introduction": "Intelligent Transportation System (ITS) is the application which incorporates electronic, computer, and communication technologies into vehicles and roadways for increasing traffic safety, reducing congestion, and thus improving people's life quality. Due to the ease of maintenances and high flexibility in automatic traffic parameter extraction via image processing, there have been a number of different vision-based systems proposed in the literature [1][2][3][4]. For example, Beymer et al. [5] proposed a vehicle tracking algorithm to estimate traffic parameters based on corner features. Additionally, Gupte et al. [1] proposed a region-based approach to track and classify vehicles based on the establishment of correspondences between regions and vehicles. However, in their approaches, only two categories, i.e., cars and non-cars, are handled. Therefore, Sullivan et al. [2] proposed a 3D model matching scheme to classify vehicles into various types like wagons, sedan, hatchback, etc. Although 3D features obtained from stereo cameras may be useful for categorizing various types of vehicles, the inherent correspondence problem makes them unfeasible for real-time applications.[3]Except the correspondence problem, several environmental variations will much affect the accuracy of the whole surveillance systems. For examples, shadows will cause two vehicles connect together and lead to the failure of vehicle classification. In addition, perspective distortion will lead to that the vehicle features such as length, width, height are not constant when vehicles move. More importantly, many different vehicle types have similar features, which cause that most approaches can classify vehicles only into two simple categories, i.e., cars and non-cars.In this paper, we propose a novel vehicle surveillance system to detect, track, and classify vehicles into different classes. First of all, this system uses a technique of image differencing to detect different vehicles from video sequences. Then, a Kalman filter is designed for tracking different vehicles and then obtaining their trajectories. After that, a new defined \"linearity\" feature is extracted from each vehicle. The new defined \"linearity\" feature is very useful for discriminating \"bus\" from \"track\" without using any 3D information. To reduce the occlusion problem of vehicles, this paper presents an automatic scheme to detect all possible lane dividing lines by analyzing different vehicles' trajectories. Then, the found lane dividing lines can be very effectively used for solving the problems of vehicle occlusions. In practice, due to perspective distortion, the dimension of a vehicle will gradually change when it movements. The found lane dividing lines also can provide important information to scale the vehicle dimension. After feature extraction, an optimal classifier is then designed for accurate vehicle classification. When designing the optimal classifier, we should know that vehicle features are easily affected by shadows or light changes. Since a vehicle has many appearances when it moves, these appearances can provide quite supports for designing the desired classifier. Based on this idea and the spirit of maximum likelihood estimation, an optimization classifier is then designed for robust vehicle categorization. Experimental results show the proposed method offers great improvements in terms of accuracy, robustness, and stability in traffic surveillance.  In this paper, we propose a novel traffic surveillance system for estimating traffic parameters from video sequence. This system includes an initialization stage to obtain the information of lane widths and lane dividing lines. Then, traffic parameters can be estimated through image differencing, Kalman filter, feature extraction, and classification by an optimal classifier. In what follows, details of each procedure are discussed.", "body": "Intelligent Transportation System (ITS) is the application which incorporates electronic, computer, and communication technologies into vehicles and roadways for increasing traffic safety, reducing congestion, and thus improving people's life quality. Due to the ease of maintenances and high flexibility in automatic traffic parameter extraction via image processing, there have been a number of different vision-based systems proposed in the literature Except the correspondence problem, several environmental variations will much affect the accuracy of the whole surveillance systems. For examples, shadows will cause two vehicles connect together and lead to the failure of vehicle classification. In addition, perspective distortion will lead to that the vehicle features such as length, width, height are not constant when vehicles move. More importantly, many different vehicle types have similar features, which cause that most approaches can classify vehicles only into two simple categories, i.e., cars and non-cars.In this paper, we propose a novel vehicle surveillance system to detect, track, and classify vehicles into different classes. First of all, this system uses a technique of image differencing to detect different vehicles from video sequences. Then, a Kalman filter is designed for tracking different vehicles and then obtaining their trajectories. After that, a new defined \"linearity\" feature is extracted from each vehicle. The new defined \"linearity\" feature is very useful for discriminating \"bus\" from \"track\" without using any 3D information. To reduce the occlusion problem of vehicles, this paper presents an automatic scheme to detect all possible lane dividing lines by analyzing different vehicles' trajectories. Then, the found lane dividing lines can be very effectively used for solving the problems of vehicle occlusions. In practice, due to perspective distortion, the dimension of a vehicle will gradually change when it movements. The found lane dividing lines also can provide important information to scale the vehicle dimension. After feature extraction, an optimal classifier is then designed for accurate vehicle classification. When designing the optimal classifier, we should know that vehicle features are easily affected by shadows or light changes. Since a vehicle has many appearances when it moves, these appearances can provide quite supports for designing the desired classifier. Based on this idea and the spirit of maximum likelihood estimation, an optimization classifier is then designed for robust vehicle categorization. Experimental results show the proposed method offers great improvements in terms of accuracy, robustness, and stability in traffic surveillance.  In this paper, we propose a novel traffic surveillance system for estimating traffic parameters from video sequence. This system includes an initialization stage to obtain the information of lane widths and lane dividing lines. Then, traffic parameters can be estimated through image differencing, Kalman filter, feature extraction, and classification by an optimal classifier. In what follows, details of each procedure are discussed.Overview of the Proposed SystemVehicle Segmentation, Tracking, and Lane-Dividing DetectionIn order to simplify the problems of vehicle segmentation, this paper assumes the analyzed image frames are captured by a still camera.Vehicle Segmentation and TrackingWhen the camera is static, the moving objects can be detected through background subtraction. Assume k I and k B are intensities of the th k frame and the th k background, respectively. The difference image (, )k Dx y can be defined as follows:,where d T is the average value of (, )kk Ix y Bx y \u2212 . After thresholding, each moving vehicle can be segmented from input video frames. Then, in order to obtain the trajectory of each vehicle, a Kalman filter Lane-Dividing DetectionIn practice, due to shadows, different vehicles will often connect together. Such occlusions will disturb the accuracies of traffic parameter estimation. In what follows, we will present an automatic scheme to detect possible lane dividing lines from videos for overcoming the occlusion problem of vehicles. Let Occlusion Detection and EliminaitonAs described before, different vehicles will often connect together and form occlusions due to shadows. In order to accurately estimate traffic parameters, this paper take advantages of the found lane-dividing lines to separate occluded vehicles into different parts. Like Fig.    Vehicle ClassificationOnce all the input vehicles have been extracted, we should classify them into different categories for traffic parameter estimation. In this section, the new feature \"linearity\" of vehicles will be defined and extracted for effective vehicle classification. In what follows, details of feature extraction and classifier designing are discussed.Feature ExtractionAs described before, this paper uses \"size\" and \"linearity\" features to classify vehicles into categories. For the first feature, due to the perspective effect, the size of a vehicle will gradually change when it moves. Therefore, before classification, the size feature should be normalized in advance. Assume { } Through normalization, i h v S forms a good feature for vehicle classification. In addition to vehicle size, the \"linearity\" feature is also very important for vehicle categorization. Like Fig. Furthermore, the probability of i H belonging to k VC can be calculated by:where ( ) ( | ) where i H Q is the set of all appearances t i H .Experimental ResultsIn order to analyze the performance of the proposed method, a series of image sequences were used. Fig. Fig. 1Fig. 2LvUFig. 34. 2 \u03c3Fig. 4Fig. 5Fig. 6Table 1 :Table 2 :", "conclusions": "", "SDG": [3, 11]}, "an_inference_mechanism_using_bayes_based_classifiers_in_pregnancy_care": {"name": "An Inference Mechanism Using Bayes-based Classifiers in Pregnancy Care", "abstract": " Significant advances on smart decision support systems (DSSs) development have influenced important results on pregnancy care. Nevertheless, even considering the efforts to reduce the number of women deaths due to problems related to pregnancy, this decrease presented less impact than other areas of human development. Hypertensive disorders in pregnancy, particularly pre-eclampsia and eclampsia, account for significant proportion of perinatal morbidity and maternal mortality. In this context, this paper proposes an inference model that uses data mining (DM) techniques capable for operating in a data set to extract patterns and assist in knowledge discovery. Identifying hypertensive crises that complicate pregnancy, it can impact in a meaningful reduction the incidence of sequelae and death of pregnant women. Comparison between two Bayesian classifiers is performed in this work to better classify the hypertensive disorders severity. Results showed that Na\u00efve Bayes classifier had an excellent performance, presenting better precision and F-measure, compared to the other experimented classifiers. Even finding a good performance to predict hypertensive disorders, other Bayesian methods need to be evaluated, as well as other DM techniques such as those based on artificial intelligence (AI) and tree-based methods.", "keywords": "eHealth,Hypertension,Pregnancy,Decision Support Systems,Data Mining,Bayes Methods", "introduction": "According to the World Health Organization (WHO), hypertensive disorders of pregnancy afflict about 10% of all pregnancies around the world. These are the leading causes of morbidity, disability, and death among mothers and babies . These complications during pregnancy were an important cause of mortality in Latin America and the Caribbean, contributing to 22.1% of all maternal deaths in this region [1]. With providing timely and efficient care, the majority of deaths related to these complications could be avoided. Thus, optimization of health care for pregnant women to prevent and treat hypertensive disorders is needed.[2]As reported by the National High Blood Pressure Education Program, hypertension in pregnancy is classified into one of the following five categories: (i) Chronic hypertension, (ii) Preeclampsia, (iii) Chronic hypertension with superimposed preeclampsia, (iv) Gestational hypertension, and (v) Transient hypertension . These categories are critical to differentiating preeclampsia, a pregnancy-specific syndrome of exaggerated vasoconstriction and reduced organ perfusion, from preexisting chronic hypertension. Nevertheless, this complex multifactorial syndrome, that occurs in about 5 to 7% of pregnancies worldwide, has not an etymology established yet, i.e., this disease has still no agreement about its classification as well as on the timing of its occurrence during pregnancy. To evaluate this disorder is necessary to define the blood pressure status. If the pregnant woman is hypertensive, the health expert assesses its severity, possible secondary causes, and damage presence in organs, to plan treatment strategies. The treatment of chronic cases in the first trimester of pregnancy is critical since fetal loss rate is about 50% and maternal mortality is significant in these cases. Preeclampsia is more common in pregnant women that already suffered from chronic hypertension, with an incidence of approximately 25%. To prevent more severe problems is recommendable the early recognition of high-risk pregnancies, a constant clinical, laboratory, and intensive monitoring when indicated. In this sense, information and communication technologies (ICT) play a key role to improve the quality of life of pregnant women. With the development of intelligent systems for risk pregnancy monitoring, health experts can identify serious problems caused by gestational hypertension at its early stages, saving lives of both mothers and babies. Several technology solutions are already being deployed to combat preeclampsia in their most critical condition [3], [4]. Many approaches have achieved proper evaluation but are still unable to reduce the critical situation of maternal and fetal deaths by themselves, mainly, in developing countries. Smart DSSs are considered a goof tool capable to contribute to this goal. Then, this paper proposes an inference model that uses data mining (DM) techniques capable for operating in a data set to extract patterns and assist in knowledge discovery. Identifying hypertensive crises that complicate pregnancy, it can impact in a meaningful reduction the incidence of sequelae and death of pregnant women.[5]The paper is organized as follows. Section II presents related works discussing used methods on identifying high-risk pregnancies. Section III presents two modeling proposals that use Bayes-based classifiers capable of identifying hypertensive disorders as from its symptoms. Section IV performs the performance evaluation study of these methods and the results analysis considering the proposed plans. Finally, Section V provides the conclusion and suggestions for further works.", "body": "According to the World Health Organization (WHO), hypertensive disorders of pregnancy afflict about 10% of all pregnancies around the world. These are the leading causes of morbidity, disability, and death among mothers and babies As reported by the National High Blood Pressure Education Program, hypertension in pregnancy is classified into one of the following five categories: (i) Chronic hypertension, (ii) Preeclampsia, (iii) Chronic hypertension with superimposed preeclampsia, (iv) Gestational hypertension, and (v) Transient hypertension The paper is organized as follows. Section II presents related works discussing used methods on identifying high-risk pregnancies. Section III presents two modeling proposals that use Bayes-based classifiers capable of identifying hypertensive disorders as from its symptoms. Section IV performs the performance evaluation study of these methods and the results analysis considering the proposed plans. Finally, Section V provides the conclusion and suggestions for further works.II. RELATED WORKThis topic discusses recent works that are significant efforts to identify hypertensive disorders risks on pregnancy, and then, researches that use Bayes-based classifiers to collect valuable information from a data set.LARIISA platform is an intelligence solution that aims to specify and implement, from an analytical and experimental study of DM methods, data warehouse, ontologies, and mashups Recent studies found hereditary factors in occurrence of preeclampsia. Dutta et al. conduct a study about critical genes that cause preeclampsia using microarray gene expression data The leading causes of preeclampsia are not known. However, its risk factors have been already defined. Cheng et al. study the effects of these risk factors on the gestational age The presence of the diastolic notch in the Doppler of uterine artery is the best predictive signal after twenty weeks of pregnancy. III. MODELING PROPOSALS USING BAYES-BASED CLASSIFIERSHypertension in pregnancy occurs when there are highpressure levels in pregnant women. Pregnancy-induced hypertension refers to the onset of hypertension as a result of pregnancy, occurring after 20 weeks of pregnancy and disappearing until six weeks after childbirth. Hypertension is the increase of blood pressure above 140/90mmHg. With the pregnant woman seated, the health expert should measure the blood pressure and confirm, after the rest period, for three times. Gestational hypertension diagnosis occurs when diastolic blood pressure is above 90mmHg or when it has a blood pressure increase above 15 mmHg of the value measured before first 20 weeks of gestation. Preeclampsia is the hypertension occurrence accompanied by loss of protein in the urine (proteinuria) after 20 pregnancy weeks with or no edema. About the protein loss, significant proteinuria is considered with the values equal or greater than 300mg of protein in urine collected for 24 hours. The diagnosis is clinical and laboratory: measurement of blood pressure, edema examination, and protein quantity in urine. Figure A. The Na\u00efve Bayes ClassifierThe Na\u00efve Bayes classifier is applicable in health care when there is a set of attributes that represents each risk factor. Each one of these attributes occurs in a particular disorder hypertensive. This classifier based on Bayes theorem is used to determine the probability of each hypertensive disease from symptoms never seen based on training examples. Shaikh et al. develop an intelligent DSS using DM techniques to predict heart diseases According to a Bayes-based method, this work aims to classify new cases according to its most probable classification of hypertensive disorder in pregnancy given its set of symptoms using the Equation ( (1)In the Eq. ( (2) Thus, the Na\u00efve Bayes classifier is a simplification, which is given by equation ( B. The Averaged One-dependence Estimators classifier (AODE)The Averaged One-dependence Estimators classifier (AODE) adopts another approach to minimize the dependence of attributes. In fact, it extends the structure of the Naive Bayes classifier also including the relationship of each attribute to with other. Figure In healthcare, Kov\u00e1cs and Hajdu use a novel strategy for the segmentation of the vascular system in retina images This method seeks to estimate the probability of each class given a specified set of features , using the equation ( (4)In the eq. ( IV. PERFORMANCE EVALUATION AND RESULT ANALYSISThe healthcare dataset used for classification includes twenty-five hypertension disorders cases, each one with eleven attributes provided by health experts.To evaluate the performance of the proposed approaches, the 5-fold cross-validation method was used. This technique assesses the ability of generalization of a model from a data set. In problems where the goal of the modeling is the prediction, this method is widely used. The primary aim is to estimate how much a model is accurate. This research used a confusion matrix, the cross-validation approach, and the F-measure to evaluate the accuracy of the models constructed by both classifiers. The confusion matrix is a table where TP (true positives), TN (true negatives), FP (false positives), and FN (false negatives) are represented. The cross-validation method allows the use of all the database data for training and testing. This work adopted five folds in this method. F-measure is the mean between precision and recall, measuring the ability to recognize negative and positive cases.Based on V. CONCLUSION", "conclusions": "This paper discussed a model for hypertensive diseases in risk pregnancies using DM techniques. For identification of the mortality leading cause during pregnancy, namely, the preeclampsia, two classifiers were considered and evaluated. Both of them presented a good performance. The main result of this study shown that minimizing the dependence of all the attributes they reduced the accuracy of the model. Further works will discuss the relationship between nodes, and they will evaluate other classifiers. Decision trees and AI based classifiers can be the key to find a better relationship between nodes, making possible a more precise model. Experimenting these models on a larger database will also show if the amount of data positively affects the accuracy. This work was an initial effort to develop a better intelligent mechanism, which aims to provide smart governance in the decision-making process by various actors in the public health area. For health professionals that work with pregnancy is important to be conscious of the medical history and changes in the pregnant woman's clinical state because these changes may not come always with a high blood pressure, complicating the decision-making process. A more specialized care is essential for preeclampsia monitoring with the purpose to personalize assistance for the prevention, promotion, and health recovery. To minimize complications and fatal consequences specific strategies with agility and efficiency are needed. In this context, information and communication technologies are essential and play a key role to reduce these consequences.", "SDG": [3]}, "applications_of_deep_learning_in_biomedicin": {"name": "Applications of deep learning in biomedicine", "abstract": " Increases in throughput and installed base of biomedical research equipment led to a massive accumulation of -omics data known to be highly variable, high-dimensional, and sourced from multiple often incompatible data platforms. While this data may be useful for biomarker identification and drug discovery, the bulk of it remains underutilized. Deep neural networks (DNNs) are efficient algorithms based on the use of compositional layers of neurons, with advantages well matched to the challenges -omics data presents. While achieving state-of-the-art results and even surpassing human accuracy in many challenging tasks, the adoption of deep learning in biomedicine has been comparatively slow. Here, we discuss key features of deep learning that may give this approach an edge over other machine learning methods. We then consider limitations and review a number of applications of deep learning in biomedical studies demonstrating proof of concept and practical utility.", "keywords": "deep learning,deep neural networks,RBM,genomics,transcriptomics,artificial intelligence,biomarker development", "introduction": "The amount of biomedical data in public repositories is rapidly increasing  , but the heterogeneous nature of this data renders integrative analysis increasingly difficult. 1 Computational biology methods are essential and routinely used in various fields of biomedicine from biomarker development to drug discovery 2 , and machine learning methods are extensively and increasingly applied 3 . Deep learning is a broad class of machine learning techniques that shows particular promise in extracting high level abstractions from the raw data of very large, heterogeneous, high-dimensional datasets. This is precisely the type of data biology now has to offer.4Here, we review deep learning as a versatile biomedical research tool with many potential applications, including the resurrection of cold repository datasets for new use in drug discovery and biomarker development. We first define basic concepts and the rationale for applying these techniques to biological data. We then discuss various implementation considerations and follow with a review of recent biomedical studies that have used deep learning, while highlighting areas of biomedicine that could benefit from this approach, in particular those important for biomarker development and drug discovery. Finally, we discuss limitations and future directions.", "body": "The amount of biomedical data in public repositories is rapidly increasing Here, we review deep learning as a versatile biomedical research tool with many potential applications, including the resurrection of cold repository datasets for new use in drug discovery and biomarker development. We first define basic concepts and the rationale for applying these techniques to biological data. We then discuss various implementation considerations and follow with a review of recent biomedical studies that have used deep learning, while highlighting areas of biomedicine that could benefit from this approach, in particular those important for biomarker development and drug discovery. Finally, we discuss limitations and future directions.Basic concepts of deep learningMachine learning, or learning that occurs without explicit programming, can take place in one of two forms: conventional, \"shallow\" learning (neural networks with a single hidden layer or support vector machines), or deep learning (neural networks with many hierarchical layers of non-linear information processing). Deep learning was recently reviewed in detail by LeCun et al The importance of this is most readily apparent in the areas where deep learning has shown to be useful: image and language recognition One can imagine the impossible effort of annotating the millions of images that machines can now accurately identify. That machines can now distinguish images of two nearly identical objects or complete a sentence is all possible increasingly with help from deep learning. These and other recent developments in DNN architectures have boosted enthusiasm within the machine learning community, with unprecedented performance in many challenging tasks Why deep learning may benefit biomedical researchWith some imagination, parallels can be drawn between biological data and the types of data deep learning has shown the most success with-namely image and voice data. A gene expression profile, for instance, is essentially a \"snapshot,\" or image, of what is going on in a given cell or tissue under given conditions, with patterns of gene expression representative of physical states in a cell or tissue in the same way that patterns of pixelation are representative of objects in a picture.In the same way that two similar but categorically different images must be discerned by deep learning algorithms regardless of background or position, two similar but categorically different disease pathologies may be difficult to distinguish if certain unimportant background conditions happen to match (e.g. tissues, time-points, individual, species, platform), thus selectivity of key differences is essential. Alternatively, one pathology may appear to differ from itelf when imposed on a variety of different experimental \"backgrounds\" and in several different states of progression, so invariance to non-target-related differences is also key. These features, selectivity and invariance, are requirements for both image recognition and gene expression analysis and are also two hallmarks of CNNs, the powerhouses of modern visual image processing. These parallels, while illustrative and hypothetical in nature, are also backed up by several practical advantages of DNNs that strengthen the case for biological application. First, DNNs require very large datasets, which biology is teeming with at this time. Secondly, DNNs are well-equipped to handle high dimensional, sparse, noisy data with non-linear relationships, all of which are common to transcriptomics and other -omics data in biology. Third, DNN have high generalization ability; once trained on a dataset they can be applied to other, new datasets; this is a requirement for binding and interpretation of heterogeneous multiplatform data, such as gene expression data.Finally, these considerations are further supported by the fact that the small number of deep learning studies in biomedicine that now exist have shown success with this method. These are to be discussed below.Importantly, despite the suitability of DNN for biological data and the potential applications, the adoption of deep learning methods in biology has been slow. This may have several explanations. While deep architectures can be exponentially more efficient than conventional models, capturing fine subtleties in the structure of the data 13 , DNN, especially recurrent networks, are very complex machines containing hundreds of millions of weights, which makes training and regularization difficult. Deep models are still not optimized, still lack an adequate formulation, require more research, and rely heavily on computational experimentation. It should also be emphasized that despite being able to extract latent features from the data, DNNs are black boxes that learn by simple associations and co-occurrences. They lack the transparency and interpretability of other methods and may be unable to uncover complex causal and structural relationships common in biology without some human interpretation. Nevertheless, their many benefits may outweigh these obstacles, some of which may be overcome with time.Important Considerations for Deep Learning ImplementationDeep learning represents a broad class of techniques, and one of the challenges in applying deep learning is selecting the appropriate DNN type for the task at hand. Here we briefly summarize some of the considerations in developing DNN for a particular application.Although new deep learning approaches and architectures are increasingly being proposed, most DNNs can be classified into three major categories 14 :1. Networks for unsupervised learning: designed to capture high-order data correlation by identifying jointly statistical distributions with the associated classes when available. Bayes rule can later be used to create a discriminative learning machine 2. Networks for supervised learning: designed to provide maximum discriminative power in classification problems and are trained only with labeled data; all outputs must be tagged. The importance of hyperparameter optimizationAnother important consideration of DNNs is their many hyperparameters; these are either architectural, such as layer sizes and transfer functions, optimization types, such as learning rates and momentum values, or regularization, such as the dropout probabilities for each layer or the noise level injected on inputs. Optimization of DNNs is in general extremely challenging due the large number of parameters and nonlinearities in the models. Careful fine-tuning of the numerous hyperparameters is one of the most difficult and time consuming tasks in implementing these solutions.A common approach to optimizing neural networks hyperparameters is using Bayesian optimization to maximize the validation AUC (Area Under the Roc curve) or minimize the loss function. Bayesian optimization is ideally suited for globally optimizing noisy functions while being parsimonious in the number of function evaluations. The method proposed by Snoek et al is very useful as they used Spearmint with warping Bayesian optimization assumes that the unknown function is sampled from a Gaussian Process updating the posterior distribution as observations are gathered-in this case, the AUC or the loss function. The hyperparameters of the next iteration are selected through the optimization of the expected improvement, as suggested by Deep learning studies and potential applications in biomedicineDeep learning has already shown success in a variety of biological applications. In this section, we review challenges and opportunities for deep learning in various areas of research and, where possible, review studies that apply deep learning to these problems (Table BiomarkersOne important task in biomedicine is the translation of biological data to valid biomarkers that reflect phenotype and physical states, such as disease. Biomarkers are critical in assessing clinical trial outcomes GenomicsNext Generation Sequencing (NGS) technology has allowed production of a massive amount of genomics data. Much of the analysis of this data can be performed in silico with modern computational approaches. This includes structural annotation of genomes (including non-coding regulatory sequences, protein binding site prediction, and splicing sites).One important division of genomics is metagenomics, also known as environmental, ecogenomics or community genomics. NGS technology has shed light on the natural diversity of microorganisms that are not cultivated and previously not well studied.There are several bioinformatic challenges in metagenomics. One major challenge is functional analysis of sequence data and analysis of species diversity. The use of deep belief networks and recurrent neural networks have allowed classification of both metagenomics pH data and human microbiome data by phenotype. These did not improve classification accuracy compared to baseline methods as reinforcement learning, but did provide the ability to learn hierarchical representations of a data set TranscriptomicsTranscriptomics analysis exploits variation in the abundance of various types of transcripts (messenger RNA (mRNA), long non-coding RNA (lncRNA), microRNA (miRNA), etc.) to gather a range of functional information, from splicing code to biomarkers of various diseases.Transcriptomics data are often obtained from different types of platforms (various microarray platforms, sequencing platforms) that differ by the gene set measured and method of signal detection. Many factors contribute to variability of gene expression data. Thus normalization is needed even for single platform analysis. Cross-platform analysis requires normalization techniques, which can be a major challenge Tabular data applicationsOne way in which gene expression data can be represented is in tabular form as matrices, which contain quantitative information about transcript expression. These data are high-dimensional, making statistical analysis problematic due to loss of signal to noise in the data High dimensional data can be handled in two ways:I. dimensionality reduction:A. feature extraction, for instance with SVM or Random Forest algorithms; B. feature subset selection; C. pathway analysis; II.use of methods less sensitive to high-dimensionality, such as Random Forest or deep belief networks.Methods such as Principal Component Analysis (PCA), Singular Value Decomposition, Independent Component Analysis or Non-negative Matrix Factorization are common first front approaches. However, the above-mentioned methods transform the data into a number of components that can be difficult to interpret biologically. Also, such dimensionality reduction methods extract features based on gene expression profiles regardless of interactions between genes. Pathway analysis allows reduction of the number of variables, reducing error rate and retaining more biologically relevant information Deep learning has also showed some success in handling high-dimensional matrix transcriptomics data. In one alternative approach, features from gene expression were extracted together with regions of non-coding transcripts such as miRNA; this was implemented using deep belief networks and active learning, where deep learning feature extractors were used to reduce the dimensionality of six cancer datasets and outperformed basic feature selection methods In another deep learning application, Fakoor et al. took advantage of an auto-encoders network for generalization and applied this to cancer classification using microarray gene expression data obtained from a different type of microarray platform (Affimetrix family) with different set of genes Image processing applicationsGene expression can also be stored in visual forms as images, such as image fluorescence signal from microarray or RNA in situ hybridisation fluorescence or radioactive signal. In several applications, CNNs, known for superior performance in image processing, have shown potential in improving the analysis of these images.In microrarray analysis, detection of a signal and recognition of fluorescence spots can be challenging because of variation in spot size, shape, location or signal intensity, and fluorescence signal intensity often corresponds poorly to gene or sequence expression level. In one application of deep learning techniques to this problem, a CNN was used for microarray image segmentation and demonstrated results in accuracy that resembled baseline approaches in accuracy, but with easier training and fewer requirements of computational sources Another opportunity for the application of CNNs to image-based gene expression data has been RNA in situ hybridization, a tedious technique that enables localization and visualization of gene expression in a group of cells, tissue slice, or whole organism when such manipulations are allowed. This method facilitates powerful longitudinal studies that illustrate changes in expression patterns during development. It was implemented for construction of the detailed Allen Developing Mouse Brain Atlas, which contains expression maps for more than 2000 genes, each illustrated in multiple brain sections. In the past, these were annotated manually, which was time-consuming, expensive and at times inaccurate. Recently, however, Zeng et al. performed automatics annotation using a deep pre-trained CNN SplicingYet another area of application of deep learning is splicing. Splicing is one of the major factors that provides biological diversity of proteins in eukaryotic organisms; moreover, recent studies show a connection between \"splicing code\" and various diseases Non-coding RNANon-coding RNA is another problem in biology that will require sophisticated computational methods like deep learning. Non-coding RNAs are surprisingly important, involved in the regulation of transcription, translation, and epigenetics Expression Quantitative Trait Loci analysisFinally, there is potential for deep learning in quantitative trait loci (QTL) analysis. QTL analysis identifies genetic loci containing polymorphisms that contribute to phenotypic variation of complex, polygenic traits (e.g. body weight, drug response, immune response). One such \"trait\" showing genetic variation is expression, or transcript abundance for any given gene in a given tissue and/or condition. Expression QTL (eQTL) are loci that influence genetic variation in transcript abundance. eQTL analysis has led to insights into the regulation of human gene expression, but is faced with a number of challenges. eQTL that regulate expression locally (cis-eQTL) are relatively easy to identify with a limited number of statistical tests, but loci that regulate expression of genes elsewhere in the genome (trans-eQTL) are more difficult to detect. A deep learning approach, MASSQTL, was recently implemented to solve the problem of trans-eQTL prediction using various encoded biological features, such as physical Protein Interaction Networks, gene annotation, evolutionary conservation, local sequence information and different functional elements from the ENCODE project ProteomicsCompared to transcriptomics, proteomics is a much less developed area of research, with data still scarce and fewer computational approaches available for analysis. The lack of human proteomics data and difficulty of translation of results from model organisms to humans also complicates analysis, even if similar signalencoding and transmitting mechanisms are in place.Deep learning can benefit proteomics in several ways, as some approaches do not require a large number of training cases as other machine learning algorithms. Other strengths of deep learning methods are that they build hierarchical representations of data and learn general features from complex interactions, thus benefiting proteomic and network analysis of proteins. For example, Bimodal deep belief networks have been used to predict the human cellular response to stimuli from rats' cellular response to the same stimuli, using phosphorylation data Structural Biology and ChemistryStructural biology includes analysis of protein folding, protein dynamics, molecular modeling, and drug design. Secondary and tertiary structures are important features of proteins and RNA molecules. For proteins, proper structure determination is important for enzymatic function prediction, formation of catalytic centers and substrate binding, immune function (antigen binding), transcriptional factors (DNA binding), and post-transcriptional modifications (RNA binding). Loss of proper structure leads to loss of function and, in some cases, aggregation of abnormal proteins which can lead to neurodegenerative diseases such as Alzheimer's or Parkinson's Comparative modeling, based on compound homology, is one possible way to predict protein secondary structure, but is limited by the amount of existing well-annotated compounds. Machine learning de novo prediction, on the other hand, is based on recognized patterns of compounds with well-known structure, but has not been accurate enough to be of practical use. Employing deep learning methods de novo has improved structure prediction by using protein sequencing data The constancy of three-dimensional structure is also functionally important. However, there are several proteins with no unique structure that are involved in fundamental biological processes, such as control of cell cycle, regulation of gene expression, and molecular signal transmission. Moreover, recent studies show markedness of some disordered proteins Many parameters distinguish IDP/IDR from structured proteins, thus making the prediction process challenging. This issue can be resolved using deep learning algorithms that are able to consider a wide variety of features. In 2013, Eickholt and Cheng published a sequence-based deep learning predictor, DNdisorder, that improved prediction of disordered proteins compared to state-of-the-art predictors Another important class of proteins is that of RNA-binding proteins, which bind single or double stranded RNA. These proteins participate in all kinds of post-transcriptional modifications of RNA: splicing, editing, regulation of translation (protein synthesis), and polyadenylation. RNA molecules form different types of arms and loops, secondary and tertiary structures required for recognition and formation of the connection between RNA and protein. Secondary and tertiary structures of RNA are predictable and have been used for modeling structural binding preferences and predicting binding sites of RBPs by applying deep belief networks Drug Discovery and RepurposingComputational drug biology and biochemistry are broadly applied on almost every stage in drug discovery, development and repurposing. A huge number of computational approaches for in silico drug discovery and target extension have been developed worldwide by different research groups and companies in past decades to reduce time and resource consumption. While many methods exist One of the important tasks in drug discovery is prediction of drug-target interaction. Targets (proteins) often have one or more binding sites with substrates or regulatory molecules; these can be used for building prediction models. However, including other protein sites could bring bias into the analysis. The ability of pairwise input neural network (PINN) to accept two vectors with features obtained both from protein sequences and target profiles was used by Wang et al. to compute target-ligand interaction Drug discovery and evaluation is expensive, time-consuming, and risky; computational approaches and various prediction algorithms can help reduce risks and save resources. One potential risk is toxicity; for example, liver toxicity (hepatotoxicity) is a frequent cause of removal of a drug from production. Prediction of hepatotoxicity with computational approaches could help to avoid likely hepatotoxic drugs. Using deep learning, it is possible to effectively determine compound toxicity with raw chemical structure without requiring a complex encoding process Multi-platform data (multi-omics)The ability to work with multi-platform data is a major advantage of deep learning algorithms. Since biological systems are complex, with multiple interrelated elements, the systems level integration of genomics, epigenomics, and transcriptomics data is key to extracting the most valid, biologically meaningful results. The integration process is not computationally trivial, but the payoff is a gain in biomarker specificity and sensitivity over single-source approaches.One of the major fields in computational biology that requires analysis of combined data is computational epigenetics. Only joint analysis of genome, transcriptome, methylome characteristics and histone modifications provides accurate epigenome predictions.Several investigators have developed deep learning approaches useful in analyzing data from multiple sources (Table Cancer is a broad name for a group of heterogeneous diseases, some of which are caused by genetic mutations, and as such cancer classification using multi-platform data could shed light on underlying pathology. Liang et al. developed a deep belief network model with multi-platform data for clustering cancer patients One advantage of this approach is that deep belief networks do not require data with a normal distribution, as other clustering algorithms, and genetic (biological) data is not normally distributed Finally, from the point of view of Natural Language Processing, deep learning could be very useful in navigating through the immense unstructured (research publications and patents) and structured data (knowledge annotated graphs, like Gene Ontology Challenges and Limitations of Deep Learning SystemsWhile deep learning algorithms have demonstrated advantages in recognition, classification and feature extraction from complex and noisy data, these methods have also some limitations that should be considered compared with traditional machine learning methods. These include:1.The \"black box\" problem 2.Overfitting and the need for large training datasets 3.The selection problem in choosing a type of DNN 4.The high computational costs of training 1. The \"black box.\" One of the major limitations of deep learning in biological context relates to quality control and interpretation. Most DNNs are \"black boxes\" that learn by simple asociations and co-occurrences. They have limited means with which to interpret the representations, although some, like CNNs, are also very powerful in creating high level representations. When working with image, voice or textual data, developers can rapidly test classification performance and evaluate the quality of the output data. High dimensional biological data, on the other hand, is not easy for humans to interpret and requires additional quality control and interpretation pipelines. DNNs thus lack the transparency and interpretability of other methods and are unable to uncover complex causal and structural relationships common in biology without human input.The need for large datasets.Another limitation is the requirement of large training data sets that may not be readily available. Many commonly-used machine learning methods outperform DNNs where experimental data is scarce. When datasets are not sufficiently large, one of the major challenges with training DNNs is dealing with the risk of overfitting, i.e., when training error is low but the test error is high, thus the model fails to learn a proper generalization of knowledge contained in data. There are ways to regularize the DNN, such as dropout, i.e., temporal removal of a random subset of units with their connections, which reduces conspiracy between units, but overfitting is often still a threat within small biological data sets, especially with unbiased and noisy data.The selection problem.With many types of DNNs available, task-appropriate selection is not always straightforward. While there are some tools to aid selection, such as hyperparameter optimization techniques 4. The computation costs. Finally, while DNNs require few computational resources when trained, the training process is usually computationally-intensive, time consuming and often requires access to and programming knowledge for graphics processing units (GPU). Tensorflow (http://tensorflow.org), a recent framework open sourced by Google and inspired in Theano, greatly simplifies the implementation and debugging of deep architectures. This and other frameworks are summarized in Supp. B.Thus despite the current enthusiasm for deep learning, traditional models still play an important role inomics, especially when the amount of data is not very large or when the number of variables is large and nonnumeric support vector machines or ensemble methods, like Random Forest, may be a better option.Discussion and future perspectivesDNNs have the potential to benefit a wide range of biological research applications including annotation, semantic linking and even interpretation of complex biological data in areas including biomarker development, drug discovery, drug repurposing and clinical recommendations. One area where DNNs can have major impact is transcriptomic data analysis. Several million samples of human transcriptomic data are available from almost two decades of experiments, residing in multiple repositories, including GEO, ArrayExpress, ENCODE and TCGA, as well as cell line data available from the Broad Institute Connectivity Map and LINCS projects (Figure Here, we covered several applications of deep learning methods, including CNN, deep belief networks, autoencoders, and recurrent neural networks, involving a variety of analytical tasks in biomedicine. A combination of these approaches using supervised, unsupervised and reinforcement learning applied to disparate types of biomedical data already play a key role in understanding fundamental biomedical processes and have helped lead to the development of personalized, or stratified, medicine. CNNs outperformed baseline approaches not only in classical deep learning applications, such as image recognition (microarray segmentation and gene expression annotation), but also in annotation of sequence polymorphism data in tools such as DeepSEA, DeepBind and Basset. The above mentioned tools are successfully used as frameworks in the public domain (DeepSEA-http://deepsea.princeton.edu/job/analysis/create/ or Bassethttps://github.com/davek44/Basset). Deep belief networks, as a universal technique that can also be configured to avoid overfitting, can be applied to various types of biomedical data (from structural to gene expression). Auto-encoders, with their ability to learn flexible and rich representations of data, can be successfully used for feature extraction and dimensionality reduction and as independent classifiers of gene expression data. Restricted boltzmann machines, perhaps due to relative ease in training, have been applied primarily in early works with structural data. DNN can deliver substantial improvements compared to traditional machine learning methods, e.g. nearest neighbours, boosted trees, or support vector machines, but usually require much larger data sets-sometimes surpassing tens of thousands of samples-and availability of high-performance GPU-based computational resources. They also require a level of experience in computer science and mathematics not commonly available in organizations working with biological data. Another factor impeding mainstream use of DNNs in biomedicine is communication of research results.One of the aims of this review was to introduce the concept of deep learning, a new topic for many biologists, and plant a seed of interest for those working with this data as to how deep learning may offer solutions to some of their data analysis problems. We see a lot of value in increased collaboration between biologists and the computational biology community or larger deep learning community. For readers from the machine-learning community, we aimed to review existing problems with biological data and open a discussion about the many opportunities biological data offers for application of this approach, including obstacles to overcome and many possible directions moving forward.Conclusions", "conclusions": "In conclusion, the massive scale of modern biological data is simply too large and complex for human-led analysis. Machine learning and particularly deep learning, combined with human expertise, is the only approach that stands a chance at fully integrating the multiple, huge repositories of multi-platform data. Deep learning has enabled humans to do what was previously unimagined-image recognition with millions of inputs, voice recognition and speech automation that approaches human abilities. While deep learning and particularly unsupervised deep learning is still in its infancy, particularly in biological applications, initial studies support it as a promising approach that, while not free of limitations and challenges in implementation, may overcome some of the problems with biological data and lead to new insights into the millions of indirect and interconnected mechanisms and pathways that underlie disease. ", "SDG": [3]}, "application_of_a_fuzzy_logic_control_system_for_continuous_anaerobic_digestion_of_low_buffered_acidic_energy_crops_as_mono_substrate": {"name": "Application of a Fuzzy Logic Control System for Continuous Anaerobic Digestion of Low Buffered, Acidic Energy Crops as Mono-Substrate", "abstract": " A fuzzy logic control (FLC) system was developed at the Hamburg University of Applied Sciences (HAW Hamburg) for operation of biogas reactors running on energy crops. Three commercially available measuring parameters, namely pH, the methane (CH 4 ) content, and the specific gas production rate (spec. GPR \u00bc m 3 /kg VS/day) were included. The objective was to avoid stabilization of pH with use of buffering supplements, like lime or manure. The developed FLC system can cover most of all applications, such as a careful start-up process and a gentle recovery strategy after a severe reactor failure, also enabling a process with a high organic loading rate (OLR) and a low hydraulic retention time (HRT), that is, a high throughput anaerobic digestion process with a stable pH and CH 4 content. A precondition for a high load process was the concept of interval feeding, for example, with 8 h of interval. The FLC system was proved to be reliable during the long term fermentation studies over 3 years in one-stage, completely stirred tank reactors (CSTR) with acidic beet silage as mono-input (pH 3.3-3.4). During fermentation of the fodder beet silage (FBS), a stable HRT of 6.0 days with an OLR of up to 15 kg VS/m 3 /day and a volumetric GPR of 9 m 3 /m 3 /day could be reached. The FLC enabled an automatic recovery of the digester after two induced severe reactor failures. In another attempt to prove the feasibility of the FLC, substrate FBS was changed to sugar beet silage (SBS), which had a substantially lower buffering capacity than that of the FBS. With SBS, the FLC accomplished a stable fermentation at a pH level between 6.5 and 6.6, and a volatile fatty acid level (VFA) below 500 mg/L, but the FLC had to interact and to change the substrate dosage permanently. In a further experiment, the reactor temperature was increased from 41 to 508C. Concomitantly, the specific GPR, pH and CH 4 dropped down. Finally, the FLC automatically enabled a complete recovery in 16 days.", "keywords": "anaerobic digestion,automation,beet silage,biogas,biomass,fuzzy logic control,monitoring", "introduction": "Ideal energy crops should be converted to biogas (methane) and fertilizer without use of chemicals or manure. A study of the Federal Agricultural Research Centre  has shown that at the end of the year 2007 approximately 4,000 biogas plants with a total installed electrical capacity of almost 1,300 MW were in operation in Germany (FAL, Braunschweig)(Weiland, 2006;. It is assumed that nearly 15-20% of these biogas plants in Germany are operated without the addition of manure personal communication). Acidic silage (Weiland, 2007)) is a very poor buffered substrate, with an alkalinity range of 2,500 to 6,000 mg CaCO 3 -equivalents/L. This could be regarded as a model for anaerobic digestion of other low buffered crops with a nitrogen content below 1%, like grain (wheat) or maize silage in the absence of manure. Manure is not only a source of nutrients (stabilizing the anaerobic digestion of energy crops with an extreme C/N ratio of >100), but it is also a good buffering agent with up to 21,000 mg CaCO 3 -equivalents/L.(pH 3.3-3.4There also exists the assumption in literature that the anaerobic digestion with acidic substrates having an alkalinity below 6,000 mg CaCO 3 /L should not be possible . Consequently, it was a great challenge to digest beet silage as a mono-substrate (Speece, 1996)(Scherer, 2005(Scherer, , 2006;;. It should be a precondition for extensive use of energy crops in areas without sufficient stock farming. A common approach to lower the need of manure is to use a recirculation of the effluent to improve the buffer capacity, as practiced in most of the two stage biogas plants of farmers in Germany with maize as monosubstrate Scherer et al., 2003)(Banks and Humphreys, 1998;. However, all these biogas plants still need additionally calcium oxide (lime) or occasionally intermittently manure as a buffering agent for continuous digestion Jarvis et al., 1997).(Barber, 1978)The FLC technique has been validated as a control strategy for anaerobic treatment of industrial wastewaters (Carrasco et al., 2002;Estaben et al., 1997;Guwy et al., 1997;Holubar et al., 2003;Liu et al., 2004;Mu \u00a8ller et al., 1997;Murnleitner et al., 2002;Pullammanappallil et al., 1998;Punal et al., 2001;. However, there exists relatively less information about the applications of FLC to operate and monitor the anaerobic digestion of particulate organic wastes and/or energy crops Steyer et al., 1997)(Boscolo et al., 1993;. Besides, lab-scale long-term applications of FLC for anaerobic digestion of solid matter lasting more than half a year are also seldom.Holubar et al., 2003)The objective of this study was to establish a reliable and simple FLC for anaerobic digestion of insufficiently buffered energy crops. As the most striking example, the acidic beet silage (pH % 3.3-3.4) was chosen. Only 3 widespread process measuring parameters were applied; pH, CH 4 content and the spec. GPR. As a specialty, the spec. GPR was used instead of the vol. GPR. In this article, the presented FLC is used for the first time for a continuously driven longterm fermentation of renewable energy crops (beet silage) lasting for more than 560 days under this unique Fuzzy control strategy.", "body": "Ideal energy crops should be converted to biogas (methane) and fertilizer without use of chemicals or manure. A study of the Federal Agricultural Research Centre There also exists the assumption in literature that the anaerobic digestion with acidic substrates having an alkalinity below 6,000 mg CaCO 3 /L should not be possible The FLC technique has been validated as a control strategy for anaerobic treatment of industrial wastewaters The objective of this study was to establish a reliable and simple FLC for anaerobic digestion of insufficiently buffered energy crops. As the most striking example, the acidic beet silage (pH % 3.3-3.4) was chosen. Only 3 widespread process measuring parameters were applied; pH, CH 4 content and the spec. GPR. As a specialty, the spec. GPR was used instead of the vol. GPR. In this article, the presented FLC is used for the first time for a continuously driven longterm fermentation of renewable energy crops (beet silage) lasting for more than 560 days under this unique Fuzzy control strategy.Materials and MethodsReactors and SubstrateLaboratory-scale, single-stage continuous anaerobic digesters were used for the experiments, which were constructed in the Research Center of Lifetec Process Engineering at HAW Hamburg Biogas production was continuously measured online, using a Milligascounter 1 type MGC-10 (Ritter GmbH, Bochum, Germany). Methane (CH 4 ) and carbon dioxide (CO 2 ) compositions (v/v) were measured online, using infrared sensors (BlueSens GmbH, Gaz Analyzer, Herten, Germany). The volume of gas was corrected for standard pressure and 08C (STP, e.g., generally 9% lower values than measured). The temperature, pH and the redox potential (ORP) were also continuously measured online every 2 min (averaged per 8 h). A simultaneous, FLC processed fermentation was performed with the same cooled substrate tank, to simultaneously test different temperatures in parallel The OLR was individually computed by the FLC program every 8 h/day, created by the fuzzy rules. A fuzzy tool of LabView was used to create the fuzzy rules. The fuzzy tool of LabView generated the new OLR added into relative percentage, individually after each feeding cycle. The percentage of substrate addition was compared by the FLC with the substrate addition of the last cycle. The new addition was called the new OLR and was adjusted in a selected frame, for example, 10-120% of the last feeding. This adjustment influenced the velocity of adaptation to a feeding situation, fore example, the start-up period should be performed with another adjustment than the recovery after a reactor failure.The SBS (without the leaves) and FBS (with the leaves and tops) were used as the mono-substrates. The general characteristics of the beet silages are given in Table Analytical MethodsThe volatile suspended solids (VSS) content was measured according to DIN Methods Fuzzy Logic Control (FLC)In order to make biogas plants for energy crops more reliable, a feedback controller for the whole process should be connected to the digester system. One of such a process controller is the Fuzzy based FLC as firstly proposed by The FLC can be explained by a scheme in five different stages (Fig. The FLC, which was developed at HAW Hamburg, determined the OLR, depending on the pH, the spec. GPR, together with CH 4 content The Set of Ranges for the Beet SilageBesides the fuzzy rules, the control ranges of the connected sensors (CH 4 , pH, spec. GPR) should be separately set. The basic array of the control parameters should be provided by the FLC rules, but the range settings should additionally be achieved, which define the input areas against the linguistic terms and are part of the fuzzy tool of LabView. For example, the low range for the CH 4 content in biogas is defined to be between 0% and 60% (Fig. The substrate feeding has to be strongly reduced, if this range is reached. The medium ranges could be adjusted to be between 60% and 70% of CH 4 , in which the OLR remains constant. The low critical control ranges for the CH 4 content in the biogas are a transient status between the high and the medium critical control ranges. In this case, the FLC should mildly decrease the OLR percentage, in relation to the previous substrate dosage.In the same manner as shown for the CH 4 content, the control ranges for the spec. GPR and pH have to be also selected. The ranges can obviously be changed for every different experiment.ResultsThe anaerobic mono-input digestion of FBS and SBS runs for more than 6 years, without any break, at both mesophilic (428C) and thermophilic (608C) conditions, at HAW Hamburg. After a yeast invasion the temperature of 428C was selected instead of 378C, which was also later recommended for anaerobic digestion of energy crops During this long period of continuous fermentation, no manure or sewage sludge was added to the reactors, so that the initial ammonium level coming from the inoculum decreased from 2,000 to 250-300 mg/L. Thereby the direct influence of manure on the GPR could be excluded. This process of washing out the manure lasted nearly 2 years and led to a steady-state condition The history of biogas CSTR with automatically FLC is shown in Figure The history of the same digester for 84 days later is given in Figure Further history of the same digester is summarized in Figure A new critical reactor performance is displayed on reactor day 1,028 in Figure The recorded process parameters of the single-stage CSTR from the reactor operation period 1,102-1,142 days are shown in Figure In Figure In further experiments, it was observed that the declined CH 4 content of the produced biogas was typical for less ensilaged sugar beets (without the top and the leaves). The final pH of SBS was 3.4, nearly the same for FBS (pH 3.3), but the sugar content of the mono-substrate was nearly doubled from about 75 g/kg wet matter to 20-180 g/kg wet matter, limiting the silage process and reducing the concomitant CO 2 loss. Therefore, the SBS revealed significant lower methane content than fodder beets with the top. However, the HRT also influenced the CH 4 content of the biogas. With an HRT of 27 days (start-up phase), the methane content was about 58-60% in the reactor period until 710 days During the whole test period of more than 2,000 days, the content of the organic matter (VSS) of the CSTR was regularly measured. As the reactor was completely stirred, the organic matter content reflected the microbial biomass in the reactor. Although the HRT was drastically reduced from 26 to 5.4-7.6 days, by the automatic FLC starting on reactor day 757, the organic dry weight of the CSTR increased to 0.93% on reactor day 819, and later on, to 1.93% (reactor day 1,066, with the same substrate FBS).In the last experiment, the reactor temperature was increased from 41 to 508C, in 18C steps per day (Fig. DiscussionThe FLC, which was developed at HAW Hamburg, determined the OLR, only depending on pH, the spec. GPR (as m 3 /kg VS/day) and the CH 4 content. The main previous work was to find out how many measuring parameters and fuzzy rules were necessary to obtain a stable and reliable fermentation The spec. GPR is the second input parameter, which outlines the biogas production rate in relation to feeding volume. For wastewater treatment it is difficult to relate the GPR to the input as generally the incoming water varies, but for more or less uniform fermentation of renewable biomass, the sum of dissolved volatile solids and volatile suspended solids can be determined as total organic carbon (TOC) of one charge or harvest for the input The CH 4 content is the third input parameter of the developed FLC. Yeasts and methanogens should be differentiated, since both produce biogas. If yeasts are present, the GPR could lie in the right range, but the methane content can be too low (Fig. Sometimes the redox value is supposed to be a controlling parameter A hydrogen sensor was used as an early warning system in a Fuzzy network for anaerobic wastewater treatment The FLC enabled a high throughput anaerobic digestion process for the used FBS (diluted). A maximum OLR of 15.6 kg VS/m 3 /day, at a short HRT of 6.0 days, and with a GPR of 9 m 3 /m 3 /day could be reached (Fig. Conclusions", "conclusions": "This article presents for the first time a detailed long-term study about a FLC system developed for anaerobic digestion of energy crops with poor buffering capacity as mono-input. The lab-scale experiments were carried out with fully automated and completely mixed reactors, fed every 8 h a day with extremely sour beet silage (pH 3.3-3.4), to create a FLC strategy. Three commercially available measuring parameters (pH, methane content-CH 4 and the spec. GPR) can cover most of all applications, including a careful startup process and a gentle recovery strategy after a severe reactor fall, as shown in this paper for two reactor failures and an attempt to cause a third reactor fall. The feasibility of the FLC was also proved by an artificially induced temperature shock situation. The FLC with a feeding cycle of 3 times per day also enabled a process with a high OLR and a short HRT (high throughput anaerobic digestion process). The developed FLC system did not need any complicated detector (e.g. for propionate) or self-learning network or a special mathematical model. Similar to an alarm signal, it was found sufficient to preset and to limit the hydraulic retention time by minimum and maximum amounts of substrate per feeding cycle in the fuzzy tool base. ", "SDG": [3]}, "artificial_intelligence_based_prediction_models_for_environmental_engineering": {"name": "INTUITIONISTIC FUZZY HOPFIELD NEURAL NETWORK AND ITS STABILITY *", "abstract": " Intuitionistic fuzzy sets (IFSs) are generalization of fuzzy sets by adding an additional attribute parameter called non-membership degree. In this paper, a max-min intuitionistic fuzzy Hopfield neural network (IFHNN) is proposed by combining IFSs with Hopfield neural networks. The stability of IFHNN is investigated. It is shown that for any given weight matrix and any given initial intuitionistic fuzzy pattern, the iteration process of IFHNN converges to a limit cycle. Furthermore, under suitable extra conditions, it converges to a stable point within finite iterations. Finally, a kind of Lyapunov stability of the stable points of IFHNN is proved, which means that if the initial state of the network is close enough to a stable point, then the network states will remain in a small neighborhood of the stable point. These stability results indicate the convergence of memory process of IFHNN. A numerical example is also provided to show the effectiveness of the Lyapunov stability of IFHNN", "keywords": "Intuitionistic fuzzy sets,intuitionistic fuzzy Hopfield neural network,limit cycle,stable point,Lyapunov stability", "introduction": "In [1][2], Atanassov extends Zadeh's fuzzy sets to intuitionistic fuzzy sets (IFSs) by adding an additional attribute parameter called non-membership degree. IFSs are shown to be superior to fuzzy sets in, for example, semantic expression and inference ability [3]. Various theoretical and applied researches have been performed on IFSs, such as fuzzy topology [4][5][6], multi-criteria fuzzy decision-making [7][8][9], clustering [10][11,, medical diagnosis 12][13, and pattern recognition 14][15][16].[17]Fuzzy neural networks combine fuzzy concepts and fuzzy inference rules with the architecture and learning of neural networks, and have been successfully applied in system identification , intelligent control [18][19][20], pattern classification [21] and expert system [22][23,, etc. Since IFSs have proved to be more powerful to deal with vagueness and uncertainty than fuzzy sets, some researchers have also investigated the combination of IFSs and artificial neural networks 24][25][26][27][28]. In [29], an intuitionistic fuzzy feedforward neural network (IFFFNN) was constructed by combining feedforward neural networks and intuitionistic fuzzy logic, and some operations and two types of transferring functions involved in the working process of IFFFNN were introduced. In this paper, similar to fuzzy Hopfield neural networks [29][30][31][32], a max-min intuitionistic fuzzy Hopfield neural network (IFHNN) is proposed by combining IFSs with Hopfield neural networks. The stability of IFHNN is investigated. It is shown that for any given weight matrix and any given initial intuitionistic fuzzy pattern, the iteration process of IFHNN converges to a limit cycle. Furthermore, under suitable extra conditions, it converges to a stable point within finite iterations. Finally, a kind of Lyapunov stability of the stable point of IFHNN is proved, which means that if the initial state of the network is close enough to a stable point, then the network states will remain in a small neighborhood of the stable point. These stability results indicate the convergence of memory process of IFHNN. A numerical example is also provided to show the effectiveness of the Lyapunov stability of IFHNN The rest of this paper is organized as follows. Some basic concepts of IFSs are collected in Section 2. IFHNN is defined and described in Section 3. A few stability results of IFHNN are given in Section 4. Section 5 presents a numerical example. Some brief conclusions are drawn in Section 6. Finally, proofs of the stability results are provided in an appendix.[33]", "body": "In Fuzzy neural networks combine fuzzy concepts and fuzzy inference rules with the architecture and learning of neural networks, and have been successfully applied in system identification PreliminariesAtanassov generalizes Zadeh's fuzzy sets to IFSs: Definition 1 [1] Let X be a given set. An intuitionistic fuzzy set A is an object having the formwhere the functions \u00b5 A (x) : X \u2192 [0, 1] and \u03b3 A (x) : X \u2192 [0, 1] define the membership degree and the non-membership degree respectively of the element x \u2208 X to the set A, and for every x \u2208 X, 0Specifically, when the given set X is finite, say, X = {x 1 , x 2 , . . . , x m }, IFS A can be expressed as a so-called intuitionistic fuzzy vector:Definition 2 [2] Let> |x \u2208 X} be two IFSs. Then, their conjunction, union and complement are defined respectively as 1)Long Li, Jie Yang, Wei Wu: Intuitionistic fuzzy Hopfield neural. . .2)Definition 3 [34, 35] Let X and Y be two given sets. An intuitionistic fuzzy relation R from X to Y is an IFS of X\u00d7Y characterized by the membership function \u00b5 R (x, y) and the non-membership function \u03b3 R (x, y), denoted bywhere the functions \u00b5 R (x, y) :In particular, when the given sets X and Y are finite, say, X = {x 1 , x 2 , . . . , x m } and Y = {y 1 , y 2 , . . . , y n }, the intuitionistic fuzzy relation R from X to Y can be denoted by an intuitionistic fuzzy matrix R = (r ij ) m\u00d7n , where r ij =< \u00b5 R (x i , y j ), \u03b3 R (x i , y j )>. Some operations and properties of intuitionistic fuzzy matrixes are defined below (cf. Definition 5 Let R = (< \u00b5 Rij , \u03b3 Rij >) m\u00d7n and S = (< \u00b5 Sij , \u03b3 Sij >) n\u00d7l be two intuitionistic fuzzy matrixes. The max-min composite operation \"\u2022\" of R and S is defined byProperty 1 Let R m\u00d7n , S n\u00d7l and T l\u00d7s be intuitionistic fuzzy matrixes. Then the max-min composite operation of intuitionistic fuzzy matrixes satisfies the associative law, i.e., (R \u2022 S)Intuitionistic Fuzzy Hopfield Neural NetworkIntuitionistic fuzzy Hopfield neural network (IFHNN) is a combination of IFSs and Hopfield neural networks. The basic processing units of IFHNN are intuitionistic fuzzy units, i.e., the input, output and weight signals are all IFSs. In this study, similar to fuzzy Hopfield neural networks then, the network iteration process is as follows:whereand t = 1, 2, . . . , are the discrete time steps. The network will iterate repeatedly according to (1) until a steady state is reached. The final output pattern X(\u221e) is taken as an association of the input pattern X(0).Stability ResultsDefinition 7 [37] If a sequence of states {P 1 , P 2 , . . . , P s } is generated by a feedback network with state transition operator F such that F(P 1 ) = P 2 , F(P 2 ) = P 3 , . . . , F(P k ) = P k+1 , . . . , F(P s ) = P 1 , and there does not exist a subsequence with the same property in this sequence, then this sequence is called a limit cycle and s is called the length of the limit cycle.Long Li, Jie Yang, Wei Wu: Intuitionistic fuzzy Hopfield neural. . . Definition 8 [37] If there exists a state P of a feedback network such that, for the state transition operator F of the network, F(P ) = P holds, then P is called a stable point of the network.Definition 9 [38-40] Let A = (< \u00b5 A1 , \u03b3 A1 >, < \u00b5 A2 , \u03b3 A2 >, . . . , < \u00b5 An , \u03b3 An >) and B = (< \u00b5 B1 , \u03b3 B1 >, < \u00b5 B2 , \u03b3 B2 >, . . . , < \u00b5 Bn , \u03b3 Bn >) be two intuitionistic fuzzy patterns. Define the Hamming distance between A and B asDefinition 10 [30, 37] Suppose that the intuitionistic fuzzy pattern P is a stable point of the networks. P is said to be Lyapunov stable, if for any \u03b5 > 0, there exists \u03b4 > 0, such that for every initial intuitionistic fuzzy pattern X satisfying H(X, P ) < \u03b4, H(X(t), P ) < \u03b5 holds for t = 1, 2, . . ., where X(t) is the t-th iteration state of the network. Now, we are ready to present our main results. Some comments on these theorems can be found in the next section, and the proofs are postponed to the Appendix.Theorem 1 For any given intuitionistic fuzzy weight matrix W and any given initial intuitionistic fuzzy pattern, the iteration process of IFHNN (1) converges to a limit cycle.Theorem 2 Suppose W is an intuitionistic fuzzy weight matrix of IFHNN with n units. Then, the following statements hold:(i) If W \u2286 W 2 , then the iteration process of IFHNN (1) converges to a stable point within finite iterations.(ii) If W is reflexive, then the iteration process of IFHNN (1) converges to a stable point within at most n \u2212 1 iterations.Theorem 3 Suppose intuitionistic fuzzy pattern P is a stable point of IFHNN A Numerical ExampleIn this section, an illustrative example is given to show the effectiveness of the Lyapunov stability of IFHNN. Suppose the intuitionistic fuzzy weight matrix of IFHNN isIt is easy to verify that W \u2286 W 2 . For the initial intuitionistic fuzzy pattern X(0) = (< 0.2, 0.7 >, < 0.5, 0.2 >, < 0.3, 0.4 >, < 0.7, 0.1 >, < 0.4, 0.5 >) , the iteration process of IFHNN (1) converges to a stable point X = (< 0.5, 0.2 >, < 0.5, 0.2 >, < 0.5, 0.3 >, < 0.6, 0.2 >, < 0.5, 0.2 >) at the third step.Next, we consider the Lyapunov stability of the stable point X. For this purpose, we add a random noise in [\u22120.001, 0.001] to the stable point X and end up with a new initial pattern X(0) = (< 0.5009, 0.2007 >, < 0.5001, 0.1995 >, < 0.4993, 0.3006 >, < 0.5993, 0.1995 >, < 0.4995, 0.2009 >).Then, the iteration process of IFHNN (1) converges to X = (< 0.5009, 0.2000 >, < 0.5001, 0.1995 >, < 0.5000, 0.3000 >, < 0.5993, 0.2000 >, < 0.5001, 0.2000 >)at the second step. This shows that when the initial state of the network is close enough to a stable point, the network states remain in a small neighborhood of the stable point.Conclusion", "conclusions": "A max-min intuitionistic fuzzy Hopfield neural network (IFHNN) is proposed by combining IFSs with Hopfield neural networks. In addition, the stability of IFHNN is investigated. It is shown that the iteration process of IFHNN always converges to a limit cycle for any given intuitionistic fuzzy weight matrix W and any given initial intuitionistic fuzzy pattern. In particular, it converges to a stable point within finite iterations if W \u2286 W 2 , and even within n \u2212 1 iterations if W is reflexive, where n is the number of the network units. Finally, a kind of Lyapunov stability of the stable point of IFHNN is proved, which means that if the initial state of the network is close enough to a stable point, then the network states will remain in a small neighborhood of the stable point. These stability results indicate the convergence of memory process of IFHNN. Our work in this paper is preliminary. Investigation on more profound properties and applications of IFHNN might be promising. For instance, in comparison with ordinary Hopfield neural networks, one may consider the following problems: 1) Determine the network weight matrix by using given training patterns. 2) Construct a functional such that the state sequence of the network is a minimization sequence of the functional. 3) Prove more profound convergence theorems of the iteration process. 4) Find practical applications of the network. that the membership degree and non-membership degree of every element of W k , for k = 1, 2, . . ., are taken from the set M and Q, respectively. Therefore, there are at most finite different matrixes in the matrix sequence {W k |k = 1, 2, . . . , }, which means that identical matrixes will appear in the matrix sequence {W k |k = 1, 2, . . . , } after finite composite operations. Thus, there exist two positive integers k 0 and k 1 , such that W k 0 = W k 1 . Assume without loss of generality that k 0 \u2264 k 1 . Then, the matrix sequence {W k |k = 1, 2, . . . , } converges to the limit cycle {W k0 , W k0+1 , . . . , W k1\u22121 }. Thus, for any initial intuitionistic fuzzy pattern X(0), we havei.e., the iteration process of IFHNN (1) converges to the limit cycle {X(k 0 ), X(k 0 + 1), . . . , X(k 1 \u2212 1)}. Theorem 1 is thus proved. Now we are in a position to present two lemmas to be used in our proofs of Theorem 2 and Theorem 3.for any i, j. Thus, we have for any i, jandThe combination of (2), (3) and Definition 4 leads toLemma 2 Assume that h > 0, that a i , b i \u2208 [0, 1], and that |a i \u2212 b i | < h for i = 1, 2, . . . , n. Then, the following two inequalities hold:Proof. Inequality (a) has been shown in Lemma 2.2 in , and the detail of the proof is omitted.[30]Next we prove the inequality (b) by induction on n. The inequality (b) is evidently valid for n = 1. Let us suppose that (b) is valid for n = k, i.e.,We analyze (4) by considering the following four cases.The above discussions result in, forNow we have shown by induction that the inequality (b) always holds. This completes the proof of Lemma 2.", "SDG": [3]}, "artificial_intelligence_for_management_and_control_of_pollution_minimization_and_mitigation_processes": {"name": "Artificial intelligence for management and control of pollution minimization and mitigation processes", "abstract": " The reduction of environmental pollution and the conservation and recycling of natural resources are significant social and environmental concerns. As valuable means for pollution control, minimization and mitigation remain attractive approaches. However, interactive, dynamic and uncertain features are associated with these processes, resulting in difficulties in their management and control. Artificial intelligence (AI) is an effective approach for tackling these complexities. In this study, the recent advancements of AI-based technologies for management and control of pollution minimization and mitigation processes are examined. Literature relevant to the area of application of AI to control and management of pollution minimization and mitigation processes is investigated. Especially, technologies of expert systems, fuzzy logic, and neural networks, which emerge as the most frequently employed approaches for realizing process control, are highlighted. The results not only provide an overview of the updated progress in the study field but also, more importantly, reveal perspectives of research for more effective environmental process control through the AI-aided measures. Several demanding areas for enhanced research efforts are discussed, including issues of data availability and reliability, methodology validity, and system complexity.", "keywords": "Artificial intelligence,Environment,Expert systems,Fuzzy logic,Mitigation,Minimization,Neural networks,Process control", "introduction": "Most of environmental engineering problems are related to a number of factors with multi-source, multi-layer, multi-stage, and multi-objective characteristics. Effective reflection of these complexities is currently an important issue emphasized by many public-sector decision-makers and private industries for sound management and control of pollution minimization and mitigation processes. Previously, many modeling tools have been developed for simulating processes in water/wastewater treatment plants, solid waste incinerators and air pollution control facilities. However, the uncertain, interactive and dynamic features of these processes often lead to difficulties in obtaining desired system performance. Integrated consideration that incorporates a number of uncertain and dynamic components in the study systems within a general framework rather than examining them in isolation is needed for potential improvement .(Rynk, 1992)Artificial intelligence (AI) is an effective approach for tackling the above complexities. For example, the complicated interrelationships among a number of system factors and activities can be explicated through the process of knowledge acquisition. Also, the gap between result generated from detailed modeling efforts and applicability of that result to a practical situation can be filled by building an automated system, allowing incorporation of implicit, and often qualitative considerations deemed crucial by engineers and/or operators. A knowledge-based system can perform trade-off analysis to compare the costs/benefits of economic versus environmental concerns. Besides, the modeling result usually does not satisfactorily address specific issues concerning impacts of a control action. An automated system can investigate the key variables in greater detail and provide more insight into the specific implications of a generalized solution. For effective realtime control, an expert system can provide more insight into the specific implications of a generalized solution and can complement or refine a simulation program .(Liang, 2001)Recently, some applications of AI to real-time control of pollution minimization and mitigation processes have been reported. They demonstrate an emerging area for more extensive studies. The objective of this paper is to examine the recent advancements of AI-based technologies for management and control of pollution minimization and mitigation processes. Literature relevant to the area of application of AI to control and management of pollution minimization and mitigation processes will be investigated. Especially, technologies of expert systems, fuzzy logic, and neural networks, which emerge as the most frequently employed approaches underlying AI for realizing process control, will be highlighted.This paper is organized as follows. Section 2 provides an overview of the related technologies for environmental process control. This is followed by a review of works on the development of expert systems and decision support systems that are critical to process control. Section 4 describes neural networks and their applications to the area of pollution minimization and mitigation; and Section 5 presents works that adopt a hybrid approach to system development and integrate expert systems, neural networks, and fuzzy logic. Section 6 concludes this review study.", "body": "Most of environmental engineering problems are related to a number of factors with multi-source, multi-layer, multi-stage, and multi-objective characteristics. Effective reflection of these complexities is currently an important issue emphasized by many public-sector decision-makers and private industries for sound management and control of pollution minimization and mitigation processes. Previously, many modeling tools have been developed for simulating processes in water/wastewater treatment plants, solid waste incinerators and air pollution control facilities. However, the uncertain, interactive and dynamic features of these processes often lead to difficulties in obtaining desired system performance. Integrated consideration that incorporates a number of uncertain and dynamic components in the study systems within a general framework rather than examining them in isolation is needed for potential improvement Artificial intelligence (AI) is an effective approach for tackling the above complexities. For example, the complicated interrelationships among a number of system factors and activities can be explicated through the process of knowledge acquisition. Also, the gap between result generated from detailed modeling efforts and applicability of that result to a practical situation can be filled by building an automated system, allowing incorporation of implicit, and often qualitative considerations deemed crucial by engineers and/or operators. A knowledge-based system can perform trade-off analysis to compare the costs/benefits of economic versus environmental concerns. Besides, the modeling result usually does not satisfactorily address specific issues concerning impacts of a control action. An automated system can investigate the key variables in greater detail and provide more insight into the specific implications of a generalized solution. For effective realtime control, an expert system can provide more insight into the specific implications of a generalized solution and can complement or refine a simulation program Recently, some applications of AI to real-time control of pollution minimization and mitigation processes have been reported. They demonstrate an emerging area for more extensive studies. The objective of this paper is to examine the recent advancements of AI-based technologies for management and control of pollution minimization and mitigation processes. Literature relevant to the area of application of AI to control and management of pollution minimization and mitigation processes will be investigated. Especially, technologies of expert systems, fuzzy logic, and neural networks, which emerge as the most frequently employed approaches underlying AI for realizing process control, will be highlighted.This paper is organized as follows. Section 2 provides an overview of the related technologies for environmental process control. This is followed by a review of works on the development of expert systems and decision support systems that are critical to process control. Section 4 describes neural networks and their applications to the area of pollution minimization and mitigation; and Section 5 presents works that adopt a hybrid approach to system development and integrate expert systems, neural networks, and fuzzy logic. Section 6 concludes this review study.Overview of AI-based technologies for environmental process controlApplication of AI for controlling an environmental process involves a number of subprocesses that need to be managed or automated. For example, in a pollutionmitigation-plant environment, there are several levels at which to address the problem of management and control. At the lowest level, there are instruments that monitor, sense, and manipulate process variables. The instruments are often connected to a control structure that is capable of implementing a control law. The next level is the supervisory host computer that is usually connected to some control hardware by network communications. The supervisory host computer maintains the applications that are one level above the primary control functions such as the database. The supervisory host computer may in turn be connected to a plant-wide and then the corporate-wide computer systems In the most general terms, AI is the use of computers to emulate the reasoning and decision-making processes of humans Expert systems can emulate human problem solving by representing the expertise in its knowledge base. An expert system usually consists of three major components: a knowledge base, an inference engine, and a working memory. The knowledge base contains facts and heuristics associated with the application domain. The inference engine searches the knowledge base for applicable rules, and applies the rules for solving the problem. The working memory is the repository to store the new information generated as the inference engine searches and selects rules. In addition to the three components, an expert system typically contains other components such as a user interface and explanation facility.Fuzzy logic has emerged as an alternative to classical or binary valued logic in application areas ranging from industrial process control to consumer products to aerospace and bioengineering Neural networks are a computational paradigm modeled on the human brain. The three important similarities to the brain's capabilities are the ability to filter out essential data from a larger set of data containing irrelevant information, the ability to learn from experience, and the ability to generalize from previous experience to predict new outcomes In terms of application to the process industries, both expert systems and fuzzy logic attempt to duplicate the reasoning process of one and several experts in a particular field, while neural networks try to emulate organization of the human brain and its reasoning mechanism. Expert systems can make a significant contribution to processes where knowledge can be expressed linguistically in a set of ''if-then'' rules that are definable. Fuzzy logic is applied in the process industries to control a process that is intrinsically nonlinear and multi-variabled, while neural networks are useful for making sense out of data. Whenever knowledge is felt to be lacking, a neural network can be applied to extract from the data the relationships that are inherent Expert systemsComplexities in pollution minimization and mitigation processes pose particular challenges in control, as they are non-linear, time varying, and often difficult to model. Alternative approaches to traditional control methods have yielded promising results, and expert system is one of these alternatives. Applications of expert systems to diverse fields have been well documented In It assists the operator in setting control parameters for the DAS and established criteria for evaluating process data. The second expert system assesses the status of the process from the data returned by the DAS. It does this at 3-h intervals and more thoroughly at 24-h intervals. It produces a report which summarizes the process status and lists any potential or apparent problems. At the 24h evaluation stage, it automatically adjusts control parameters as necessary.The basic programs perform tasks which are cumbersome for the expert system such as obtaining numerical information, performing calculations, and accessing data files. The control program was tested by supplying it with process data from the computer keyboard and from an electronic instrument panel, which simulates the sensors of the control system. The data were intended to produce predictable results, which could be compared to the control program's results. In nearly all cases, the control program produces results consistent with those expected. Its performance was satisfactory in that all components of the system were activated properly and information was successfully transferred among the programs. Based on the performance and capability of the control program, it was indicated that expert systems could potentially play a beneficial role in control systems for composting processes. Their utility for this purpose depends on the specific needs of each composting facility. Large facilities with inexperienced staff and a reliance on a high level of technology are most likely to benefit from an expert system. The MSS for simulation of water quality consists of two kinds of knowledge stored in the knowledge base: structural modeling knowledge and procedural modeling knowledge. The model base includes the atomic models and coupled models. The simulation engine for the MSS functions as follows. A user initiates the model simulation by sending a message to the coordinator, which triggers the simulation. The coordinator sends a message to the simulation controller, generator, and coupled models and their components or atomic models. The input values are stored in a file in the generator and can represent, for example, various conditions of water quality. The input values are sequentially transmitted from the coordinator through the input/output ports to the coupled model. The outputs are sent from the coupled model to the co-ordinator, which simultaneously transmits them to a log-file as well as sends a message to the simulation controller to update the simulation clock. The MSS is built on the SES, which serves as a generative framework for model representation, retrieval, and manipulation. A modeler provides the system with information regarding the specific problems of the site such as physical and biochemical aspects of the aquatic system. The information is matched with the production rules in the rule base, and recommendations about the proper selection of entities from the entity structure tree are generated. The modeler uses these recommendations during pruning of the entity structure tree. The basic structural elements or atomic models are the primitives. When the retrieved atomic models are linked into a model instance, it is called a coupling process. A coupled model is subsequently simulated after initialization of the input data. However, working together as components of an integrated system, they can combine to make an effective system for operational control of wastewater treatment. Neural networksA number of researchers have been working on modeling of various environmental processes for supporting further process control studies. A model is a useful engineering tool if it is able to predict reaction conditions and rates Neural computing is one of the fastest growing areas of AI. There are two key differences between neural computers and digital computers. First, neural networks are inherently parallel machines and as a result they can solve problems much faster than a serial digital computer. Secondly, many neural nets have the ability to ''learn''. The most frequently used algorithm for neural networks is back propagation. According to The neural networks that involve back propagation, recurrent or radial basis functions often result in long training time, local minima and lack of self-tuning ability in on-line applications. Due to these reasons, Ye et al. (1998) adopted a Bayesian-Gaussian neural network (BGNN) for predicting dynamic behavior of a non-linear single-input single-output system and for predicting the static performance and dynamic behavior of circulating fluidized bed boilers. A comparison of the performance of Bayesian-Gaussian BGNN versus BPNN in terms of both the static performance and dynamic behavior predictions of the two circulating fluidized bed (CFB) boilers is given in the paper. On the positive side, BGNN gives reasonably good predictions and takes less training time than the BPNN. On the negative side, BGNN requires longer execution time than the BPNN does, possibly due to the large topology of BGNN. One disadvantage of BGNN is that problems can occur when the training data set is large, because this network stores all the training data into its topology from the beginning. By contrast, the BPNNs 'compress' the training data set into the connection weights and thresholds. When new training data are introduced, the topology of the BGNN does not grow because of its selftuning ability. On the other hand, when new samples are available to retrain the BPNNs, they must be carefully screened to select the ones that contain innovative information. If a large amount of new samples containing little innovative information were used in retraining, the new samples may overwhelm the existing ones that are smaller in number but which represent crucial characteristics of some aspects of the process, thereby attenuating prediction ability of these networks.Neural networks have been frequently applied for environmental process modeling and control. De Neural-network-based control strategies are also adopted for online applications. The majority of these utilize the multi-layered feed forward network with sigmoidal or hyperbolic activation functions. However, while many online chemical process control applications have been reported in the literature, they are mostly for miniature laboratory-scale equipment only. Syu and Chen (1998) discussed implementation of an online estimation and control system undertaken as part of a response to the environmental regulations introduced in Taiwan in 1998. The system implements a BPNN adaptive controller on a continuous wastewater treatment process. The BPNN adaptive controller regulates pump rates for addition of hydrogen peroxide \u00f0H 2 O 2 \u00de and ferrous chloride to treat the chemical oxygen demands (COD) of the process. After completion of the oxidation process, an anion resin was added to coagulate and settle the suspended particles. The pH value was a major factor for determining the coagulation condition of the suspended particles and a value of 5.0 was considered the best pH for coagulation. An online control system was used to provide the minimum amount of reagents to reach the required COD.The neural network structure consisted of seven input nodes, four hidden nodes and one output node, and was of a time-delayed type. The seven input nodes are COD\u00f0t\u00de; COD\u00f0t \u00c0 3\u00de; CODi\u00f0t \u00c0 3\u00de; CODi\u00f0t \u00c0 2\u00de; CODi\u00f0t \u00c0 1\u00de; H 2 O 2 \u00f0t \u00c0 1\u00de and H 2 O 2 \u00f0t \u00c0 2\u00de; and the only output node was the predicted amount of H 2 O 2 that should be added at current control time. The backpropagation network learns by calculating an error between desired and actual output and propagating this error information back to each node in the network. The neural network was trained in a dynamic mode. For each learning cycle, a moving window of learning data with a fixed size was provided. An analysis of the reaction time, the pH value and concentrations of reagents reveal that a network structure of 7-4-1 and a data window size of 15 gives the best control performance. A review of several applications of neural networks in waste water treatment plants reveal that the process begins with modeling of process variables or the control actions. Internal checks are necessary in the neural network system to compensate for erroneous inputs. This can be implemented as a modular neural network system which checks inputs against measured values, or as an external program that compares input values against the range of input values in the training data set. If a significant deviation from the range of learning data is detected, a flag is raised. A retraining schedule can be established as a periodic check on validity of the neural network.Sefiner (1997) reviewed neural network applications for environmental control of greenhouse gas, and discussed four salient points:(i) Greenhouse gas modeling with neural networks:Traditional greenhouse gas models are based on energy and mass balances, such as quasi-steadystate energy-balance models. While the traditional model is sufficient for design purposes, it has substantial residual error because it oversimplifies the actual system. To address the inadequacy of the traditional model, The dynamic error back-propagation network (EBPN) model was used with a total of 123 data points acquired at 5-min intervals. The data were collected and separated into the training set and the test set. The former was used to update the network weights, the latter was used to evaluate the generalization ability of the network model. The input layer of EBPN consisted of four nodes representing the current and lagged values of the process input and output. The process model was used and tested within the framework of neural model prediction control (NMPC). The advantage of the NMPC methodology is that all the control computations are performed in the domain of a single network working as a forward process model, thereby avoiding the need to train a separate neural network to act as a controller.More recently, Fuzzy set theorySystems involving fuzzy logic are distinct from ''nonfuzzy'' expert systems in that the former typically involve (1) linguistic rather than numeric variables and (2) ''fuzzy'' conditional statements rather than exact expressions. Rules that incorporate linguistic and inexact data can be manipulated as a useful tool for reasoning about difficult industrial process control situations.The fuzzy logic controller is pre-programmed with a flexible set of rules and can adapt to changing conditions. It is made up of three basic elements: a fuzzifier, an inference engine, and a de-fuzzifier. The fuzzifier and de-fuzzifier translate the literal language of sensors into linguistic terms understandable by the inference engine, where the control decisions are made. The most interesting use of fuzzy logic in the process industries is to control a process that is non-linear and multi-variable in its makeup. Tay and Zhang (1999) adopted advanced neural fuzzy technology to develop a conceptual adaptive model for anaerobic treatment systems. Modeling anaerobic biological wastewater treatment systems is difficult because their performance is complex and highly dependent on the configurations of the different reactors, influent characteristics, and operational conditions. Instead of conventional kinetic modeling, the model developed by Tay and Zhang combines the robustness of fuzzy systems with the learning ability of neural networks, and can adapt to various situations. The advantage of combined neural fuzzy systems is that fuzzy systems are enhanced with the automatic tuning abilities of neural networks. However, a weakness in the neural fuzzy model is that its performance is highly dependent on the quality of training data.An anaerobic biological wastewater treatment system consists of the three phases of biological, liquid and gas treatment. Different substances are exchanged among the three phases, and products of the microbial conversion process are carried away by the effluent discharge gas. To avoid conducting the difficult analysis that such a complex system would require, the system is often treated as a ''black box''. The proposed model consists of several key components, including inputs and outputs, database and pre-processor, a fuzzy system generator, a fuzzy inference system, and an adaptive neural network representing the fuzzy system. Since it is difficult to manually pre-process the redundant and conflicting data, a fuzzy clustering method is utilized to automatically carry out the task.More recently, Hybrid intelligent systemsMany environmental systems are complex, nonlinear, and uncertain, leading to difficulties in simulating and controlling them through a single methodology. Often, hybrid technologies are desired to enhance robustness of the decision support. For example, both expert system and fuzzy logic try to emulate the reasoning process of an expert, or a set of experts, in a particular field. However, implementing fuzzy logic control as a set of rules in a knowledge base represents a significant investment in time and resources. On the other hand, neural networks can be an excellent tool for making sense out of data. Whenever knowledge is felt to be lacking, a neural network can be applied to extract from the data the inherent relationships. Therefore, in many industrial applications, expert systems, fuzzy logic and neural networks are integrated in a system to support optimized process control and management. In this section, some applications that adopt a hybrid approach to developing intelligent systems in the process industries are discussed.De la To assess the environmental impact of the Gulf War, a system for change detection was implemented which can identify two types of categorical changes: (i) a new landcover class that has emerged and (ii) changes between known classes. The change detection adaptive fuzzy (CDAF) network can be used to first derive a set of category representations that could be successfully employed to detect changes between known landcover classes. Secondly, the network can establish new category representations for unknown landcover classes, and then, it provides quantitative measures of the probability of changes in terms of category likelihood, as well as of the magnitude of changes in terms of intensity.The application of the CDAF network to assess change resulting from the Gulf War involves the following procedure: (i) training, i.e. the network is trained with spectral data for each pixel in the training set from the 1989 image along with the corresponding land cover class; (ii) classification, i.e. the trained neural network produced in step 1 was used in three ways, where the performance of the trained neural network is validated using an independent ''unseen'' test data set, the trained network was used to classify the entire 1989 image, and the trained network of 1989 was used to classify the 1991 image; (iii) change detection, i.e. fuzzy membership values for each land class for each pixel are obtained for both 1989 and 1991 classifications and are used in the detection of change. The results showed that this neural network approach gave good results compared to analysis using other methods. The hierarchical expert system is structured into the lower and upper layers, with the former consisting of a number of rule bases. To perform the task of scheduling, both deep and shallow knowledge are included in the knowledge base. The deep knowledge includes the first principles of reaction kinetics, thermodynamics, and mass and energy balances, as well as process models derived from first principles. The process models can evaluate process operations. The shallow knowledge is expressed as heuristic rules and functions. Since the available information is imprecise, the heuristic rules are fuzzy in nature and are called fuzzy heuristic rules or simply fuzzy rules.Scheduling decisions are made in two stages. First, each subsystem in the lower layer makes a locally optimal decision. Secondly, by coordinating the sub-systems, the local decisions are modified to satisfy the criterion of global optimality. The rules in the rule base at the lower layer are weighted. By co-ordinating and balancing trade-offs among the local decisions according to a number of constraints, a set of globally optimal decisions is made. The constraints include scheduling criteria, the minimum total idle time of process units in a day, satisfaction of a production plan, and waste minimization.The expert system PS-BATCH is developed for the scheduling of the batch chemical process. PS-BATCH is a question-answer system which poses a set of questions on the operating conditions of each process unit to the user. Among the variety of wastes generated in the metal finishing industry, the waste solution of electroplating plants consists of cyanide and is of particular concern because it is highly toxic. In a typical electroplating process, each work piece is cleansed to remove dirt and oil prior to plating or stripping. This involves electrocleaning, alkaline cleaning, acid cleaning and rinsing. A fuzzy expert system has been constructed to facilitate the source reduction of cyanide-waste solutions and to train plant operators in the techniques of cyanide minimization. The knowledge base of the system includes the detailed cyanide minimization strategies, which are expressed as both fuzzy and crisp (i.e. non-fuzzy) rules.The fuzzy expert system MIN-CYANIDE evaluates and prioritizes the minimization options, which include drag-out minimization, bath-life extension, rinse water reduction, replacement with a non-cyanide solution, use of an alternative plating technique, and improvement of the operating procedure. Based on the ranking, the system makes a recommendation on the most effective minimization strategy. Since hybrid models take advantage of the combination of knowledge and information from different sources, they are more complex than classical models. This may cause problems with classical optimization procedures, like dynamic programming or the application of Pontryagin's maximum principle for process optimization. These difficulties might in practice reduce the applicability of the model and consequently its benefit. The HYBNET software package is a development system for hybrid structure definition and optimization. HYBNET can combine differential equation systems, analytical mathematical relations, ANN, fuzzy expert systems or fuzzy neural networks and their variants. These become individual components in a modular model which is organized in a network structure. The novelty of HYBNET is that the entire model assumes a network structure, which implies ideas of network training developed for simple ANN can be applied at a higher level. From a practical point of view, the advantage of HYBNET is that it supports flexibility in arranging partial process models so that they can work together in solving a particular problem.The paper emphasizes that the cost-benefit ratio is a key determinant in the acceptance of model-supported process supervision, optimization and control in industrial scale production processes. Combining ANN, fuzzy rule systems and classical model approaches in a hybrid model can usually enhance its performance. An additional reason that hybrid modeling is particularly suited for production process automation is that in this application, there is usually enough data to conduct data analysis in combination with classical mathematical modeling. (i) Blackboard and layered architectures: The blackboard architecture permits several components of a knowledge-based system to communicate by means of a shared space in memory known as the blackboard. The blackboard architecture could be augmented by addition of a manager that controls the data written to the blackboard and the order in which expert modules use scarce resources to perform their tasks. The layered architectural design specifies the tasks of the expert system to be layered according to the time-scale at which events occur. (ii) Coping with real time interruption: Coping with realtime processes can be difficult. While there are a few strategies that can be adopted to improve real-time performance, none of them can guarantee adequacy of the performance. To do so, it is necessary to investigate performance profiles of the reasoning processes, to determine how much the results of the reasoning can be improved with allocating more processing time. (iii) Distributed architectures: In a distributed architecture, the blackboard or layered architectures would distribute the knowledge sources to different processors, with each one managing its own data exchange. This in fact describes the multi-agent architecture that consists of several, possibly physically distributed expert modules, each of which is capable of some control over its communication and decision-making. In a process control system, these agents would likely have the same goal, namely the safe and efficient production of some manufactured goods. However, at a lower level, it is possible that conflicts would arise among the agents.In these three types of architectures, a variety of AI methods can be incorporated. One of the earliest successes in the application of AI to process control involved the use of a fuzzy rule-based expert system. Many knowledge-based systems for process control use rule-based inference or fuzzy logic to cope with at least part of their control strategy. In addition, the neural network approach has recently emerged as a process control method. Current work in Japan and the United States is seeking to combine fuzzy control with neural networks.De For the biological treatment process for contaminated water, some knowledge about the inherent biochemistry of the process is usually available, either in the form of energy balances or other equations describing development of the process. However, these equations often involve variables that are not quantifiable. Therefore, a feed-forward neural network is used in conjunction with the 'specifiable' part of the model. The neural network estimates the unobservable kinetic parameters from the state at time t: This information plus the estimated kinetic parameters at time t is then fed into the known model to obtain the state at time t \u00fe 1: Hybrid neural networks, sometimes called 'gray box models', can be trained with fewer data and extrapolate better than pure neural networks.The experiments demonstrated the potential for using a hybrid model for forecast and simulation of an environmental process. When sufficiently large training sets of data are available, pure neural networks give an accurate model of the process. However, when limited data are available, as frequently happens in real life, the hybrid model gives significantly better accuracy, particularly on extrapolation. The part of the model that is assumed known acts as a constraint on the estimation, producing extrapolation far more reliable than those generated using a purely empirical model.Adding more constraints to the model is generally beneficial. The five most common types of neural network classifiers are PDF, global and local neural network, nearest neighbor and rule forming. PDF classifiers provide good performance when the PDF of the input features are known and when there is sufficient training data. Global and local neural network classifiers are both suitable for applications in which probabilistic outputs are desired. Nearest-neighbor classifiers are best suited for problems in which fast training and adaptation are essential. Finally, rule-based classifiers and decision trees are most suitable when a minimal-sized classifier is desired and when simple explanations for classifier decisions are desired.LNKnet integrates all of these methods. The software can be used at any of the three levels described as follows. The point-and-click graphical user interface can facilitate rapid experimentation and interaction with the various classifiers on new databases. Users who want to execute long batch jobs can edit and run the shell scripts produced by the point-and-click interface. Finally, users with knowledge of C programming can work at the source-code level. At this level, C source code of LNKnet can be embedded in a user application program. LNKnet contains more than 20 neural network, pattern-classification, and feature-selection algorithms. The algorithms can be trained and tested on separate data or tested with automatic cross-validation. The software is written in C and runs under the UNIX operating system. A number of research laboratories including the Lincoln Laboratory of Massachussettes Institute of Technology have used LNKnet successfully for many diverse applications, including talker identification, talker-gender classification, hand-printed character recognition, underwater and environmental sound classification, image spotting, seismic-signal classification, medical diagnosis, galaxy classification, and fault detection.More recently, Neural network models are often considered 'black boxes' due to their inability to provide explanations. To compensate for this weakness, De Veaux et al. (1999) proposed a hybrid system of neural network and first principle differential equation. This combination is sometimes called a 'grey box'. The inclusion of first principle knowledge in this hybrid model is shown to improve substantially the stability of the model predictions. When sufficiently large training sets of data are available, pure neural networks give an accurate model of the process. However, when only limited data are available, as typically is the case in real life, the hybrid model gives significantly better accuracy, particularly on extrapolation. The part of the model that is assumed known acts as a constraint on the estimation, producing extrapolation far more reliable than those generated using a purely empirical model of neural networks.Research perspectives on AI-aided process controlProcess control for environmental systems is an interdisciplinary concept, involving a variety of factors with multi-component, multi-phase, multi-period, and multi-criterion characteristics. The goal is to obtain desired environmental targets with minimized capital and operating costs. However, the possibility for accomplishing this goal could be affected by many constraints, limitations and complexities that exist in the pollution control systems. Facing these challenges, it is essential to identify effective approaches for realizing the desired targets. In the following, the detailed perspectives will be discussed.(1) Data availability: The insufficiency of information about pollution sources, minimization and mitigation measures, water quality records and economic data often hinder the development of effective models or expert systems. In many situations, information of scaled and field works was insufficient for supporting the AI-aided process modeling and control studies. Thus, researchers have to work within limited scopes where the required data are available, constraining the adoption of techniques of comprehensive analysis, integrated modeling, global optimization, and systematic consideration. It is suggested that improvement in data availability and quality is pre-requisite for useful process control studies.(2) Data reliability: Many environmental data are subject to significant problems of uncertainties, inconsistencies, and errors. In fact, reliability and uncertainty are direct opposites of each other. That is, high reliability usually corresponds to low uncertainty, but it is, in fact, difficult to achieve both high reliability and low uncertainty at the same time. However, it is well recognized that high uncertainty means high system complexity, and low reliability could be worse than no data. To obtain improved reliability and certainty, solid works on validation of input data before they are used for further AI-related studies are desired.(3) Validity of methodology: A variety of techniques have been developed for supporting missions of environmental process management and control. However, problems of reliability and suitability exist. In process simulation and prediction, due to the existence of uncertainties and the lack of quantified economic information, most of neural network models can only reflect part of the impact factors. These affect accuracy of the prediction. Therefore, when neural network models are used for providing decision support, the researchers have to conduct solid on-site works to gain as much insight of the study system as the process managers before claiming that they are wiser and can do better jobs. Simply staying in offices and conducting modeling computations and theoretical imaginations based on documented reports and data will not work.(4) System complexity: Environmental processes generally have interactive, dynamic and uncertain features. Complexities exist in determination of system parameters, reflection of interactive relationships, formulation of AI-aided modeling approaches, interpretation of research outputs, and implementation of recommended activities. Often, to quantify and control such systems, a number of simplifications were made by adopting, for example linear, continuous, static, and/or deterministic assumptions. These simplifications, however, are subject to risks of system errors and failures. How to effectively reflect these complexities without taking these risks has been a challenging question facing the modeling researchers. Obviously, using any single technique to solve such problems could be subject to risks. Therefore, whenever possible, it is recommended to use an integration of more than one tool to tackle different aspects of these complexities. This paper has presented some hybrid approaches that incorporate diverse techniques. These can be more flexibly adopted to address more open-ended investigations. For example, the ANN technique can be integrated with fuzzy logic to tackle more complex problems. The ability of fuzzy logic to model qualitative knowledge and represent knowledge in a logical form can compensate for the weakness in ANN of embedding knowledge implicitly in their structural parameters Conclusions", "conclusions": "Several demanding areas for enhanced research efforts are discussed, including issues of data availability and reliability, methodology validity, and system complexity. In general, expert systems and AI techniques can be categorized according to problem solving or task types. Along this dimension, the majority of AI applications for pollution mitigation and minimization fall into four broad task groups: (i) process control, (ii) prediction or estimation, (iii) process modeling or simulation, and (iv) process management that involves some comparison and evaluation of cost-benefit ratios of different solution scenarios. Among these, the most frequently tackled tasks are process control, prescription, and prediction. When sufficient data are available, the neural network technique often generates good results for system prediction. On the other hand, expert systems are often used for the task of prescription and control which involve process modeling and optimization. However, many environmental processes are complex with interactive, dynamic and uncertain features, leading to difficulties in using a single technique to solve such problems. Therefore, integrated or hybrid approaches are desired for reflecting such complexities and accomplishing the desired targets.Problems of environmental pollution have caused severe stress on the human society and economic progress. Pollution abatement through minimization and mitigation measures is an important approach for addressing such a challenge, where effective process control would be essential for obtaining desired efficiencies. Recently, AI technologies have been found to excel in environmental systems that are complex, non-linear, and uncertain. This study investigates and analyzes some recent advancement of these technologies. In particular, technologies of expert systems, fuzzy logic, and neural networks have emerged as the most frequently employed approaches for realizing process control. The results not only provide an overview of progress in the study field but also, more importantly, reveal opportunities for research in order to realize more effective environmental process control through incorporation of AI technologies.", "SDG": [3, 11]}, "automated_and_real_time_segmentation_of_suspicious_breast_masses_using_convolutional_neural_network": {"name": "Automated and real-time segmentation of suspicious breast masses using convolutional neural network", "abstract": " In this work, a computer-aided tool for detection was developed to segment breast masses from clinical ultrasound (US) scans. The underlying Multi U-net algorithm is based on convolutional neural networks. Under the Mayo Clinic Institutional Review Board protocol, a prospective study of the automatic segmentation of suspicious breast masses was performed. The cohort consisted of 258 female patients who were clinically identified with suspicious breast masses and underwent clinical US scan and breast biopsy. The computer-aided detection tool effectively segmented the breast masses, achieving a mean Dice coefficient of 0.82, a true positive fraction (TPF) of 0.84, and a false positive fraction (FPF) of 0.01. By avoiding positioning of an initial seed, the algorithm is able to segment images in real time (13-55 ms per image), and can have potential clinical applications. The algorithm is at par with a conventional seeded algorithm, which had a mean Dice coefficient of 0.84 and performs significantly better (P< 0.0001) than the original U-net algorithm.", "keywords": "", "introduction": "Breast cancer is the most common cancer among American women after skin cancer, and is a leading cause of death with an estimate of 40,450 cases in 2016 [1,. Additionally, more than half of the cases of breast cancer occur in the developing world, with a mortality rate inversely related to the country's wealth 2]. Various imaging modalities are used for screening breast tissue with the goal of early detection (e.g., mammography, US, and magnetic resonance imaging). Annual breast cancer mammography screening is recommended by the American Cancer Society for women between the ages of 45 and 54, with biennial screening after the age of 54 [2]. Mammography is discouraged in women younger than 45 due to the risk radiation poses and likelihood of outweighed benefits. Furthermore, younger, premenopausal women have denser breasts compared to older, postmenopausal women, which makes interpretation of mammograms more difficult [3]. Young patients with high breast density and family history of cancer often undergo magnetic resonance imaging or US examination for cancer screening, thus, US plays a major role for patients who cannot undergo mammography examination [4]. In addition, US is commonly used as a secondary screening modality to further inspect suspicious breast masses identified by mammography. US imaging is a relatively inexpensive, noninvasive, and widely available medical imaging technique used for breast cancer screening, with growing use in developing countries [5]. Ultrasonographers use the morphological and textural features to identify suspicious masses. These visual cues can be shape, margin, echo pattern, posterior features, presence of calcifications, or architectural distortion [6]. The suspicious mass is then scored based on the Breast Imaging Reporting and Data System (BI-RADS) scale. The BI-RADS scale is a purely visual system developed to quantify cancer suspicion in breast masses and is the basis for recommending core needle biopsy or continuous monitoring if the mass is suspected to be of low suspicion. With the aid of mammography, localization of breast masses with US is done relatively effortlessly; however, in the absence of a mammogram, many challenges arise when finding breast masses only with US, as US requires a meticulous scanning of the entire breast. A computer-aided detection system could reduce the time sonographers spend finding breast masses, thus making the localization and segmentation process more efficient.[7]Algorithms to segment breast masses on ultrasound imaging in two and three dimensions have been proposed. The majority of the algorithms use a seeded boundary, which is a rough estimate of the mass boundary drawn on a single B-mode frame or an initial point seed to initiate the segmentation algorithm. Some examples include, a leak plugging algorithm to find diffused and partially diffused boundaries based on a pre-specified seed [8,, region-growing algorithms that grow regions based on an initial seed and eventually converge to the segmented boundaries 9][9][10][11][12], active contour model and its variations [13][14][15], a level set algorithm which uses the principle of active contour energy minimization [16][17][18], a twostage active contour method based on an initial point seed [19], an automated particle swarm optimization clustering algorithm which does not require an initial seed but is computationally costly and not suitable for live imaging implementation [20], and a segmentation algorithm based on the cellular automata principle which requires an initial seed [21]. Marking a seed is a trivial task when reviewing cases retrospectively, but is a major impediment for segmentation during live imaging. Correct segmentation of breast masses is very important, as determining malignancy of a mass is critically dependent on the mass morphological features (e.g., shape, smoothness of boundary). Therefore, any automated approach to classify breast masses should first be able to accurately identify the mass boundary.[22]Deep learning takes advantage of improvements in graphics processing unit's computing power to develop larger and more complex neural networks capable of performing visionbased tasks comparable to humans and, in some cases, exceeding human performance . Deep learning algorithms have been previously used for classification of benign and malignant breast masses [23][24,. In this paper, we propose a Multi U-net algorithm to segment suspicious breast masses on US imaging. The proposed algorithm builds up on existing deep learning based segmentation algorithm 25]. The segmentation algorithm is introduced first, followed by implementation on 258 patients and comparison with conventional seed based algorithm and original U-net algorithm.[26]", "body": "Breast cancer is the most common cancer among American women after skin cancer, and is a leading cause of death with an estimate of 40,450 cases in 2016 Algorithms to segment breast masses on ultrasound imaging in two and three dimensions have been proposed. The majority of the algorithms use a seeded boundary, which is a rough estimate of the mass boundary drawn on a single B-mode frame or an initial point seed to initiate the segmentation algorithm. Some examples include, a leak plugging algorithm to find diffused and partially diffused boundaries based on a pre-specified seed Deep learning takes advantage of improvements in graphics processing unit's computing power to develop larger and more complex neural networks capable of performing visionbased tasks comparable to humans and, in some cases, exceeding human performance Materials and methodsPatientsClinical US Images were taken using 2 different commercial clinical US machines: LOGIQ E9 (General Electric; Boston, USA) and IU22 (Philips; Amsterdam, Netherlands). No specific probe, center frequency, or gain settings were specified for image acquisition. Written consent was obtained from all patients along with proper institutional review board approval from Mayo Clinic, while being HIPPA complaint. Patients older than age 19 undergoing biopsy after US imaging for breast cancer were included in the prospective study. Patients with breast implants, abnormalities, and who previously underwent any breast surgical procedures were excluded from the study. A total of 258 patients participated, resulting in 433 US clinical images from multiple orientations. US images with calipers or region of interest (e.g., boxes) were excluded. One hundred twenty-four (124) masses were malignant and one hundred thirty-four masses (134) were benign, as confirmed by biopsy. Table Table The training set consisted of 337 images, the validation set consisted of 35 images, and the test set consisted of 61 images. The sets were divided such that individual patients appear in only one set. Images were manually segmented by a trained sonographer with thirty one years of experience and were used as gold standard. Identifying breast mass boundaries is a subjective process; therefore, having an experienced professional is of critical importance.PreprocessingThe clinical US images were down sampled to 208 by 208 pixels with zero padding to preserve the image aspect ratio. The US images were taken with different imaging voltages, gain settings, and different transducers, thus resulting in variation in B-mode intensity values. To standardize the images, standard scores for each image were calculated by subtracting the mean value of the image from each pixel followed by division with the standard deviation of the image.AlgorithmFig Data augmentationOne of the major concerns when using deep learning is overfitting; this is particularly true for convolutional neural networks Malignant Pathologies Sample size, nDuctal carcinoma in situ 5Invasive ductal carcinoma (IDC) 76Invasive lobular carcinoma (ILC) 14Invasive mammary carcinoma (IMC) 28Lymphoma 1Serous carcinoma 1Metaplastic carcinoma 1 https://doi.org/10.1371/journal.pone.0195816.t002Table Benign Pathologies Sample size, nCellular fibroepithelial lesion 2Clustered apocrine cysts 7Cyst 3Fat necrosis 9Fibroadenoma 52  While augmenting data these US features must be preserved. Thus, horizontal flipping and equal axis zooming are the only data augmentation techniques used.Fibrocystic changes 12Post-processingPost-processing was used to improve performance of the network. Equally weighted binary pixels from the ten-fold cross-validated Multi U-net models were averaged and a threshold was used to implement majority voting. A majority voting threshold of 0.5 was used, as justified later in the discussion section. Majority voting removes uncertainty of finding the minima associated with random initialization of the individual U-nets.Segmentation evaluationThe proposed algorithm was evaluated using the Dice coefficient (similarity index), true positive fraction (TPF), and false positive fraction (FPF). All three parameters range between 0 and 1; values closer to 1 are better for the Dice coefficient and TPF, and values closer to 0 are better for FPF. Box plot distributions showing the performance of the above mentioned three parameters against different pathologies and BI-RADS were also examined. The dominant pathologies of benign and malignant cases were additionally analyzed separately (i.e., fibroadenoma and invasive ductal carcinoma (IDC), respectively).Comparison with conventional seeded algorithm and original Unet algorithmTo compare the performance of Multi U-net algorithm with conventional seeded algorithm, a distance regularized level set segmentation (DRLS) algorithm ResultsThe mean and standard deviation value of the Dice coefficient, TPF, and FPF achieved during testing is presented in Table Review of selected casesThe results of 6 different cases are reviewed to demonstrate the ability and the limitations of the algorithm.     detects the central region of the cyst, along with the hypoechoic region surrounding it (Dice coefficient of 0.78). DRLS algorithm (Dice coefficient of 0.74) also fails to identify the sharp extension of the cyst and performs poorer than Multi U-net due to the initial seed, which did not include the cystic extension. Performance of original U-net algorithm is lower than Multi U-net algorithm with a Dice coefficient of 0.56. The original U-net algorithm identifies an isolated region beneath the suspicious mass. However, the Multi U-net algorithm is able to avoid that as this outlying region may exist in only a few U-nets and the majority voting can remove such outlying regions.DiscussionThe paper presents the performance of Multi U-net segmentation algorithm for suspicious breast masses. The Multi U-net algorithm segments the test images in real time with a mean Dice coefficient of 0.82, which is on par with other seed-based segmentation algorithms (DRLS, Dice coefficient of 0.84). The performance of original U-net algorithm is significantly poorer than Multi U-net algorithm even though the image size used in training of original Unet algorithm is larger than Multi U-net algorithm. Unlike its contemporary seeded algorithms, the Multi U-net algorithm does not require an initial seed and could be used in applications requiring minimum user interaction. The abilities and the limitations of the algorithm are exhibited through the help of selected review cases. The complexity in a deep learning algorithm does not depend on the algorithm itself, but in the data that is used to train the algorithm. Deep learning algorithm's performance increases as the algorithm is trained on more diverse and unique cases. The data gathered from the past and futures studies can be further used to improve the performance of the algorithm.As shown in Table From Fig 2(a) and 2(b), we observed that the median Dice coefficient and TPF are higher for malignant pathologies compared to benign pathologies; however, FPF was higher for malignant pathologies compared to benign pathologies (Fig From Fig 3(a) and 3(b), it is evident that the performance of the Multi U-net algorithm was better for BI-RADS 3 and 4 compared to BI-RADS 5. Suspicious masses with irregular shapes and margins are usually assigned BI-RADS 5, and are difficult cases for the algorithm to capture the intricate details. Similar to our observation from Fig 2 we observe that the TPF for DRLS algorithm is higher or comparable to Multi U-net algorithm. However, the FPF for DRLS is lower than Multi U-net algorithm which implies the overestimation of suspicious mass by Multi U-net and under estimation by DRLS.The performance of DRLS algorithm is highly dependent on the initial seed as shown in case 4 and case 6. The DRLS algorithm has leverage over the Multi U-net algorithm in our implementation because the initial seed was just an eroded version of the manually segmented expert's mask. This was intentionally done to highlight the performance of Multi Unet algorithm when the seed has been selected by a user with good knowledge about the suspicious mass. However, the initial seed may not work in favor of DRLS algorithm when the size of the suspicious mass is too small or if the mass has spiculations, narrow extensions or irregular boundaries.The better performance of original U-net algorithm for case 3 and case 4 can be attributed to the larger image size used in training the algorithm compared to Multi U-net which has a coarser resolution, as the contours are scaled to the original image size before overlaying on the B-mode image. For majority of the review cases the performance of original U-net is poorer than Multi U-net algorithm. This improvement in Dice coefficient can be mainly attributed to ten-fold cross validation and majority voting technique.Selection of majority voting threshold is important for optimizing the performance of ten cross fold validated Multi U-net algorithm. Since, the initialization for all the ten U-nets is random they converge to different local minima's, resulting in ten unique maps for the same image. At lower values of majority voting threshold the algorithm is overestimating the suspicious mass region resulting in low Dice coefficient values as seen in The original U-net is prone to predicting a larger region than the actual lesion. The penultimate output of the U-net algorithm assigns probability (probability that the pixel belongs to the lesion) to each pixel which is then converted into a binary probability using the sigmoid classifier. The center of the segmented part is predicted with higher probability. However, the periphery pixels have lower probability thus lower confidence and are prone to be spurious. Using multi U-net enables to increase the probability of the pixels on the periphery by trimming the periphery based on a majority voting threshold. The performance of original U-net is poor compared to multi U-net algorithm. However, the performance of original Unet algorithm with the same hyper parameters as multi U-net algorithm improves as seen in model 1 to 10 from Fig Hypoechoic suspicious masses are sometimes surrounded with hyperechoic boundaries. There is an ongoing deliberation on the selection of these hyperechogenic regions as part of a suspicious mass Advancements in suspicious mass segmentation offer potential benefits in their classification. Additionally, deep learning techniques that seek to diagnose suspicious masses are heavily dependent on accurate segmentation to capture boundary-related features. Improved accuracy in real-time automatic segmentation will enable the development of live automatic classification of suspicious masses. Unlike classification algorithms, which need bigger data sizes, segmentation algorithm can work with smaller data size as the sample size of the data is not just the number of images but the number of images multiplied by image size as each pixel is individually classified into normal or suspicious mass.  LimitationsClinical 2D US images provide only a planar view of the in-vivo tissue. The planar view of the tissue varies with angle, orientation and pre-compression. The same mass appears different, depending on how the above mentioned parameters are changed. The manual segmentation masks were created by a sonographer who had access to live imaging and thus viewed the suspicious mass by sweeping over it in real time as well as rocking, heeling, and toeing to view it from different angles, orientations, and at different pre-compression levels. Pre-compression changes the contrast of the mass with respect to the surrounding tissue. The angle of inclination changes the posterior acoustic shadowing and enhancement. Orientation changes the cross-section that is being examined. The Multi U-net algorithm has access to only a single frame, which limits the ability of the algorithm to better delineate the boundaries of the suspicious mass. Also, currently the algorithm treats the images from different cross-sections of the same suspicious mass as independent cases; thus, the information from different cross-sections is not combined. More images of the same suspicious mass acquired with different imaging parameters can improve the performance of the algorithm further. Unique pathologies which can be mixture of two different pathologies are not readily available in the small training set and are hard to segment as shown in case 4. A larger training set inclusive of various pathologies can further improve the performance of segmentation algorithms.Conclusion", "conclusions": "The Multi U-net algorithm can segment suspicious breast masses in real time without the need for an initial seed, and performs on par with contemporary seeded algorithms (DRLS). A significant improvement is obtained over original U-net by using the multi U-net algorithm. The increment is due to combined contribution of better hyper parameter selection and use of tenfold cross validation technique. The performance of the algorithm can be further improved with a bigger dataset and can be extended to diagnosis of suspicious masses in the future. The algorithm is independent of US machine and can be used in any commercially available clinical system. ", "SDG": [3]}, "automatic_coronary_artery_calcium_scoring_in_cardiac_ct_angiography_using_paired_convolutional_neural_networks": {"name": "Medical Image Analysis", "abstract": " The amount of coronary artery calcification (CAC) is a strong and independent predictor of cardiovascular events. CAC is clinically quantified in cardiac calcium scoring CT (CSCT), but it has been shown that cardiac CT angiography (CCTA) may also be used for this purpose. We present a method for automatic CAC quantification in CCTA. This method uses supervised learning to directly identify and quantify CAC without a need for coronary artery extraction commonly used in existing methods.", "keywords": "Coronary,artery,calcifications,Automatic,calcium,scoring,Convolutional,neural,network,Cardiac,CT,angiography", "introduction": "Cardiovascular disease (CVD) is the global leading cause of death. The amount of coronary artery calcification (CAC) as quantified in cardiac CT -the calcium score -is a strong and independent predictor of CVD events .( Yeboah et al., 2012 )In a clinical cardiac CT exam, a calcium scoring CT (CSCT) scan and a coronary CT angiography (CCTA) scan are typically both acquired. The CCTA scan is used for stenosis detection or identification of non-calcified plaque, and the CSCT scan is used to determine the calcium score . However, it has been shown that CAC may also be quantified in CCTA. In a study by ( Hecht, 2015 ) , 85% of patients with a high calcium score in CSCT also had a high calcium score in CCTA (specificity 99%). Moreover, Pavitt et al. (2014) showed excellent agreement between CVD risk categories based on calcium scoring in CCTA and categories based on calcium scoring in CSCT (Cohen's linearly weighted \u03ba = 0 . 93 ).Mylonas et al. (2014)A recent survey reported typical radiation doses of 1 mSv for CAC scoring in CSCT , while modern techniques allow CCTA acquisitions with 1.5 mSv radiation dose ( Messenger et al., 2015 ). Hence, performing calcium scoring in CCTA and omitting acquisition of the CSCT scan could reduce the radiation dose of a cardiac CT examination by 40-50% ( Al-Mallah et al., 2014 ).( Voros and Qian, 2012 )In clinical practice, CAC is standardly quantified in CSCT by manual identification of groups of connected voxels in the coronary artery that are above a 130 HU threshold and subsequent automatic 3D region growing . This procedure is not applicable to CCTA, due to intravascular contrast material that typically enhances the arterial lumen well beyond 130 HU ( Agatston et al., 1990 ) and (f)). Hence, higher global detection thresholds, ranging from 320 HU ( Figs. 1 (b) to 600 HU ( Otton et al., 2012 ) have been proposed to emulate CAC scoring in CCTA. However, these fixed thresholds do not consider variations in lumen attenuation in CCTA, which might occur depending on protocols, scanners or contrast agents ( Figs. ( Glodny et al., 2009 )) and (g)). This variation can be taken into account by using patient-specific or scan-specific attenuation thresholds, based on HU values taken from a ROI in the ascending aorta 1 (c or the proximal coronary arteries ( Mylonas et al., 2014 ) and (h)).( Pavitt et al., 2014 ) ( Figs. 1 (d)Manual identification of CAC in cardiac CT requires substantial expert interaction, which makes it time-consuming and infeasible for large-scale or epidemiological studies. To overcome these limitations, (semi)-automatic calcium scoring methods have been proposed for CSCT (see e.g.  ; I\u0161gum et al. (2007) ; Kurkure et al. (2010) ; Shahzad et al. (2013) and Wolterink et al. (2015a ) ). Ding et al. (2015) ) provide a comparison of (semi-)automatic methods for calcium scoring in cardiac CT exams. Similarly, methods have been developed for auto-  Wolterink et al. (In press oversegments CAC in both images. (c),(g) \u03b8 manual = 600 HU ( Agatston et al., 1990 ) misses one CAC lesion in (c) and oversegments CAC in (g). (d), (h) a patient-specific threshold based on attenuation in the ascending aorta ( Glodny et al., 2009 )  matic calcium scoring in CCTA. These methods typically require a (semi)-automatically extracted segmentation of the coronary arteries. Based on this segmentation, CAC has been identified as deviation from a trend line through the lumen intensity ( Mylonas et al., 2014 )( Wesarg et al., 2006;, as voxels in the extracted arteries with intensities above a patient-specific HU threshold Ahmed et al., 2014 ), or as deviations from a model of non-calcified artery segments ( Te\u00dfmann et al., 2011 ). ( Eilot and Goldenberg, 2014 ) did not use a model or threshold to identify CAC, but trained classifiers to identify CAC lesions along an extracted coronary artery centerline. Coronary artery tree extraction methods generally show good performance, but they have been reported to fail in patients with complex anatomy, in the distal segments of the coronary arteries, in scans with motion or noise artifacts and in scans with occlusions in the coronary arteries. In addition, severe CAC deposits affect the performance of artery extraction algorithms, restricting their applicability in CAC identification Mittal et al. (2010). Manual correction of incorrectly segmented coronary arteries is often time-consuming and tedious.( Schaap et al., 2009 )We propose identification of CAC without initial coronary artery tree extraction. In contrast to previously proposed methods, our algorithm uses supervised learning to directly identify CAC in CCTA. Supervised learning using nearest-neighbor, SVM and randomized decision tree classifiers has been previously applied to CAC identification in CSCT (e.g. ( Isgum et al., 2012;Wolterink et al., 2015a;). However, these methods cannot be applied in CCTA, as they classify potential CAC lesions, extracted using a clinical 130 HU threshold. In CCTA, it is non-trivial to distinguish between CAC and attenuated lumen, and the application of a predefined single detection threshold to extract potential CAC lesions is not feasible. Instead, the proposed method identifies CAC voxels to segment lesions.Shahzad et al., 2013 )CAC voxel identification in CCTA is a challenging and extremely unbalanced classification problem. The proposed algorithm therefore first limits the volume-of-interest (VOI) to a bounding box around the heart, extracted using our previously proposed algorithm . Thereafter, voxels in this VOI are classified using convolutional neural networks (ConvNets). Recently, ConvNets have been successfully used in natural image classification, image segmentation and object detection. In addition, they have been used in several medical image analysis tasks, for example knee cartilage segmentation ( de Vos et al., 2016 ) lymph node detection ( Prasoon et al., 2013 ), brain tissue segmentation ( Roth et al., 2014 ), and pulmonary nodule classification ( Stollenga et al., 2015 ). In the proposed algorithm, ConvNets automatically extract texture features from triplanar 2.5D or volumetric 3D input samples, which are combined with spatial features derived from a normalized coordinate system defined in the VOI. To classify voxels as CAC or non-CAC, a pair of ConvNets is used. These ConvNets are linked by training and together are called a ConvPair. The first ConvNet identifies voxels likely to be CAC. Such voxels are further classified by the second ConvNet, which distinguishes between CAC and CAC-like negatives. We propose a purely convolutional Con-vNet architecture, which allows for fast evaluation times and can be directly applied to arbitrarily sized CCTA images. In addition, we present experiments showing that combinations of different architectures can achieve higher CAC identification performance than individual architectures.( Ciompi et al., 2015 )We have previously proposed a method for CAC scoring in CCTA using a combination of a ConvNet and a Random Forest classifier . This work extends our previous work in several ways. First, the classification procedure has been modified. Our previously proposed method used a ConvNet for voxel classification and a Random Forest classifier for lesion classification. The current method uses two sequential ConvNets for voxel classification. Second, in our previous work, candidate voxels for classifica-tion were selected based on the image intensity histogram. In the current work, we classify all voxels within the VOI, regardless of intensity, hence no assumptions are made about CAC HU values. Third, location features were previously extracted using a timeconsuming elastic registration preprocessing step. In the current method, this registration step is omitted in favor of our very fast ConvNet-based bounding box detection technique ( Wolterink et al., 2015b ). Fourth, in our previous work we only evaluated triplanar 2.5D input with one input size. In the current work, we provide a comparison between 2.5D and volumetric 3D input, between input with different sizes, as well as experiments with ensembles combining these input representations. Fifth, the ConvNet architecture in our previous work required a time-consuming scan algorithm with many redundant operations for neighboring candidates. Here, we use a purely convolutional network for efficient voxel classification. Finally, in this work an evaluation on a substantially larger set of scans has been performed, and a thorough comparison with clinically used CSCT CAC scores, as well as interobserver variability, are provided.( de Vos et al., 2016 )", "body": "Cardiovascular disease (CVD) is the global leading cause of death. The amount of coronary artery calcification (CAC) as quantified in cardiac CT -the calcium score -is a strong and independent predictor of CVD events In a clinical cardiac CT exam, a calcium scoring CT (CSCT) scan and a coronary CT angiography (CCTA) scan are typically both acquired. The CCTA scan is used for stenosis detection or identification of non-calcified plaque, and the CSCT scan is used to determine the calcium score A recent survey reported typical radiation doses of 1 mSv for CAC scoring in CSCT In clinical practice, CAC is standardly quantified in CSCT by manual identification of groups of connected voxels in the coronary artery that are above a 130 HU threshold and subsequent automatic 3D region growing Manual identification of CAC in cardiac CT requires substantial expert interaction, which makes it time-consuming and infeasible for large-scale or epidemiological studies. To overcome these limitations, (semi)-automatic calcium scoring methods have been proposed for CSCT (see e.g. We propose identification of CAC without initial coronary artery tree extraction. In contrast to previously proposed methods, our algorithm uses supervised learning to directly identify CAC in CCTA. Supervised learning using nearest-neighbor, SVM and randomized decision tree classifiers has been previously applied to CAC identification in CSCT (e.g. CAC voxel identification in CCTA is a challenging and extremely unbalanced classification problem. The proposed algorithm therefore first limits the volume-of-interest (VOI) to a bounding box around the heart, extracted using our previously proposed algorithm We have previously proposed a method for CAC scoring in CCTA using a combination of a ConvNet and a Random Forest classifier DataIn this study, clinically obtained cardiac CT exams of 250 consecutively scanned patients were included. Each exam consists of a CSCT and a CCTA scan, made on a 256-detector row scanner (Philips Brilliance iCT, Philips Medical, Best, The Netherlands). The CSCT scans were acquired using a standard calcium scoring protocol with 120 kVp tube voltage and 55 mAs tube current, with ECGtriggering and without contrast enhancement. Reconstructed sections had 3.0 mm spacing and thickness. The CCTA scans were acquired with 120 kVp tube voltage and 210-300 mAs tube current, with ECG-triggering and contrast enhancement. Reconstructed sections had 0.45 mm spacing and 0.90 mm thickness. In both CSCT and CCTA, in-plane resolution was 0.4-0.5 \u00d7 0.4-0.5 mm.The set of 250 cardiac CT exams was divided into two sets. The first 50 exams were used to train an algorithm that detects bounding boxes around the heart. The remaining 200 exams were used to train and evaluate a voxel classification algorithm that identifies CAC in these bounding boxes. Two expert observers provided annotations in all (observer O 1 ) or in a subset (observer O 2 ) of the exams.In each of the 50 cardiac CT exams used to train the bounding box detection algorithm, observer O 1 manually determined a 3D rectangular bounding box around the heart in the CCTA scan. This bounding box included the pericardial sac, from below the pulmonary artery to the apex in the craniocaudal direction.In each of the 200 cardiac CT exams used to train and evaluate the voxel classification algorithm, manual reference annotations for CAC were obtained in the CCTA and the CSCT scan. Manual annotations in the CCTA scans were obtained similarly to the methods proposed in Observer O 1 annotated CAC in all 200 cardiac CT exams. These annotations were considered the reference standard, used for training and evaluation of the voxel classification algorithm. Observer Please cite this article as: J.M.  O 2 annotated CAC in a subset of 100 cardiac exams. These annotations were used to determine inter observer variability and for comparison with the automatic method.MethodCAC was identified by voxel classification. Besides CAC, a typical CCTA scan contains many other voxels of appearance similar to CAC. These include extracardiac lesions like bones such as ribs, calcifications in the descending aorta and calcified lymph nodes, as well as intracardiac calcifications such as those in the mitral and aortic valve ( Fig. The proposed algorithm is illustrated in Fig. PreprocessingCCTA scans are generally acquired with a standardized scan length in the craniocaudal direction ranging from mid-pulmonary artery to diaphragm ConvNets label a target voxel based on a square or cubic input patch centered at that voxel. For this, it is beneficial to have identical receptive fields along all image axes. Therefore, images cropped to the determined bounding box are resampled with B-spline interpolation to 0.45 mm isotropic voxels -the standard slice spacing in our data set. Finally, to allow robust ConvNet training, all data is rescaled to the mean and standard deviation of HU values in the training images.Voxel classificationAll voxels in the VOI are considered candidates for CAC. The proposed ConvNets take one or multiple patches of size w voxels centered at the candidate voxel as input and extract features based on that input. Using these features, the voxel is assigned a probability of being CAC p CAC . Input patches are either 2.5D, i.e. three 2D patches from orthogonal image planes centered at the voxel, or 3D, i.e. volumetric patches centered at the voxel.Features in a ConvNet are typically extracted by a stack of convolution layers, while classification is done by a stack of fullyconnected hidden layers. The proposed method uses a purely convolutional ConvNet architecture for both the feature extraction stack and the classification stack, suitable for both 2.5D and 3D input. Fig. Depending on the size and the dimensionality of the input patches, the architecture of the feature extraction stack is generated as follows. Each convolutional layer with the exception of the last one consists of 16 small convolution kernels of 3 \u00d7 3 voxels in 2.5D or 3 \u00d7 3 \u00d7 3 voxels in 3D. Choosing multiple stacked small convolution kernels over one larger convolution kernel has been shown to have two advantages Please cite this article as: J.M.  In both the 2.5D and the 3D ConvNet, features are concatenated with x, y and z location features and connected to an output layer through one hidden layer.Second, stacking small kernels reduces the number of trainable parameters, and hence the chance of over-fitting. Convolutions are valid, i.e. no zero-padding is applied after convolution, so that each convolution reduces the input size by 2 voxels along each axis. In the final convolution layer, 32 convolution kernels reduce the input size to 1 voxel along each axis. The 32 obtained features are used for classification. Each convolution layer was followed by a rectified linear unit (ReLU) activation function The convolutional stack does not include any max-pooling downsampling layers. These layers are typically used in image classification and object detection to rapidly decrease the input size, to reduce the number of weights in the network to prevent overfitting and to introduce spatial invariance. However, the spatial invariance introduced by pooling could mean that neighboring voxels are assigned the class label that is most expressed in that location. This in turn could lead to over-or undersegmentation of CAC lesions, which are generally small. In addition, the absence of pooling layers means that the convolutional stack is purely con-volutional. Hence, the convolutions may be applied to full images, thereby avoiding redundant convolution operations.A 2.5D ConvNet contains three convolutional stacks, which independently process axial, sagittal and coronal input. The three networks share weights, i.e. one 2D network was used for feature extraction in the three orthogonal planes, similar to the shared weight multi-scale approach in The features extracted by the convolutional stack are used to classify the target voxel as either CAC or non-CAC. The extracted features might only provide limited spatial information. However, studies on automatic CAC scoring in non-contrast-enhanced cardiac CT have shown that location information is essential for CAC identification The feature vector serves as input to one fully-connected hidden layer. This hidden layer is connected to an output layer with a softmax function to predict probabilities p CAC and 1 \u2212 p CAC . To regularize the network, Dropout is applied before and after the hidden layer.Training strategyCCTA scans contain many more negative (background) than positive (CAC) samples, representing a heavily unbalanced classification problem. Identifying CAC among all voxels in a cardiac VOI in CCTA poses two challenges. The vast majority of negatives such as those representing lung or fatty tissue, share very few similarities with CAC. Hence, given sufficiently descriptive features, they might easily be discarded. Other negatives, such as bone (e.g. sternum), calcifications in the ascending aorta and coronary artery lumen enhanced by contrast material, are more challenging to distinguish from CAC. Our method uses a ConvPair, a pair of Con-vNets, each of which have a specific task. ConvNet 1 focuses on detection of CAC-like voxels, ConvNet 2 identifies CAC voxels among these candidates.The two ConvNets are trained in sequence. ConvNet 1 is trained first, using all voxels in the VOIs of the calcium scoring training images. This ConvNet learns to discard the vast majority of negative voxels. For each calcium scoring training image, a CAC candidate mask is obtained using ConvNet 1 . This mask contains CAClike voxels, but no negatives such as lung tissue or fatty tissue. Subsequently, ConvNet 2 is trained using only the samples in the CAC candidate mask. To leverage already learned knowledge, the ConvNet 2 is initialized as the final version of ConvNet 1 and training is resumed using the samples from the CAC candidate mask. Hence, ConvNet 1 and ConvNet 2 share their architecture but not their trainable parameters.During  ImplementationAll ConvNets were implemented using Theano EvaluationReference lesions were segmented using 26-connected 3D region growing of voxels above a patient-specific intensity threshold Given that CAC lesions in CCTA were created differently in the reference and automatic results, they might not contain the same voxels. Therefore, true positive lesions were automatically found lesions having overlap with the reference lesions. False positive lesions were those in the automatic result having no overlap with the reference lesions. False negative lesions were lesions identified in the reference that had no overlap with any automatically found lesion.In addition to an evaluation of the ability of the method to detect individual lesions, its ability to determine per patient Agatston, volume and mass CAC scores was established. The Agatston score weighs calcified plaque area by peak intensity, and was computed as s \u2208 S a s \u2022 d s \u2022 z 3 . 0 , where S is the set of slices containing the lesion, a s is the lesion area (in mm 2 ) in s, d s is a density factor based on the lesion's peak intensity in s (130-199 HU: 1, 200-299 HU: 2, 300-399 HU: 3, \u2265 400 HU: 4), and z is the image slice increment. A linear scaling factor z 3 . 0 is standardly used to correct for image slice increments different than the 3.0 mm increment for which the method was originally developed Automatically obtained volume and mass scores in CCTA were compared with reference volume and mass scores in CCTA by observer O 1 . In current clinical practice, CAC burden is determined in CSCT. Hence, automatically obtained volume and mass scores in CCTA were compared with reference volume and mass scores in CSCT. In addition, manual volume and mass scores defined by observer O 1 and observer O 2 in CCTA scans were compared with the reference scores in CSCT. Finally, to establish interobserver agreement, volume and mass scores in CSCT and CCTA scans were compared between the observers. Agreement between two measurements was determined using the intra-class correlation coefficient (ICC) for absolute agreement, with 95% confidence interval. In addition, Bland-Altman plots were generated and the bias and limits of agreement ( \u00b11.96 SD) were reported.In clinical practice, patients are assigned to a CVD risk category based on their Agatston score. Because CVD risk categorization is not defined for Agatston scores in CCTA, agreement between standard Agatston score based risk categorization in CSCT and the Agatston score in CCTA was determined as follows: patients were first categorized based on their CSCT Agatston score according to standard risk categories (Very low: 0, Low: 1-100, Intermediate: 101-40 0, High: > 40 0). Thereafter, patients were ranked based on their CCTA Agatston score. Based on this ranking, to each of four CCTA risk categories the same number of patients was assigned as in the corresponding CSCT category. Patients who ended up in the same category based on both CSCT and CCTA Agatston score were correctly categorized, patients who were assigned to a different category were incorrectly categorized. Categorization accuracy and Cohen's linearly weighted \u03ba were computed.Please cite this article as: J.M. Experiments and resultsThe set of 250 exams was divided into four sets. First, a set of 50 exams was used to train the bounding box extraction algorithm. Second, a training set of 90 exams was used to train the ConvNet pairs for CAC classification. Third, a validation set of 10 exams was used to optimize hyperparameters of the ConvNets. Finally, a test set of 100 exams was only used to evaluate the performance of the method. For the test set, annotations by both observer O 1 and O 2 were available.Bounding box extractionBounding box extraction took 6.9 \u00b1 0.5 s, discarding up to 80% of the original CCTA image. Fig. Experimental settingsNetwork weights were initialized according to the procedure specified by Four ConvPairs were trained, namely for 2.5D and 3D input patches with size w = 15 and w = 25 . Table Table 2Lesion identification with respect to the reference standard in CCTA. Four ConvPairs with dimensionality 2.5D or 3D, and input patch size w = 15 or w = 25 were trained. Ensembles of Conv-Pairs were formed by averaging of probabilities. Bullet points indicates membership of an ensemble. Lesion identification sensitivity is reported, as well as the average number of false positive (FP) lesions per scan.2.5D 3DSens. FP/scanwork with size w = 15 . In 3D, the difference in required processing time between input sized w = 15 and w = 25 was larger.Lesion identificationConvPairs are specified by input dimensionality and input size, and results are presented for merged output of their members ConvNet 1 and ConvNet 2 . Automatically extracted lesions were compared to the reference standard in CCTA test scans, which in total contained 260 CAC lesions.Table In all cases, ensembles make fewer FP errors per scan at a similar sensitivity level. The ensemble with w = 25 outperforms the ensemble with w = 15 , and the 2.5D ensemble outperforms the 3D ensemble. An ensemble of all ConvPairs provides the best result, at 71% lesion sensitivity and 0.48 FP error per scan.  lumen in the left anterior descending (LAD) artery was missed ( Fig. The effect of ConvNet 1 and ConvNet 2 on voxel classification was investigated. Fig. To investigate the effect of the normalized x, y, z\u2212coordinates on CAC identification, an additional ConvPair was trained with the best performing architecture, i.e. 2.5D input with w = 25 , adapted to omit the feature coordinates. ROC analysis showed that at all sensitivity levels, this ConvPair made slightly more FP errors than the ConvPair with location features.Per patient CAC quantificationFor each patient, the volume and mass scores in CCTA were determined by observer O 1 , observer O 2 and the automatic method. Also, for the automatic method, Agatston scores in CCTA were determined. Automatic results were based on the complete ensemble of trained architectures.Fig. Fig. For CAC mass score, values in CCTA were lower than the reference in CSCT for the two observers, but not for the automatic method. Bland-Altman bias and limits of agreement were \u221210 . 6 ( \u221252 . 7 -73.9) for observer O 1 , 19.0 ( \u221299 . 1 -137.0) for observer O 2 , and \u22120 . 2 ( \u221238 . 7 -38.3) for the automatic method. The ICC was 0.895 (0.837-0.932) for observer O 1 , 0.761 (0.650-0.838) for observer O 2 , and 0.944 ( 0 . 918 -0.962) for the automatic method.The confusion matrix in  linearly weighted \u03ba = 0 . 83 . No patient was more than one category off. The test set contained 48 patients with zero CAC and 52 patients with a positive CAC score. The automatic method identified 43/48 patients with zero CAC. Conversely, the automatic method correctly identified 48/52 patients with a positive CAC score. Missed CAC lesions in the remaining patients were small and low-density lesions, with the exception of one patient whose scan contained a CAC lesion in the RCA which was deformed due to a motion artifact and therefore missed by the algorithm ( Fig. Interobserver agreementBoth observers scored CAC in the full test set. To score CAC in CCTA, a patient-specific CAC threshold was determined using a manually placed ROI in the ascending aorta. This threshold showed Please cite this article as: J.M.   Agreement between CAC scores of the two observers in both CCTA and CSCT was excellent. For CAC volume and mass in CCTA, the ICC between the two observers was 0.928 (0.891-0.952) and 0.949 (0.922-0.966), respectively. Bland-Altman analysis ( Fig. Observer O 2 identified 45/48 patients with zero CAC score in CCTA as annotated by observer O 1 , the reference. Conversely, observer O 2 identified 51/52 patients with a positive CAC score as annotated by observer O 1 .Comparison with previous methodsThe proposed method was compared with results reported by other previously published algorithms on CAC scoring in CCTA. Table Please cite this article as: J.M. Discussion", "conclusions": "A method for automatic coronary artery calcium scoring in coronary CT angiography employing convolutional neural networks has been presented. In contrast to previously proposed methods for CAC scoring in CCTA, our method does not require coronary artery extraction. Instead, CAC voxels are directly identified using pairs of ConvNets.Automatically obtained as well as reference CAC volume scores in CCTA were lower than in CSCT. This is in accordance with previous studies ( van der Bijl et al., 2010;. However, automatically obtained CAC mass scores showed a strong correlation ) with reference CAC mass scores in CSCT. A comparison of CVD risk categories based on reference Agatston scores in CSCT and automatically obtained Agatston scores in CCTA showed excellent agreement, with 83 % of patients assigned to their reference risk category ( \u03ba = 0 . 83 ). Hence, patients with a high reference Agatston score in CSCT also had a high automatically determined Agatston score in CCTA. In addition, discrimination between patients with zero CAC and patients with a positive CAC score was good. Large scale studies have shown that patients with zero CAC have an excellent prognosis Mylonas et al., 2014 ), underlining the clinical relevance of this distinction.( Joshi et al., 2012 )False positive errors were caused by high intensity voxels in the coronary artery lumen. Lumen attenuation may be very different among patients, thereby posing a challenge for accurate CAC segmentation. In our test set, CAC detection thresholds determined based on the attenuation of the ascending aorta ranged from 333 to 930 HU. The method made some false positive errors, for example calcified lymph nodes, that are less likely to occur in methods using coronary artery extraction. Nevertheless, although our method does not use coronary artery extraction, it is likely that the ConvNet implicitly learns a representation of tubular structures. Calcifications in the coronary arteries were identified, but calcifications in the aorta, with similar intensity and shape characteristics but a different context, were not a source of false positive errors.A VOI containing the heart was determined using a ConvNetbased bounding box extraction algorithm. Although this VOI primarily contained cardiac structures, non-cardiac structures such as ribs (see Fig.  ) were occasionally partially included due to the rectangular nature of the identified VOIs. Alternatively, segmentations which more closely follow the boundaries of the heart may be obtained using for example graph cuts 6, morphological operations ( Funka-Lea et al., 2006 ) or atlasbased methods ( Kurkure et al., 2010 ). The results showed that for the current application a 3D rectangular bounding box was sufficient. Moreover, this bounding box was successfully acquired in all cases, with an average processing time of 6.8 \u00b1 0.5 s, compared to 13.2 m reported by ( Zhuang et al., 2015 ) . Results presented by de Vos et al. ( Zhuang et al. (2015)) illustrate that the method tightly follows a predefined standard anatomical VOI. Finally, the method only required retraining with manually defined 3D bounding boxes in 50 CCTA images.2016Similarly to our previous work, x, y, z -coordinates were used to describe the location of each candidate in the image . While in our previous work these features were crucial for accurate scoring, we found that here they only moderately affected the performance of the method. The reason for this may be two-fold. First, the patches provided sufficient texture information for voxel classification. Second, the VOI was sufficiently limited in size that location features did not provide much additional information. Nevertheless, x, y, z -coordinates would likely be valuable to provide artery-specific CAC scores, potentially leading to better prediction of CVD events ( Wolterink et al., 2015b ). It is likely that the proposed method could straightforwardly be extended to such a multi-class analysis. However, this would increase the complexity of the method and it would therefore require a larger training set size than available in the presented work.( Brown et al., 2008 )The purely convolutional network architecture used in this study allows training with patches and testing with whole images. To improve efficiency during ConvNet training,  proposed end-to-end training with whole images, i.e. by minimizing the difference between a predicted and a reference label map for a whole image instead of a single sample. However, the extreme imbalance in our classification problem necessitates balanced sampling of negative and CAC candidates during training. In whole image training, a sampling mask should therefore be applied to the predicted and reference label maps, which would reduce the overlap between patches and the potential increase in efficiency. Furthermore, considering whole images and not patches as samples would reduce the number of possible training batches. Therefore, in this study ConvNets were trained with mini-batches containing balanced samples from random training images.Long et al. (2015)The proposed network architecture ( Fig.  ) does not include any pooling layers, hence no spatial invariance is introduced. Therefore, the ConvNets were trained to predict a label only for the voxel at the center of the odd-sized input patch and not for other voxels in patch that might have a different label. The experiments showed a benefit of larger input patches in terms of specificity, with input patch sizes of 15 and 25 voxels corresponding to receptive fields of 6.75 mm vs. 11.25 mm along each axis. Note that the typical diameter of a coronary artery is 4 mm 3. Therefore, a larger patch size provides a wider margin around a coronary artery, which likely allows reduction of FP errors. In this work, only two input sizes were evaluated. As shown by the experiments, smaller inputs would not be likely to provide better results. Larger inputs might provide better results, but this effect might be mitigated by the increase in the number of trainable parameters and the limited number of training samples. Hence, a multiscale approach using a combination of small high resolution input patches to provide detailed local analysis and larger low resolution Please cite this article as: J.M.  ( Dodge et al., 1992 )[m5G;April 29, 2016; input patches to provide spatial context might further improve the method, while keeping the number of trainable parameters low. Our method used either 2.5D or 3D input. We did not use individual 2D planar inputs. Although these are highly efficient, they fail to capture the volumetric aspect of the data. Volumetric 3D patches can provide more information, but their size may pose computational challenges. In our experiments, 3D testing took substantially more time than processing with 2.5D, on average 147 s vs. 52 s for ConvNet 1 and 201 s vs. 107 s for ConvNet 2 . Similarly, training with 3D input took much longer. In addition, with a limited number of training samples, as is often the case in medical image analysis, 3D ConvNets are more likely to overfit to the training data. Our experiments showed a performance drop between 2.5D and 3D architectures in terms of CAC lesion identification in CCTA. It is likely that the number of trainable parameters and input voxels for the 3D patch causes the network to overfit, considering that the number of parameters was high compared to the number of positive training samples. In other applications in medical image analysis, e.g. 19:42 ], 2.5D input outperformed 3D input as in our experiments. As a potential means to overcome the limitations of 3D input patches, yet capture volumetric information, ( Prasoon et al., 2013 ) proposed 3D kernels with a reduced number of trainable parameters. Zheng et al. (2015) extended 2.5D input by using 9 rotated 2D views for ConvNet-based lung nodule classification, and hypothesized that, to some extent, more 2D views may lead to better performance. However, this advantage may be problem-specific and not applicable to voxel classification. In future work, we will further investigate the trade-off between complexity and performance of different input representations for voxel classification. In addition, the current data set could be enlarged to provide a more diverse set of training samples.Setio et al. (2016)It has previously been shown in 2D natural image classification that ensembles of ConvNet models can outperform individual models, e.g. by  . In our experiments, ensembles of ConvPairs with different dimensionality or input size improved lesion identification in all cases. It is likely that these models captured different aspects of the data, and hence were prone to make different errors. In future work, we will investigate to what extent the combination of models with identical architectures improves results, compared to models with different architectures that were combined in the present study.Krizhevsky et al. (2012)A clinical standard for CAC scoring in CCTA is lacking, and intensity thresholds have been determined in various ways ( Glodny et al., 2009;Otton et al., 2012;Mylonas et al., 2014;. The proposed method was trained using manual annotations in CCTA, based on a patient-specific threshold of mean aorta + 3 SD aorta determined with a ROI in the ascending aorta. As a consequence, in several cases, high variability in lumen HU values in the CCTA image caused oversegmentation of the CAC lesion. In the current work, we did not correct for this oversegmentation in the reference. A comparison between automatically determined and manually determined CAC volume and mass scores in CCTA showed that the automatic method generally determined lower scores. Pavitt et al., 2014 ), calcium was annotated fully manually, by contouring of calcified lesions, an extremely time-consuming process. The performance of our method is likely to increase by using such annotations, i.e. by removing noisy labels from the training data set.In ( van der Bijl et al., 2010 )In large studies, CAC scores in CSCT have been shown to be highly predictive of cardiovascular events . For CAC scores in CCTA, such studies have as of yet not been performed. Agatston and volume scores in CCTA are typically much lower in CCTA than in ( Yeboah et al., 2012 ). CSCT images are reconstructed with 3.0 mm slice spacing, while CCTA images typically have sub-millimeter slice spacing. Therefore, due to partial volume effects high-density CAC appears to have a larger volume in CSCT than in CCTA, but a higher peak intensity in CCTA than in CSCT. The Agatston score uses a stepwise function, which assigns the same weight to all voxels above 400 HU, thus not weighing higher peak intensities in CCTA accordingly. In previous studies, Agatston scores and CAC volume scores in CCTA were converted to modified Agatston scores using empirically determined linear conversion factors CSCT ( van der Bijl et al., 2010 )( Mylonas et al., 2014;. In this study, we primarily evaluated our method using the mass score, which weighs intensities linearly, and therefore better captures differences between CSCT and CCTA. The mass score has previously been shown to yield high correlations between CAC quantification in CSCT and CCTA Schuhbaeck et al., 2015 ) and to have lower inter-scan variability than the volume and Agatston score ( Hong et al., 2002 ). In addition, we did compute unmodified Agatston scores in CCTA to assign patients to CVD risk categories. Although these scores were lower than Agatston scores in CCTA, they ranked patients almost equally, indicating that CAC quantification in CCTA may be used to assign patients to CVD risk categories, without the application of a conversion factor.( Hoffmann et al., 2006 )ConvNets are pattern recognition networks, which means that they learn from examples and hence, are not likely to correctly classify unfamiliar samples. An example of this was found in our test set, which contained large calcified lymph nodes in the pericardium. It is likely that the number of scans in the training set was too small to capture all the variability expected in coronary CT angiography scans. Therefore, in future work a larger set of images covering a larger variety of examples will be included.The presented method performed very fast voxel classification. Bounding box extraction took on average 7 s per patient. Average processing time for the best performing ConvPair was 46 s for ConvNet 1 and 28 s for ConvNet 2 . Because our method is fully automatic, it shows potential for application in large-scale studies, as well as in a clinical settings where immediate processing would allow for a smooth workflow. To the best of our knowledge, the relation between CAC scores in CCTA and clinical CVD outcome and/or CVD risk categorization is not yet known. The presented automatic method allowing quick analysis might allow such studies.In conclusion, CAC can be accurately automatically identified and quantified in CCTA using the proposed pattern recognition method. This might obviate the need to acquire a dedicated CSCT scan for CAC scoring, which is regularly acquired prior to a CCTA, and thus reduce the CT radiation dose received by patients.", "SDG": [3]}, "automatic_license_plate_recognition": {"name": "Automatic License Plate Recognition", "abstract": " Automatic license plate recognition (LPR) plays an important role in numerous applications and a number of techniques have been proposed. However, most of them worked under restricted conditions, such as fixed illumination, limited vehicle speed, designated routes, and stationary backgrounds. In this study, as few constraints as possible on the working environment are considered. The proposed LPR technique consists of two main modules: a license plate locating module and a license number identification module. The former characterized by fuzzy disciplines attempts to extract license plates from an input image, while the latter conceptualized in terms of neural subjects aims to identify the number present in a license plate. Experiments have been conducted for the respective modules. In the experiment on locating license plates, 1088 images taken from various scenes and under different conditions were employed. Of which, 23 images have been failed to locate the license plates present in the images; the license plate location rate of success is 97.9%. In the experiment on identifying license number, 1065 images, from which license plates have been successfully located, were used. Of which, 47 images have been failed to identify the numbers of the license plates located in the images; the identification rate of success is 95.6%. Combining the above two rates, the overall rate of success for our LPR algorithm is 93.7%.", "keywords": "Color edge detector,fuzzification,license number identification,license plate locating,license plate recognition (LPR),self-organizing (SO) character recognition,spring model,topological sorting,two-stage fuzzy aggregation", "introduction": "A UTOMATIC license plate recognition (LPR) plays an important role in numerous applications such as unattended parking lots , [31], security control of restricted areas [35], traffic law enforcement [8], [7], congestion pricing [33], and automatic toll collection [5]. Due to different working environments, LPR techniques vary from application to application. Most previous works have in some way restricted their working conditions [20], such as limiting them to indoor scenes, stationary backgrounds [9], fixed illumination [30], prescribed driveways [7], [22], limited vehicle speeds [26], or designated ranges of the distance between camera and vehicle [1]. The aim of this study is to lessen many of these restrictions.[23]Of the various working conditions, outdoor scenes and nonstationary backgrounds may be the two factors that most influ-Manuscript received December 11, 2002;  ence the quality of scene images acquired and in turn the complexity of the techniques needed. In an outdoor environment, illumination not only changes slowly as daytime progresses, but may change rapidly due to changing weather conditions and passing objects (e.g., cars, airplanes, clouds, and overpasses).revised December 8, 2003.In addition, pointable cameras create dynamic scenes when they move, pan or zoom. A dynamic scene image may contain multiple license plates or no license plate at all. Moreover, when they do appear in an image, license plates may have arbitrary sizes, orientations and positions. And, if complex backgrounds are involved, detecting license plates can become quite a challenge.Typically, an LPR process consists of two main stages: 1) locating license plates and 2) identifying license numbers. In the first stage, license plate candidates are determined based on the features of license plates. Features commonly employed have been derived from the license plate format and the alphanumeric characters constituting license numbers. The features regarding license plate format include shape, symmetry , height-towidth ratio [15], [23], color [25], [17], texture of grayness [25], [2], spatial frequency [25], and variance of intensity values [26], [8]. Character features include line [10], blob [34], the sign transition of gradient magnitudes, the aspect ratio of characters [13], the distribution of intervals between characters [12], and the alignment of characters [28]. In reality, a small set of robust, reliable, and easy-to-detect object features would be adequate.[32]The license plate candidates determined in the locating stage are examined in the license number identification stage. There are two major tasks involved in the identification stage, character separation and character recognition. Character separation has in the past been accomplished by such techniques as projection , [11], morphology [30], [2], [10] relaxation labeling, connected components [28], and blob coloring. Every technique has its own advantages and disadvantages. Since the projection method assumes the orientation of a license plate is known and the morphology method requires knowing the sizes of characters, these two approaches are not appropriate for our application because of their required assumptions. Relaxation labeling is by nature iterative and often time consuming. In this study, a hybrid of connected components and blob coloring techniques is considered for character separation.[25]There have been a large number of character recognition techniques reported. They include genetic algorithms , artificial neural networks [17], [2], [16], fuzzy c-means [26], support vector machine [25], Markov processes [16], and finite automata [6]. These methods can be broadly classified into iterative and noniterative approaches. There is a tradeoff between these two groups of approaches; iterative methods achieve better accuracy, but at the cost of increased time complexity. In this study, we pay more attention to accuracy than time complexity whenever a choice has to be made between them. For this, we developed our own character recognition technique, which is based on the disciplines of both artificial neural networks and mechanics.[1]The rest of this paper is organized as follows. In Section II, the types of license plates to be considered are described, followed by the fundamental idea of the proposed LPR technique. The two primary stages of the proposed technique, license plate location and license number identification, are discussed in detail in Sections III and IV, respectively. Experimental results are presented in Section V. Concluding remarks and ideas for future work are given in Section VI.", "body": "A UTOMATIC license plate recognition (LPR) plays an important role in numerous applications such as unattended parking lots Of the various working conditions, outdoor scenes and nonstationary backgrounds may be the two factors that most influ-Manuscript received In addition, pointable cameras create dynamic scenes when they move, pan or zoom. A dynamic scene image may contain multiple license plates or no license plate at all. Moreover, when they do appear in an image, license plates may have arbitrary sizes, orientations and positions. And, if complex backgrounds are involved, detecting license plates can become quite a challenge.Typically, an LPR process consists of two main stages: 1) locating license plates and 2) identifying license numbers. In the first stage, license plate candidates are determined based on the features of license plates. Features commonly employed have been derived from the license plate format and the alphanumeric characters constituting license numbers. The features regarding license plate format include shape, symmetry The license plate candidates determined in the locating stage are examined in the license number identification stage. There are two major tasks involved in the identification stage, character separation and character recognition. Character separation has in the past been accomplished by such techniques as projection There have been a large number of character recognition techniques reported. They include genetic algorithms The rest of this paper is organized as follows. In Section II, the types of license plates to be considered are described, followed by the fundamental idea of the proposed LPR technique. The two primary stages of the proposed technique, license plate location and license number identification, are discussed in detail in Sections III and IV, respectively. Experimental results are presented in Section V. Concluding remarks and ideas for future work are given in Section VI.II. LPRIn this section, the styles of license plate that are considered in this study are discussed, followed by a brief description of the proposed LPR process. Table As shown in Table Fig. In Sections III and IV, we look at the details of the license plate locating module and the license number identification module.III. LICENSE PLATE LOCATING MODULEA. Basic ConceptsA flowchart for the license plate locating module is shown in Fig. Next, the RGB space of the input color image is transformed into the HSI space. Let and denote the (red, green, blue) and (hue, saturation, intensity) values of an image pixel, respectively. The transform from to It is inevitable that maps , , , and are less than perfect in view of noise, measurement error, and imperfect processing. In order to compensate for this drawback, we appeal to the soft computing techniques rooted in fuzzy (for license plate location) and neural (for license number identification) disciplines. Let , , , and be the fuzzy versions of , , , and . The entries in the fuzzy maps represent the degrees of belonging to a license plate. A two-stage fuzzy aggregator is introduced to integrate the maps. In the first stage, fuzzy maps , , and are integrated. The resulting map next combines with in the second stage leading to a single map, denoted . The reason of using the two-stage aggregator is because the intrinsic characteristics (related to color) of , , and are different from that (related to edge magnitude) of . Afterwards, based on map , interesting regions are located in the input image, which have locally maximal values. License plate candidates are then determined to be those interesting areas whose sizes are large enough.B. Color Edge DetectionThe color edge detector focuses on only three kinds of edges (i.e., black-white, red-white and green-white edges). Consider a black-white edge, and suppose that the input RGB color image has been normalized into an image. Ideally, the values of a white pixel and a black pixel should be and , respectively. Their differences are either or , so all the components of the difference vector between a white and a black pixel will have the same sign. This property is considerably stable under environmental variations. A black-white edge pixel is then defined based on this property as follows. An image pixel is regarded as a black-white edge point if all of the signs of components of the difference vector between the pixel and one of its neighbors are the same, i.e., , , where is the set of neighbors of the image pixel. We also store its edge magnitude defined as . Edge magnitudes will be exploited later to derive fuzzy edge maps.In a similar way, an image pixel is considered to be a red-white edge point if its difference vector for some satisfies the following conditions: 1) and 2) and . The magnitude of the edge pixel is defined as . Finally, an image pixel is regarded to be a green-white edge pixel if for some , 1) and 2) and . Its edge magnitude is determined by . Image pixels, which are not edge points, are given zero edge magnitudes.C. Fuzzy MapsThe basic idea of generating a fuzzy map from a given map (e.g., , , , or ) is as follows. Since every map encodes some characteristic about the scene, the entry of any cell in the map expresses the degree of the cell possessing the property. In order to highlight the cells corresponding to the objects of interest (e.g., license plates), we assign large entries to those cells that are compatible with the known characteristics of the objects. Such large entries indicate a high degree of existence of an interesting object. We call the resultant map the characteristic map of the original map.Since the input data (both the given map and the object characteristics) are not perfect, uncertainties should be taken into account during the computation of the characteristic map. Fuzzy sets have been known to provide an elegant tool for modeling uncertainties 1) Map: Consider the universal set of hue values. Suppose that the object of interest has a color whose corresponding hue value is . Given an entry in map , say , the membership degree, , of the entry belonging to fuzzy set \"like the object\" can be written where is a positive constant. If the given entry is equal to that of the interesting object , then the degree of membership is 1. As the difference between the hues increases, the degree of membership decreases to an asymptotic value of 0. Recall that there are four colors (black, white, red and green) utilized in the license plates that are of interest. Let and be the hue values for red and green, respectively. Note that the hues of achromatic colors (i.e., all levels of grey, including black and white) are not defined since the denominator of the equation for hue in (1) is zero. Therefore, we will highlight red and green, but not white and black based on map . The membership function of fuzzy map is finally defined as 2) Map: Since fuzzy map can only express the colors red and green, we need other means to handle black and white. According to the in (1), all achromatic colors have the same saturation . In addition, this value is smaller than that of any chromatic color. Based on these two facts, we generate a fuzzy map from map for distinguishing between chromatic and achromatic colors. The membership function of is defined as 3) Map: While chromatic and achromatic colors can be separated from each other based on their saturation values, black and white have to be further differentiated from other achromatic colors. For this, we count on the intensity map . Since the intensity values of black and white correspond to the two extreme values on the intensity axis of the HSI coordinate system, the following function emphasizes the colors with intensity values close to the two extremes This assumes that the working environment has an average intensity of 0.5. However, both black and white will be distorted under some circumstances. For example, a white color may appear grey in a dark environment, as will a black color in a bright environment. To compensate for such distortion, the constant 0.5 in the above equation may be replaced with the average value, , of map . We then define the membership function of fuzzy map as (4)4)Map: Based on fuzzy maps , , and image areas with black, white, red, or green colors can be distinguished. However, a large portion of these areas has nothing to do with license plates. Edge maps play a crucial role in discriminating against irrelevant regions. Since there are many close-by edges in a license plate and distributed in a repetitive manner, an image pixel whose neighbors possess large edge magnitudes will have a high possibility that it belongs to a license plate. Hence, we define the membership function of fuzzy edge map as D. Fuzzy AggregationEach fuzzy map provides information for locating license plates in the input image. There are two ways to draw a conclusion from a set of maps. In the first, intermediate decisions are made on the basis of individual maps and a final conclusion is drawn from the intermediate decisions. In the second, multiple maps are first integrated into a single map, and a final conclusion is then drawn from the integrated map. Since the first method involves both numerical computations and symbolic decisions, we prefer the second approach, which includes only numerical computations. Following the second approach, fuzzy maps , , , and are integrated into a single map, , with decisions being made on the basis of . A two-stage fuzzy aggregator is introduced for this purpose.In the first stage of the aggregator, fuzzy maps , , and are integrated cell by cell. Let , , and be the entries of the corresponding cells in fuzzy maps , , and . The aggregator integrates the entries by Recall that in the definition of a fuzzy map a large entry indicates a high degree of possibility that the entry belongs to a license plate. However, if the majority of cells having small variations in the entry (i.e., having nearly uniform distribution of entries) are in a fuzzy map, the usefulness of the map for detecting license plates deteriorates. To see this, consider a picture taken in the evening or on a rainy day. On the whole, the picture will look dim. The overall intensities of image pixels tend to be small, which in turn leads to large saturation values throughout the picture [see IV. LICENSE NUMBER IDENTIFICATION MODULEA. Fundamental Idea Fig. Since the camera may be rolled and/or pitched with respect to license plates, it is desirable that their images be transformed to a predefined size and orientation before performing license number identification. However, without information about relationships between the camera and working environments, the transformations can only be conducted blindly or by trial-anderror. In the proposed method since the transformation step is omitted, it is inevitable that difficulties in the subsequent steps will increase.Considering a license plate candidate, it is first binarized. Since some information will somehow be lost during binarization, a variable thresholding technique previously proposed by Nakagawa and Rosenfeld In order to eliminate undesired image areas, a connected component algorithm is first applied to the binarized plate candidate. The aspect ratios of connected components are then calculated. The components whose aspect ratios are outside a prescribed range are deleted. Then an alignment of the remaining components is derived by applying the Hough transform to the centers of gravity of components. The components disagreeing with the alignment are removed. If the number of remaining components is still larger than a prescribed number (eight in practice), connected components are deleted one at a time starting with the smallest. Here, we choose eight as the prescribed number because a license number consists of five or six characters and characters may be broken. The removal process continues until either of two conditions is satisfied. Either the number of remaining components equals the prescribed number, or a dramatic change size from the previously removed component to the current one under consideration is encountered. We assume that noise components are much smaller than characters.The above procedure does not guarantee that each of the surviving components will correspond to an individual character. A component may be due to noise, an incomplete character, a distorted character, or characters that appear to touch. To distinguish them, we utilize attributes of license plates, including the aspect ratios of individual characters, the regular intervals between characters, and the number of characters constituting license numbers. We refer to these attributes collectively as the structural constraints of license plates. We also introduce the operators of delete, merge, split and recover into the character segmentation procedure. Note that characters may be missed during license plate location and binarization. The recover operator is introduced to retrieve missing characters.During processing the segmentation procedure applies the first three operators (delete, merge, and split) to the set of surviving components in an attempt to determine if a component satisfies the structural constraints of license plates. If such a component can be determined, the character recognition procedure is invoked to identify a character from the component. The above process repeats until no character can be extracted from the set of surviving components. Thereafter, if the number of extracted characters is less than the number of characters in license numbers, the recover operator starts at the outermost characters of those detected and searches for characters along the alignment of the known characters. The search continues until no character can be retrieved within an extent determined by the average width of characters as well as intervals between characters. Next, the collection of identified characters is verified in the confirmation stage, where the compositional semantics of license numbers plays an important role. The set of characters will be deemed to form a valid license number if it agrees with the compositional semantics.B. Optical Character RecognitionIn this subsection we discuss the character recognition procedure. Since, as already mentioned, license plates may be bent and/or tilted with respect to the camera, characters extracted from such license plates may be deformed. Furthermore, input characters may be noisy, broken or incomplete. Character recognition techniques should be able to tolerate these defects. In this study, we develop our own character recognition approach to suit our particular application. The proposed approach consists of three steps: character categorization, topological sorting, and self-organizing (SO) recognition. In the first step, the input character is distinguished as numerical or alphabetical. This is easily accomplished by referring to the compositional semantics of license numbers. In the next step, the topological features of the input character are computed and are compared with those of prestored character templates. Compatible templates will form a test set, in which the character template that best matches the input character is determined. The template test is performed by a SO character recognition procedure.1) Topological Sorting: The topological features of characters utilized in this study include the number of holes, endpoints, three-way nodes, and four-way nodes (see Fig. 2) Template Test: The templates in the test set are matched against the input character and the best match is determined. The template test is primarily accomplished using a SO character recognition approach, which is based on Kohonen's SO neural network Let be the set of character templates surviving from the topological sorting of an unknown input character. Let denote the computed dissimilarities between the unknown character and the character templates. It is natural that the character template having the smallest dissim- ilarity with the unknown character is taken to be the class to which the unknown character belongs.a) SO Neural Model: In this subsection, we brief the key components of the Kohonen SO neural model, which will be used in the later practical implementation. Referring to Fig. Let be the weight of the link between SO neuron and input neuron . The weight vector for is , where is the number of input neurons. Let denote an external stimulus. The input to due to the stimulus is , where represents a position vector. Let be the weight of the connection between SO neurons and located at and , respectively. The input to due to lateral interaction is . Next, the winner together with its neighbors, say set , engage in a group learning process. During this process a neuron close to the winner will gain a high rate of learning while a neuron located far from the winner will have a low rate of learning. The rate of learning is governed by the Gaussian function . The learning rule for the neurons in is then defined as ( b) Practical Implementation: To begin, we group characters into three categories, referred to as 0-hole, 1-hole, and 2-hole, according to the number of holes contained in the characters. Each category has its own associated SO neural network, which contains 40 SO neurons and two input neurons. The difference among the three neural networks is primarily in their configurations of SO layer (see Fig. Referring to the example shown in Fig. Suppose that a template (\"L\" in the example) chosen from the test set for the input character is to be matched against the character. Rather than the entire template, just its contour serves as the stimulating pattern, which repeatedly innervates the neural network until its synaptic weights stabilize. In our implementation the contour points are fed into the input layer of neural network one at a time. Consider an input contour stimulus point . The SO neurons of the network compete for the stimulus. The winner is determined by , where 's are the weight vectors of the 40 SO neurons. The winner and its first-and second-order neighbors, call them set , join in the following learning process c) Remarks: The proposed character recognition approach has difficulty distinguishing character pairs (8, B) and (O, D) especially when they are distorted. To overcome this, we predefine an ambiguity set containing the characters 0, 8, B and D. For each character in the set, the nonambiguous parts of the character are specified (see Fig. Our character recognition method gives different measurements of dissimilarity for the same character with different tilt angles with respect to the camera. Currently, this issue has not troubled us because the characters extracted from the images of license plates are all in a nearly upright position. But, we may improve our algorithm to deal with this by introducing a normalization step to transform license into a prescribed orientation prior to license number V. EXPERIMENTAL RESULTS Two groups of images have been collected for our experiments. The first contains 639 images (640 by 480 pixels) taken from 71 cars of different classes. For each car, nine images were acquired from fixed viewpoints whose positions are illustrated in Fig. The second group contains 449 images (768 by 512 pixels), some of which are shown in Fig. A common failure of the locating module is the failure to detect the boundaries of license plates. This occurs when vehicle bodies and their license plates have similar colors. For this, boundaries of license numbers were often extracted instead of the entire license plates. License plate images may be kept for further testing if the computed aspect ratio of the rectangular area containing the license numbers agrees with the constraint. The location success rate achieved with the second group of images (449 images) is 96.7%. Combining this rate with the rate  (98.8%) of the first group (639 images), the overall rate of success for the license plate locating module is 97.6%.The license plates (1061 plates in all), whose dimensions range from (320 95) to (80 45) pixels, extracted in the previous experiment were used in the experiment for license number identification. The images of license plates are rarely perfect (see Fig. The success rate for identification with the set of 1061 license plates is 95.6%. Combining this rate with the location success rate (97.9%), the overall rate of success for our LPR algorithm is 93.7%.Currently, our LPR algorithm is running on a Pentium IV-1.6 GHz PC. The license plate location module takes about 0.4 seconds to find all license plate candidates in an image. However, the license number identification module takes about two seconds (primarily due to the neural-based OCR process running on a sequential computer) to process one license plate candidate. If there are multiple plate candidates located in an image, the total processing time may not be adequate for real-time applications. Several things may be considered to compensate for the high time complexity. Firstly, since the proposed technique assumes no prior information about license numbers, if such information is available (e.g., databases of license numbers), both the processing time and the recognition rate will improve. Secondly, if parallel machines, such as transputers, n-cubes, or PC clusters, can be used, multiple license plate candidates and character templates will be able to be processed in parallel. As a consequence, the topological sorting step becomes unnecessary. Finally, some firmware components of the algorithm could be replaced by hardware.VI. CONCLUDING REMARKS AND FUTURE WORK", "conclusions": "Compared to most previous work that in some way restricted their working conditions, the techniques presented in this paper are much less restrictive. The proposed LPR algorithm consists of two modules, one for locating license plates and one for identifying license numbers. Soft computing techniques rooted in fuzzy (for license plate location) and neural (for license number identification) disciplines were introduced to compensate for uncertainties caused by noise, measurement error and imperfect processing. Although the proposed algorithm is concerned with the license plates of one specific country, many parts in the algorithm are readily extended to use with license plates of other countries. Specifically, since color and edge are two fundamental features of license plates, the color edge detector introduced in the locating module is readily adapted to other color schemes by replacing the color parameters embedded in the detector. Since numerals and Roman letters are commonly used to form license numbers, the proposed SO OCR technique is applicable to any similarly constituted license plates.It is well known that a mixture of top-down (expectation-driven) and bottom-up (data-driven) procedures often perform better than either in isolation. Currently, the locating and identification modules both perform in somewhat of a hybrid top-down and bottom-up manner. Location determination is guided by both the color information of license plates and the compositional semantics of license numbers, while identification is based on prebuilt templates and the compositional semantics. A higher degree of combining top-down with bottom-up processing may be used in some applications, such as the control of restricted or secure areas, the detection of stolen vehicles, and the management of car pools, where license information of the cars of interest can be known a priori.In our future work, techniques for deriving intrinsic images (e.g., illumination, reflectance and depth images) from a scene image or a number of input images are recommended. Intrinsic images containing only one intrinsic characteristic of the scene are viewpoint dependent and can be of great use for many visual inferences, such as image segmentation, view-based template matching, and object reconstruction. Recall that at the identification stage we have omitted a normalization step to trans-form extracted license plates to a prescribed size and orientation. Adding this step would improve the performance of license number identification. However, normalization requires knowing the boundaries of either license plates or license numbers. The former may be invisible if vehicle bodies and license plates have similar colors, while detecting boundaries of license numbers can be error-prone. We leave these issues to be considered in future study. Furthermore, the proposed neural approach for character recognition is basically unsupervised. In general, supervised methods can outperform unsupervised ones if rich training sets are available. We may later investigate supervised approaches.A number of strategies have been introduced to reduce the time complexity of the proposed LPR algorithm. The color edge detector reduces the processing time by ignoring irrelevant edges at an early stage; the topological sorter limits the set of template candidates for character test at the identification stage. Obviously, there are more things that can be done to improve the processing time. However, in order to make our techniques applicable to real-time applications in less restrictive working conditions, the topics regarding replacing firmware components with hard-wired ones and using parallel machines should be studied.", "SDG": [3]}, "autonomous_vehicle_perception_the_technology_of_today_and_tomorrow": {"name": "Transportation Research Part C", "abstract": " Perception system design is a vital step in the development of an autonomous vehicle (AV). With the vast selection of available off-the-shelf schemes and seemingly endless options of sensor systems implemented in research and commercial vehicles, it can be difficult to identify the optimal system for one's AV application. This article presents a comprehensive review of the state-of-the-art AV perception technology available today. It provides up-to-date information about the advantages, disadvantages, limits, and ideal applications of specific AV sensors; the most prevalent sensors in current research and commercial AVs; autonomous features currently on the market; and localization and mapping methods currently implemented in AV research. This information is useful for newcomers to the AV field to gain a greater understanding of the current AV solution landscape and to guide experienced researchers towards research areas requiring further development. Furthermore, this paper highlights future research areas and draws conclusions about the most effective methods for AV perception and its effect on localization and mapping. Topics discussed in the Perception and Automotive Sensors section focus on the sensors themselves, whereas topics discussed in the Localization and Mapping section focus on how the vehicle perceives where it is on the road, providing context for the use of the automotive sensors. By improving on current state-of-the-art perception systems, AVs will become more robust, reliable, safe, and accessible, ultimately providing greater efficiency, mobility, and safety benefits to the public.", "keywords": "Automotive,sensors,Autonomous,vehicles,Intelligent,vehicles,Localization,and,mapping,Machine,vision,Sensor,fusion", "introduction": "Autonomous vehicle (AV) technology is making a prominent appearance in our society in the form of advanced driver assistance systems (ADAS) in both research and commercial vehicles. These technologies aim to reduce the amount and severity of accidents, increase mobility for people with disabilities and the elderly, reduce emissions, and use infrastructure more efficiently . One of the major motivations accelerating the advancement of AV technologies is their insusceptibility to humanrelated errors, such as distraction, fatigue, and emotional driving, which currently cause approximately 94% of accidents according to a statistical survey completed by the National Highway Traffic Safety Administration (NHTSA) (Fagnant and Kockelman, 2015).(Singh, 2015)As research, testing, and deployment of vehicles with AV technology is escalating around the world, the development of standardized guidelines and regulations has become a major focus to ensure safe integration into society. The U.S. Department of Transportation and the NHTSA have recently adopted the Society of Automotive Engineers international standard for automation levels which define autonomous vehicles from Level 0 (the human driver has full control) to Level 5 (the vehicle completely drives itself) . Currently, due to limitations and high costs of available sensors, most commercial vehicles only include Level 1 to Level 2 autonomy, which require constant driver attention and control. The autonomous features in these vehicles generally consist of emergency braking, blind spot detection, and/or lane keeping. Nonetheless, Level 3 autonomous features are available in the Tesla Model S and Model X. However, recent accidents have initiated concerns regarding the drivers' understanding and capability of using the technology safely (Transportation, 2016).(Krisher and Durbin, 2016)Presently, a major concern is the occurrence of new, unsafe driving practices as a result of drivers who do not understand or are not aware of how the AV technologies work (Kyriakidis et al., 2017;. Furthermore, in order for autonomous features and vehicles to have significant, far-reaching effects in improving safety, mobility and efficiency, the public must understand the capabilities of the technology Lu et al., 2016)(Kyriakidis et al., 2015(Kyriakidis et al., , 2017;;. This includes important factors such as the limitations of the technology, the application for the technology, and the appropriate scenarios to use and/or rely on the technology.Lu et al., 2016)As a brief overview, autonomous vehicle navigation can be visualized as five main components (Fig. ): Perception, Localization and Mapping, Path Planning, Decision Making, and Vehicle Control 1. Perception uses sensors to continuously scan and monitor the environment, similar to human vision and other senses (Cheng, 2011). Localization and mapping algorithms calculate the global and local location of the ego-vehicle and map the environment from sensor data and other perception outputs (Maurer et al., 2016). Path planning determines possible safe routes for the ego-vehicle based on perception, and localization and mapping information (Maurer et al., 2016). The decision-making component is responsible for calculating the optimal route based on the possible paths, the current vehicle state, and the environment information (e.g., road attributes, weather conditions, road signs, etc.) (Katrakazas et al., 2015). The vehicle control module will then calculate the appropriate vehicle command (torque, acceleration, steering wheel angle, etc.) in order to follow the optimal route decision, such as a lane change, a right turn, or another maneuver (Maurer et al., 2016). It is important to note that the autonomous navigation process is a high frequency recursive process. This allows AVs to effectively handle high-speed motion and dynamic objects, such as pedestrians, motorcycles, and cars (Gruyer et al., 2016b)). An overview of the autonomous navigation process is shown in Fig. (Julier and Durrant-Whyte, 2003. This paper will focus on the \"Perception\" and \"Localization and Mapping\" stages.1This paper aims to provide a comprehensive review of the state-of-the-art AV perception technology to address a lack of synthesized information about sensor, hardware, and algorithm requirements for effective AV perception. In general, robust and reliable perception, and localization and mapping are required in order to make accurate and reliable decisions for vehicle control. This paper provides a review of the current sensor technology used for perception, as well as an overview of the methods used for localization and mapping.In essence, this paper aims to answer the following questions:\u2022 Which sensors are currently used in prominent research and commercial vehicles?\u2022 What are the current advantages and shortcomings of the sensors?\u2022 Which localization and mapping techniques are being used in research and commercial vehicles with respect to sensing the egovehicle's environment?\u2022 What are the shortcomings of these localization and mapping methods and how can they be improved?\u2022 What are the current areas of research that need to be addressed? \u2022 How will this technology evolve in the future?The paper is organized as follows. Section 2 introduces the background of AV research. Section 3 discusses the capabilities and shortcomings of automotive sensors commonly used in research and commercial vehicles. Section 4 provides an overview of localization and mapping techniques. Section 5 discusses the future of research in AV perception, providing useful insight for experienced AV researchers and engineers, and Section 6 concludes the review paper. Note that, although obstacle detection and tracking, road detection and tracking, ego-localization, and environment estimation are subcomponents of AV perception, these topics are considered outside of the scope of this paper and will not be discussed in detail. Further information about these topics may be found in .Zhu et al. (2017)It is worth to note that this paper's Localization and Mapping section was included to provide the context for how the ego-vehicle senses its surroundings and to illustrate how perception and sensing are closely related to localization and mapping. For example, an AV must know where it is on the road (through localization) to better estimate which objects are in its surroundings. A more practical example might be if an AV has sensed (through localization and mapping) that it is nearing a crosswalk, the vehicle should prioritize sensing for pedestrians around the crosswalk (which can only be done if the vehicle has localized itself). The link between perception and sensing, and localization and mapping works in the opposite direction as well. For example, if a vehicle senses road markings to the left and right of the vehicle, then the vehicle is likely to be in the center of the road, and thus, localization algorithms should be updated accordingly. Furthermore, the sensor technology (e.g., GPS, IMU, LIDAR, etc.) discussed in the Perception and Automotive Sensing section informs how localization and mapping algorithms are implemented, as discussed in the Localization and Mapping section.Finally, note that the purpose of this paper is to provide an overview of current AV perception research and to identify areas of AV perception that require further work. This paper is meant to be used as a guide for future research, as well as provide essential information about perception for newcomers to the AV field. Thus, detailed analyses or rankings of perception algorithms and current sensors (which change regularly with the rapid advancement of research in this field) are outside of the scope of this paper. Nonetheless, the reader is strongly encouraged to investigate such analyses further.This article can also benefit researchers through providing tables and figures that synthesize AV research in the following areas:\u2022 Table : challenges in AV research and the extent to which the challenges have been addressed1\u2022 Table : advantages and disadvantages of common passive and active sensors \u2022 Table 2: sensor arrangements in prominent research and commercial vehicles \u2022 Table 3: autonomous features available in commercial vehicles \u2022 Table 4: localization and mapping methods used by prominent research vehicles \u2022 Fig. 5, 4 and 5: various autonomous navigation processes \u2022 Figs. 2 and 3: AV history and milestones1Researchers may refer to these figures and tables when developing presentations, deciding on future research areas, choosing AV sensors, or developing new autonomous features.", "body": "Autonomous vehicle (AV) technology is making a prominent appearance in our society in the form of advanced driver assistance systems (ADAS) in both research and commercial vehicles. These technologies aim to reduce the amount and severity of accidents, increase mobility for people with disabilities and the elderly, reduce emissions, and use infrastructure more efficiently As research, testing, and deployment of vehicles with AV technology is escalating around the world, the development of standardized guidelines and regulations has become a major focus to ensure safe integration into society. The U.S. Department of Transportation and the NHTSA have recently adopted the Society of Automotive Engineers international standard for automation levels which define autonomous vehicles from Level 0 (the human driver has full control) to Level 5 (the vehicle completely drives itself) Presently, a major concern is the occurrence of new, unsafe driving practices as a result of drivers who do not understand or are not aware of how the AV technologies work As a brief overview, autonomous vehicle navigation can be visualized as five main components (Fig. This paper aims to provide a comprehensive review of the state-of-the-art AV perception technology to address a lack of synthesized information about sensor, hardware, and algorithm requirements for effective AV perception. In general, robust and reliable perception, and localization and mapping are required in order to make accurate and reliable decisions for vehicle control. This paper provides a review of the current sensor technology used for perception, as well as an overview of the methods used for localization and mapping.In essence, this paper aims to answer the following questions:\u2022 Which sensors are currently used in prominent research and commercial vehicles?\u2022 What are the current advantages and shortcomings of the sensors?\u2022 Which localization and mapping techniques are being used in research and commercial vehicles with respect to sensing the egovehicle's environment?\u2022 What are the shortcomings of these localization and mapping methods and how can they be improved?\u2022 What are the current areas of research that need to be addressed? \u2022 How will this technology evolve in the future?The paper is organized as follows. Section 2 introduces the background of AV research. Section 3 discusses the capabilities and shortcomings of automotive sensors commonly used in research and commercial vehicles. Section 4 provides an overview of localization and mapping techniques. Section 5 discusses the future of research in AV perception, providing useful insight for experienced AV researchers and engineers, and Section 6 concludes the review paper. Note that, although obstacle detection and tracking, road detection and tracking, ego-localization, and environment estimation are subcomponents of AV perception, these topics are considered outside of the scope of this paper and will not be discussed in detail. Further information about these topics may be found in It is worth to note that this paper's Localization and Mapping section was included to provide the context for how the ego-vehicle senses its surroundings and to illustrate how perception and sensing are closely related to localization and mapping. For example, an AV must know where it is on the road (through localization) to better estimate which objects are in its surroundings. A more practical example might be if an AV has sensed (through localization and mapping) that it is nearing a crosswalk, the vehicle should prioritize sensing for pedestrians around the crosswalk (which can only be done if the vehicle has localized itself). The link between perception and sensing, and localization and mapping works in the opposite direction as well. For example, if a vehicle senses road markings to the left and right of the vehicle, then the vehicle is likely to be in the center of the road, and thus, localization algorithms should be updated accordingly. Furthermore, the sensor technology (e.g., GPS, IMU, LIDAR, etc.) discussed in the Perception and Automotive Sensing section informs how localization and mapping algorithms are implemented, as discussed in the Localization and Mapping section.Finally, note that the purpose of this paper is to provide an overview of current AV perception research and to identify areas of AV perception that require further work. This paper is meant to be used as a guide for future research, as well as provide essential information about perception for newcomers to the AV field. Thus, detailed analyses or rankings of perception algorithms and current sensors (which change regularly with the rapid advancement of research in this field) are outside of the scope of this paper. Nonetheless, the reader is strongly encouraged to investigate such analyses further.This article can also benefit researchers through providing tables and figures that synthesize AV research in the following areas:\u2022 Table \u2022 Table Researchers may refer to these figures and tables when developing presentations, deciding on future research areas, choosing AV sensors, or developing new autonomous features.History and background informationSince before the twenty-first century, researchers and industrial leaders have been competing to develop the first fully autonomous vehicle that is robust, reliable and safe enough for real-world and high-speed driving environments. Major contributors to early AV research can be attributed to AV tests and competitions held around the world. These competitions provided opportunities for industry and researchers to assess the capabilities and boundaries of AVs in various driving environments. However, more importantly, they identified major difficulties and shortcomings in AV software and hardware, some of which remain unresolved today.One of the first long-distance AV road tests, \"No Hands Across America,\" was introduced in 1995 Moreover, through these tests, vehicle developers noted many areas in AV technology requiring significant improvement. These areas included image processing and other perception techniques; driving in complex, urban scenarios and poor weather conditions; and improving erroneous obstacle and road marking detection In 2003, the next major competition was initiated by the Defense Advanced Research Projects Agency (DARPA) which required vehicles to drive without the aid of road markings through an off-road desert course After the DARPA Grand Challenges, AV research steadily increased (see Fig. \u2022 RA VisLab Intercontinental Autonomous Challenge(VIAC) (2010) \u2022 LA\u2022 Vehicle platooning without a priori information\u2022 LA\u2022 Autonomous driving without a priori information \u2022 Real-world platooning using V2V communication\u2022 RA perception, and localization techniques to localize the vehicle in the environment and detect, classify and track obstacles Although the DARPA Urban Challenge tested the AVs in scenarios much closer to everyday driving, these challenges still lacked important and common roadway obstacles, such as pedestrians and cyclists. Additionally, the AVs were not required to detect traffic lights or signs, as these were omitted from the challenge or provided by RNDF files To address a lack of real-world AV testing, in 2010, the VisLab Intercontinental Autonomous Challenge (VIAC) was initiated. In this challenge, two AVs (a leader and a follower) drove across multiple countries, encountering urban, off-road, and highway scenarios, as well as a myriad of weather conditions. Impressively, the vehicles autonomously drove through these situations with no a priori maps or prior knowledge about standard road shapes and sizes for guidance. Instead, the follower vehicle detected the leader \u2022 Good for classification \u2022 Difficult to measure distances \u2022 Provides additional information about the environment (color, texture, etc.) \u2022 Sensitive to lighting conditions Stereovision\u2022 Obstacle detection and classification\u2022 Depth perception similar to human eyes (effective at close range) \u2022 Lane detection (via intensity measurements)\u2022 Multi-layer LIDAR allows robust 3D construction \u2022 Difficulty detecting highly reflective objects \u2022 Poor very near (< 2 m) measurement Radar\u2022 Obstacle detection\u2022 Direct distance measurements \u2022 Long range radar have small FOVs \u2022 High accuracy \u2022 Direct distance measurements \u2022 Poor angular resolution Connected vehicle technology, such as vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication, may be able  Note: Not all vehicles were included in this table due to the fast growth and vastness of the industry and research in this area.Table 4Available autonomous features in high-end commercial vehicles.Vehicles Autonomous Freeway DrivingAutonomous Lane ChangeSemi-Autonomous ParkingSemi-Autonomous BrakingBMW750i xDrive (BMW, 2017; Sherman, 2016) \u2713 \u2713 \u2713 Ford (high-end production vehicles) to resolve some of the existing problems resulting from heavy reliance on a priori information. V2I communication could provide a network for intersections, road signs and construction signs to transfer important infrastructure information, such as road layout changes, speed limits, and traffic light information to AVs In 2011, a challenge in the Netherlands called the \"Grand Cooperative Driving Challenge\" (GCDC) aimed to accelerate \"cooperative driving\" technology. In this event, teams of AVs cooperatively drove along a contained highway and intersection, transmitting signals to each other to indicate their intentions. The event' s objective was to complete the challenges quickly (in terms of throughput), fuel-efficiently, and stably through employing V2V and V2I communication Table Research VehiclesA priori method SLAM-based methodAudi' s research vehicle a Heavily reliant on certain a priori information, but not highly detailed, pre-created maps.  Other influential projects include the Programme for European Traffic with Highest Efficiency and Unprecedented Safety (PROMETHEUS) The SARTRE project, which ran from 2009 to 2012, was a large-scale connected-vehicle project. In this project, aspects of vehicle platooning, from the strategies and safety benefits of vehicle platooning to the efficiency and aerodynamics of vehicle platooning, were investigated. A major outcome of this project was the development and implementation of safe connected, AV platoons, which drove in various European countries including Sweden and Spain Although each of these challenges and projects (see Fig. \u2022 AV perception in poor weather and lighting conditions \u2022 AV perception in complex urban environments \u2022 Autonomous driving without heavy reliance on a priori perception data \u2022 Utilization of connected vehicle technology to improve accuracy, certainty and reliability of perception \u2022 Development of safety measures in case of faulty sensors/perception A summary of the challenges exposed by the AV projects and competitions is provided in Table Perception and automotive sensorsSimilar to human drivers, AVs must be able to continuously observe their surroundings and accurately calculate their location on both a local (relative to the curb, obstacles, intersections, etc.) and global (which street, neighborhood, city, etc.) scale. To provide AVs with these abilities, sensors are installed in and around the vehicle.Automotive sensor technology overviewAs an overview, automotive sensing falls into three main categories: self-sensing, localization, and surrounding-sensing Proprioceptive and exteroceptive sensors can be categorized as either active or passive sensors Localization typically uses a combination of sensors such as GPSs, IMUs, odometers, and cameras (by matching between primitives and a map, i.e., SLAM) for high precision results The combination of low-cost IMUs, which can localize reliably for short periods of time (such as through tunnels), and GPSs, which can localize reliably for long periods of time, but may lose connection when in remote areas or through tunnels, can effectively reduce positioning errors and provide localization information during GPS outages In general, AV developers use different combinations of light-, sound-, and/or vision-based sensors for detection of the vehicle' s surroundings. These sensors can be either passive or active with most current passive sensors being less expensive than active sensors, such as the active sensor, LIDAR Nevertheless, vision-based systems have their own shortcomings. A key concern with vision-based systems is that computing distance information requires complex algorithms, whereas active sensors can directly determine this information with high accuracy. Stereovision (which consists of two cameras mounted at a horizontal distance from one another) can provide 3D information such as depth; however, when the point of interest is far away (e.g., on the horizon), the images collected by each camera become effectively the same, and are no longer able to provide the required information for 3D perception For more information about the advantages and disadvantages of common passive and active sensors, please refer to Table To compensate for individual shortcomings, sensors can be coupled via \"multisensor fusion.\" Multisensor fusion's benefits, including improving perception accuracy, reliability, and robustness, are widely known and used in AV research Sensor fusion is also not restricted to fusing data from multiple sensors. Sensor fusion can also be performed by fusing data from multiple readings of a single sensor to obtain a more reliable output Sensor fusion is widely used for a number of reasons including more reliable data outcomes, more coverage, applicability, and above all, lower initial investment (equipment cost) though at a higher computation cost. The lattermost reason is often why sensor fusion is preferred over using single, highly accurate sensors. The initial monetary cost of a highly accurate sensor is generally significantly higher than the cost of two low-cost sensors, which can generally achieve similar or better results to single sensor algorithms via sensor fusion To further illustrate the usefulness of sensors based on their accuracy and cost, consider the IMU. An cost effective IMU ($10-$100) alone is generally not accurate enough for any robotics or AV applications, but can be used in a sensor fusion system to increase accuracy to a useful level. A mid-level IMU ($1000-$2000) may be sufficient for certain navigation problems (short-term motion estimation), but it is neither accurate enough to be used as a sole sensor (i.e., dead reckoning) nor affordable for widespread AV use. Finally, high-end IMUs (>$10000), which are often used in aircrafts, are quite accurate, but remain beyond the reach of AV applications due to high initial costs The sensor arrangements chosen for AV research and commercial vehicles are noticeably linked to the vehicles' specific autonomous application and desired maneuvers. For example, the vehicles in the DARPA Urban Challenge were generally outfitted with multiple, expensive LIDAR and radar sensors, but lacked sonar sensors, since the challenges did not focus on low-speed, precise maneuvers (such as parallel parking). Conversely, many production vehicles, such as the Tesla Model S and the Mercedes-Benz S class, include ultrasonic (sonar) sensors for parking maneuvers, but do not use LIDAR, due to current high costs. Infrared cameras, which are used to detect pedestrians and other obstacles at night, are predominantly found on production vehicles, possibly since research vehicles may primarily drive during the day. Other commercial vehicles, such as Renault, Volvo, and Audi, use Mobileye technology. This technology consists of multiple mono-cameras with different focal points. It is affordable and efficient for motorways, albeit less effective in suburban and urban situations Furthermore, Table Areas for automotive sensor and perception improvementEven though sensor technology has rapidly advanced, there are still many areas that require improvement before level five autonomy can be achieved. Three major challenges related to sensor technology that remain unresolved are: (1) perception in poor weather conditions, (2) perception in changing and unfavorable lighting conditions and (3) human perception of automotive sensors.Perception in poor weather conditionsPerception in poor weather conditions such as snow, heavy rain, and fog is an important AV research topic as these scenarios continuously prove to be problematic even for human drivers. In snowy conditions, it has been found that both vision-based and LIDAR-based systems have extreme difficulty As noted in Table In Perception in changing and unfavorable lighting conditionsLens-flares, large shadows and other unfavorable lighting conditions also present difficulties for perception. For example, vision systems may confuse large shadows to be parts of other objects Other perception systems attempt to address light condition problems by relying on a priori information about the environment. Stanford updated their research vehicle, Junior, to be able to detect traffic lights in differing lighting conditions using an a priori list of traffic light locations Other methods may instead rely on active sensors, such as LIDAR, to overcome the lighting condition problem. Such sensors do not require external light for perception, allowing them to detect obstacles in poor light and at night Human perception of automotive sensorsIn order for autonomous features to effectively increase safety, there must be a certain level of public acceptance and understanding of AV technology, including an understanding of the capability of AV sensors. As discussed previously, automotive sensors are integrated into vehicles to assist human drivers with tasks such as detecting obstacles, maintaining a set speed, and emergency braking. When humans do not understand the capability of sensors or the sensors are insufficient for the driving task, the human driver may either rely too much on the sensor or disregard the sensor's readings entirely. Thus, it is important for sensors to be sufficient for the driving task and as capable and robust as possible. Furthermore, human drivers must be informed of the capability of sensors to prevent misunderstandings that could potentially cause accidents.To further illustrate the need for improved sensors as well as increased public understanding, the following example is provided. Long-range automotive radar, which typically has the longest range of all AV active sensors, only has a range of 4.5-7.5 s at highway speeds (150-250 m range Thus, even though sensor ranges may in fact be sufficient, it is important to ensure the public understands the technology' s capacity (e.g., its ability to adequately detect obstacles in typical conditions) as well as its limitations (e.g., its inability to detect obstacles past a certain distance or in poor weather conditions). According to Evidently, it is important to ensure that the sensors being used are as capable and robust as possible, and that the public understands the benefits of AV technology and how to use this technology effectively. Nonetheless, since the vast topic of human factors (HF) in automation is outside the scope of this paper, additional articles of interest that address this topic include Although many commercial and research vehicles contain varied sensor combinations to increase perception reliability and robustness, much work still needs to be done before level five autonomy is completely achieved. Sensor reliability in poor weather conditions, sensor cost (especially active sensor cost), and the public' s understanding of autonomous sensors and systems must improve before fully autonomous commercial vehicles should be available for public use. Furthermore, monitoring systems must be developed to detect and identify system failures (e.g., faulty sensors/actuators, algorithmic errors) and degraded operating conditions to ensure the AV' s sensors are working correctly while on the road. Although this topic is not the focus of this paper, it will be briefly discussed in the next section, as it is important to consider when developing any AV technology.Sensor failure and operating condition monitoringOnce AVs can robustly perceive the environment, they will also need to be able to detect and identify sensor and perception failures and/or degradations. An analogy can be made between a human driver perceiving an illuminated engine light and deciding to safely pull over, and an AV determining that one of its sensors is compromised and safely maneuvering the vehicle off the road.To address this difficulty in aerospace systems, often multiple identical sensors are incorporated to increase redundancy, validate each sensor' s correct operation through comparison, and thus determine whether any of the sensors are faulty. However, duplicating sensors can be costly and inefficient In autonomous road vehicle studies, researchers have found some success using similar model-based methods An alternative proposed in Perception AlgorithmsThis section provides a brief, high-level overview of algorithms currently used in AV perception. Note that this paper does not aim to review or survey perception algorithms in detail (other papers, including Localization and mappingLocalization and mapping has been a popular research topic for many years. It has evolved from stationary, indoor mapping for mobile robot applications, to outdoor, dynamic, high-speed localization and mapping for AVs. The initial problem researchers faced was determining how to measure the environment while moving to create reliable and accurate maps for successful navigation through indoor areas. This required the robot to determine where it was relative to obstacles and walls while simultaneously creating (and storing) an accurate map. This process became known as Simultaneous Localization and Mapping (SLAM) Initial algorithmsInitial SLAM algorithms were generally variants of occupancy grid maps or topological/feature-based maps After the initial SLAM problem was solved, the next difficulty was determining whether obstacles were dynamic or stationary. In initial SLAM algorithms, once a robot recognized an obstacle, the robot would always avoid that area on the map, regardless if it was a stationary or moving object. To address this challenge, algorithms needed to determine not only the likelihood of an object actually being there (depending on the reliability and accuracy of the robot' s sensors), but also the likelihood of an obstacle being there in the future. One early method used a feature-based map that determined where features were presently, and predicted where objects would move in the next algorithmic cycle To move towards AV localization and mapping, researchers began developing road-based algorithms. These were initially based on robotic SLAM algorithms; however, important differences between robotic and road vehicle mapping (such as the environment surrounding the vehicle and the speed of the vehicle) became apparent. Specifically, the majority of the robotic algorithms were designed to map indoor, highly structured, well-lit environments, rather than outdoor, variably-lit, road-based environments (in which AVs need to localize themselves globally (on the earth) as well as locally (on the road)). Furthermore, AVs operate at highwayspeeds, rather than indoor walking speeds, which require faster and more efficient mapping algorithms.Global and local localizationUnlike indoor robots, AVs must be able to: In In other techniques, AVs map and localize while driving. This is known as Simultaneous Localization and Mapping (SLAM) Increasing the efficiency of localization and mappingSince vehicles drive at much higher speeds than indoor robots, AVs require faster and more efficient algorithms. This is typically addressed through one of two methods: (1) by using pre-created, detailed and/or HD dynamic maps (leaving only localization to be completed during the autonomous driving) and ( A Priori and HD Mapping and LocalizationMany state-of-the-art vehicles, such as the Google, Uber and Navya Arma vehicles, use a priori mapping methods In Google' s vehicles, which also use a priori mapping and localization methods, have autonomously driven over 2.4 million kilometers, including driving through complex situations such as intersections An emerging technique under the umbrella of a priori mapping and localization includes HD mapping. HD maps are based on a priori data; however, they are dynamic, updated in the cloud, and utilize V2I and V2V communication to improve accuracy. Essentially, HD maps provide centimeter-accurate a priori map information to AVs, and AVs provide the cloud-based HD maps with updated map information based on collected sensor data. The maps are continually updated in this manner offline Simultaneous localization and mapping (SLAM)Vehicle localization performed while on the road (i.e., the localization portion of Simultaneous Localization and Mapping (SLAM)) does not rely heavily on a priori information. This allows AVs to continuously observe the environment and readily adapt to new situations. Consequently, localization through SLAM requires more computationally intensive algorithms and may be subject to more uncertainty depending on the sensors used and surroundings. These algorithms must differentiate between static objects in the environment (which can help with localization) and dynamic objects (which should not be used for localization, since they move) without any prior knowledge. Many of these algorithms currently rely on road marking detection for local localization, which pose problems when the road markings have faded, are partially occluded (by weather or other objects), or are non-existent Various probabilistic techniques have been researched for discriminating between static and dynamic objects in the environment using SLAM-based methods. Early techniques include vehicles from the DARPA Urban Challenge. Carnegie Mellon University' s vehicle, Boss, differentiated between static and dynamic obstacles by flagging them as either \"moving,\" \"not-moving,\" or \"observed moving,\" and then transferred these obstacles from a static map to a dynamic map and vice versa, allowing the vehicle to plan different routes based on obstacles' predicted motions Although Carnegie Mellon' s entry did not pre-map the Urban Challenge course (and thus can be considered to be using online, SLAM-based techniques), it relied heavily on a priori information, such as road shape and accurate road position Other vehicles have used similar SLAM-based techniques. VisLab developed multiple AVs that relied on perception (such as camera vision) for local navigation and used sparsely detailed maps for global localization When developing SLAM-based autonomous driving algorithms, it is important to consider current human driver behavior and ensure AVs can safely react to this behavior In In In situations with multiple vehicles, V2V technology can optimize SLAM-based, lane-level localization. A Markov-based information sharing approach is used to communicate GPS data between vehicles, resulting in improved lane-level localization over single-GPS localization in The Tesla Model S is a well-known example of a semi-autonomous commercial vehicle that primarily uses real time SLAM-based techniques. The Tesla' s autopilot feature allows the vehicle to drive itself along freeways and highways, performing lane changes and autonomously adjusting its speed (under constant human supervision). However, the Tesla Model S cannot drive by itself through complex situations, such as intersections Recent emerging autonomous navigation approachesUp to this point, it has been assumed that the autonomous navigation process includes the following stages: environmental perception, localization and mapping, path planning, decision-making, and vehicle control. However, some researchers are developing AV algorithms that skip the localization and mapping and path planning stages 1 and rely purely on environmental perception to make driving decisions. Thus, instead of the process depicted in Fig. One of the first examples of an AV using the behavior reflex approach is the Autonomous Land Vehicle in a Neural Network (ALVINN) In the direct perception approach, the vehicle completes sections of the mapping-related computation (for example, determining the vehicle' s current angle on the road and the distance to surrounding vehicles and lane markings) but does not create a complete local map or any detailed trajectory plans. Thus, this approach skips the majority of the localization and mapping stage and the entirety of the path planning stage. This process is depicted in Fig. In Nonetheless, the direct perception approach developed in Chen et al. ( predictively useful. This provides grounds for testing more complex machine learning techniques with the direct perception approach Furthermore, The direct perception, deep learning approach to autonomous driving provides an interesting alternative to classical AV techniques. Depending on the results of future research, this approach may change how AV perception is managed and achieved.Future research areas and AV technology advancementsThe previous sections in this article provided information about AV history, AV perception sensors, and AV localization and mapping while alluding to areas of research that need improvement. This section provides an in-depth summary of areas of AV perception requiring further improvement as well as information about recent related advancements in technology.Automotive sensor research areas and advancementsThis section provides a summary of future areas of research to improve AV sensors and information about recent advancements in AV sensors. The following list summarizes areas related to AV sensors needing further development:\u2022 Improving detection and reducing uncertainty in poor lighting and weather conditions \u2022 Improving detection and reducing uncertainty in complex environments \u2022 Reducing uncertainty in sensor data by cross-verifying obstacle locations and signals through:-Further development of sensor fusion algorithms -Use of more sensors and sources for sensor fusion to build multiple layers of environment modeling -V2V and V2I communication\u2022 Ensuring the public understands sensor capacity and limitations \u2022 Using more passive sensors (versus active sensors) or developing effective algorithms to counteract the increased density and therefore interference of active sensor signals\u2022 Decreasing the overall cost of AV sensor systems by: -Further developing sensor fusion algorithms using low-cost sensors -Utilizing new low-cost, highly-effective sensors that may be introduced in the near future (e.g., new LIDAR from Innoviz \u2022 Developing fault detection and isolation systems for automotive sensors and algorithms \u2022 Using multisensor data fusion in order to limit the impact of sensors' drawbacks and exploit the advantages of each sensor through using the sensors' complementarity and redundancy in order to improve accuracy, certainty, and reliabilityAs several key challenges remain unresolved for AV perception, many companies are competing for a piece of the AV technology market. Osram Opto Semiconductors just recently announced the development of a four channel LIDAR with a mass production price predicted to be less than fifty dollars Additionally, Seegrid, a manufacturing robotics company, uses stereovision as the primary sensor for autonomous navigation Tesla uses a similar concept (and used to utilize Mobileye technology Localization and mapping research areas and advancementsIn terms of localization and mapping, continued research to increase the reliability, efficiency and robustness of the algorithms is required before vehicles can reliably localize themselves in all driving situations. This may be completed by improving a priori based and/or SLAM-based localization and mapping methods. Additionally, it is expected that localization and mapping techniques will continually be updated as the improved automotive sensing techniques become available. Further research focused on improving a priori mapping and localization methods may include:\u2022 Developing a system to compensate for the lack of large, interconnected, highly detailed databases that will be required for autonomous driving based on a priori maps, potentially through HD map technology\u2022 Addressing poor localization in snow, rain and fog; in areas with fewer landmarks (such as on bridges or along long straight sections of roads); and along roads that have undergone large changes (such as construction sites) through: -Improved a priori information about such situations and landmarks, and/or -Improved SLAM through better sensing techniques and sensors\u2022 Incorporating cooperative driving techniques (V2V and V2I) into localization and mapping to reduce uncertainty and improve vehicle safety -How old is too old for an a priori map? -Should vehicles legally be allowed to drive using an old map? -How will vehicles become aware of traffic pattern changes, such as construction zones? SLAM-based methods also must be improved in terms of efficiency, accuracy, reliability, and robustness for complex situations. Further research to improve localization and mapping in real time may include:\u2022 Improving the efficiency of local localization and mapping algorithms to be used in real time (and not analyzed after the fact, as in Satzoda and Trivedi (2015))\u2022 Improving the robustness of SLAM in poor weather conditions \u2022 Enhancing GPS reliability and accuracy, or reducing the cost of DGPS technology \u2022 Utilizing further V2V and V2I communication to reduce localization uncertainty Although the DARPA Urban Challenge attempted to address problems with a priori-based localization by preventing the entrants from pre-mapping road sections, the vehicles that placed first and second still relied heavily on a priori information, such as generic road structure and road markings. Furthermore, the entries that did not rely heavily on a priori information also did not compete in complex situations, such as urban areas with pedestrians and cyclists. Other vehicles that localize through SLAM have partially addressed such situations, including the Tesla, but still lack the ability to autonomously drive through any situation other than straightforward highway/freeway driving.Determining a proper balance between relying on a priori information to increase algorithm efficiency and current perception data to increase the vehicle' s ability to adapt to new situations remains an unresolved challenge in AV research. Certain aspects of a priori data have proven to be necessary, such as address coordinates, whereas other a priori data is useful, but unnecessary, such as road shape or detailed, pre-mapped 3D images. Future AVs will need to rely on a combination of current perception data and a priori information; however, it is clear that further research and development is required to provide reliable, accurate, and robust enough perception algorithms for full autonomy. In fact, perception stages remain essential to guaranteeing the development of efficient, fully automated applications.Furthermore, in recent works the direct perception approach and other deep learning driving techniques have been proposed to improve performance (without relying on a priori information) by streamlining the autonomous navigation process by eliminating the localization and mapping and path planning stages or using end-to-end deep learning techniques Although sensor technologies allow AVs to outperform human perception in many areas, AVs are not yet ready to provide level five autonomy. With the aforementioned improvements, however, AVs will be much closer to providing the efficiency, mobility, and safety benefits predicted.Conclusions", "conclusions": "In this paper, an overview of AV sensor technology, localization and mapping techniques, and future perception research areas were presented. Although current perception systems implemented in Level 1 to Level 3 autonomous systems have been shown to increase vehicle safety, there is still much to improve upon before fully autonomous vehicles will be available to the public. The three main areas requiring improvement presented in this paper include: (1) reduction of uncertainty in perception, (2) reduction in cost of perception systems, and (3) operating safety for sensors and algorithms. A fourth area not discussed in this paper, but that is of equal importance, includes the efficiency of computational methods and algorithms for AV perception. These methods comprise a large amount of research conducted in the AV field. Researchers are continually aiming to improve the efficiency of detection and classification, localization and mapping, and other AV perception related algorithms (Huang et al., 2017;. All in all, with further research and development of AV perception systems, AVs will likely be driving on public roads while increasing driving safety, sustainability and mobility in the near future.Jagannathan et al., 2017)", "SDG": [3]}, "avec_2017_real_life_depression_and_affect_recognition_workshop_and_challenge": {"name": "\u2022General and reference \u2192 Performance; \u2022Computing methodologies \u2192 Biometrics", "abstract": " Real-life depression, and a ect\" will be the seventh competition event aimed at comparison of multimedia processing and machine learning methods for automatic audiovisual depression and emotion analysis, with all participants competing under strictly the same conditions. e goal of the Challenge is to provide a common benchmark test set for multimodal information processing and to bring together the depression and emotion recognition communities, as well as the audiovisual processing communities, to compare the relative merits of the various approaches to depression and emotion recognition from real-life data. is paper presents the novelties introduced this year, the challenge guidelines, the data used, and the performance of the baseline system on the two proposed tasks: dimensional emotion recognition (time and value-continuous), and dimensional depression estimation (value-continuous).", "keywords": "A ective Computing,Social Signal Processing,Automatic Emotion/Depression Recognition", "introduction": "e 2017 Audio-Visual Emotion Challenge and Workshop (AVEC 2017) will be the seventh competition event aimed at comparison of multimedia processing and machine learning methods for automatic audiovisual analysis of emotion and depression, with all participants competing under strictly the same conditions .[19, 26-28, 30, 31]e goal of the Challenge is to compare the relative merits of the approaches for audiovisual emotion recognition and severity of depression estimation under well-de ned and strictly comparable conditions, and establish to what extent fusion of the approaches is possible and bene cial. e main underlying motivation is the need to advance emotion recognition and depression estimation for multimedia retrieval to a level where behaviors expressed during human-human, or human-agent interactions, can be reliably sensed in real-life conditions, as this is exactly the type of data that applications would have to face in the real world.AVEC 2017 shall help raise the bar for emotion and depression detection by challenging participants to estimate levels of depression and a ect from audiovisual data captured in real-life conditions, and will continue to bridge the gap between research on emotion and depression recognition and low comparability of results.", "body": "e 2017 Audio-Visual Emotion Challenge and Workshop (AVEC 2017) will be the seventh competition event aimed at comparison of multimedia processing and machine learning methods for automatic audiovisual analysis of emotion and depression, with all participants competing under strictly the same conditions e goal of the Challenge is to compare the relative merits of the approaches for audiovisual emotion recognition and severity of depression estimation under well-de ned and strictly comparable conditions, and establish to what extent fusion of the approaches is possible and bene cial. e main underlying motivation is the need to advance emotion recognition and depression estimation for multimedia retrieval to a level where behaviors expressed during human-human, or human-agent interactions, can be reliably sensed in real-life conditions, as this is exactly the type of data that applications would have to face in the real world.AVEC 2017 shall help raise the bar for emotion and depression detection by challenging participants to estimate levels of depression and a ect from audiovisual data captured in real-life conditions, and will continue to bridge the gap between research on emotion and depression recognition and low comparability of results.Novelties and Challenge Guidelinese A ect Sub-Challenge (ASC) is based on a novel database of human-human interactions recorded 'in-the-wild': SEWA 1 data set. Hence, audiovisual signals were not recorded with high-quality equipments and in dedicated laboratory rooms with ideal recording conditions, but in various places (e. g., home, work place) and with arbitrary personal equipments. In addition to those new challenging conditions tailored to real-life applications of a ective computing technologies, we introduce the prediction of likability, along the usual (time-and value-continuous) emotional dimensions: arousal and valence. e Depression Sub-Challenge (DSC) is a re ned rerun of the AVEC 2016 challenge \u2022 A ect Sub-Challenge (ASC) participants are required to perform fully continuous a ect recognition of three a ective dimensions: Arousal, Valence, and Likability, where the level of a ect has to be predicted for every moment of the recording. e competition measure is the concordance correlation coe cient (CCC) where \u03c1 is the Pearson correlation coe cient between two time series (e. g., prediction and gold-standard), \u03c3 2x and \u03c3 2 is the variance of each time series, and \u00b5 x and \u00b5 are the mean value of each.\u2022 Depression Sub-Challenge (DSC): participants are required to assess the depression severity of the interviewed subject, where the target depression severity is based on the self-report PHQ-8 scores recorded prior to every humanagent interaction. For the DSC, performance in the competition will be measured using the root mean square error (RMSE). Participants in the competition, however, are also encouraged to provide classi cation output whether the participant also scored as depressed or not depressed according to the PHQ-8 score, i. e., score >= 10. In addition, participants are also encouraged to report on overall accuracy, correlation with the PHQ-8 score, average precision, and average recall to further analyse their results. As an additional novelty over the AVEC 2016 DSC, we encourage participants to provide symptom predictions, i. e., values of 0-3 for each of the eight questions on the PHQ-8 depression inventory 2 .1 h p://sewaproject.eu 2 h p://patienteducation.stanford.edu/research/phq.pdf Both Sub-Challenges allow contributors to nd their own features to use with their regression algorithm. In addition, standard feature sets are provided for audio, video, and text separately, which participants are free to use. e labels of the test partition remain unknown to the participants, and participants have to stick to the de nition of training, development, and test partition. ey may freely report on results obtained on the development partition, but are limited to ve trials per Sub-Challenge in submi ing their results on the test partition. Ranking will rely on the scoring metric of each respective Sub-Challenge, i. e., RMSE for the DSC, and CCC for the ASC.To be eligible to participate in the challenge, every entry has to be accompanied by a paper submi ed to the AVEC 2017 Data Challenge and Workshop, and presenting the results and the methods that created them, which will undergo peer-review by the technical program commi ee. Only contributions with a relevant accepted paper will be eligible for challenge participation. e organisers will not participate in the Challenge themselves, but will re-evaluate the ndings of the two best performing systems of each Sub-Challenge. e remainder of this article is organised as follow: we introduce the corpus and baseline features for the ASC and the DSC in Section 2 and Section 3, respectively, baseline methods and results obtained for the two Sub-Challenges are then presented in Section 4, before concluding in Section 5.AFFECT ANALYSIS CORPUSe corpus used in the AVEC 2017 ASC is a subset of the Sentiment Analysis in the Wild (SEWA) database 1 . is data set consists of audiovisual recordings of subjects showing spontaneous and natural behaviours. All recordings were collected 'in-the-wild', i. e., using standard webcams and microphones from the computers in the subjects' o ces or homes. e subset of the SEWA database exploited for the ASC is the video chat recording of German subjects.Subjects participated in pairs and were given the task of discussing a commercial they have just viewed. e commercial was a 90 seconds long video clip advertising a tap. e participants were allowed to discuss arbitrary aspects of the commercial, e. g., if it was produced well, if it was too long, or the usefulness of the product itself.e maximum duration of the dyadic conversation was 3 minutes, but participants were free to stop the video chat at any time. Each conversational partner was required to know their chat partner beforehand (relatives, friends, or colleagues), in order to ensure an unreserved discussion. e data set was recorded using an online platform through the OpenTok API 3 .e subset of the SEWA database used for the ASC consists of 32 pairs in total, i. e., 64 subjects.e data is provided in three partitions (Training, Development, and Test), where both partners of one video chat appear in the same partition. Di erent combinations of gender are included, cf. Table Emotion Analysis Labelse video chat recordings were annotated time-continuously in terms of the emotional dimensions arousal, valence, and liking, i. e., how much a subject expresses a positive or a negative a itude while speaking, either with respect to the commercial, the advertised product, or any other ma ers discussed. e annotation process was conducted by 6 annotators (3 female, 3 male) aged between 20 and 24. No annotator is present in the database and all were German native speakers. Each dimension was annotated separately; the video chat recordings of each subject were shown in random order to each annotator, who was asked to rate the current expressed emotional dimension using a joystick on a continuous scale.In order to create one unique gold-standard from the annotations, the six single annotations for each dimension were processed in the following manner. First, as the ratings from the joysticks are nonuniform, a Hermitian resampling to the nal annotation sample rate (100 ms, 10 fps) was performed. e resulting contours are then normalised to the range of \u22121 to +1, based on the peak amplitude of the joysticks and median ltered (with a width of 3 samples).en, in order to a enuate the e ect of a di erent interpretation of the scale, the normalised and ltered ratings are standardised to the average standard deviation of all annotators.One single gold-standard EWE for each audio-visual sequence n and dimension d is then formed exploiting the evaluator weighted estimator (EWE) approach based on the inter-rater agreement, similar to the basic approach described by Schuller where the index k denotes the annotator k = 1, 2, . . . , K and y n,d,k denotes the pre-processed annotation values. e annotator-speci c weight r n,d,k is a di erent one for each sequence and dimension and computed as follows. First, the pairwise linear correlation coe cients (CC) r n,d,(k,k i ) between the annotations of all raters k are computed, as well as the autocorrelation (obtained when k = k i ). en, the mean pairwise correlation for each annotator k is computed using:e weight of the unreliable annotators, i. e., where r n,d,k < 0 is then set to zero (r n,d,k = 0). is ensures that negatively correlated annotations are not taken into account in the gold-standard e resulting coe cients are nally normalised with respect to their sum to obtain an annotator-speci c weight:e average inter-rater agreement 1N N n=1 r n,d,k for each annotator k and dimension d is given in Table Emotion Analysis Baseline FeaturesBelow we describe the features that were extracted for the A ect Analysis sub-challenge.  As the LLDs capture only very local information in time, a segment-level representation of the features is required, especially when static predictors are used, such as Support Vector Machine (SVMs). In the ASC, two di erent types of segment-level features are provided, using the eGeMAPS LLDs: functionals (as de ned in the eGeMAPS feature set) and bag-of-audio-words (BoAW). e la er feature type was introduced for text features originally, but has been successfully applied also to others modalities, such as the audio and the video domain Di erent methods can be employed to create a codebook of audio words VideoFeatures. e video feature set consists of three types of features related to the position and expression of the subjects' face:(1) Face orientation (pitch, yaw, and roll) in degrees (3 features) (2) Pixel coordinates for 10 eye points (x and y coordinates, i. e., 20 features in total) (3) Pixel coordinates for 49 facial landmarks (x and y coordinates, i. e., 98 features in total) e facial features have been extracted for each video frame (frame step 20 ms) using the C face tracker To obtain a segment-level representation, bag-of-video-words (BoVW) were computed from the normalised facial features using the same segment lengths as for the BoAW (6 seconds). e process to generate the BoVW is the same as described above, however, separate codebooks and histograms were created for each three facial feature type, with a codebook size of 1 000 each, resulting in a nal segment-level feature vector of length 3 000.Text Features.In addition to audio and video features, a bag-of-words feature representation based on the transcription of the speech are generated with XBOW and used as additional features. e dictionary for these textual features is learnt from the training partition taking only the terms with at least two occurrences into account. is results in a dictionary of 521 words, where only unigrams are considered. As for the acoustic and facial features, the histograms are created over a segment of 6 s in time and the logarithm is taken from the term frequencies. In total, the bag-of-text-words (BoTW) features contain 521 features.DEPRESSION ANALYSIS CORPUSe Distress Analysis Interview Corpus -Wizard of Oz (DAIC-WOZ) database is part of a larger corpus, the Distress Analysis Interview Corpus (DAIC) ese interviews were collected as part of a larger e ort to create a computer agent that interviews people and identi es verbal and non-verbal indicators of mental illness Depression Analysis Labelse level of depression is labelled with a single value per recording using a standardised self-assessed subjective depression questionnaire, the PHQ-8 Depression Analysis Baseline FeaturesIn the following sections we describe how the publicly available baseline feature sets are computed for either the audio or the video data. For ethical reasons, no raw video is made available.Audio Features.For the audio features we utilised CO-VAREP (v1.3.2), a freely available open source Matlab and Octave toolbox for speech analyses ese methods have been successfully shown to be correlated with psychological distress and depression \u2022 Voice quality: Normalised amplitude quotient (NAQ), quasi open quotient (QOQ), the di erence in amplitude of the rst two harmonics of the di erentiated glo al source spectrum (H1H2), parabolic spectral parameter (PSP), maxima dispersion quotient (MDQ), spectral tilt/slope of wavelet responses (peak-slope), and shape parameter of the Liljencrants-Fant model of the glo al pulse dynamics (Rd) \u2022 Spectral: Mel cepstral coe cients (MCEP0-24), harmonic model and phase distortion mean (HMPDM0-24) and deviations (HMPDD0-12).In addition to the feature set above, raw audio and transcripts of the interview are being provided, allowing the participants to compute additional features on their own. For more details on the shared features and the format of the les participants should also review the DAIC-WOZ documentation.Video Features.Based on the OpenFace 7 framework [2], we provide di erent types of video features 8 : 7 h ps://github.com/TadasBaltrusaitis/OpenFace 8 For additional information consult the challenge manual provided a er entering the challenge.\u2022 Facial landmarks: 2D and 3D coordinates of 68 points on the face, estimated from video \u2022 Histogram of oriented gradients (HOG) features on the aligned 112x112 area of the face \u2022 Gaze direction estimate for both eyes \u2022 Head pose: 3D position and orientation of the head \u2022 Action units (AUs): {AU01, AU02, AU04, AU05, AU06, AU09, AU10, AU12, AU14, AU15, AU17, AU20, AU25, AU26}CHALLENGE BASELINESFor transparency and reproducibility, we use standard and opensource algorithms for both Sub-Challenges; toolbox 9 . We describe below how the baseline system was de ned and the results we obtained for each modality, as well as on the fusion of all modalities.EmotionAn emotion recognition baseline system is obtained using the BoAW, BoVW, and BoTW features with a segment length of 6 seconds described above and a Support Vector Regression (SVR).Generally, time-continuous annotations obtained in real-time su er from a certain delay as the annotators cannot react on changes in the shown emotion immediately To train the SVR, the L library DepressionWe computed the depression severity baseline using random forest regression. e only hyper-parameter in this experiment was the number of trees \u2208 {10, 20, 50, 100, 200}. For both audio and video the best performing random forest has 10 trees. Regression was performed on a frame-wise basis as the classi cation and temporal fusion over the interview was conduced by averaging of outputs over the entire screening interview. Fusion of audio and video modalities was performed by averaging the regression outputs of the unimodal random forest regressors. e performance for both root mean square error (RMSE) and mean absolute error (MAE) for Development and Test sets is provided in Table CONCLUSION", "conclusions": "We introduced AVEC 2017 -the fourth combined open Audio/Visual Emotion and Depression Severity Assessment Challenge. It comprises two Sub-Challenges: (i) ASC, where the level of a ective dimensions of arousal, valence, and -for the rst time -likability has to be infered from audiovisual data collected 'in-the-wild' during human-human interactions, and (ii) DSC, where a self-reported level of depression needs to be estimated from audiovisual data collected during human-machine interactions. is manuscript described AVEC 2017's challenge conditions, data, baseline features and results. By intention, we opted to use open-source so ware and the highest possible transparency and realism for the baselines, by refraining from feature space optimisation, using less number of trials as given to participants for reporting results on the test partition. In addition, baseline scripts have been made available in the data repositories, which should help improving the reproducibility of the baseline results.  Any opinions, ndings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily re ect the views of the United States Government, and no o cial endorsement should be inferred.", "SDG": [3]}, "a_fuzzy_logic_based_model_to_predict_biogas_and_methane_production_rates_in_a_pilot_scale_mesophilic_uasb_reactor_treating_molasses_wastewater": {"name": "A fuzzy-logic-based model to predict biogas and methane production rates in a pilot-scale mesophilic UASB reactor treating molasses wastewater", "abstract": " A MIMO (multiple inputs and multiple outputs) fuzzy-logic-based model was developed to predict biogas and methane production rates in a pilot-scale 90-L mesophilic up-flow anaerobic sludge blanket (UASB) reactor treating molasses wastewater. Five input variables such as volumetric organic loading rate (OLR), volumetric total chemical oxygen demand (TCOD) removal rate (R V ), influent alkalinity, influent pH and effluent pH were fuzzified by the use of an artificial intelligence-based approach. Trapezoidal membership functions with eight levels were conducted for the fuzzy subsets, and a Mamdani-type fuzzy inference system was used to implement a total of 134 rules in the IF-THEN format. The product (prod) and the centre of gravity (COG, centroid) methods were employed as the inference operator and defuzzification methods, respectively. Fuzzy-logic predicted results were compared with the outputs of two exponential non-linear regression models derived in this study. The UASB reactor showed a remarkable performance on the treatment of molasses wastewater, with an average TCOD removal efficiency of 93 (\u00b13)% and an average volumetric TCOD removal rate of 6.87 (\u00b13.93) kg TCOD removed /m 3 -day, respectively. Findings of this study clearly indicated that, compared to non-linear regression models, the proposed MIMO fuzzy-logic-based model produced smaller deviations and exhibited a superior predictive performance on forecasting of both biogas and methane production rates with satisfactory determination coefficients over 0.98.", "keywords": "Molasses,wastewater,Up-flow,anaerobic,sludge,blanket,Fuzzy-logic,Non-linear,regression,Modeling", "introduction": "Distillery effluents from sugar cane factories using molasses as a raw material are characterized by highly organically polluted wastewaters, and can have serious impacts on the environment if disharge untreated. Molasses-based distilleries are one of the most polluting industries generating large volumes of high-strength wastewater . Apart from high organic content, distillery wastewater also contains high concentration of nutrients in the form of nitrogen (1660-4200 mg/L), phosphorus (225-3038 mg/L) and potassium (9600-17,475 mg/L) [1] that can lead to eutrophication of receiving water bodies [2][1,. The raw molasses wastewater is also characterized by moderately acidic (pH 4-5), very high total chemical oxygen demand (TCOD) (65,000-130,000 mg/L), high concentration of mineral salts and has a bad smell and dark brown color as the melanoidin pigment 3][1,[3][4]. This dark color hinders photosynthesis by blocking sunlight and is deleterious to aquatic life [5]. Moreover, studies focused on the water quality of a river contaminated with distillery effluent displayed high biological oxygen demand (BOD) values of about 1600-21,000 mg/L within a radius of 8 km [6][1,3,. Adequate treatment is therefore imperative before the effluent is discharged 7][1,3,4,.8]With environmental regulations becoming more stringent, regulatory compliance has also become a matter of increasing concern to the fermentation and food industries. Therefore, several types of processes have recently been proposed for treating the molasses wastewater to improve the quality of the final discharge in terms of residual pollutant contents. Sirianuntapiboon and Prongtong  have conducted studies on the removal of color substances in molasses wastewater by using combined biological and chemical treatment processes. The study concluded that color substances can be removed by simple coagulants (such as CaO for stillage and FeCl 3 for anaerobic treated molasses wastewater (An-MWW)). But to increase the color removal efficiency in coagulation step, the authors suggested the pretreatment of molasses wastewater by aeration with or without sludge-added for about 96 h. In another study conducted by Pena et al. [9], color removal from biologically pretreated molasses wastewater by means of chemical oxidation with ozone were explored. The study concluded that ozonation was an effective treatment to remove color but less effective to remove organic matter. The authors reported that, depending on the applied ozone dosage, color removal from 71% to 93% and TCOD reduction from 15% to 25% were reached after 30 min reaction time. In a recent study, Zhang et al. [5] proposed a novel UASB-MFC-BAF (up-flow anaerobic sludge blanket reactor-microbial fuel cell-biological aerated filter) integrated system for simultaneous bioelectricity generation and high-strength molasses wastewater treatment. The study concluded that TCOD, sulfate and color removal efficiencies of the proposed system were achieved of 53.2%, 52.7% and 41.1%, respectively. Moreover, each unit of this system had respective function and performed well when integrated together. Sirianuntapiboon and Prasertsong [4] carried out investigations on the treatment of molasses wastewater by acetogenic bacteria BP103 in a sequencing batch reactor (SBR) system. They used acetogenic bacteria BP103 cells as the absorbent for melanoidin pigment (MP) and molasses wastewater. The study concluded that the strain showed the highest TCOD, BOD 5 TKN (total Kjedahl nitrogen) and MP removal efficiencies of 65.2%, 82.8%, 32.1% and 50.2% on average, respectively. In another recent work, Liang et al. [10] applied coagulation/flocculation process in the polishing treatment of molasses wastewater on a bench-scale. The study concluded that ferric chloride was found to be the most effective in removing melanoidins from bio-treated molasses wastewater, achieving color and TCOD removal efficiencies of 98% and 89%, respectively. Finally, Sohsalam and Sirianuntapiboon [8] applied a surface flow constructed wetland (SFCW) system with Cyperus involucratus, Typha augustifolia and Thalia dealbata J. Fraser to treat An-MWW under the organic loading rates (OLR) between 612 and 1213 kg BOD 5 /ha-day. The proposed SFCW system showed that the highest SS (suspeded solids), BOD, TCOD, total phosphorus, ammonia nitrogen, nitrate nitrogen and molasses pigments removal efficiencies of 90-93%, 88-89%, 67%, 70-76%, 77-82%, 94-95% and 72-77% were detected under the lowest OLR of 612 kg BOD 5 /ha-day, respectively. Detailed information on several other existing and advances methods applied to the treatment of molasses-based distillery wastewater can be found in a comprehensive review of Satyawali and Balakrishnan [3].[1]In recent years, anaerobic digestion technology has become a technology of growing importance, especially for high-strength wastewater [11,. Although anaerobic digestion has been regarded as one of the beneficial and advantageous processes, particularly in treatment of highly polluted wastewaters, however this type of treatment is thought to be a difficult system due to its instability and complexity in solving this problem in a short time in a real-scale plant.12]Modeling is a valuable tool in both design and operation of biological treatment plants, and can be used for process optimization and testing of control strategies at a reasonable cost. Hence, modeling helps to develop a better understanding of the treatment processes and provides a significant potential for solving operational problems as well as reducing operational cost in a specific treatment process. Moreover, model results can be evaluated for different operating data before transferring the concepts to a full scale plant . In the literature, there are numerous studies such as integral dynamic modeling of the UASB reactor [13], mathematical simulation of the sludge blanket of UASB reactor [14], dynamic modeling of a singlestage high-rate anaerobic reactor [15], and mathematical modeling of a batch anaerobic digestion [16], conducted about the comprehensive and complex models to control and simulate several anaerobic treatment systems. In a recent study, Pontes and Pinto [17] conducted studies on the analysis of integrated kinetic and flow (or hydraulic) models for two anaerobic digesters, the UASB and the EGSB (expanded granular sludge bed) reactors. In the study, the flow models were found to be quite different for the UASB and EGSB reactors. Since many of the parameters used in the UASB reactor flow model can be critical for an accurate simulation, the study concluded that the volume variations of the sections of the UASB reactor were necessary to accurately describe the behaviour of such digester in non-steady-state. Because of the mechanisms associated with anaerobic processes are not adequately understood to formulate reliably, many of conventional models require simplifications of the process representation for a better understanding of the underlying phenomena in anaerobic digestion. Hence, more simple and useful models are required to overcome complexity and applicability [18]. Biyikoglu et al. [19] have reported that conventional numerical methods require much time to obtain accurate results, and sometimes it is not possible to reach a solution due to convergence problems regarding the type of governing equations and boundary conditions.[20]The performance of anaerobic digestion processes is complex and highly dependent on the configurations of the different reactors, and varies significantly with different influent characteristics and operational conditions [19,. Therefore, the system must be continuously monitored and controlled due to its instability in circumstance conditions, particularly in terms of biogas or methane production rates, providing an indication of the overall anaerobic biomass activity in the process 21]. Since anaerobic digestion process is very susceptible to fluctuations in process inputs such as organic loading rates, influent pH, and toxic organic compounds, biogas or methane production rates are highly dependent on the applied process conditions. Therefore, the complicated interrelationships among a number of system factors in the process may be explicated through a number of attempts in developing a representative knowledge-based prediction model allowing the investigation of the key variables in greater detail.[22]Because of their speed and capability of learning, robustness, predictive capabilities and non-linear characteristics, several artificial intelligence-based modeling techniques, such as artificial neural networks [23][24][25], fuzzy-logic [26][27,, adaptive neuro-fuzzy inference systems 28][19,, have recently been conducted in the modeling of various real-life processes in environmental engineering field. Among these methods, fuzzy-logic methodology has been successfully applied in a variety of ecological and environmental applications, ranging from mapping to modeling, evaluation and prediction tasks 29]. Fuzzy-logic-based models have also been conducted by many researchers as an established and promising method for modeling of various types of anaerobic processes [30][31][32][33][34][35][36][37][38]. However, there are no systematic papers in the literature specifically devoted to a study of an artificial intelligence-based modeling of biogas and methane production rates in a pilot-scale mesophilic UASB reactor treating molasses wastewater using the fuzzy-logic technique. Such an artificial intelligence-based control of real-time gas production rates may provide several potential advantages such as protection of the system from possible risks associated with significant fluctuations in influent characteristics, optimization of the process at a reasonable cost, providing a rapid evaluation and estimation of emissions on energetic basis, and development of a continuous early-warning strategy without requiring a complex model structure and tedious parameter estimation procedures. Therefore, clarification of the place of the present subject in the scheme of fuzzy-logic methodology can be considered as a particular field of investigation to evaluate in realtime biogas and methane production rates that are necessary to control the anaerobic process and to establish fault diagnosis.[39]Considering the above-mentioned facts, the specific objectives of this study were: (1) to develop a fast predicting MIMO (multiple inputs and multiple outputs) fuzzy-logic-based model for the estimation of biogas and methane production rates in a pilot-scale mesophilic UASB reactor running under various organic, hydraulic and alkalinity loading conditions; (2) to compare the proposed artificial intelligence-based model with the conventional non-linear regression approach by means of various descriptive statistics; and(3) to verify the validity of the MIMO fuzzy-logic model by several additional testing data sets which were not used in training the model.", "body": "Distillery effluents from sugar cane factories using molasses as a raw material are characterized by highly organically polluted wastewaters, and can have serious impacts on the environment if disharge untreated. Molasses-based distilleries are one of the most polluting industries generating large volumes of high-strength wastewater With environmental regulations becoming more stringent, regulatory compliance has also become a matter of increasing concern to the fermentation and food industries. Therefore, several types of processes have recently been proposed for treating the molasses wastewater to improve the quality of the final discharge in terms of residual pollutant contents. Sirianuntapiboon and Prongtong In recent years, anaerobic digestion technology has become a technology of growing importance, especially for high-strength wastewater Modeling is a valuable tool in both design and operation of biological treatment plants, and can be used for process optimization and testing of control strategies at a reasonable cost. Hence, modeling helps to develop a better understanding of the treatment processes and provides a significant potential for solving operational problems as well as reducing operational cost in a specific treatment process. Moreover, model results can be evaluated for different operating data before transferring the concepts to a full scale plant The performance of anaerobic digestion processes is complex and highly dependent on the configurations of the different reactors, and varies significantly with different influent characteristics and operational conditions Because of their speed and capability of learning, robustness, predictive capabilities and non-linear characteristics, several artificial intelligence-based modeling techniques, such as artificial neural networks Considering the above-mentioned facts, the specific objectives of this study were: (1) to develop a fast predicting MIMO (multiple inputs and multiple outputs) fuzzy-logic-based model for the estimation of biogas and methane production rates in a pilot-scale mesophilic UASB reactor running under various organic, hydraulic and alkalinity loading conditions; (2) to compare the proposed artificial intelligence-based model with the conventional non-linear regression approach by means of various descriptive statistics; and(3) to verify the validity of the MIMO fuzzy-logic model by several additional testing data sets which were not used in training the model.Materials and methodsSource of molasses wastewater and feed preparationMolasses was taken from a commercial sugar factory located in Adapazari, Turkey and stored in the refrigerator at 4 \u2022 C to minimize substrate decomposition before the experiment. The content of molasses used as feedstock (in g/kg molasses) was Total Chemical Oxygen Demand (TCOD): 800-900, ammonia nitrogen (NH 3 -N): 25-80, oxygen: 4-8, K + : 40-60, and Na + : 150-200, respectively. Weight percentages of other components in molasses (%) were determined as saccarose: 50, betaine: 5.5, K 2 O: 4.7, and others: 39.8, respectively.Satyawali and Balakrishnan It is noted that addition of other micronutrients (such as nickel and selenium) to the feed material in the form of their salts may also play an important role on the growth of microorganisms and help to increase biogas production.UASB set-up and operationThe molasses wastewater was anaerobically treated in a pilotscale UASB reactor under different organic and hydraulic loading conditions. The internal diameter, total height and total tank capacity of the system were 20 cm, 190 cm and 90 L, respectively. All parts of the reactor was made of ANSI 316 stainless steel. The reactor had a conical bottom of 20 cm length and a feed inlet pipe of about 1.0 cm diameter to avoid chocking during operation. An outlet weir was provided at the top (1.85 m), which was connected to an outlet pipe to the effluent collection tank. The reactor was equipped with five sampling ports, localized at 0.30, 0.45, 0.60, 0.75 and 0.90 from the bottom of the system. The diameter of each sampling port was about 1.5 cm.Biogas was collected from the headspace on the top of the reactor via a gas collecting system. The gas collecting and measuring system consisted of a gas-solid-liquid (GSL) separator (made from inverted plastic funnels of 15 cm diameter), a gas collecting pipe, a glass water trap used as hydrogen sulfide (H 2 S) scrubber and a wettip gas meter. The reactor was kept under mesophilic conditions (35.2 \u00b1 0.7 \u2022 C) by circulating the hot water through the external reactor jacket with a Fisher Isotemp 2100 (Fisher Company, Pittsburgh) immersion circulator. Heated water was pumped through the jacket surrounding the reactor and glass wool was used as an isolation material.Ward et al. Depending on the feed flow rates (from 40.5 to 165.1 L/day), the reactor was operated in a semi-continuous mode feeding (i.e. twice an hour for 15 min) by pumping of the fresh feed into the reactor and collecting effluent samples daily. The reactor was run for a period of about 2 years under various organic and hydraulic loading rates. Influent and effluent sampling was carried out once the steady-state period was achieved. In feeding, different target hydraulic retention times (HRTs) were achieved using a peristaltic pump (ColeParmer, Masterflex \u00ae ). Sufficient up-flow velocity was maintained to achieve proper fluidization inside the reactor. During the feeding of the reactor, the feeding tank was occasionally agitated with a glass rod to prevent the sedimentation of suspended solids, as well as to make the feeding solution homogeneous. In order to increase the efficiency of the digestion process, the reactor were seeded with anaerobic sludge (about 25% of the working volume) taken from the Kartonsan Factory Anaerobic Treatment Facility (Corlu, Istanbul, Turkey). A detailed schematic of the experimental set-up is depicted in Fig. Representation of model parametersIdentification of parameters that could be used for monitoring the biological treament system is an important factor for efficient operation of the anaerobic digestion processes. Choosing the most appropriate model components representing the behaviour of the studied process can help to recognize possible technical faults and to reduce operating costs of plants in the planning stage Effect of organic loading rateThe organic loading rate (OLR) is an important parameter significantly affecting microbial ecology and characteristics of anaerobic systems. This parameter integrates the operational characteristics of the reactor, and bacterial mass and activity into the volume of media Effect of volumetric TCOD removal rateThe volumetric TCOD removal rate (R V ) is a function of influent flow rate, working volume of the reactor, and the difference between influent and effluent substrate concentrations Effect of alkalinityAlkalinity refers to the ability of a solution to resist changes in pH. Alkalinity is important because as acid is added to solution, carbonates will contribute hydroxide ions, which tend to neutralize the acid. This is known as the buffering effect of alkalinity Effect of pHIt has been determined that an optimum pH value for anaerobic treatment lies between 5.5 and 8.5 Biogas and methane production ratesIn any anaerobic digester, effectiveness of the process is usually represented in terms of biogas production rate Fuzzy-logic methodologyA general fuzzy system has basically four components: fuzzification, fuzzy rule base, fuzzy output engine, and defuzzification Finally, in the defuzzification step, linguistic results obtained from the fuzzy inference are translated into a real value by using the rule base provided where (y i ) d is the defuzzified output, y i is the output value (or the centroidal distance from the origin) in the ith subset, and (y i ) is the membership value of the output value in the ith subset. For the continuous case, the summations in Eq. ( The situations of uncertainties in fuzzy-logic are defined via giving appropriate membership functions to the elements of the set that represent the situation. The value of the variation between 0 and 1 (the highest level) for each element is called membership degree and its value in subset is called membership function In this work, Fuzzy Logic Toolbox was used to create and to edit the present Fuzzy Inference System (FIS) within the framework of MATLAB \u00ae V7.0. The steady-state data obtained from the experimental study were allocated into fuzzy sets to represent different levels of crisp numerical variables. Input and output variables were built by using the FIS Editor, and fuzzified with trapezoidal membership functions (trapmf). Fig. In this study, a total of 134 rules were established with the Fuzzy IF-THEN Rule Editor, and a decision from the combinations of input membership functions (premise part or the antecedent block) to output membership functions (consequent part) was made by experience and the steady-state experimental data set. As an example, Table In the applications of the fuzzy system in both control and forecasting, there are two types of fuzzy inference systems, namely, Mamdani-type Non-linear modeling studyIn this study, a non-linear modeling study was also carried out to appraise the performance of the UASB reactor treating molasses wastewater by means of biogas and methane production rates. The steady-state experimental data was evaluated by DataFit \u00ae scientific software (version 8.1.69, Copyright \u00a9 1995-2005 Oakdale Engineering, RC167) containing 298 two-dimensional (2D) and 242 three-dimensional (3D) non-linear regression models. As similarly done in our previous studies Analytical procedureInfluent and effluent pH values were measured by a pH meter (Thermo Orion 210). Total Chemical Oxygen Demand (TCOD), volatile fatty acids (VFA) and alkalinity analyses were conducted by the procedures described in the Standard Methods Results and discussionUASB processOn the basis of the cross-sectional area of the reactor (314.16 cm 2 ) and applied feed flow rates (40.5-165.1 L/day, mean: 73.4 (\u00b134) L/day), hydraulic loading rates (L H ) were controlled between 1.29 and 5.26 m 3 /m 2 -day, with an average value of 2.34 (\u00b11.09) m 3 /m 2 -day. The UASB reactor were conducted with different HRTs between 0.36 and 1.48 days. Imposed volumetric organic loading rates (OLR) ranged from 1.95 to 16.56 kg TCOD/m 3 -day, with a mean value of 7.40 (\u00b14.26) kg TCOD/m 3 -day.Depending on various organic and hydraulic loading conditions, daily biogas production rates ranged between 46 and 753 L/day. Daily methane (CH 4 ) production rates ranged from 36 to 490 L/day with a mean value of about 160 L/day. Percentages of the typical components in biogas, such as CH 4 and CO 2 , ranged between 50-70% and 16-44%, with mean values of about 68 (\u00b16)% and 27 (\u00b15)%, respectively. Moreover, high volumetric TCOD removals (R V ) were achieved between 1.85-15.97 kg TCOD removed /m 3 -day, with an average value of 6.87 (\u00b13.93) kg TCOD removed /m 3 -day. Result also showed that the rounded TCOD removal percentages ranged from 84 to 98%, with an average value of 93 (\u00b13)%.The average values of influent and effluent pH were about 5.01 (\u00b10.20) and 6.82 (\u00b10.21), respectively. The increase in pH can be attributed to the anaerobic bio-convertion of amino acids contained in feed wastewater to ammonia Mudunge Consequently, the molasses wastewater was satisfactorily treated by means of a high-rate anaerobic process, specifically with the use of UASB reactor. Although relatively high incoming OLRs were imposed to the system, the UASB reactor demonstrated a stable performance on the anaerobic treatability of molasses wastewater, and no process failure was recorded. This should be due to acclimatization of both acidogens and methanogens to the gradual flow regime after a well controlled adaptation period.Prediction of biogas and methane production ratesIn this work, the developed MIMO fuzzy-logic-based model and the non-linear regression analysis-based model were applied to predict biogas and methane production rates obtained from the steady-state experimental data. In the non-linear study, one exponential model and two first-order polynomial models were obtained for prediction of both biogas and methane production rates. Results are summarized in Table Regression variable results including standard error, the tstatistics and the corresponding p values for the best-fit model are summarised in Table The larger t-ratio indicates the more significant parameter in the regression model. Moreover, the variable with the lowest p value is considered the most significant The obtained PSE, IA and MB values were in line with those reported by others Conclusions", "conclusions": "The pilot-scale UASB reactor showed a noticeable performance on the treatment of molasses wastewater under various organic and hydraulic loading conditions. TCOD removal efficiencies ranged between 84% and 98%, and high volumetric TCOD removal rates (R V ) ranging from 1.85 to 15.97 kg TCOD removed /m 3day) were achieved. TCOD mass balance revealed that over 92% of influent organic matters imposed to the system were transformed to biogas on average. Although relatively high OLRs (1.95-16.56 kg TCOD/m 3 -day) were imposed to the system, no process failure was observed.On the basis of the experimental findings, a real-world modeling study was conducted as an important objective to develop an artificial intelligence-based model that could make a reliable prediction on both biogas and methane production rates. For five different model components (OLR, R V , ALK inf , pH inf , pH eff ) the proposed MIMO fuzzy-logic model showed precise and effective predictions with satisfactory correlation coefficients over 0.96. Moreover, two exponential non-linear regression models were also developed as the best-fit models to appraise the performance of the UASB reactor treating molasses wastewater by means of biogas and methane production rates. Non-linear regression variable results showed that R V and effluent pH had more importance than other model components in prediction of both biogas and methane production rates.Descriptive performance indices clearly indicated that the proposed MIMO fuzzy-logic-based model showed a superior predictive performance on forecasting of both biogas and methane production rates compared to non-linear regression model. The applicability of the fuzzy-logic model is very simple and there is no need to define the complex reactions and their mathematical or biochemical equations. Moreover, due to highly non-linear structure of the fuzzy-logic model model, it was shown that a complex system such as anaerobic digestion could be easily modelled. Since fuzzy-logic methodology gave encouraging estimation results for the online control of a pilot-scale system, it is believed that this kind of model will help the control engineer to evaluate in real-time production rates that are necessary to control the anaerobic process and to establish fault diagnosis before transferring the concepts to a full scale plant.On the basis of the advantages of artificial intelligence-based modeling approach, for future studies, an improved MIMO fuzzylogic-based model, including additional inputs and outputs, will be developed to estimate parameters that are not measured on-line in the process, as well as to evaluate the effects of unexpected input changes on the outputs. Furthermore, different types of membership functions and their combinations will also be tested to enhance the prediction performance of the proposed diagnosis system based on fuzzy-logic.", "SDG": [3, 11]}, "a_fuzzy_logic_model_for_biogas_generation_in_bioreactor_landfills": {"name": "A fuzzy logic model for biogas generation in bioreactor landfills", "abstract": " A fuzzy logic model was developed to simulate the effect of leachate recirculation and sludge addition on the biogas generation in anaerobic bioreactor landfills. The model was designed using a fuzzy logic system (FLS) which incorporated 3 input variables (time, leachate recirculation, and sludge addition) and a single manipulated output (biogas generation rate). The biogas production rate was measured during the experiment and was increasing proportionally with the rate of both leachate recirculation and sludge addition. The experimental work involved the operation of six simulated laboratory-scale bioreactors for over a year under different operating schemes. The experimental results were employed in formulating the fuzzy rule base, calibrating the model, and verifying its predictions. Then, the model was validated against other measured data that was compiled from published studies. The FLS model simulations demonstrated high correlation with the experimental observations.", "keywords": "", "introduction": "Leachate recirculation and sludge addition are among the most effective techniques used in enhancing the design and operation of bioreactor landfills. In both methods, the concept is to control and manipulate the influencing factors, specifically moisture content and nutrients, in a positive manner to accelerate the biodegradation of municipal solid waste (MSW). One special advantage of leachate recirculation is lowering the treatment cost and environmental impacts of the high strength leachate as the organic component of leachate is reduced by the active biological communities within the refuse mass.Several studies have developed mathematical models to simulate the quantity and quality of biogas generated from landfills (El-Fadel et al. 1989;Peer et al. 1992;Lay et al. 1998;). In addition, stochastic modeling was used to simulate landfill processes White et al. 2004(Copty et al. 2004;. Thus far, most of these models are practically inapplicable as they are complicated and require extensive data inputs. Furthermore, the heterogeneity in MSW characteristics as well as the complex processes taking place within the landfill add difficulty in assessing the individual and coupled effect of various parameters in the system. Therefore, this study targeted an entirely different approach which is based on fuzzy logic modeling. The fuzzy logic can offer advantages in dealing with systems that are complex, ill structured, and best described qualitatively Zacharof and Butler 2004)).(Ibrahim 2004The fuzzy logic is a generalization of Boolean logic implementing the concept of partial truth or uncertainty. Within the fuzzy set theory, an element can have a gradual membership to different sets. The system behaviour is described by defining fuzzy sets, fuzzy rules or so-called IF -THEN rules, and applying a fuzzy inference scheme. The generation of a fuzzy logic system (FLS) model can be based both on experts' knowledge and experimental data. Over the last few decades, applications of fuzzy logic have reached almost every area of science, engineering, business, and high-tech industries.In this study, the FLS was employed to develop a model for MSW biodegradation in terms of biogas generation. The time, leachate recirculation and sludge addition were set as the controlled input variables and the biogas generation was the only manipulated output variable. The data obtained from the present experimental investigation was employed in building and calibrating the rule base of the fuzzy inference system. The model was validated by examining its predictions against measured data that were compiled from the literature (Bae et al. 1998;. The statistical analyses used to test model adequacy included linear regression between actual and predicted data and mean squared deviation (MSD) measures.San and Onay 2001)", "body": "Leachate recirculation and sludge addition are among the most effective techniques used in enhancing the design and operation of bioreactor landfills. In both methods, the concept is to control and manipulate the influencing factors, specifically moisture content and nutrients, in a positive manner to accelerate the biodegradation of municipal solid waste (MSW). One special advantage of leachate recirculation is lowering the treatment cost and environmental impacts of the high strength leachate as the organic component of leachate is reduced by the active biological communities within the refuse mass.Several studies have developed mathematical models to simulate the quantity and quality of biogas generated from landfills The fuzzy logic is a generalization of Boolean logic implementing the concept of partial truth or uncertainty. Within the fuzzy set theory, an element can have a gradual membership to different sets. The system behaviour is described by defining fuzzy sets, fuzzy rules or so-called IF -THEN rules, and applying a fuzzy inference scheme. The generation of a fuzzy logic system (FLS) model can be based both on experts' knowledge and experimental data. Over the last few decades, applications of fuzzy logic have reached almost every area of science, engineering, business, and high-tech industries.In this study, the FLS was employed to develop a model for MSW biodegradation in terms of biogas generation. The time, leachate recirculation and sludge addition were set as the controlled input variables and the biogas generation was the only manipulated output variable. The data obtained from the present experimental investigation was employed in building and calibrating the rule base of the fuzzy inference system. The model was validated by examining its predictions against measured data that were compiled from the literature Materials and methodsExperimental setupThe experimental setup consisted of six simulated anaerobic bioreactor landfill models made from Plexiglas\u00ae with dimensions of 150 mm in diameter and 550 mm in height. The schematic representation of the simulated bioreactor is shown in Fig. To achieve a representative sample of MSW commonly disposed in a landfill, waste was collected from the curbside of the city of Ottawa. The major components of this waste were paper (36\u20226%), food (36\u20222%), and yard trimmings (27\u20222%). The collected waste was shredded manually to a size of 5 to 10 mm and then mixed uniformly. The shredded solid waste was filled in 100 mm layers and compacted to a density of 350 kg/m 3 . The final height of the waste inside each bioreactor was 400 mm. The total mass and volume of waste in the bioreactors were 2\u20225 kg and 7 L, respectively. In addition, a layer of 15 mm diameter marbles was placed at the bottom of each bioreactor. This supporting layer was used as a leachate drainage system and to prevent clogging of the leachate outlet.The simulated rainfall was maintained at a rate of 2 L/week. Leachate was collected in a 15 L tank and was then recycled daily to the top of the bioreactors starting from the third week of operation. The sludge addition was carried out through the leachate recirculation system and using anaerobically digested and thickened waste activated sludge from the Robert O. Pickard Environmental Centre (ROPEC) municipal wastewater treatment plant in Ottawa.The volume of gas produced in the simulated bioreactors was measured using a wet tip meter device. The number of tip readings was converted into a rate of biogas generation using a calibration curve.Operating schemesThe operational conditions of the six bioreactors, notated as R1 to R6, are outlined in Table Fuzzy logic controllerA static fuzzy logic controller (FLC) structure was designed to model the biodegradation of MSW. As illustrated in Fig. Inputs included time, leachate recirculation, and sludge addition. The crisp values of the input variables were obtained from the conducted experimental work.Fuzzification is to map the observed inputs to fuzzy sets in the universe of discourse. The fuzzification strategy involves the following: (i) acquiring the crisp values of the three input variables, (ii) mapping the crisp values of the input variables into the corresponding universes of discourse and finally (iii) converting the mapped data into suitable linguistic terms so as to make it a compatible fuzzy sets representation.Database is provided by defining the membership functions (MF) of the fuzzy sets used as values for each system variable. Membership functions must be defined for each input and output variable and are represented by a real number ranging between 0 and 1. Figure Fuzzy inference engine is required to determine the fuzzy output and to compute the rules along with the membership function of the fuzzy input. The MAX-MIN fuzzy inference technique was applied to compute a numerical value representing the aggregate effect of all that was triggered by an input value.Defuzzification occurs as part of the last stage of fuzzy inference. Typically, it involves weighting and incorporating a number of fuzzy sets in a calculation which gives a single crisp value for the output. The defuzzification method used in this study is the centroid method. In this method, the defuzzified value, \u03bc, can be calculated as:where r is the total number of rules, \u03bc ci is the degree of membership of the output fuzzy set i, and C i is the value associated with the peak of output fuzzy set i.Model simulationThe fuzzy inference system was developed using the MATLAB\u2122 7\u20220 program through the Fuzzy Logic Toolbox. The simulation was designed and run at discrete variable steps using Simulink\u2122 software, which work concurrently with the fuzzy logic toolbox.The flowchart of the developed simulation is illustrated in Fig. Model evaluation criteriaIn addition to graphical assessment, the model was evaluated statistically using a group of criteria that was established prior to the evaluation process. These criteria included linear regression between actual and predicted data and MSD measures. The regression estimates of the intercept (a) and the slope (b) are good indicators of accuracy; the simultaneously closer to zero the intercept is and the slope is to unity, the higher the accuracy. On the other hand, the A fuzzy logic model for biogas generation in bioreactor landfills Abdallah, Fernandes, Warith and Rendra correlation coefficient (R) is a good indicator of precision; the higher the R, the higher the precision Results and discussionExperimental data analysesThe experimental data of biogas production rate was plotted together with the model simulations for the simulated bioreactors in Fig. For the trapezoidal MF: parameters a and d locate the \"feet\" of the trapezoid, whereas parameters b and c locate the \"shoulders.\" For the S-shaped MF, parameters a and b locate the extremes of the sloped portion of the curve. For the triangular MF, parameters a and c locate the \"feet\" of the triangle and parameter b locates the peak. For the Gaussian MF, parameters \u03c3 and c are the variables in the symmetric Gaussian function, f (x;\u03c3,c) = exp{\u2212(x \u2212 c) 2 /(2\u03c3 2 )}.  comparing its predictions to the experimental data that were used already in creating the fuzzy rules and calibrating the membership functions. Referring to Fig. The comparison plotted for R2 was largely similar to the one for R4.The simulation results of R2 duplicated the experimental data with the exception at the peak. Slight discrepancies of around 0\u20225 L/week could be recognized during weeks 10 to 17 and 27 to 35 of the experiment. The model developed for R5 and R6 was the same due to the fact that these bioreactors were replicates. The FLS model was most successful in predicting biogas production rates for R6. Model validationModel validation was used to examine the applicability of the simulation model under a wide range of operating conditions. Data used for model validation were compiled from two published studies on laboratory-scale bioreactor landfills. The first experimental study, by The second experimental study, by The validation process was run on the experimental data of the sludge-added lysimeter and the leachate-recycled reactor in the first and second experiments, respectively. Table In the first study, the model overestimated the biogas generation by an average of 20% during the periods of gradual increase and decrease of the rate. In contrast, the model underestimated the peak and produced a constant generation rate during this part. This could be due to the fact that, in this study, the sludge addition rates between week 30 and week 40 were higher than the maximum recycling rate that was defined in the model. As these rates were not expressed in the fuzzy rules, the FLS model applied the maximum defined recycling rate scenario constantly during that period.Despite that, the model predicted accurately the average measured production through that period and eventually followed the decreasing trend of the actual data.In the second experiment, the pattern of the biogas generation was anomalous; the curve started with a major generation rate followed by gradual decrease to the dip. Afterwards, the production rate started to increase steadily. This irregular shape was produced as a result of the operating scheme that was followed in that particular experimental study. This actually added to the merit of the FLS model validation process. The model predicted the general trend of the production pattern adequately despite the fact that it underestimated the generation rate during most of the operation period.Statistical analysesThe evaluation criteria and main features of the statistical tests that were selected for this study were previously discussed in the Model Evaluation Criteria section. Figure Based on the MSD partitioning shown in Table The FLS model achieved an acceptable normalized RMSE of 13\u202272% for the present experiment, whereas, it achieved a significantly high percentage (42\u202234% in average) for the validation data sets. These shortcomings could have been avoided if more data and operating scenarios were implemented in the construction and calibration processes of the fuzzy rules and membership functions of the FLS model.Conclusions", "conclusions": "The main objective of this work was to develop a fuzzy logic system that is capable of simulating the biogas generation in an anaerobic bioreactor landfill. The developed model went through several steps to evaluate its potentials and reveal its weak points. In the verification step, the FLS model proved to simulate perfectly the experimental data; this was confirmed statistically. On the other hand, the validation process revealed certain weaknesses in the model. Although the model predictions were in reasonable agreement with the validation data sets, it produced significant difference between simulated and measured data in terms of the normalized RMSE.The overall model simulation proved significant concurrence with the experimental results indicating the model reliability in capturing the pertinent features of the system. The validation process showed that the fuzzy logic system functions better when the modeled system is fully described under all possible operating conditions. However, the validation process proved the model flexibility in dealing with atypical operating scenarios such as irregular biogas generation trends and extreme recirculating rates. Based on these findings, the application of the fuzzy logic system in modeling the MSW biodegradation process can be considered as a successful simulating technique that implicitly describes the large number of complex physical and biochemical processes that occur within the bioreactor landfill.", "SDG": [3, 11]}, "a_preeclampsia_diagnosis_approach_using_bayesian_networks": {"name": "A Preeclampsia Diagnosis Approach using Bayesian Networks", "abstract": " Hypertension is the main cause of maternal death. Preeclampsia can affect pregnant women before or during pregnancy. Identification of patients with higher risk for preeclampsia allows some precautions that are taken to prevent its severe disease and subsequent complications. In medicine, there are different situations that deal with a large range of information, which needs a thorough assessment to be able to help experts in the decision-making process. Smart decision support systems allow grouping all existing information and finding pertinent information from it. Bayesian networks offer models that allow the information capture and handle situations of uncertainty. This paper proposes the construction of a system to support intelligent decision applied to the diagnosis of preeclampsia using Bayesian networks to help experts in the pregnant's care. The processes of qualitative and quantitative modeling to the construction of a network are also presented. The main contribution of this work includes the presentation of a Bayesian network built to help decision makers in moments of uncertainty in care of pregnant women.", "keywords": "Decision support systems,Bayesian networks,Pregnancy,Hypertencion,Modeling IEEE ICC 2016 -Communication QoS, Reliability and Modeling Symposium", "introduction": "The quality of care provided by health services for pregnant women is the most important action to control maternal mortality. Access to these services presents a major impact on these reductions, since they have enough quality to identify risks. Information systems offer the ability to monitor and evaluate the pregnant health notifying the experts in good time about a given complication that can occur during pregnancy. This allows the physicians make better decisions on diagnosis and establish better medical procedures and treatment.The most frequently diseases during pregnancy are infectious, especially those that reach the urinary tract. These diseases can cause severe complications like increasing the risk of miscarriage and anticipation of the birth labor. However, the main concerns of obstetricians are related to metabolic syndromes such as preeclampsia and gestational diabetes, which are more fatal for both mothers and babies.Preeclampsia occurs when a pregnant woman has high blood pressure (above 140/90 mmHg) at any time after the 20 th pregnancy week and disappears before 12 weeks postpartum . Besides high blood pressure, other complications such as excessive protein in the urine should occur in a diagnosis of preeclampsia. In [1], the authors identify high-risk pregnancy complaints and propose a method that analyses the Doppler signal to identify these conditions. Conclusions show that complications with pregnancy are associated with hypertensive disorders (preeclampsia), intra-uterine growth restriction of fetus, and gestational diabetes mellitus. In [2], the importance of a reliable diagnosis with accurate measurement of blood pressure and proteinuria is discussed. The authors also present cases where preeclampsia goes undiagnosed due to lack of appropriate equipment and limited resources in laboratories. Cheng et al. [3] analyze the relationship between the blood pressure and risk factors of pregnant women. To determine the effects over gestational age a varying coefficient model was established. Results show that effects of known risk factors change with gestational age. This result is an important knowledge for understanding the causes of gestational hypertension. In [4] the authors investigate the alterations in the progress of a normal pregnancy and pregnancy disorders associated with hypertension. This research work uses the Joint Symbolic Dynamics method. Mukherjee et al. [5] use Discriminant Analysis and k-means clustering to predict preeclampsia based on lipid parameters. This technique is used to separate the pregnant women in two groups named preeclampsia and control, so that a new patient can be classified into any of these groups according to estimated values of the parameters. In this context, several efforts have been performed in order to develop a system for giving support to experts in pregnant's care. Then, this work aims to investigate how Bayesian networks can support clinicians to identify high-risk pregnancy. It proposes the use of a statistical model based on Bayesian networks to better classify the seriousness of a problem helping the decision-makers in uncertainty moments. These systems applied to healthcare offer the possibility to monitor and evaluate pregnant's health and notify any complications that can occur during the pregnancy in a due time. The main contribution of this work includes a proposal of a smart system based on Bayesian networks to support decision makers in pregnant women monitoring using the Noisy-or classifier.[6]The remainder of this paper is organized as follows. Section II describes the use of Bayesian networks on healthcare, while Section III shows the network modeling and the construction of the tables of probabilities. Performance comparison of the proposed method, in comparison with other available approaches, and results analysis are considered in Section IV. Finally, Section V provides the conclusion and suggestions for future works.", "body": "The quality of care provided by health services for pregnant women is the most important action to control maternal mortality. Access to these services presents a major impact on these reductions, since they have enough quality to identify risks. Information systems offer the ability to monitor and evaluate the pregnant health notifying the experts in good time about a given complication that can occur during pregnancy. This allows the physicians make better decisions on diagnosis and establish better medical procedures and treatment.The most frequently diseases during pregnancy are infectious, especially those that reach the urinary tract. These diseases can cause severe complications like increasing the risk of miscarriage and anticipation of the birth labor. However, the main concerns of obstetricians are related to metabolic syndromes such as preeclampsia and gestational diabetes, which are more fatal for both mothers and babies.Preeclampsia occurs when a pregnant woman has high blood pressure (above 140/90 mmHg) at any time after the 20 th pregnancy week and disappears before 12 weeks postpartum The remainder of this paper is organized as follows. Section II describes the use of Bayesian networks on healthcare, while Section III shows the network modeling and the construction of the tables of probabilities. Performance comparison of the proposed method, in comparison with other available approaches, and results analysis are considered in Section IV. Finally, Section V provides the conclusion and suggestions for future works.II. BAYESIAN NETWORKS ON HEALTHCAREThis section addresses the use Bayesian networks in healthcare. The use of this technique on the proposal presented in this paper is also considered.Bayesian networks are a methodology for the construction of systems that rely on probabilistic knowledge. These systems function with uncertain and incomplete knowledge through of Bayesian Probability Theory. Teles et al. Kachroo et al. Based on the related literature analysis, next section will describe a mathematical model based on Bayesian networks that can assist decision makers in uncertain times to diagnose and evaluate the gravity of hypertension in pregnant women.III. BAYESIAN NETWORKS MODELING OF HYPERTENSIONInformation about the diagnosis of hypertension in pregnant women is essential to create a smart system designed to support the decision-making in healthcare. In this proposed model the network nodes three groups can be considered: i) Risk factorsvariables that activate physiological mechanisms; ii) Physiological mechanisms -functioning model of diseases related to preeclampsia; and iii) Symptoms/exams -physical manifestations of disease and test results. The network nodes are presented in Table A subsequent stage of a Bayesian network structure creation is the specification of their probabilities. These probabilities can be obtained in two ways: from specialists or from an automatic learning process from a database. It is also possible to combine the two alternatives. Marginal probabilities are the easiest way to find and correspond to nodes without parents. These probabilities correspond to the prevalence of diseases in pregnant seeking medical assistance. Graphically, these nodes are represented in the upper part of the network. Based on the research of Kumar et al. A Bayesian network is defined by its structure and the corresponding probabilistic model. It determines unequivocally the joint distribution for the variables describing as mentioned in Eq. 1.(1)For certain types of nodes, the conditional probabilities can be calculated from other probabilities instead of being specified directly. The classifier Noisy-or allows such calculation. For medical problems, such representation is appropriate when there is a disease with several risk factors/causes or a symptom caused by various diseases. Using a node D with (true) and (false) representing a disease, its causes are , the probabilities for D are given by the joint table conditional probabilities (Eq. 2).(2)The Noisy-or model allows the calculation of the joint table of conditional probabilities from probability given in (2) for each parent node , whilst respecting the restriction on relatives laid earlier. Table IV. PERFORMANCE EVALUATION OF THE PROPOSED METHODFor the evaluation and validation of the proposed method a case study is performed. It will use information about symptoms of patients in different hypertension severity. For the study 20 pregnant women were recruited. Fig. A. Case 1: Patient with preeclampsia/eclampsia diagnosticsIn this case, the pregnant woman had high blood pressure (BP > 140/90 mmHg), pulmonary edema, hyperflexia, headache, nausea or vomiting, giddiness and proteinuria (>3,5g/24h). The network presented in Fig. B. Case 2: Patient with chronic hypertension with superimposed preeclampsia.The chronic hypertension with superimposed preeclampsia presents i) emergence of proteinuria (> 0,3g/24h) after the gestational age of 20 weeks in a patient with chronic hypertension; ii) an additional increase in proteinuria in those who have had increase previously; iii) a sudden increase in blood pressure in who had previously controlled levels; or iv) clinical or laboratory abnormality characteristic of preeclampsia. In this case the pregnant woman presented high blood pressure (BP > 140/90 mmHg), hyperflexia, headache, giddiness, oliguria, and traces of proteinuria. also presents a probability of 66,7% for chronic hypertension with superimposed preeclampsia. This probability can be improved with the inclusion of new data. The presented model helps to better understand the patient's condition, assisting the diagnostic or prognostic decisions in order to reduce the uncertainty of current or future condition of the patient. This approach also requires varied sources as medical knowledge and experience. This approach requires more clinical data to be better evaluated and compared with other systems. However, with a large volume of data another type of probabilistic approach is required. Considering all these conditions, data mining can be a way to improve certainty in the moment of decision-making.Preeclampsia is very difficult to diagnose because it can occur even without an increase of blood pressure and without the presence of protein in the urine, but research is advancing and the joint cooperation including technology, knowledge and experience of health experts is an important path to tread.V. CONCLUSION AND FUTURE WORK", "conclusions": "This work focused on the construction of a smart system designed to support a medical decision for pregnant healthcare. The proposed decision support system used probabilistic concepts for decision-making. A Bayesian network for the diagnosis of preeclampsia was presented. The network structure was obtained from medical references. Noisy-or model was also considered in this work. The operation of the network shows that, in certain cases, this type of modeling can be used profitably, especially when it has a large number of parents, and when parents have characteristics in common.Further research work will consider other Bayesian classifiers and evaluate the network using real cases and the corresponding experts evaluation. This evaluation will provide different views regarding parts of the network and will contribute for further deployments. It is also proposed to carry out practical experiments with the network, sensitivity analysis, and development of a user interface and corresponding application.", "SDG": [3]}, "boosting_approach_for_maternal_hypertensive_disorder_detection": {"name": "A Boosting Approach for Maternal Hypertensive Disorder Detection", "abstract": " Pregnancy is the delicate stage in every woman's life cycle. The changes in the health during this period may lead to risk in pregnancy. That is a high risk pregnancy is one that endangers the women's or baby's health or life. Hypertensive disorders are the most common cause of these pregnancy related complications. In the present era, it's a major challenge in health care because their results in increasing maternal and fetal death. There exist various techniques widely used in data mining for the early identification of health diseases. Better treatment can be provided in the early stage by the early identification of hypertensive disorders. The main intent of this paper is to pinpoint the present complications of a pregnant woman by applying a boosted random forest approach for predicting hypertensive disorders. This classification method helps to predict the risks, diagnose and thereby can decrease maternal and fetal mortality, which remains as a large problem in much of the developing nation.", "keywords": "classification,hypertensive disorder,data mining,pregnancy", "introduction": "Pregnancy is an experience of motherhood, which needs much more medical care and attention. Women who were healthy and normal before getting pregnant may experience some risk of problems with them. Hypertension occurs when women have high blood pressure. Having hypertension at any point can lead to health problems, but especially during the cycle of gestation. It could have dangerous outcomes for mother and child, if the adequate care is not taken to control it effectively. The mortality rate in the world has reduced. Women are still dying due to pregnancy complications. Hypertension and its disorders are a leading cause of concern and its prevalence is increasing in developing countries. Therefore, getting identification at an early stage and regular pregnancy care can help to decrease the risk for problems before they become more serious.There are wide varieties of factors that complicate pregnancy. The maternal and fetal conditions include maternal smoking, blood pressure, mean arterial pressure, body mass index, excessive protein in urine, gestational age, etc. Also women face higher risk due to obesity, woman over the age 40 or under 20, multiple births, woman with diabetes and kidney disease, etc. The maternal hypertensive disorders in pregnancy can be classified into various classes like preeclampsiaeclampsia, gestational hypertension, chronic hypertension and preeclampsia superimposed on chronic hypertension. Preeclampsia (PE) is a condition that pregnant women develop with high blood pressure accompanied by proteinuria. This condition generally appears after the twentieth week of gestation. It may develop into the more severe state called eclampsia. Development of high blood pressure in the second half of pregnancy without proteinuria or other symptoms of preeclampsia is Gestational hypertension (GH). A medical complication developed before pregnancy or before the gestation week 20 is called chronic hypertension. Women with Chronic high blood pressure diagnosed before pregnancy develops a rapid increase in excess protein in the urine or blood pressure results in chronic hypertension with superimposed preeclampsia. The effect of high blood pressure in pregnancy varies based on these disorders and factors. Data mining techniques helps for the extraction of medical data and their analyzing. Its main aspect is pattern discovery. Classification is a basic task in this field of knowledge discovery in database. The classification in medical diagnostic problem assigns a disease label to the instance. Many researches are going on to find a long term health effect of maternal hypertensive disorders. So it is needed to develop an efficient and better method for identifying, diagnosing and treating women at risk. This paper is outlined as follows: section II focuses on literature survey. Section III involves the proposed system where the model is explained.", "body": "Pregnancy is an experience of motherhood, which needs much more medical care and attention. Women who were healthy and normal before getting pregnant may experience some risk of problems with them. Hypertension occurs when women have high blood pressure. Having hypertension at any point can lead to health problems, but especially during the cycle of gestation. It could have dangerous outcomes for mother and child, if the adequate care is not taken to control it effectively. The mortality rate in the world has reduced. Women are still dying due to pregnancy complications. Hypertension and its disorders are a leading cause of concern and its prevalence is increasing in developing countries. Therefore, getting identification at an early stage and regular pregnancy care can help to decrease the risk for problems before they become more serious.There are wide varieties of factors that complicate pregnancy. The maternal and fetal conditions include maternal smoking, blood pressure, mean arterial pressure, body mass index, excessive protein in urine, gestational age, etc. Also women face higher risk due to obesity, woman over the age 40 or under 20, multiple births, woman with diabetes and kidney disease, etc. The maternal hypertensive disorders in pregnancy can be classified into various classes like preeclampsiaeclampsia, gestational hypertension, chronic hypertension and preeclampsia superimposed on chronic hypertension. Preeclampsia (PE) is a condition that pregnant women develop with high blood pressure accompanied by proteinuria. This condition generally appears after the twentieth week of gestation. It may develop into the more severe state called eclampsia. Development of high blood pressure in the second half of pregnancy without proteinuria or other symptoms of preeclampsia is Gestational hypertension (GH). A medical complication developed before pregnancy or before the gestation week 20 is called chronic hypertension. Women with Chronic high blood pressure diagnosed before pregnancy develops a rapid increase in excess protein in the urine or blood pressure results in chronic hypertension with superimposed preeclampsia. The effect of high blood pressure in pregnancy varies based on these disorders and factors. Data mining techniques helps for the extraction of medical data and their analyzing. Its main aspect is pattern discovery. Classification is a basic task in this field of knowledge discovery in database. The classification in medical diagnostic problem assigns a disease label to the instance. Many researches are going on to find a long term health effect of maternal hypertensive disorders. So it is needed to develop an efficient and better method for identifying, diagnosing and treating women at risk. This paper is outlined as follows: section II focuses on literature survey. Section III involves the proposed system where the model is explained.II. LITERATURE SURVEYThis section survey about existing work which make use of data mining methods for finding reliable information about pregnant women having hypertensive diseases. Some of these studies find solutions for healthcare limitations in maternal care domain.M. W. Moreira, J. J. Rodrigues, A. M. Oliveira and K. Saleem M. W. Moreira, J. J. Rodrigues, A. M. Oliveira and K. Saleem A probabilistic knowledge based system called Bayesian network has a significant role in healthcare. Diagnosis of the pregnancy disorder preeclampsia Severe maternal morbidity can be predicted early using techniques like logistic regression In Development of a predictive model is used for the classification of risk related to hypertension using decision tree III. PROPOSED SYSTEMIn the proposed system we have put forward a novel method for detecting the pregnancy disorders. Women with maternal disorders have different changes in her body compared to normal one and this creates threat to her and infant life. In this model focus is on women suffering from maternal hypertensive disorders. Data mining techniques are widely employed to reach a decision or to classify or identify the disorder. A. Random ForestThe tree based classifier called random forest which is a multitude of decision trees is mainly used for several types of classification tasks. The prediction of several trees is combined in it essentially. Random features and random inputs produce good results in classification. When we use random forest, a user has to determine only two parameters. They are the number of trees to be used and the number of variables to be randomly selected from the available set. Let the assumed number of training cases or observations in the training set is N. Then N observations are sampled at random, but with replacement. If there are K total input variables, the input variables which are used at random to find the decision at a node of the tree k should be less than K. They create best possible split to develop a decision tree model. Each tree is grown to the largest extent possible.The process of splitting is performed with the measures like information gain or Gini index. The method of splitting affects trees accuracy. It splits the node into two or more sub nodes. The decision tree grows using a set of samples. Its growth starts from the root node and divides recursively by using splitting rules until it reach a stopping condition. Each individual tree gives a classification, a prediction. The leaves are outputting the prediction. Each of the tree is made with a sample of the data in the average of many decision trees. That is, once each of the trees is classified, the value that has majority among them is taken as the output.B. Boosted Random ForestIn the proposed model we can apply a boosting technique for the maternal disease detection. It's a method used to boost single trees into strong learning algorithms. Finally it combines the classifiers by letting them vote on a final prediction. We can achieve accuracy by combining the random features with method of boosting. Boosted trees try to improve the model fit over different trees by considering past fits. The classifiers focus on the cases which are incorrectly classified in the last round.Algorithm is as follows: When we introduce boosting algorithm into random forest, decision tree weighting and updating weights are considered. The training set weight is initialized into 1/N where N is taken as the size of training set. It also follows the same procedure of random forest in tree construction. The splitting process can be done with information gain. The information gain IG can be calculated as: And are the sample sets at the left and right child node. The splitting process is continued recursively until the information gain is zero or until the stopping criteria .The probability distribution is stored at the leaf node. If the decision tree training is completed then we can estimate the class label. The misclassification rate is expressed as error rate. We compute the error rate of decision tree. Based on the error calculation ER we calculate the weight W.= (2)Where ER is the error rate of the decision tree and M is the number of classes. If W > 0 the weight is updated, otherwise the tree is rejected. When an input is given its output is stored in the leaf nodes. That is, they produce the classification result. Class with highest probability is chosen. The hypertensive disorder class prediction will be the weighted majority vote.The main intent of boosted random forest is to maintain the generality. The boosting method will help to attain good classification performance. With regard to standard input we are able to find individuals as having mild, moderate and severe hypertension and can classify in the correct disorder type.IV. CONCLUSION", "conclusions": "Data mining methods are essential in the medical field for prediction. The area of data mining has established itself as the major part of computer science and contributes a way to improve the decision making process. It has shown significant potential for future improvements. In this proposed work designing an approach called boosted random forest to classify the hypertensive pregnancy disorder for the early prediction. This work can bring a drastic change in medical fields, especially in the maternal care domain by effectively classifying the disorders. As future work we can improve the performance and accuracy using other classification techniques and expanding size of datasets.", "SDG": [3]}, "critical_reasons_for_crashes_investigated_in_the_national_motor_vehicle_crash_causation_survey": {"name": "TRAFFIC SAFETY FACTS Crash \u2022 Stats", "abstract": " The National Motor Vehicle Crash Causation Survey (NMVCCS), conducted from 2005 to 2007, was aimed at collecting on-scene information about the events and associated factors leading up to crashes involving light vehicles. Several facets of crash occurrence were investigated during data collection, namely the precrash movement, critical pre-crash event, critical reason, and the associated factors. A weighted sample of 5,470 crashes was investigated over a period of two and a half years, which represents an estimated 2,189,000 crashes nationwide. About 4,031,000 vehicles, 3,945,000 drivers, and 1,982,000 passengers were estimated to have been involved in these crashes. The critical reason, which is the last event in the crash causal chain, was assigned to the driver in 94 percent (\u00b12.2%) \u2020 of the crashes. In about 2 percent (\u00b10.7%) of the crashes, the critical reason was assigned to a vehicle component's failure or degradation, and in 2 percent (\u00b11.3%) of crashes, it was attributed to the environment (slick roads, weather, etc.). Among an estimated 2,046,000 drivers who were assigned critical reasons, recognition errors accounted for about 41 percent (\u00b12.1%), decision errors 33 percent (\u00b13.7%), and performance errors 11 percent (\u00b12.7%) of the crashes.", "keywords": "", "introduction": "Databases such as the National Automotive Sampling System (NASS) Crashworthiness Data System (CDS) do not provide information on pre-crash scenarios and the reason underlying the critical pre-crash events. In 2005, the National Highway Traffic Safety Administration (NHTSA) was authorized under Section 2003(c) of the Safe, Accountable, Flexible, Efficient, Transportation Equity Act: A Legacy for Users (SAFETEA-LU) to conduct a national survey to collect on-scene data pertaining to events and associated factors that possibly contributed to crash occurrence. NHTSA's National Center for Statistics and Analysis (NCSA) conducted NMVCCS from July 3, 2005 Crashes were investigated at the crash scene to collect driver, vehicle, and environment-related information pertaining to crash occurrence, with a focus on driver's role. The targeted information was captured mainly through four data elements: (i) movement prior to critical pre-crash event (i.e., the movement of the vehicle immediately before the occurrence of the critical event); (ii) critical pre-crash event (i.e., the circumstance that led to vehicle's first impact); (iii) critical reason for the critical pre-crash event (i.e., the immediate reason for the critical event, which is often the last failure in the causal chain of events leading up to the crash); and (iv) the crashassociated factors (i.e., the factors that are likely to add to the probability of crash occurrence). This was done with reference to the crash envelope that comprises of a sequence of events, referring to the above data elements, which eventually led to the crash. This Crash\u2022Stats presents some statistics related to one of the four data elements, namely \"critical reason for the critical precrash event.\" The data obtained through the sample of 5,470 NMVCCS crashes and the weights associated with them were used to obtain national estimates of frequencies and percentages along with their 95-percent confidence limits, as presented in the following sections., to December 31, 2007.", "body": "Databases such as the National Automotive Sampling System (NASS) Crashworthiness Data System (CDS) do not provide information on pre-crash scenarios and the reason underlying the critical pre-crash events. In 2005, the National Highway Traffic Safety Administration (NHTSA) was authorized under Section 2003(c) of the Safe, Accountable, Flexible, Efficient, Transportation Equity Act: A Legacy for Users (SAFETEA-LU) to conduct a national survey to collect on-scene data pertaining to events and associated factors that possibly contributed to crash occurrence. NHTSA's National Center for Statistics and Analysis (NCSA) conducted NMVCCS from Critical Reasons for the Critical Pre-Crash EventThe critical reason is the immediate reason for the critical pre-crash event and is often the last failure in the causal chain of events leading up to the crash. Although the critical reason is an important part of the description of events leading up to the crash, it is not intended to be interpreted as the cause of the crash nor as the assignment of the fault to the driver, vehicle, or environment.A critical reason can be assigned to a driver, vehicle, or environment. Normally, one critical reason was assigned per crash, based upon NMVCCS researcher's crash assessment. The critical reason was assigned to the driver in an estimated 94 percent (\u00b12.2%) of the crashes (Table Critical reason attributed to driversThe critical reason was assigned to drivers in an estimated 2,046,000 crashes that comprise 94 percent of the NMVCCS crashes at the national level. However, in none of these cases was the assignment intended to blame the driver for causing the crash. The driver-related critical reasons are broadly classified into recognition errors, decision errors, performance errors, and non-performance errors. Statistics in Table Critical reason attributed to vehiclesThe critical reason was assigned to vehicles in an estimated 44,000 crashes comprising about 2 percent of the NMVCCS crashes, though none of these reasons implied a vehicle causing the crash. There were no detailed inspections of vehicles during the NMVCCS on-scene crash investigation; the vehiclerelated critical reasons were mainly inferred through external visual inspection of the vehicle components. This resulted in only mostly external, easily visible factors (tires, brakes, steering column, etc.) that were cited as the few vehicle-related critical reasons. The related statistics may not, therefore, be representative of the role of other internal vehicle related problems that might have led to the crash. Of the small percentage (2%) of the crashes in which the critical reason was assigned to the vehicle, the tire problem accounted for about 35 percent (\u00b111.4%) of the crashes. Brake related problems as critical reasons accounted for about 22 percent (\u00b115.4%) of such crashes. Steering/suspension/ transmission/engine-related problems were assigned as critical reasons in 3 percent (\u00b13.3%) of such crashes. Other vehiclerelated problems coded as critical reasons were assigned in about 40 percent (\u00b124.0%) percent of such crashes. Critical reason attributed to environmentThe critical reason was assigned to about 2 percent of the estimated 2,189,000 NMVCCS crashes. However, none of these is suggestive of the cause of the crash. Table Table 1 . Driver-, Vehicle-, and Environment-Related Critical Reasons", "conclusions": "", "SDG": [3]}, "data_derived_soft_sensors_for_biological_wastewater_treatment_plants_an_overview": {"name": "Data-derived soft-sensors for biological wastewater treatment plants: An overview", "abstract": " This paper surveys and discusses the application of data-derived soft-sensing techniques in biological wastewater treatment plants. Emphasis is given to an extensive overview of the current status and to the specific challenges and potential that allow for an effective application of these soft-sensors in full-scale scenarios. The soft-sensors presented in the case studies have been found to be effective and inexpensive technologies for extracting and modelling relevant process information directly from the process and laboratory data routinely acquired in biological wastewater treatment facilities. The extracted information is in the form of timely analysis of hard-to-measure primary process variables and process diagnostics that characterize the operation of the plants and their instrumentation. The information is invaluable for an effective utilization of advanced control and optimization strategies.", "keywords": "Water,quality,monitoring,Soft-sensors,Data-driven,models,Wastewater,treatment", "introduction": "During the recent decades, the increased awareness about the negative impact of eutrophication in the quality of water bodies (see, e.g.,  and the advances in environmental technology have given rise to more stringent wastewater treatment requirements and regulations Ansari et al., 2010)(Olsson et al., 2005;. In wastewater treatment plants (WWTPs), the tightening treatment regulations are leading towards the addition of new unit processes and towards the renewal of the existing ones. A typical example in municipal WWTPs is the update of the ammonia removal process towards total nitrogen removal; this is usually achieved through the conversion of the biological reactor from a single aerated tank to a sequence of anoxic and aerobic zones. The subsequent increase in operational and management investments, mostly associated to energy consumption and chemical dosing, stimulates modern WWTPs to face the challenges of maintaining and improving effluent quality, while guaranteeing efficient and safe operations and optimizing the costs. A major requirement for achieving these goals relies on the availability of real-time measurements of key or primary process indicators. These indicators are needed to efficiently monitor the operation of the plants in terms of influent and effluent quality, process and instrument performance and economic efficiency, with immediate implications for environmental compliance, safety, management planning and profitability. The real-time availability of primary indicators is invaluable for an effective utilization of advanced process control and optimization strategies in WWTPs.Olsson, 2012)The conventional approach to the monitoring problem in WWTPs relies upon on-line and off-line analysis of the primary variables. The primary variables are typically concentrations of ammonia, nitrates and total nitrogen, phosphates and total phosphorus, suspended solids, biochemical and chemical oxygen demand, as well as others process variables like the sludge blanket level. Such variables are hard-to-measure and their availability is often associated with expensive capital and maintenance costs, as well as being characterized by time-delayed responses that are often unsuitable for real-time monitoring. For instance, the organic compounds are still typically monitored by off-line laboratory measurements, of which the analysis of biochemical oxygen demand requires several days. Moreover, the harsh conditions in biological treatment processes such as the Activated Sludge Process (ASP) make reliable field measurements challenging. Already in an early survey where fifty wastewater treatment facilities in the USA were considered  and in a publication on the state-of-the-art in wastewater treatment control (Molvar et al., 1976), the problems of on-line instrumentation were discussed. The typical problems included solids deposition, slime build-up and precipitation, which gave rise to poor performance and a frequent need for maintenance of the instrumentation. During the recent decades, considerable development in on-line instrumentation has taken place (see e.g., (Olsson, 1977)Vanrolleghem and Lee, 2003;. In spite of the recent advances, such as in situ nutrient sensors and luminescent dissolved oxygen sensors, instruments still tend to get fouled Olsson, 2012). Nevertheless, trustworthy real-time analyses for many key variables is not there yet.(Olsson, 2012)Due to the progress in measurement, automation and communication technologies, WWTPs are also becoming highly instrumented and many on-line easy-to-measure process variables are routinely acquired. The easy-to-measure or secondary variables are typically pressure, temperature, flow rate, and level measurements, as well as conductivity, turbidity, pH, and, perhaps dissolved oxygen. The secondary variables can be extensively used for characterizing the operational conditions of the unit processes and they offer an inexpensive opportunity to extract primary information useful to monitor both the processes and the instruments. For example, being the primary variables necessarily related to some of the secondary variables, their availability offers the opportunity to develop process models capable to reconstruct such a relationship and thus also to estimate the primary variables. Such models are at the core of virtual instruments often referred to as software-or soft-sensors; that is, computer programs that model the input information encoded in the secondary variables and output information related to the primary variables, in a similar fashion to their hardware counterparts . On the basis of their internal model, soft-sensors are often divided into two main classes, phenomenological and data-driven. Phenomenological softsensors are based on the first principle process models, whereas data-driven soft-sensors are built around process models derived from data. Hybrid soft-sensors combine these two modelling approaches.(Kadlec et al., 2009)In wastewater treatment, the most commonly used first principle models belong to the Activated Sludge Model (ASM) family  proposed by the IWA Task Group on Mathematical Modelling for Design and Operation of Biological Wastewater Treatment. Moreover, the IWA Task Group on Good Modelling Practice has created a unified protocol for enhancing the quality of activated sludge modelling and dealing with uncertainties associated with the phenomenological approaches (Henze et al., 2000). Because capable to describe both linear and nonlinear phenomena and to provide information on the internal states of the process, the detailed phenomenological modelling approach has proven to be efficient, for example, in wastewater treatment process design, renovation, employee training, optimization of the plant operation and understanding the system's behaviour and interactions of the components (Rieger et al., 2012)(Hauduc et al., 2009;. The ASM models have also been used in softsensor design, for instance by Phillips et al., 2009) and Sp\u00e9randio and Queinnec (2004). However, there are major challenges in using the first principle models for real-time applications. For example, characterizing the organic matter and determining the rate constants for the volatile fatty acid (VFA) uptake in wastewater is challenging, expensive and time-consuming and, yet, fundamental for successful model calibration Grau et al. (2007)(Dochain and Vanrolleghem, 2001;Petersen et al., 2003;Sin, 2004;. Moreover, the high-dimensionality of detailed phenomenological models results in an enormous computational requirements and ill-conditioned problems due to the interaction between fast and slow dynamics Hauduc et al., 2011).(Dochain and Vanrolleghem, 2001)The large amount of process data routinely measured and collected in modern WWTPs permits data-driven modelling as an interesting alternative for soft-sensor design. A data-derived softsensor is an inputeoutput model, where the inputs usually consist of easy-to-measure secondary variables in the form of plant's signals.In the soft-sensor, the input information is modelled and the internal model is used to return output information associated with the hard-to-measure primary variables. Different families of models have been popular in designing the data-derived soft-sensors, which are commonly used for applications such as on-line prediction, process monitoring and process fault detection, and sensor monitoring and reconstruction . Today, data-driven soft-sensors are becoming more common in the wastewater treatment sector, even though they are still not as widespread as, for instance, in the process industry where soft-sensors are extensively exploited (see, e.g., (Kadlec, 2009)Fortuna et al., 2007;. Softsensors have also been used in the other environmental domains, where the open distributed architectures for sensor networks Kadlec et al., 2009) provide an increasing amount of the available data. For instance, environmental data has been applied for softsensors aiming at real-time anomaly detection in the meteorological signals (Douglas et al., 2008)(Hill et al., 2009; and at prediction of the ammonia concentration in a river downstream the sewage and WWTP outlets Hill and Minsker, 2010). In the wastewater treatment facilities, the data-derived soft-sensor applications range from the proposals where a small number of variables are used for modelling (such as in (Masson et al., 1999)Marsili-Libelli, 1990;Lumley, 2002;Puig et al., 2005;\u00c4ij\u00e4l\u00e4 and Lumley, 2006; to the studies where a larger number of measurements are processed together with a model (for instance, Cecil and Kozlowska, 2010)Teppola et al., 1999b;Rosen and Lennox, 2001;Aguado et al., 2007a;Lee et al., 2008;, which in particular are in the scope of this review. Datadriven applications in wastewater treatment are included in earlier review papers, where their extent, however, is limited. Their limited amount is due to the main focus of these review publications being on the application of phenomenological modelling Corona et al., 2013)(Yoo et al., 2001;Gernaey et al., 2004; or on the use of a specific data-driven modelling family Banadda et al., 2011)(Khataee and Kasiri, 2011;.Yetilmezsoy et al., 2011)In this paper, we aim to provide an extensive overview of the applications of data-derived soft-sensors in wastewater treatment and to present a general guideline for the development of dataderived soft-sensors. The paper is organized as follows. Section 2 introduces briefly the biological wastewater treatment process types, which are used in the case studies, and characteristics of WWTP operation data. In Section 3, we give an overview of the practical steps to be undertaken in the design of data-derived softsensors. A review of publications focussing on the data-derived soft-sensor applications in wastewater treatment systems is given in Section 4. Next, Section 5 contains a discussion on our findings concerning the applications of the data-derived soft-sensors in WWTPs and the current status and future challenges of softsensing in the field of operation. A nomenclature of the terminology is provided in Table .1", "body": "During the recent decades, the increased awareness about the negative impact of eutrophication in the quality of water bodies (see, e.g., The conventional approach to the monitoring problem in WWTPs relies upon on-line and off-line analysis of the primary variables. The primary variables are typically concentrations of ammonia, nitrates and total nitrogen, phosphates and total phosphorus, suspended solids, biochemical and chemical oxygen demand, as well as others process variables like the sludge blanket level. Such variables are hard-to-measure and their availability is often associated with expensive capital and maintenance costs, as well as being characterized by time-delayed responses that are often unsuitable for real-time monitoring. For instance, the organic compounds are still typically monitored by off-line laboratory measurements, of which the analysis of biochemical oxygen demand requires several days. Moreover, the harsh conditions in biological treatment processes such as the Activated Sludge Process (ASP) make reliable field measurements challenging. Already in an early survey where fifty wastewater treatment facilities in the USA were considered Due to the progress in measurement, automation and communication technologies, WWTPs are also becoming highly instrumented and many on-line easy-to-measure process variables are routinely acquired. The easy-to-measure or secondary variables are typically pressure, temperature, flow rate, and level measurements, as well as conductivity, turbidity, pH, and, perhaps dissolved oxygen. The secondary variables can be extensively used for characterizing the operational conditions of the unit processes and they offer an inexpensive opportunity to extract primary information useful to monitor both the processes and the instruments. For example, being the primary variables necessarily related to some of the secondary variables, their availability offers the opportunity to develop process models capable to reconstruct such a relationship and thus also to estimate the primary variables. Such models are at the core of virtual instruments often referred to as software-or soft-sensors; that is, computer programs that model the input information encoded in the secondary variables and output information related to the primary variables, in a similar fashion to their hardware counterparts In wastewater treatment, the most commonly used first principle models belong to the Activated Sludge Model (ASM) family The large amount of process data routinely measured and collected in modern WWTPs permits data-driven modelling as an interesting alternative for soft-sensor design. A data-derived softsensor is an inputeoutput model, where the inputs usually consist of easy-to-measure secondary variables in the form of plant's signals.In the soft-sensor, the input information is modelled and the internal model is used to return output information associated with the hard-to-measure primary variables. Different families of models have been popular in designing the data-derived soft-sensors, which are commonly used for applications such as on-line prediction, process monitoring and process fault detection, and sensor monitoring and reconstruction In this paper, we aim to provide an extensive overview of the applications of data-derived soft-sensors in wastewater treatment and to present a general guideline for the development of dataderived soft-sensors. The paper is organized as follows. Section 2 introduces briefly the biological wastewater treatment process types, which are used in the case studies, and characteristics of WWTP operation data. In Section 3, we give an overview of the practical steps to be undertaken in the design of data-derived softsensors. A review of publications focussing on the data-derived soft-sensor applications in wastewater treatment systems is given in Section 4. Next, Section 5 contains a discussion on our findings concerning the applications of the data-derived soft-sensors in WWTPs and the current status and future challenges of softsensing in the field of operation. A nomenclature of the terminology is provided in Table Wastewater treatment plantsMunicipal wastewater treatment aims at reducing the amounts of nutrients, organic matter (determined as Biochemical Oxygen Demand (BOD), Chemical Oxygen Demand (COD) or Total Organic Carbon (TOC)) and Suspended Solids (SS) that influent wastewater contains. This is typically carried out by using several unit processes, including biological, chemical and physical treatment methods. The core of the treatment line is a biological reactor such in the ASP, where a high concentration of activated sludge consisting mainly of bacteria and protozoa is recycled in zones with different Dissolved Oxygen (DO) conditions, especially for nitrogen removal purposes. In most of the municipal WWTPs, primary and secondary clarifiers are applied for separation and thickening sludge. To maintain the microbiological population in the bioreactor, the sludge from the secondary clarifiers is recirculated into the biological reactor. Phosphorus removal is typically achieved using chemical precipitation, but process configurations targeting for Enhanced Biological Phosphorus Removal (EBPR) also exist. In the EBPR process, polyphosphate-accumulating organisms are selectively enriched in the bacterial community and an additional anaerobic process stage and a more sophisticated sludge recirculation scheme are required. In addition, anaerobic digestion is a typical process for excess sludge treatment where the amount of organic matter in the sludge is considerably reduced and the biogas generated during the digestion process is often used for producing electricity and heat.Also in many industrial sectors, biological wastewater treatment is widely applied. For instance in the pulp and paper industry, ASP is the treatment process most commonly used (see, e.g., In addition to the ASP, also other kinds of treatment processes have been used in the reviewed papers and they are briefly described. Sequencing Batch Reactors (SBRs) containing activated sludge are operated in a different number of phases such as filling, aeration, mixing, settling and decanting. The phases and their lengths are determined depending on the treatment targets. In the Membrane Bioreactors (MBRs), suspended solids are separated with membranes instead of secondary clarifiers as in the conventional ASPs. Considerably larger Mixed Liquor Suspended Solids (MLSS) concentrations can be used in the MBR due to getting rid of the operational constraints set by the clarifier units. Trickling filters are aerobic processes, where wastewater flows through permeable medium to which microorganisms are attached. The Moving Bed Biofilm Reactor (MBBR) technology is based on the carrier media that provides a large protected surface area for the biofilm attachment and that moves freely in the bioreactor where wastewater is treated in. The aerated Submerged Biofilm Reactors (SBFRs) are filled with porous support medium through which wastewater and air flow. Specifically, the support medium on which biofilm grows is maintained under the total immersion of the water flow in the SBFRs. In the Lagoon Treatment Plants (LTPs), earthen basins are used as reactors and typically they require a large surface area.Floating surface aerators are used to provide the required oxygen and mixing in the lagoons. In the Single reactor system for High activity Ammonium Removal Over Nitrite (SHARON) process, partial nitrification, i.e. oxidation of ammonium to nitrite, is established. Nitrate formation in the reactor is prevented by adjusting temperature, pH, and retention time and, thus, the aeration requirement of SHARON is smaller in comparison with a conventional complete nitrification process. Several types of bioreactors operated in anaerobic or anoxic conditions have also been considered in the reviewed papers. Anaerobic Digestion (AD) is the process where sewage is treated, for instance, in septic tanks mainly for reducing the organic content of wastewater. In the Upflow Anaerobic Sludge Blanket (UASB) process, wastewater is introduced at the bottom of the reactor and it flows upwards through a sludge blanket composed of biologically formed granules or particles in the absence of oxygen. Anaerobic Filters (AFs) treat wastewater that is led through a bed of medium on which anaerobic bacteria grows. Typically, their operation targets on the removal of organic matter but, for example, in anaerobic post-treatment filters the goal may be reducing the nitrate content in wastewater in the presence of an external carbon source. In the Anaerobic Fluidized Bed Reactors (AFBRs), bacteria are immobilized on small fluidized medium though which wastewater flows and the process is operated in an up-flow mode in order to achieve fluidization.The process data of a municipal WWTP has typical diurnal and seasonal trends, e.g., in influent flow rate, concentrations and temperature Due to the demanding conditions in the biological WWTPs, the real-time measured process data usually contains missing and anomalous observations Soft-sensor developmentA data-derived soft-sensor is conventionally described as an inputeoutput process model. The model inputs consist of easy-tomeasure secondary variables in the form of plant's signals and measurements and, sometimes, numerically encoded expert knowledge. The model outputs consist of information associated with the hard-to-measure primary variables. In the soft-sensor, the input and output process information is modelled empirically and the internal model is used to return the outputs when only the inputs are available.The range of tasks that can be fulfilled by data-derived softsensors is broad and mainly dictated by the nature of the available input information, by the information that we are interested to output and the typology of the inputeoutput model. The original and still most prominent application area is the on-line prediction of process variables that can be only measured either at low sampling rates or off-line. In this case, the inputs are those variables that are easy to measure and the outputs are estimates of the variables that are hard to measure; since the inputeoutput relationship is encoded in the data used to calibrate the model, the soft-sensor model is used to reconstruct it and then to estimate the output variables when new inputs are available. At its core, this type of soft-sensors addresses a supervised learning problem in the form of regression or classification. The other typical application areas are related to monitoring the state of the process and to monitoring the state of the instrumentation. In this case, the inputs are again those variables that are easy to measure and the outputs are information on the operation of the process and the instruments, in the form of diagnostics and status characterization; since the output information is usually hidden in the data, the soft-sensor model is used to explore the data and then to estimate the outputs when new inputs are available. At its core, this typology of soft-sensors usually addresses an unsupervised learning problem in the form of dimensionality reduction or clustering.This section overviews the practical steps to be undertaken in the design of data-derived soft-sensors. An overview is given in Fig. Data acquisitionIn modern WWTPs, the historical process and laboratory data are routinely acquired and stored in the data acquisition system of the plant. The data can be easily retrieved and data collection and subsequent data inspection are the first steps in soft-sensor development. During the initial inspection, a preliminary exploration of the measurements is performed, in order to overview the prominent structures in the data (e.g., redundancies, taxonomies, functionalities and time-delays existing between variables and observations) and to identify the presence of obvious problems (e.g., locked variables, missing and drifting data and measurements outside the operating range of the instruments). In addition, periods of instrument calibrations and unit process maintenance are also annotated together with a selection of representative operations (e.g., steady states and periodicities, transients and seasonal disturbances). The inspection typically requires a large amount of manual work and expertise in the underlying processes, coupled with an extensive exploration of time series, scatter plots, and histograms of data.Data pre-processingRemarkable characteristics of the data acquired in wastewater treatment facilities are redundancy and possibly insignificance, let alone the presence of other disturbances that corrupt the measurements. Very often, the amount and quality of the data together with their high-dimensionality can be a limiting factor for the development of the soft-sensors. Therefore, it is necessary to prepare the data before they are processed by the soft-sensor's model. Process understanding and a priori knowledge is required in this phase; the plant operators and experts can provide valuable information on the relevance of the variables and the observations. Such a knowledge can be supported and complemented by many statistical techniques for variable selection and sample selection. In the following, some of the most commonly used techniques for these tasks are briefly overviewed.Variable selectionThe choice of the input variables that influence the model output is a crucial stage. Variable selection consists of choosing those secondary variables that are most informative for the process being modelled, as well as those that provide the highest generalization ability. This step is fundamental because data models are built from a finite number of observations and having a model with too many inputs and hence too many parameters, may lead to overfitting and induce large computational times. In addition, describing a process in terms of a few selected variables allows to retain interpretability.The most commonly used techniques for variable selection are often categorized as filtering, wrapping and embedded methods The criterion is often based either on statistical dependence or model accuracy measures. The simplest and most popular ways to measure the interactions between sets of variables are correlation and its multivariate extension obtained with Canonical Correlation Analysis In many situations the computational applicability of criterionsearch schemes is limited by the too large number of inputs. An alternative approach to reduce the dimensionality of the problem consists of producing a small number of combinations of the original variables. New variables may be constructed on the basis of process knowledge (e.g., using balance calculations and averaging) or they can be derived using statistical projection methods for dimensionality reduction Sample selectionWhen analysing real data, it is not uncommon that some observations are different from the majority. Such observations are often called outliers. They may be due to data acquisition mistakes or they correspond to exceptional process circumstances and, in general, one can distinguish between two main types of outliers: i) obvious outliers are observations that violate physical or technological limitations and ii) non-obvious outliers are observations that do not violate any bound but still lay out of typical ranges. Sample selection consists of discarding or pinpointing outlying observations because not necessarily representative of normal operations and because their use may be detrimental for the performances of the soft-sensor model A detailed multi-phase procedure for obtaining high-quality WWTP data was recently proposed by A conceptually different approach to sample selection can be seen in the application of clustering and classification methods Model designModel design is a critical step in soft-sensor development, for the structure of the model at the core defines the specific application task and it confirms the designer's assumptions on the problem being studied. Moreover, the selection of the model parameters determines the generalization abilities of the soft-sensor. However, a consistent approach to the task does not exist so far and, the model structure and its parameters are often selected in an ad hoc manner for each soft sensor Despite the lack of a theoretically superior approach to model design, two main tasks can be recognized: i) model structure selection and ii) model training, calibration and testing. The common practice suggests to start with simple model types, assess their potential and performances and then gradually increase complexity, as long as significant improvements are observed. Furthermore, it is important that the models are not only accurate, but also simple and computationally efficient, interpretable and with low maintenance cost. While performing the task it is fundamental to optimize the model parameters in terms of generalization performances, through a validation of the results on independent data before testing the results. In the following the main family of model structures are overviewed and then the optimization of their parameters discussed using a standard method like cross-validation.3.3.1. Model structure selection 3.3.1.1. Models for on-line prediction. Such models address the problem of reconstructing the functionality existing between the easy-to-measure inputs and the hard-to-measure process outputs. Usually, the inputs and the outputs take continuously varying real values and their relationships can be modelled as a regression problem. Less common is the case where the outputs take categorical values; in this case, their relationship can be modelled as a classification problem. In the following we will focus only on the most common regression models and we refer the reader interested on classification to the book by The simplest regression techniques assume the existence of a linear inputeoutput relationship and they fit a linear model to reconstruct it. Most of the commonly used techniques pertain to multivariate statistics Other nonlinear methods do not necessarily rely on any assumption on the inputeoutput relationships. Nevertheless, they are widespread among researchers and practitioners. Methods based on supervised artificial neural networks 3.3.1.2. Process and sensor monitoring. Such models address the problem of detecting, identifying and diagnosing normal and abnormal behaviours in the processes and in the field instruments, using the easy-to-measure process variables as inputs. The diagnostics comprise the hard-to-measure outputs and, usually, no prior information about them is available and it must be extracted from data using dimensionality reduction and clustering approaches. In the less common case where the output information is available, either as real or categorical values, the inputeoutput relationship can be modelled as a regression or a classification problem, respectively. Again, we will mostly focus only on the most common models for dimensionality reduction and clustering.In order to detect the occurrence of any variation having an exceptional or identifiable cause, univariate statistical control charts have been traditionally used to monitor a small number of process variables. Examining one variable at a time, as though they were independent, however, makes interpretation and diagnosis extremely difficult in environments where a large number of variables are continuously varying relative to one another. However, when the number of variables is large, one often finds that they are also highly dependent on one another and common methods for reducing the dimensionality of the problem are the already mentioned linear approaches based on PCA and PLS.In the monitoring of the continuous processes and the hardware sensors, the conventional and adaptive PCA and PLS methods are popular for reducing the dimensionality of the variables of interest. The use of PCA methods enables process monitoring in a lowdimensional space defined by the principal components retained in the model at the price of losing the information encoded in the less significant principal components discarded. An established technique to isolate the variable(s) responsible for the detected anomalies is to study their contributions to the statistics considered for the analysis of model residuals. Another traditional monitoring approach is to use low-dimensional scatter plots defined by the most significant principal components, that include most of the information of the original variables, for observing the transitions in the process or in the relationships between the supervised sensors. When using PLS approaches for process monitoring, the output variables are usually hard, or impossible, to determine in real-time and they indicate the presence of anomalous situations. In wastewater treatment, such variables are, for instance, the indicators of the sludge settling properties determined by field and laboratory experiments.In the batch process, an additional dimension to the data structure is addressed by the batch, the other dimensions representing time and the variables. The Multiway PCA (MPCA) and PLS (MPLS) are commonly used for dimensionality reduction when multiway data is considered, for instance in the cases of the batch processes The Kohonen Self-Organizing Map (SOM, Clustering methods Since most of the basic data-derived techniques cannot deal with missing data directly, a strategy for their replacement has to be designed and implemented. A data imputation approach, which is primitive and not recommended but commonly applied, is to replace the missing values with the mean values of the affected variable. Another non-optimal approach is to skip the data samples consisting of variable or variables with the missing values. Also, imputing a missing value by a linear interpolation between the preceding and following existing values is a problematic approach especially when several sequential values are missing. More efficient approaches based on multivariate statistics of the data perform the reconstruction of the missing values from other variables of the affected samples Model training, validation and testingMost of the model types discussed in this section are characterized by a number of basic parameters and a number of hyper-, or meta-, parameters that define their structure and optimize it in terms of its generalization performances. The basic parameters of the models are, for instance, the regression coefficients of linear regression methods, the connection weights of neural and neurofuzzy systems, the loading components in multivariate statistical methods like PCA and PLS, among the others. The meta-parameters are, on the other hand, the number of components to be retained in methods like PCA and PLS, the regularization parameter in linear shrinkage methods like LARS and LASSO, the number of neurons and layers in neural and neuro-fuzzy systems, the number of clusters, among the others. Before a model is able to operate on new unseen observations, it has to be trained to estimate its basic parameters and it has to be validated to optimize its meta-parameters. Model validation is a highly important step in soft-sensor development, in which the designer estimates how well the model will perform on new data.Ideally, if enough data were available, the soft-sensor developer would set aside a validation set and use it to assess a model whose basic parameters are calibrated on a training set, for different values of its meta-parameters. After finding the optimal set of metaparameters, the developer would then calibrate the model to ultimately set its basic parameters, using all the available learning data (i.e., both the training and the validation set). The resulting model is eventually assessed on an independent test set of data. However, it may be difficult to obtain a sufficient amount of historical data for the for learning the model according to the aforementioned procedure. In such a situation, the soft-sensor developer has to resort on error-estimation techniques, like the simple and widely used cross-validation Although cross-validation is the most popular approach to optimize the generalization performances of data-derived softsensors, alternative techniques based on statistical resampling methods like bagging Model maintenanceAfter the successful design, it is not uncommon to observe a degradation of the performance of a data-derived soft-sensor. Such a degradation is often due to changes in process and instrumental characteristics or operating conditions. In wastewater treatment applications, the reason for this may be, e.g., variations in influent wastewater composition, temperature and flow rate, instruments recalibrations or operational changes inside the plant. To overcome such limitations, soft-sensors should be regularly maintained and updated as the system characteristics change but, their manual and repeated redesign should be avoided due to the heavy workload.Most of the soft-sensors currently found in full-scale environments do not provide any automated mechanisms for their maintenance. To automatically cope with changes in process characteristics and operating conditions, a number of data-derived approaches have been however designed and are available for the developer Soft-sensor applications in WWTPsData-derived methods can be used for (i) the on-line prediction of the primary process variables, (ii) process monitoring and process fault detection, and (iii) hardware-sensor monitoring and providing back-up for them, e.g., during the periods of sensor faults, maintenance and calibration. In this section, the reviewed case studies are organized according to the soft-sensor application categories.In the beginning of each subsection representing the aforementioned application areas, we review a few representative studies without separating them based on the modelling methods. Next, the contributions belonging to the categories are further arranged considering the different families of modelling methods employed. Among the multivariate methods, case studies applying conventional methods are introduced first and, after that, studies concentrating on advanced techniques such as adaptive, nonlinear and multiway extensions are presented. As for the ANNs, we first introduce publications utilizing FFNNs as the most popular techniques used in WWTPs, then studies using other types of supervised ANNs and, finally, works employing unsupervised ANNs. Eventually, studies where neuro-fuzzy techniques and hybrid models are proposed for soft-sensor design are introduced. The modelling methods applied in the reviewed publications and their amounts are represented in Fig. On-line predictionA common application for the data-derived soft-sensors in wastewater treatment systems involves predicting the primary process variables. The soft-sensor applications for prediction tasks in the reviewed publications are summarized in Table Some of the most relevant works can be found in continuous WWTPs. In the earlier soft-sensor applications the SS, BOD and COD concentrations have been popular predicted outputs, whereas in more recent publications it has been more common to estimate nutrient concentrations. This indicates the progress in wastewater treatment technology, where modern-day municipal WWTPs are typically designed for nutrient removal and, therefore, reliable information on the nutrient concentrations in the process has become of interest to the plant operators. The secondary variables typically used as inputs in the case studies dealing with the continuous processes include flow rates, pH, temperature and DO, SS and nutrient concentrations measured in different locations of the process. As for the batch processes, the typical target has been to predict the nutrient concentration trends during the aerobic and anoxic phases using simple on-line measurements, e.g., DO, pH and Oxidation Reduction Potential (ORP) as the model inputs and, further, to employ the estimates for controlling the lengths of the phases. In both continuous and batch problems, the most commonly used modelling method for the reconstruction of the desired outputs has been ANNs, especially FFNNs, followed by multivariate statistical methods based on PLS.A FFNN was applied for the estimation of NH 4 eN and NO 3 eN concentrations in a municipal ASP by As for the batch processes, a study by Multivariate statistical modelsMultivariate statistics, in particular those based on PLS, are one of the typical techniques used as soft-sensors for prediction tasks. In an early application in a municipal ASP, conventional PLS was used for estimating Total Phosphorus (TP) and COD concentrations and turbidity in the effluent Artificial neural network modelsAs already pointed out, ANN techniques have been popular in soft-sensor for prediction of process variables in the biological WWTPs.Most commonly the applications in this research area have considered the use of FFNNs in full-scale ASPs. In an early study, More recently, Grey modelling and FFNN techniques were applied for the prediction of SS and COD concentrations in the effluent of a continuous sequencing batch ASP treating hospital wastewater Case studies targeting at prediction of the variables using FFNNs in laboratory and pilot-scale processes have also been presented. The effluent concentrations of a pilot-scale MBR treating industrial wastewater were predicted by using a cascade-forward ANN model by Soft-sensors aiming at on-line prediction have been investigated by employing FFNNs in simulated treatment processes. In an interesting study by Apart from FFNNs, also other types of supervised ANNs have been used for estimation purposes. In a study by As for the unsupervised ANN methods, Neuro-fuzzy systemsNeuro-fuzzy systems, especially ANFIS models, have been applied for prediction tasks in various wastewater treatment processes. In a detailed study by Hybrid modelsHybrid models combining different modelling approaches on the system level provide interesting alternatives for soft-sensor design. In an early study, Process monitoring and process fault detectionVarious methodologies have been employed in the publications concentrating on the process monitoring and process fault detection in WWTPs. The reviewed studies targeting for monitoring the treatment processes are summarized in Table Many of the early soft-sensor applications in wastewater treatment considered monitoring bulking sludge episodes, which diminish the effluent wastewater quality and disturb the operation of the ASP. For instance monitoring SVI or its variants, that describe sludge settling properties and indicate presence of the bulking sludge, have been the goal of several studies. Taking into account the fact that the bulking sludge episodes do not usually appear rapidly, it is reasonable that their detection was among the first process fault monitoring the data-derived models were substantially dependent on the laboratory measurements. Together with the increase of the real-time measurements in WWTPs, the proposals for process monitoring have focused on a wider range of anomalies, which appear more abruptly. In addition, the monitoring of the states in SBRs has been a popular research objective, particularly, aiming at optimized control on the lengths of the phases. The usual secondary variables used for monitoring tasks are flow rates, pH, temperature and DO, SS and nutrient concentrations measured in different locations of the process. Most typically, multivariate methods and the SOM combined with clustering algorithms have been applied for process monitoring purposes.The actual pioneering work on the applications of multivariate methods in soft-sensor design in the wastewater treatment sector has been done for monitoring ASPs in the pulp and paper industry by Multivariate statistical modelsAmong multivariate statistics, the conventional PCA technique has been used for process state identification especially in earlier applications. The conventional PLS combined with an auto-correlation function was used for modelling DSVI and the COD, nitrogen and phosphorus reductions in the ASP for the treatment of paper mill wastewater in order to detect the various process shifts by Adaptive extensions of multivariate techniques have been proposed for WWTP monitoring in a number of publications. The work of Multiway methods have been popular in particular in the SBR case studies. Another approach presented by Artificial neural network modelsAs for the ANN methods, Fuente and Vega (1999) used the frequency content of the fault-indicating signals and a FFNN for process fault detection in a municipal ASP. In particular, faults in aeration turbines were monitored and the results demonstrated the reliability of the proposed method. The operating conditions and relationships between the process variables were monitored in a municipal ASP with the SOM by Fuzzy systemsFuzzy methods have also been proposed for process monitoring in wastewater treatment. Sensor monitoringAnother task for soft-sensors in WWTPs is the validation, fault detection and diagnosis of the hardware instrumentation. The sensor monitoring applications in the reviewed publications and a few conference publications that are not reviewed in this paper (marked with an asterisk) are summarized in Table Typically, advanced multivariate approaches have been applied to the identification of reasons for sensor faults, such as bias, drift, complete failure and precision degradation. Most commonly, monitoring of the NH 4 eN and NO 3 eN sensors in ASPs have been studied in the reviewed publications. The scarcity of the reviewed papers in this application area and the fact that all of them are relatively recent suggests that sensor monitoring in wastewater treatment facilities is still an emerging research field. In addition, data from a full-scale WWTP was used only in one of the publications whereas the majority of them considered simulated processes. In an ANN-based application, Caccavale et al. ( DiscussionApplications in the wastewater treatment facilitiesRecently, the majority of the soft-sensor applications in wastewater treatment have been associated with prediction tasks. This can be observed in Fig. As for the process types in the case studies shown in Fig. The useful soft-sensor applications ease with the operation of the treatment processes, for instance, by providing beneficial monitoring tools and helping in reducing the operational costs or reaching the treatment requirements. The desired monitoring tools often depend on the type of the process. For example, considering SBRs, they typically relate to the recognition of the optimal lengths of the phases, while in ASPs they may concern the detection of abnormal process states. As for the cost reduction, the on-line variable estimates used for supporting the process control may give advantages, e.g., in avoiding excessive aeration or chemical dosing. In the modern-day plants, soft-sensors for such purposes typically aim at the prediction of nutrient concentrations in various locations of the treatment line, or at monitoring the hardware instruments measuring the nutrient contents. On the other hand, the  treatment requirements e.g., on the local legislation, the size of the plant and the characteristics of the wastewater, which affect case-specifically the types of useful soft-sensors. For instance, the shifting of the typical treatment requirements from organic matter to nutrients in the municipal sector during the recent decades has also made soft-sensor estimates of nutrient concentrations more preferable. In the industrial sector, the removal of compounds specific to the field of industry is regulated by the authorities. Therefore, the estimates of such specific compounds has been of special interest in the soft-sensor design and shown to be useful for the plant operation. The appropriate modelling methodologies for soft-sensor design are typically casespecific and they depend on the problem and process considered as well as the available process data.Modelling approaches used for the soft-sensor designDifferent multivariate and ANN methodologies have typically been used for the data-driven soft-sensor development in wastewater treatment, with the supervised ANN techniques having been the most popular. The shares of the data-derived methods applied for soft-sensor design in the reviewed publications are presented in Fig. The most applied methods used for different tasks in the continuous and batch processes in the studies conducted after the year 2005 have been collected in Table Due to the dynamic and nonlinear nature of the wastewater characteristics and treatment process conditions, the conventional linear multivariate techniques have often been found to be unsatisfying methods for soft-sensor development in the wastewater sector as such. Therefore, a number of adaptive and nonlinear PCA and PLS extensions have been proposed and shown to be more feasible for soft-sensor design in the reviewed papers. In particular, adaptive PCA and PLS approaches based on, e.g., moving window or recursive techniques have been demonstrated to overcome the difficulties associated with the changing process conditions, however, with an increased computational cost. An additional challenge in the adaptive methods based on the moving window techniques is the choice of an adequate time window length. As for the nonlinear extensions, researchers have established, for instance, FPLS and KPLS to be adequate for prediction tasks in biological WWTPs. On the other hand, in all the case studies the performance of nonlinear methods has not been shown superior to the simpler linear methods. In those cases, employing the simpler methods is suggested due to their lighter computational burden. The researchers have also indicated the multiway extensions of PCA and PLS useful, in particular, for the monitoring and analysis of SBRs, and found the multiscale approaches to extract the features of treatment processes in different time-scales. In addition, the conventional and multiway PCA methods have been popular preprocessing techniques applied in 10% of the ANN-based soft-sensors proposed in the studies, which indicates the potential of the PCA techniques in the compression of information.In the publications where the performances of multivariate and supervised ANN methods have been compared for prediction tasks in WWTPs, usually the ANN methods have been found to be more feasible. Especially FFNNs with various architectures and backpropagation training algorithms have been popular among the soft-sensor developers. Even though in most of the case studies the FFNN estimates in the biological plants have been shown accurate, some drawbacks of the methodology have been notified. Those include the extent of the training data required and the challenges in simulating outputs outside the range of the training data. For instance, the FFNN estimates resulting only in satisfactory accuracies due to an insufficient amount of training data has been reported in several publications. Moreover, the structure of the ANNs is not easily interpretable and, therefore, they are not very useful for learning interactions between the process variables. Another challenge with FFNN modelling is defining the topology of the network, which is often done based on trial and error. In addition to the supervised ANNs, the researchers have applied unsupervised SOM models successfully for monitoring, analysis and preprocessing tasks in a number of publications.In several case studies, neuro-fuzzy models such as ANFIS have outperformed the supervised ANN models when tested for prediction accuracy in the dynamic conditions of WWTPs. Also, the structure of ANFIS may be easier to interpret than, e.g., the structure of FFNNs due to the IF-THEN rules, but a drawback of the ANFIS methodology is a strong computational power required. Hybrid models, where different modelling methods are fused on the system level, have been shown to be efficient tools for soft-sensor tasks in many of the reviewed publications. Particularly, the hybrid techniques have been indicated to enhance the strengths of the individual modelling methods and to overcome their limitations. The features, pros and cons of the basic data-derived methods used for soft-sensor design in the reviewed publications are summarized in Table Factors limiting the use of soft-sensors and needs for the future researchEven though full scale WWTP data has been used in most of the case studies, only a small minority of the publications reported a practical implementation of the proposed methods (see Tables In such cases, it may feel more confident for the operators to rely on the conventional hardware sensors and their maintenance provided by the instrument suppliers when available. Instead, if the plant operators have more technical competence and they are research-orientated, it is more likely that they are open-minded for novel techniques such as soft-sensors. In the best situations, the operators are actually involved in the soft-sensor development, they have recognized problems that can potentially be solved by soft-sensing and, therefore, they are willing to test and implement these techniques (see e.g., Based on these considerations, the future research targeting for practical implementations of soft-sensors in WWTPs would be important for increasing the awareness of these alternatives to the conventional measurement and monitoring solutions. The research related to soft-sensors should also follow the development of the state-of-art hardware sensors, concerning their dependability, feasibility and expense, and the typical measurements used for the process control. Thus, the primary variables whose measurement reliabilities may be bottlenecks in the efficient process operation can be recognized along the technical development that will take place. Based on this knowledge, soft-sensors for the relevant applications can be designed. Moreover, the research should focus on solving the real-life problems in WWTPs rather than developing complicated methodologies motivated by theoretical interests. This is also to say that the soft-sensor solutions should be selected based on the problem at hand and they should be relatively simple.Conclusion", "conclusions": "In this review, we focused on data-derived soft-sensor applications in biological wastewater treatment facilities. After introducing briefly the treatment processes used in the reviewed case studies and the typical characteristics of process data in WWTPs, a general guideline for soft-sensor design was provided. The presented soft-sensor applications were divided into prediction, process monitoring and process fault detection, and sensor monitoring categories according to the main objectives of the softsensor design.To summarize, the data-driven soft-sensors have become more popular as a greater number of real-time measurements have been applied in municipal and industrial WWTPs. Typical soft-sensor applications in the treatment plants include the prediction of the primary process variables that are hard to measure reliably or with reasonable costs using hardware instrumentation. Often, these types of soft-sensors are proposed for estimating the concentrations of nutrients or organic matter. Due to the increased amount of real-time process measurements, monitoring individual process variables has become troublesome using, for instance, univariate control charts. Hence, applications where a large amount of data is compressed into visual low-dimensional monitoring tools that are informative and easy to interpret has become valuable for plant operators for the recognition of the operational states. Another purpose of the data-driven soft-sensors is hardware instrumentation monitoring and providing a back-up system during instrument down-time.The data-derived techniques typically used for soft-sensor design in wastewater treatment include different multivariate statistical and ANN methods. According to the reviewed studies, the conventional approaches of PCA and PLS are usually not able to satisfactorily catch the dynamic and nonlinear behaviour of the biological WWTPs. For this reason, the adaptive and nonlinear extensions of multivariate methods have been proposed for modelling treatment processes in many of the recent studies. As for the batch processes, multiway PCA and PLS have been widely used methods. ANNs, in particular the FFNN methods employing backpropagation learning algorithms, have been the most popular techniques applied for data-driven soft-sensor development in the reviewed publications, especially for estimation tasks. In addition, neuro-fuzzy systems and hybrid methods have become more common in wastewater treatment modelling applications. ", "SDG": [3, 11]}, "deep_learning_for_image_based_cancer_detection_and_diagnosis_\u2212a_survey": {"name": "Pattern Recognition", "abstract": " In this paper, we aim to provide a survey on the applications of deep learning for cancer detection and diagnosis and hope to provide an overview of the progress in this field. In the survey, we firstly provide an overview on deep learning and the popular architectures used for cancer detection and diagnosis. Especially we present four popular deep learning architectures, including convolutional neural networks, fully convolutional networks, auto-encoders, and deep belief networks in the survey. Secondly, we provide a survey on the studies exploiting deep learning for cancer detection and diagnosis. The surveys in this part are organized based on the types of cancers. Thirdly, we provide a summary and comments on the recent work on the applications of deep learning to cancer detection and diagnosis and propose some future research directions.", "keywords": "", "introduction": "Cancer is a major reason to cause death in the world  and a survey performed by American Cancer Society (ACS) shows that approximate 600,920 people are expected to die from cancers in USA in 2017 [1] . Thus, fighting against the cancers is a big challenge faced by both research scientists and clinic doctors [2].[ 3 ]Early detection plays a key role in cancer diagnosis and can improve long-term survival rates. Medical imaging is a very important technique for early cancer detection and diagnosis. As is well known, medical imaging has been widely employed for early cancer detection, monitoring, and follow-up after the treatments  . However, manual interpretation of enormous number of medical images can be tedious and time consuming and easily causes human bias and mistakes. Therefore, from early 1980s, computer-aided diagnosis (CAD) systems were introduced to assist doctors in interpreting medical images to improve their efficiency [4] .[5]In CAD systems with medical imaging, machine learning techniques are widely employed for cancer detection and diagnosis. In order to adopt machine learning techniques, feature extraction is generally a key step. Different feature extraction methods have been investigated for different imaging modalities and different cancer types [6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21] . For example, in breast cancer detection, bilateral image subtraction, difference of Gaussian, and Laplacian of Gaussian filter have been adopted as feature extractor to detect mass regions in the mammograms [22][6][7][8][9] . However, the previous studies mainly focus on developing good feature descriptors combined with machine learning techniques for context learning from medical images. These methods based on feature extraction have a lot of weakness. The weakness limits the further improvement of performance of the CAD systems. In order to overcome the weakness and improve the performance of the CAD systems, the importance of representation learning has been emphasized instead of feature engineering in recent years [10][23, . Deep learning is one type of representation learning techniques that learns hierarchical feature representation from image data. One advantage of deep learning is that it can generate high level feature representation directly from the raw images. In addition, with the support of massive parallel architecture, Graphic Processing Unites (GPUs), deep learning techniques have gained enormous success in many fields in recent years, i.e. image recognition, object detection, and speech recognition. For example, recent studies show that CNNs 24] achieve promise performance in cancer detection and diagnosis.[25]This paper aims to introduce some popular deep learning techniques and provide an overview of the applications of deep learning for cancer detection and diagnosis. ", "body": "Cancer is a major reason to cause death in the world Early detection plays a key role in cancer diagnosis and can improve long-term survival rates. Medical imaging is a very important technique for early cancer detection and diagnosis. As is well known, medical imaging has been widely employed for early cancer detection, monitoring, and follow-up after the treatments In CAD systems with medical imaging, machine learning techniques are widely employed for cancer detection and diagnosis. In order to adopt machine learning techniques, feature extraction is generally a key step. Different feature extraction methods have been investigated for different imaging modalities and different cancer types This paper aims to introduce some popular deep learning techniques and provide an overview of the applications of deep learning for cancer detection and diagnosis. Basic concepts of CNNs, FCNs, SSAE, and DBNsMost of the successful image-based deep learning models were designed based on CNNs, FCNs, SSAE, and DBNs. This section introduces the basic concepts of the four architectures.Convolutional neural networks (CCNs)CNNs belong to feedforward neural networks where a signal flows through the network without forming cycles or loops, which can be expressed as In the convolutional layer, f is composed of multiple convolution kernels ( g 1 ... g k \u22121 , g k ). Each g k represents a linear function in the k th kernel, which can be represented as follows where ( x, y, z ) denotes the position of pixel in input I, W k denotes the weight for the k -th kernel, m, n , and w denote the height, width, and depth of the filter. In the activation layer, f is a pixelwise non-linear function, i.e. rectified linear unit (ReLU) An example of a single pipeline CNN model is shown in Fig. Fully convolutional networks (FCNs)The major difference between FCNs and CNNs lies in that FCNs replace the fully connected layer with upsampling layer and deconvolutional layer Auto-Encoder (AEs)Autoencoders (AEs) belong to another type of neural networks which are used for unsupervised learning where \u03c3 is the activation function and sigmoid function is generally employed:In the decoding stage, the representation h is decoded to reconstruct the output \u02c6x through a new weight matrix W h, \u02c6 x and a new bias b h, \u02c6x :where \u03c3 is the activation function. Note that W h, \u02c6x can be defined as the transpose of W x, h , or as a new learnable parameter matrix depending on different applications. An AE is trained to minimize  Sparse autoencoders (SAEs) are a special type of AEs where sparsity is introduced into the hidden units by making the number of nodes in the hidden layer bigger than that in the input layer An SSAE is a stack of SAEs with only encoding part and they are often trained in a greedy fashion Deep belief network (DBNs)Deep Belief Network (DBN) is a probabilistic generative model which is constructed by a stack of Restricted Boltzmann Machines (RBMs) instead of AEs argmaxwhere Z is a partition function. Contrastive divergence algorithm, which combines Gibbs sampling and gradient descent, is used to optimize an RBM In general, CNN has demonstrated its superior performance in image recognition problems. However, the input of CNN architecture is limited to relatively small images due to fully connected layers. It restricts its capability being directly used on large images. Instead, FCN does not have any fully connected layer and it can be applied to images of virtually any sizes. Cancer detection and diagnosis using deep learning techniquesThis paper is intended to provide a comprehensive survey of recent studies on applying deep learning for cancer detection and diagnosis. For the survey, we organize the survey based on the type of cancers.Breast cancerIn recent years, a bunch of papers have been published about the application of deep learning to breast cancer detection and diagnosis. In Skin cancerSkin cancer is a typical common cancers and some effort s based on deep learning have been done to develop algorithms to help diagnose the disease in recent years. In Prostate cancerProstate cancer has a highly rate of diagnosed among male and is the third leading cause of death in men Brain cancerBrain tumor is a solid mess that grows uncontrolled, and it may occur anywhere in the brain. Some research has been done on the applications of deep learning for brain cancer detection and diagnosis. Gao et al. studies deep learning for the classification of tumor cells in CT brain images Colonial cancerColorectal cancer (CRC) is the third most common cancer in both men and women. In order to help medical institutions diagnosing the colorectal cancer, researchers are trying to use deep learning methods for colonic polyp detection. Ribeiro et al. explored CNNs for colonic polyp classification Deep learning for other types of cancersDeep learning has also found applications in other types of cancers, such as cervical cancer, bladder cancer, liver cancer, and so on. The cytology based screening method, such as Pap tests, is a common method for the detection of cervical cancer in its early stage Bladder cancer has the highest recurrence rate among all the cancers. Diagnosis and bladder cancer could be developed through deep learning methods. Cha et al. proposed a deep CNN based algorithm for bladder segmentation to aid the detection of bladder cancer in CT urography (CTU) Liver cancer is a primary cancer, which means the cancer starts in the liver rather than migrating to the liver from another organ or section of the body. Gibson et al proposed a CNN based system to achieve automatic segmentation of liver from laparoscopic videos which could help diagnosing liver cancer Accurate segmentation of glands is an important task for morphological statistics of gland tumors. Current CAD systems for gland analysis are facing several challenges, such as variation of glandular morphology, difficulty of separating individual objects, and degeneration of glandular structures in malignant cells The number of circulating tumor cells (CTC) in blood plays an important role in early diagnosis of cancer when tumors are invisible. Mao et al. presented a deep CNN based system for automatic imaged-based CTC detection. In the proposed system, a training Chen et al. Besides the papers mentioned above for specific cancer detection and diagnosis, there are also some review papers summarizing the image-based methods for cancer detection or diagnosis. Bernal et al. presented a complete validation study comparing eight different polyp detection methods that were proposed on endoscopic vision challenge in MICCAI 2015 Summary and discussion", "conclusions": "", "SDG": [3]}, "deep_learning_model_for_classifying_drug_abuse_risk_behavior_in_tweets": {"name": "Deep Learning Model for Classifying Drug Abuse Risk Behavior in Tweets", "abstract": " Social media such as Twitter can provide urgently needed drug abuse intelligence to support the campaign of fighting against the national drug abuse crisis. We employed a targeted tweet collection approach and a two-staged annotation strategy that combines conventional annotation with crowdsourced annotation to produce annotated training dataset. In this demo, we share deep learning models trained in a boosting manner using the data from the two-staged annotation method and unlabeled data collection to detect drug abuse risk behavior in tweets.", "keywords": "drug abuse detection,social media,deep learning,Twitter", "introduction": "A Public Health Emergency has been declared  with the rising trend of drug abuse in the US in recent years. The most recent National Survey on Drug Use and Health (NSDUH) [1] reported that an estimated 10.6% of the total population aged 12 and above misused illicit drugs within one month prior to the interview in 2016. In the same year, the Centers for Disease Control and Prevention (CDC) [2] reported 42,249 deaths due to opioid drugs, which outnumbered the deaths by motor vehicle accidents, suicides and homicides. On top of that, heroin alone caused more deaths than firearms did in 2015 and the number is rising.[3]In 2017, the Department of Health and Human Services (HHS) announced a new \"Opioid Strategy\" , where \"strengthening public health surveillance\" was among its five priorities. In contrast with surveillance methods based on clinical records from hospitals and traditional surveys, social media monitoring can provide more real-time surveillance capabilities to improve public health awareness. Twitter has been shown to be an excellent data source in many healthrelated projects [5][6][7]. The goal of our ongoing research is to develop an automated machine learning system to detect tweets related to drug abuse risk behavior in near real-time. However, it is a challenging task to detect and classify tweets mentioning drug abuse risk behaviors due to: (1) Sparsity of drug abuse-related tweets among the 500 million daily tweets;[8](2) The short and ungrammatical nature of tweets; and  Limited availability of annotated tweet data for training machine learning models.(3)This paper addresses these challenging issues and achieves three objectives: (1) Build a system that can effectively collect drug abuse-related tweets at a large-scale; (2) Design an annotation strategy (drug abuse vs. non-drug abuse) that enables creating datasets at a much lower cost than traditional methods; and (3) Develop a deep learning model that can accurately classify tweets into drug abuse risk behaviorpositive or negative tweets to support drug abuse monitoring.", "body": "A Public Health Emergency has been declared In 2017, the Department of Health and Human Services (HHS) announced a new \"Opioid Strategy\" (2) The short and ungrammatical nature of tweets; and This paper addresses these challenging issues and achieves three objectives: (1) Build a system that can effectively collect drug abuse-related tweets at a large-scale; (2) Design an annotation strategy (drug abuse vs. non-drug abuse) that enables creating datasets at a much lower cost than traditional methods; and (3) Develop a deep learning model that can accurately classify tweets into drug abuse risk behaviorpositive or negative tweets to support drug abuse monitoring.II. METHODSWe first collected over 3 million raw tweets using the Twitter Streaming API during January 2017. To keep a balance between coverage and quality of the collected tweets, we used three types of keywords as filters: (1) Prescription and illicit drug names, e.g. marijuana, oxycontin, heroin, LSD, etc.; (2) Slang drug terms, e.g. barbs, crack, blunt, etc.; and (3) Drug abuse-related activities, behaviors, and syndromes, e.g. high, stoned, dizziness, etc. Over 800 keywords were used.A set of 1,794 human-annotated tweets, jointly annotated by two professors and three students, were used as seed dataset. The seed dataset was used to train an SVM classifier, which was then applied to unlabeled tweets to derive 4,985 positively predicted tweets. These 4,985 tweets were reviewed and annotated through Amazon Mechanical Turk.For classification, raw tweets were first tokenized and stemmed, then each word was vectorized into 300-dimension vectors using the pre-trained \"GoogleNews\" Word2vec model. Then, a Convolutional Neural Network (CNN) model III. EXPERIMENTAL RESULTSOur strategy produced a balanced and reliable dataset with 3,102 positive and 3,677 negative tweets in total. We tested and compared our boosted deep learning models with boosted traditional and state-of-the-art machine learning approaches.386Our model achieved 86.53% accuracy, 88.6% recall, and an 86.63% F1-score using Monte Carlo Cross Validation.Figures Figure IV. CONCLUSION", "conclusions": "We have presented three key components of a drug abuse tweet surveillance system: a large-scale data collection and annotation component, our boosting deep learning model that detects drug abuse tweets, and the geospatial and semantic analytics component. Our deep learning classification model achieved the state-of-the-art performance with a two-staged tweet annotation method for labeling more tweets at a lower cost without sacrificing quality. The semantic analysis of identified positive drug abuse behaviors uncovers many drug terms to be used for ontology refinement, and our geospatial analysis allows hotspot identification.The Amazon Mechanical Turk labeled dataset is available at: https://github.com/hu7han73/DrugAbuseLabeledTweets.    ", "SDG": [3]}, "development_of_an_evidence_based_ethical_decision_making_tool_for_neonatal_intensive_care_medicine": {"name": "Development of an Evidence-Based Ethical Decision-Making Tool for Neonatal Intensive Care Medicine", "abstract": " The goal of this research project is to combine our intelligent decision-aid systems with a patient decisionsupport tool to provide more information to physicians, nurses and parents when they are facing very difficult, ethical decisions regarding the care or management of neonatal intensive care (NICU) patients. Our two artificial intelligence approaches, one using case-based reasoning and the other artificial neural networks, may provide critical information such as estimates of the likelihood of survival and the use and duration of artificial ventilation. These estimates, in addition to other factors such as birth weight, gestational age and the presence of major complications, may provide eritical information to health care givers and parents to decide whether to initiate intensive care for the infant, or whether to terminate it if it has already been initiated.", "keywords": "Ethics,neonatal medicine,decision-aid system", "introduction": "Neonatal medicine has been available for over 30 years in the developed world and has been providing specialized and intensive care to premature babies and critically ill newboms to improve their health and survival [l].  define intensive care as \"the use of invasive treatment intended to save or extend the life of a neonate who might otherwise die from organ failure. It includes artificial ventilation, other forms of organ support and artificial nutrition delivered by invasive means.\" They add that this type of care must be delivered by a specialized team.Larcher and Hird [2]Since the early days, great progress has been seen in the capabilities of neonatal intensive care units (NICU) in terms of the sophistication of the equipment that allows \"more rapid and precise diagnosis, effective monitoring, and specific therapy\" . A new specialization has also arisen:[I]The neonatologist. The impact of these intensive care services has been mixed. There has been \"a substantial reduction of mortality in premature infants ... the rate of handicap or significant morbidity appears to have remained steady or declined in survivors of NICU of nearly all gestational ages and weights\" [I]. However, the author also states that the rate of prematurity, low birth weights, and the rate of birth defects has not declined in the US since the appearance of NICUs [ 11. On the question of birth defects, a report states that 3 or 4 babies out of 100 are bom with some type of birth defect in the US .[3]A serious concem fa'r health care givers and for parents remains: To whom should this intensive care be administered and in what circumstances should it be withdrawn? In a recent article, Tyson [4] stated: \"Despite the success of newborn intensive care, a vexing ethical question remains: Which pre-term infants are so malformed, sick, or immature that newbom intensive care should not be administered?\" This sta1.ement has lead several researchers to ponder upon ways in which this question can be answered. Are there factors relating these babies' health status that can guide physicians and parents to make the decision either not to begin treatment or to end it if it has been started?The term 'extremely low birth weight infants' is used to refer to infants who weigh 1000 grams or less; infants who, if they survive, usually need a respirator for more than a month and remain in hospital for more than 100 days [4]. Gestational age is another factor that could be considered in these cases and a recommendation is to begin intensive care for infants 25 weeks gestational age or greater and not for infants of 22 weeks or less ; for infants of 23-24 weeks, treatment should be decided with the parents [4,6]. Of course these decisions also depend on whether the infant's condition deteriorates or if a serious complication exists such as a large intracranial hemorrhage. \"Among infants of the same birth weight, those with the most advanced gestational age are the most mature, least likely to die, and thus, most likely to benefit from neonatal intensive care'' European countries appear to be much less aggressive than the US in using neonatal intensive care and place more emphasis on prenatal care. Tyson [4] states that in Denmark, for example, neonatal intensive care is not recommended for infants below 25 or 26 weeks of gestational age and in Sweden for infants under 600 grams. In developing countries, if available, it would not be provided for infants weighing less than 1000 grams. Tyson admits that there is little information documenting actual practice in administration of this type of care in the US. h Canada, management of the wom,an with threatened birth of an infant of extremely low gestational age is guided by a joint statement from the following national organizations and their committees: The Fetus and Newbom Committee of the Canadian Paediatric Society (CPS) and the Matemal-Fetal Medicine Committee of the Society of Obstetricians and Gynecologists of Canada [5]. This guideline was published in 1994; however, similar to the US situation, there is little data in Canada on actual practices regarding this issue.[7]", "body": "Neonatal medicine has been available for over 30 years in the developed world and has been providing specialized and intensive care to premature babies and critically ill newboms to improve their health and survival [l]. Since the early days, great progress has been seen in the capabilities of neonatal intensive care units (NICU) in terms of the sophistication of the equipment that allows \"more rapid and precise diagnosis, effective monitoring, and specific therapy\" The neonatologist. The impact of these intensive care services has been mixed. There has been \"a substantial reduction of mortality in premature infants ... the rate of handicap or significant morbidity appears to have remained steady or declined in survivors of NICU of nearly all gestational ages and weights\" [I]. However, the author also states that the rate of prematurity, low birth weights, and the rate of birth defects has not declined in the US since the appearance of NICUs [ 11. On the question of birth defects, a report states that 3 or 4 babies out of 100 are bom with some type of birth defect in the US A serious concem fa'r health care givers and for parents remains: To whom should this intensive care be administered and in what circumstances should it be withdrawn? In a recent article, Tyson [4] stated: \"Despite the success of newborn intensive care, a vexing ethical question remains: Which pre-term infants are so malformed, sick, or immature that newbom intensive care should not be administered?\" This sta1.ement has lead several researchers to ponder upon ways in which this question can be answered. Are there factors relating these babies' health status that can guide physicians and parents to make the decision either not to begin treatment or to end it if it has been started?The term 'extremely low birth weight infants' is used to refer to infants who weigh 1000 grams or less; infants who, if they survive, usually need a respirator for more than a month and remain in hospital for more than 100 days [4]. Gestational age is another factor that could be considered in these cases and a recommendation is to begin intensive care for infants 25 weeks gestational age or greater and not for infants of 22 weeks or less ~41.Larcher and Hird [Z] mention that some guidelines exist for the withdrawal of life-sustaining treatment and that such guidance can be helpful, but \"it does not abolish controversy and ambiguity.\" These authors also say that: \"Clinicians require guidance that is practical, reasonably specific, but not prescriptive ... and that it is ethically acceptable to offer intensive care until a clearer view of the baby's prognosis and the wishes of the parents can be defined.\"The discussions above indicate a critical need to provide physicians with more accurate estimations of the likelihood of the infant surviving, the estimated duration of artificial ventilation, and length of stay in the NICU (LOS) , If these estimates could be provided with acceptable accuracy, in addition to the factors mentioned above regarding birth weight, gestational age and the presence of major complications, then guidelines could be developed to enhance the decision-making on whether to provide neonatal care to an infant or not, or to withdraw it when the circumstances warrant this decision.PROPOSED APPROACHESOur research group has designed two complementary artificial intelligence approaches to estimate outcomes for NICU patients. First, a casebased reasoner (CBR) mtches a new patient anival in the NlCU to the most similar patients from a large database of patients from the Canadian Neonatal Network's NICU database with the input variables from SNAP-I1 (Score for Neonatal Acute Physiology, Version 2: lowest blood pressure, lowest temperature, lowest p02ifi02 ratio, lowest serum pH, presence of seizures and lowest urine outputall parameters measured within first 12 hours of admission) and SNAPPE.11 (SNAP with Perinatal Extensionplus birth weight, small for gestational age (SGA) status and Apgar score at 5 minutes) The CBR can display outcomes, complications, and medical information of the matched patients that may guide physicians in their diagnosis or in their selection of therapies for the newly admitted infant. The system is designed to be user-friendly and the outputs mimic the process physicians use in decision-making so that their use is intuitive. Table Ill. DISCUSSIONThe two artificial intelligence tools described above provide information that add to the known admission data such as gestational age, birth weight, and apparent defects. The tools provide an estimation of mortality, whether the infant will be on a respirator for a very long period of time (Tyson states a month as indicative of major problems) and /or stay in the hospital for a long period (Tyson mentions 100 days for this variable). Being able to predict these outcomes may help in the decision-making process for admission to a NICU and whether the infant is likely to benefit from intensive care or not [4]. Gestational age, birth weight, estimating mortality, artificial ventilation, length of stay were all variables mentioned in the literature that can aid clinical decision-making in the NICU. However, it is certain that the decision-support systems would increase their usefulness if they could also predict other rare outcomes, those that would better guide physicians and parents in their decision-making process in the NICU. Some of the outcomes that should be added to our systems are: Intraventricular hemorrhage (IVH), perivenhicular leukomalacia (PVL), broncho-pulmonary dysplasia (BPD), necrotising entero-colitis (NEC), and retinopathy of prematurity (ROP). In cases where the greatest uncertainty remains, care could be initiated, and estimations could be provided at regular intervals to re-assess the situation on prognosis and outcomes.Our work to date has shown the usefulness of ANNs for estimating the outcomes listed in Table The next step to improve our systems would consist in adding the rare outcomes previously listed that would be relevant to the decision-making process of administering or withdrawing neonatal intensive care. In order to achieve the goal of estimating these additional rare outcomes, we have developed and tested a method to replace missing values in our NICU database. Using only complete records restricts greatly the number of cases containing these rare outcomes and so extending the case number to our entire database of over 20,000 NICU patient cases will now allow us to train and test our ANNs to predict these additional outcomes [ 191.Until this step has been completed, we plan to use the results from the Canadian Neonatal Network whose SNAP-I1 score is a predictor of IVH (intraventricular hemorrhage) and CLD (chronic lung disease) in neonatal intensive care patients. In their article, Chien et al. report the percent Contribution to predictive power of various factors to predict 1VH:as follows: Gestational age is 41 %; SNAP-I1 is 30 %; outborn status is 23 %; and Apgar at 5 minutes is 6 %. For predicting CLD, the percent contribution to predictive power ae: 54 % for gestional age; 30 % for SNAP-II; 9 % for 'small for gestional age'; 2 % for the 5-minute Apgar and 2 % for outbom status; it is 3 % for a male infant [20].Our research group is currently designing an expert system that will incorporate the predictive tool (ANNs for each outcome of interest), then merge this information for a global view of the situation in a manner in which physicians wish to see it. The information will then be transferred in a manner that parents can understand (both in terms of language, content, and the speed at which this information is provided to them. The generic tool will be adapted to various situations and ensure that parents are included in the decision-making process and are able to reach a consensus with the physicians, without coercion. The decision-support tool will be sttited to the parents' needs, values, and level of information that can be communicated effectively to them. Dr. OConnor's generic tool (at University of Ottawa) that supports patient-centered decision-making will be the base on which this prototype is constructed V. CONCLUSION", "conclusions": "Our research to dzte has led to the development of prototypes that will allow physicians to better predict certain outcomes in neonatal intensive care and thus use this information when they counsel families on prognosis and the desirability of initiating or on withdrawing treatment when conditions dictate this approach. It will be very important in our future work to include parents in the decision loop. An expert system is in initial stages of development to complement the predictive tools described above. This system will take into consideration the parents' values and manner and tempo in which the information should be provided to tham.", "SDG": [3]}, "early_prediction_of_severe_maternal_morbidity_using_machine_learning_techniques": {"name": "Early Prediction of Severe Maternal Morbidity Using Machine Learning Techniques", "abstract": " Severe Maternal Morbidity is a public health issue. It may occur during pregnancy, delivery, or puerperium due to conditions (hypertensive disorders, hemorrhages, infections and others) that put in risk the women's or baby's life. These conditions are really difficult AQ1 to detect at an early stage. In response to the above, this work proposes using several machine learning techniques, which are considered most relevant in a bio-medical setting, in order to predict the risk level for Severe Maternal Morbidity in patients during pregnancy. The population studied correspond to pregnant women receiving prenatal care and final attention at E.S.E Cl\u00ednica de Maternidad Rafael Calvo in Cartagena, Colombia. This paper presents the preliminary results of an ongoing project, as well as methods and materials considered for the construction of the learning models.", "keywords": "Severe maternal morbidity,Machine learning,Logistic regression", "introduction": "The term Severe Maternal Morbidity (SMM) includes a set of complications that can have a severe adverse effect on women and baby health, and happen during pregnancy, delivery, or puerperium. When any of these appear, it is necessary to provide the patient with immediate attention, in order to avoid death . Although maternal health outcomes have shown positive variation, complications of pregnancy still are an important public health issue. Each year around 585.000 women die during pregnancy, delivery or puerperium worldwide [10], and annually close to 50 million complications in maternal health are registered, and approximately 300 million women suffer from short and long-term illnesses and injuries related to pregnancy, childbirth and postpartum [5]. Currently, there is an epidemiological surveillance strategy which consists in identifying SMM cases, reporting them to the public surveillance system (SIVIGILA) [10], and following them up. This allows to characterize SMM and have a better understanding about the main factors of risk in the population and devise policies to help lower incidence. However, the number of SMM cases continue to be very high.[16]Studies conducted to identify causes of SMM show that this condition is related with hypertensive disorder, hemorrhage, and infections. The main risk factors associated with occurrence of SMM are black race, obesity, multi parity and backgrounds of previous cesarean sections and presence of co-morbidities [3,6,7,.14]The development of adequately sensitive and specific predictive tests for these outcomes has received significant focus in perinatal research. According to the literature, machine learning approaches are used frequently to identify patterns and make predictions. Specially in medicine, logistic regression [11,, support vector machines 13], neural networks [1] have been used successfully.[9]The World Health Organization and PanAmerican Health Organization during the last decades have tried to reduce mortality and Severe Maternal Morbidity. For this, the action plan of 2012-2017 was proposed . It consists in strengthening information systems and monitoring of maternal health in the countries of the region. The reduction of maternal mortality is a millennium goal and a national purpose. Actions, as epidemiological surveillance, the availability of statistical data, and the identification of risk factors related to these events, have contributed to its decrease.[3]Institutions and doctors strive to avoid SMM, because it is not easy to detect and prevent such situations. Especially, when the volume of pregnant women is quite high in a day, or when novice doctors do not have enough experience. Even with the implementation of the above actions, failure to meet the stated goal persist. Because of that it is necessary to implement new mechanisms for early warning and monitoring of SMM cases.This paper proposes the use of machine learning techniques to build a risk classifier for SMM. With this, we expect to have early detection of morbidity cases, providing support for medical staff in decision-making, to enable a timely intervention of patients. This would help reduce the risk the mother and baby may have during this stage, and in turn to reduce social and economic repercussions.The paper is organized as follows. Section 2 presents related work. Section 3 shows the methods and materials used in our approach. Section 4 shows the preliminary results of our on-going research. Finally, Sect. 5 states the conclusions of this paper.", "body": "The term Severe Maternal Morbidity (SMM) includes a set of complications that can have a severe adverse effect on women and baby health, and happen during pregnancy, delivery, or puerperium. When any of these appear, it is necessary to provide the patient with immediate attention, in order to avoid death Studies conducted to identify causes of SMM show that this condition is related with hypertensive disorder, hemorrhage, and infections. The main risk factors associated with occurrence of SMM are black race, obesity, multi parity and backgrounds of previous cesarean sections and presence of co-morbidities The development of adequately sensitive and specific predictive tests for these outcomes has received significant focus in perinatal research. According to the literature, machine learning approaches are used frequently to identify patterns and make predictions. Specially in medicine, logistic regression The World Health Organization and PanAmerican Health Organization during the last decades have tried to reduce mortality and Severe Maternal Morbidity. For this, the action plan of 2012-2017 was proposed Institutions and doctors strive to avoid SMM, because it is not easy to detect and prevent such situations. Especially, when the volume of pregnant women is quite high in a day, or when novice doctors do not have enough experience. Even with the implementation of the above actions, failure to meet the stated goal persist. Because of that it is necessary to implement new mechanisms for early warning and monitoring of SMM cases.This paper proposes the use of machine learning techniques to build a risk classifier for SMM. With this, we expect to have early detection of morbidity cases, providing support for medical staff in decision-making, to enable a timely intervention of patients. This would help reduce the risk the mother and baby may have during this stage, and in turn to reduce social and economic repercussions.The paper is organized as follows. Section 2 presents related work. Section 3 shows the methods and materials used in our approach. Section 4 shows the preliminary results of our on-going research. Finally, Sect. 5 states the conclusions of this paper.Related WorkEvery time, it is more frequent to find the use of machine learning in medicine, especially for classification problems. Some studies of prediction of at least one of the major diseases associated with SMM are mentioned below.In Poon et al. Logistic regression was used and a detection rate around 90 % for early preeclampsia was obtained, and a false-positive rate of 5 %.In Park et al. In Nanda et al. In Farran et al. According to the reviewed literature there is evidence that the results obtained from the implementation of machine learning for classification problems in medicine are quite satisfactory. The authors of this paper have not been able to find similar studies or proposals, using machine learning as a tool to help to avoid or mitigate the risk of SSM.Methods and MaterialsParticipantsA retrospective cohort study was done through clinical histories of prenatal controls obtained between 2014 and 2015. The population selected for this study include patients with ages between 12 and 45 years who had at least one control at E.S.E Cl\u00ednica de Maternidad Rafael Calvo and whose labor was cared for in this institution.Cohort patients were classified according SMM outcome, in two groups: patients who did not present SMM, and patients which presented SMM (which also were reported to the public healthcare surveillance system). For the first group, we used random sampling, and for the second group, we used convenience sampling. This method is known as mixed sampling Data SetThe construction of the machine learning model was based on features or risk factors. These factors were selected according to the risk factor characterization described by Latin American Center of Perinatology (Centro Latino Americano de Perinatolog\u00eda, CLAP), compared to the 2015 SMM protocol from the Colombia Ministry of Health and Social Protection (Ministerio de Salud y de Protecci\u00f3n Social) For the data set construction we used Google Forms. Two forms were designed, the first one to record Obstetrics Gynecology (OBGYN) and sociodemographic background data, and the second one to record diagnoses for each gestational week, using the International Classification of Diseases (ICD-10) codification The data set is being built with help of sixth year medicine students. They were trained on SMM, review of medical records and filling out Google forms. The manual review of prenatal medical records of each patient is necessary, because the information is sometimes scattered and not totally centralized on the hospital information system. To perform preliminar training and validation, two patient groups were generated according to outcomes (SMM and not SMM). First group was sub-sampled in order to reduce class imbalance, given that the number of patients that exhibited SMM is lower than the non SMM group. The collection process is still underway, and subsequent validation will be carried to monitor progress and performance of the predictive models.Statistical Data AnalysisThe filtering features allow to select the set of variables that represent variability in the occurrence of SMM. Once the variables are defined, and the database constructed, we proceeds to do an analysis statistical to obtain a database only with the information of the variables that we considered predictors for the model. We use descriptive statistics to identify the frequency with which diagnoses are presented, and multi-factor analysis of variance (ANOVA) to determine which variables are more likely to be considered predictive for the model that we want to implement. It was tested with different levels of confidence for the group of variables most likely had to influence the behavior of the response variable. Learning ModelsThis section lists some of the machine learning techniques most commonly used for classification or pattern recognition.Logistic Regression. The logistic regression has been historical an important tool for data analysis in medical investigation and epidemiology. This allows differentiating between some classes, in terms of a set of numerical variables, as a predictor. The basic goals of a logistic regression model are:-Get an unbiased or adjusted estimate of the relationship between variable dependent (or result) and an independent variable. -Simultaneously evaluate several factors that are allegedly related somehow (or not) with the dependent variable. -Build a model and get a hypothesis prediction purpose or calculating risk. Results can be interpreted as the probability for the input to belong to the positive class, p(Y = 1|x; \u03b8). In this study, Logistic Regression was trained with Cross-Entropy loss. A L2 regularization penalty was introduced to account for model complexity and avoid over-fitting:whereIt is possible to increase its capabilities by applying a polynomial transform to the input, in which case the decision boundary can be non-linear and handle more difficult problems.In this work, we used L2 Regularized Logistic Regression with polynomial transform of 2nd degree. The model was implemented using the Scikit-Learn Python Machine Learning Library, version 2.7.11. ResultsStatistical AnalysisThis paper presents preliminary results of the project that is still ongoing. Achieving determine the variables set related to the occurrence of extreme maternal morbidity. The entire population is 1838 patients. 72 belong to the first group, and the remain 1766 belong to the second group. For a 95 % level with a confidence interval of 4.5, we obtained a total sample of 377 patients. Once the system is trained, we will validate the obtained data in 145 patients. Initially, we performed a descriptive data frequency analysis of the patients who developed SMM.We perform an analysis of frequency of the diagnosis. As we can see in Fig. Followed by this, we did the frequency analysis by trimester. In the first trimester only the diagnostic group Z30-Z39 had high frequency. For the second trimester, the results show that the most frequent group are Z30-Z39, O30-O48, O20-O29, E65-E68, and N70-N77. In the third trimester is obtained that the diagnostic groups most frequently are Z30-Z39, O30-O48, O10-O16, N70-N77, O95-O99, D50-D64, and O60-O75. After having a notion of the data trend, we decided to carry out an analysis of variance ANOVA to the data of the patients with SMM and not SMM. It was organized per trimester. Similarly, the diagnostics were grouped according to SMM. The analysis of the first trimester was done initially with levels of confidence of 95 %, 90 % and finally 85 %. The results indicate that none of the variables have a high probability of being connected with the SMM. It notes that the diagnosis Z30-Z39 shows more likely to have any connection with the response variable, but it is the default diagnostic.The analysis of the data obtained for the second trimester was tested with the 90 %, 80 % and 75 % confidence levels, and the results are shown in Table Finally to make the analysis of ANOVA without taking into account the trimester was tested with confidence levels of 95 % and 80 %. Results are shown in Table TrainingThe data set was divided into training set and test set, the first corresponding to 80 % (178 instances) and the other 20 % (44 instances). The learning model was trained using 5-fold Stratified Cross-Validation. We Used L2 Regularized In the Fig. 1. Take the original data set and to use Recursive Feature Elimination for logistic regression, in this way obtain the best predictors to the model. 2. To Prove with polynomial transform higher than 2nd degree. 3. If the above options fail, we need to try a vector support machine. 4. After, we need to compare the results of ROC graphics for polynomial transform and support vector machine and select the model that shows the best especificity and sensibility.Conclusions", "conclusions": "Severe maternal morbidity remains a public health-care problem that affects much the pregnant women population, and in many cases, it is possible to avoid this. The problem lies in the early identification of risk patients who have finished in SMM. In response to the above mentioned, this paper presents the usage of the logistic regression for SMM detection. It is a pattern recognition technique commonly used in the medical field to solve problems of classification, prediction and identification of patterns. By the using of this technique, it is expected to build a tool for risk identification or risk classification of a patient having SMM. The goal is to provide a timely and adequate attention to each patient depending on the risk level to be determined. With the implemented logistic regression model, we obtained regular results, for this, we to continue to proving others techniques of machine learning from to obtain a model with best results.", "SDG": [3]}, "epidemiology_from_tweets_estimating_misuse_of_prescription_opioids_in_the_usa_from_social_media": {"name": "Epidemiology from Tweets: Estimating Misuse of Prescription Opioids in the USA from Social Media", "abstract": " Background The misuse of prescription opioids (MUPO) is a leading public health concern. Social media are playing an expanded role in public health research, but there are few methods for estimating established epidemiological metrics from social media. The purpose of this study was to demonstrate that the geographic variation of social media posts mentioning prescription opioid misuse strongly correlates with government estimates of MUPO in the last month. Methods We wrote software to acquire publicly available tweets from Twitter from 2012 to 2014 that contained at least one keyword related to prescription opioid use (n = 3,611,528). A medical toxicologist and emergency physician curated the list of keywords. We used the semantic distance (SemD) to automatically quantify the similarity of meaning between tweets and identify tweets that mentioned MUPO. We defined the SemD between two words as the shortest distance between the two corresponding word-centroids. Each word-centroid represented all recognized meanings of a word. We validated this automatic identification with manual curation. We used Twitter metadata to estimate the location of each tweet. We compared our estimated geographic distribution with the 2013-2015 National Surveys on Drug Usage and Health (NSDUH). Results Tweets that mentioned MUPO formed a distinct cluster far away from semantically unrelated tweets. The state-bystate correlation between Twitter and NSDUH was highly significant across all NSDUH survey years. The correlation was strongest between Twitter and NSDUH data from those aged 18-25 (r = 0.94, p < 0.01 for 2012; r = 0.94, p < 0.01 for 2013; r = 0.71, p = 0.02 for 2014). The correlation was driven by discussions of opioid use, even after controlling for geographic variation in Twitter usage. Conclusions Mentions of MUPO on Twitter correlate strongly with state-by-state NSDUH estimates of MUPO. We have also demonstrated that a natural language processing can be used to analyze social media to provide insights for syndromic toxicosurveillance.", "keywords": "Social media,Epidemiology,Misuse,Opioids,Natural language processing,Computational linguistics", "introduction": "Approximately 35 million Americans over age 12 used prescription opioids for nonmedical reasons at least once in the last year . The misuse of prescription opioids (MUPO) is Electronic supplementary material The online version of this article (doi:10.1007/s13181-017-0625-5) contains supplementary material, which is available to authorized users. associated with adverse hormonal and immune system effects, abuse, and addiction [1]. American healthcare costs of MUPO increased from $53.4 billion in 2006 [2] to $70.4 billion in 2013 [3].[4]Americans turn to online resources and social networks to discuss healthcare issues; 72% of US web users have sought health information online within the past 12 months; 34% of adult web users have read or shared health concerns or commentary on social platforms [5,. Nearly three out of four Americans use at least one social networking site 6].[7]Social media platforms, such as Twitter, Facebook, or YouTube, facilitate the exchange of short messages, via desktop, laptop, tablet, or smartphone. Messages exchanged on these platforms have previously been successfully analyzed for syndromic surveillance of infectious diseases  and sentiment analysis of the treatment of migraine headaches [8]. Twitter is an online news and social networking service where users post messages, called Btweets,^and reply to tweets that others send. Tweets are limited to 140 characters. Anyone can read publicly posted tweets. Only registered users can post tweets. Users access Twitter through its website interface or mobile device app. Among social networks, the microblogging platform of Twitter offers several advantages for digital epidemiology; its users tend to write frequent, short messages (tweets) on a wide variety of topics, users often indicate their location and other demographic information, messages are publicly searchable, by default, and the Twitter platform is frequently used via desktops, laptops, and mobile devices [9]. The use of social media to study the epidemiology of drug use has focused on using social media as a source of material for qualitative analysis, as a means to digitally acquire large amounts of data, often from online forums, that experts then process entirely manually. Prior analyses include an exploration of the demographics of well-defined communities [10], the frequencies of keywords related to stimulant abuse [11] or alcohol [12], and surveys of drugs mentioned in online discussion forums [13]. A limitation of all of these studies is that comparing the findings of these studies to established findings is not straightforward; for example, it is difficult to relate the frequency of words to prevalence of use in the population. This difficulty hinders validation of social media as an emerging data source for public health research.[14]Our aim was to determine whether Twitter could provide data on MUPO that agreed with government survey data, establishing Twitter as a potential longitudinal source for syndromic surveillance. We used the National Survey on Drug Usage and Health (NSDUH) as our standard for comparison. The NSDUH is conducted by professional interviewers, confidentially surveying residents from a random sample of US households, in person, over the course of about an hour about their substance use . Each year, NSUDH surveys approximately 70,000 people. A secondary objective was to evaluate the potential of social media for toxicosurveillance in a scalable and automated fashion so that our approach could be readily adapted and extended. We hypothesized that the geographic distribution of tweets about MUPO would closely correspond to that of NSDUH survey data about MUPO.[1]", "body": "Approximately 35 million Americans over age 12 used prescription opioids for nonmedical reasons at least once in the last year Americans turn to online resources and social networks to discuss healthcare issues; 72% of US web users have sought health information online within the past 12 months; 34% of adult web users have read or shared health concerns or commentary on social platforms Social media platforms, such as Twitter, Facebook, or YouTube, facilitate the exchange of short messages, via desktop, laptop, tablet, or smartphone. Messages exchanged on these platforms have previously been successfully analyzed for syndromic surveillance of infectious diseases Our aim was to determine whether Twitter could provide data on MUPO that agreed with government survey data, establishing Twitter as a potential longitudinal source for syndromic surveillance. We used the National Survey on Drug Usage and Health (NSDUH) as our standard for comparison. The NSDUH is conducted by professional interviewers, confidentially surveying residents from a random sample of US households, in person, over the course of about an hour about their substance use MethodsWe performed a prospective study of the incidence of discussions on MUPO using publicly available data from Twitter. The Institutional Review Board (IRB) approved this study at the authors' institutions. Figure Tweet Preprocessing Twitter provides an application programming interface (API) that enables programmatic consumption of its data. An API is an access point allowing researchers to collect automatically data that have been made publicly available. The Twitter Streaming API allows unrestricted access to all public tweets matching any given filter criteria in real time. For example, using the keyword filter of BAdderall,^all tweets mentioning that substance are collected. We acquired two types of tweets from Twitter, tweets that contained the keywords in Table & Lemmatization: All words in the tweet were converted to their associated lemma, or dictionary form. LemmaThe form of the word, without inflections, that would be found in the dictionary, for example Bchild^not Bchildren\u015c topword A word with no intrinsic semantic value, for example, Ba,^Bthe,^Bof.^Additional words may be stopwords in one context but not another.API Application Program Interface; method to allow programs to access the data of other programs without human interface OntologyA formal description of the semantic relationship between wordsGitHub repository An online cloud storage and code-sharing community.Resource for open-source softwareSemantic similarity A quantification of the similarity in meaning between two phrases Twitter Streaming APIAn API that provides real-time access to tweets. As soon as a user emits a publicly available tweet, it becomes available to the Streaming API.Semantic similarity matrixA two-dimensional grid where each square denotes the semantic similarity between two pieces of text (tweets in this context). Each square in the grid is specified by two co-ordinates, canonically called the ith and jth coordinates, counting from 0. For example, the lower right square of a 2 \u00d7 2 grid would be identified as 11.CentroidMean position of all points in a cluster, analogous to center of mass in physical objects.Comparing Tweets To quantify the similarity in meaning (semantic similarity) between tweets, we used a straightforward extension of Jiang-Conrath similarity In keeping with terminology from machine learning, we termed the weighting factors the semantic kernel (see Supplemental section BJiang-Conrath Similarity and WordNet^for more detail).Computing the Context of TweetsThe context in which a word occurs helps specify which meanings of that word are most germane. We took context into account by weighting the combinations of meanings of each word by the relative frequency with which all synonyms of the meaning of a word occur in the text. For example, if a text excerpt contains twice as many words pertaining to drugs as to aviation, then the meaning of high as in intoxicated with marijuana receives twice as much weight as as high as in elevated in altitude.We excluded tweets for which we could not calculate the SemD (3.2% for 2012, 2.5% for 2013, and 3.1% for 2014), generally because those tweets contain too few recognizable words (for example, Bonereallylongword^cannot be processed, whereas Bone really long word^can).We identified clusters of tweets as tweets with correlated semantic distance values, using k-means clustering Tweet Curation Independently, one emergency physician (NG) and one medical toxicologist (AM) manually curated the same 5% random sample of all tweets we acquired, rating each tweet as Brelated or Bnot related^to MUPO.^We did this to identify whether the clusters identified using SemD had any toxicologic meaning. Two examples of tweets rated as Brelated to MUPO^-censored for profanity but not for nonstandard orthography-are as follows:1. 420 blaze it How abot yo grow up and shoot heroin like an adlt, oxy sh*t 2. percocet's keep me motivated, good weed keep me motivatedExamples of tweets rated as Bnot related to MUPO^are as follows:1. Knee x-rayed and been given some pain killers. Waiting to see dr now. Was such a lovely afternoon. 2. Thank yo! Hx How are you today? I hope everything is amazing.3.Try something new today (not heroin) and f*ck the world. \u263a\u263a 4. Today I get to place a british boy, a heroin addict, and a bookish girl next door in one day.Geocoding Tweets We estimated a tweet's location in three ways. If metadata contained latitude and longitude coordinates, we directly used them. In our sample, approximately 2% of tweets contained explicit coordinates of latitude and longitude. This level of explicit geocoding is consistent with prior studies Scaling To compare data from NSDUH and Twitter, we scaled each data set by the population in each state. For NSDUH, we divided the number of respondents in each state who endorsed MUPO by the total number of respondents in that state. For Twitter, we divided the number of MUPO tweets by the total number of tweets geolocated to that state.To allow comparison on the same scale, we scaled each data set by the formula z = (x \u2212 min(x)) / (max(x) \u2212 min(x)), where min (or max) refers to the minimum (maximum) and x refers to the Twitter or NSUDH data set. The resulting variables range between 0 and 1.Sample Size Calculation Our central statistical test a comparison of the difference between two proportions with independent samples. We chose our chance of false positives (alpha) at 0.01. We adjusted this alpha for the simultaneous comparison of three hypotheses (whether Twitter and NSDUH were comparable for each age group defined by NSDUH) using a Bonferroni correction factor of 3, yielding a final alpha of 0.0033. We chose our initial chance of false negatives (beta) at 0.01, yielding a power (1-beta) of 99%. We chose a more stringent than usual power, in consideration of the novelty of the approach. Choosing a more stringent power also mitigates the effect of unequal sample sizes on the chance of false negatives. Using estimates from the previous 10 NSDUH, we estimated the prevalence of MUPO to be around 2%. We assumed that the Twitter rate would be comparable, i.e., 1.9 to 2.1%. We chose this small difference so that our study would be powered to detect even small differences between Twitter and NSDUH. A sample size calculation using those parameters yielded a suggested sample size of 1,696,621 across all age groups for each year. While we had no control over the number of respondents in NSDUH, we obtained the extra n necessary from Twitter.Principal Component Analysis Principal component analysis (PCA) identifies the largest sources of variance in the data and allows high-dimensional data to be visualized in two dimensions Software All analyses were performed with available opensource software or custom software (written by MC) in the Python programming language ResultsFor 2012, we obtained approximately 1.3 million unique English language tweets from the Streaming API that discussed MUPO. For 2013 and 2014, we obtained approximately 1.1 million and 1.2 million tweets, respectively. These account for 0.00065% of the annual volume of tweets. Of those, we obtained geographic information for 85,328 (2012), 64,112 (2013), and 79,442 (2014). The NSDUH surveys approximately 70,000 individuals (each person interviewed is a proxy for approximately 4500 US residents Figure The agreement between Twitter and NSDUH could be confounded by population density. To account for this, we assessed the correlation between unscaled Twitter and NSDUH data. None of these correlations were significant (Table DiscussionThe purpose of the study was to determine whether data from social media could accurately estimate the geographic location and relative prevalence of MUPO when compared to an established epidemiologic gold standard (i.e., the NSDUH).  We used a novel application of natural language processing, the kernel-weighted semantic distance (SemD), automate content analysis. Our approach leverages the observation that discussions on Twitter about have linguistic characteristics that distinguish them from other discussions The main finding of this study is that Twitter and NSDUH provide significantly correlated estimates of the geographic distribution of MUPO over a discrete time period. The strongest correlation occurred between data from Twitter and NSDUH data from those aged 18-25. The correlation increased from 2012 to 2013 and then decreased from 2013 to 2014, although these differences were not statistically significant. This work demonstrates that social media can be used to estimate fundamental epidemiologic quantities, in contrast to prior work that used social media to define a population or estimate quantities that might correlate with established epidemiologic metrics such as prevalence.Data on the epidemiology of MUPO traditionally come from government surveys, such as the annual National Survey on Drug Usage and Health. Social media may provide a complementary source of data, especially on nonmedical substance usage in certain age groups (particularly adolescents, teens, and young adults). Users of social networks often publicly broadcast their location and information about their peers and behaviors. Further information about these users, such as age, can be inferred from patterns of communication and association with other users. There are challenges to extracting data from social media data, which are of comparably lower quality than government-sponsored survey data. Discussions of substance use on social media often use slang and highly referential language. Users may post misleading messages to portray a pattern of substance use that they associate with social status Limitations We used processed versions of the tweets that regularized spelling, ignored emoticons, and changed the part of speech of some of the words. This increases the number of tweets that we could analyze at the cost of possibly distorting or overlooking synonymy, sarcasm, irony, and hyperbole.Our data are subject to sampling bias. The Twitter API provides a random 1% sample of all tweets at any given time. Although we are unaware of any published literature on this, anecdotal evidence from multiple groups suggests that successive samples from the Twitter API are not independent. Only 1-2% of the tweets encoded by the Twitter API contain explicit latitude and longitude coordinates. We used Python module Carmen to increase the number of tweets with geographic information. Carmen infers location based on metadata and the text of the tweet, which may add another layer of bias. Our calculation of the semantic distance also uses the text of the tweet. The accuracy of Carmen is already known to depend on the amount of metadata and length of text of a tweet. These limitations notwithstanding the correlation between Twitter and NSDUH did not statistically significantly vary over 3 years, suggesting that the correlation we found is stable.This paper describes an agreement between social media and government surveys; however, it provides no insight into mechanisms underlying this agreement. Our conceptual hypothesis is that people discuss on social media what they intend to do in the physical world. This hypothesis has held for research involving cardiovascular mortality Further work is necessary to correlate the geographic variation noted in this paper with geographic variation in policies and laws on controlled substances, mental health and addiction services, and known risk and protective factors. As geolocation algorithms improve, it would be desirable to look at trends in usage at the more granular levels of a city or Congressional district. The compilation of a time-series of usage will help further establish our method and may allow novel insights.Conclusions", "conclusions": "We used Twitter data to estimate the geographic variation in discussions on MUPO. We found that our estimates agreed with national survey data, suggesting that social media can be a reliable additional source of epidemiological data regarding substance use. Furthermore, we have demonstrated that techniques from machine learning can be used to analyze social media to canvass larger segments of the general population and potentially yield timely insights for syndromic surveillance.", "SDG": [3]}, "graph_convolutional_policy_network_for_goal_directed_molecular_graph_generation": {"name": "Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation", "abstract": " Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models to find molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goaldirected graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184% improvement on the constrained property optimization task.", "keywords": "", "introduction": "Many important problems in drug discovery and material science are based on the principle of designing molecular structures with specific desired properties. However, this remains a challenging task due to the large size of chemical space. For example, the range of drug-like molecules has been estimated to be between 10 23 and 10 60 . Additionally, chemical space is discrete, and molecular properties are highly sensitive to small changes in the molecular structure [32]. An increase in the effectiveness of the design of new molecules with application-driven goals would significantly accelerate developments in novel medicines and materials.[21]Recently, there has been significant advances in applying deep learning models to molecule generation [15,38,7,9,22,4,31,27,34,. However, the generation of novel and valid molecular graphs that can directly optimize various desired physical, chemical and biological property objectives remains to be a challenging task, since these property objectives are highly complex 42] and non-differentiable. Furthermore, the generation model should be able to actively explore the vast chemical space, as the distribution of the molecules that possess those desired properties does not necessarily match the distribution of molecules from existing datasets.[37]Present Work. In this work, we propose Graph Convolutional Policy Network (GCPN), an approach to generate molecules where the generation process can be guided towards specified desired objectives, while restricting the output space based on underlying chemical rules. To address the challenge of goal-directed molecule generation, we utilize and extend three ideas, namely graph representation, reinforcement learning and adversarial training, and combine them in a single unified framework. Graph representation learning is used to obtain vector representations of the state of generated graphs, adversarial loss is used as reward to incorporate prior knowledge specified by a dataset of example molecules, and the entire model is trained end-to-end in the reinforcement learning framework.Graph representation. We represent molecules directly as molecular graphs, which are more robust than intermediate representations such as simplified molecular-input line-entry system (SMILES) , a text-based representation that is widely used in previous works [40][9,22,4,15,38,27,. For example, a single character perturbation in a text-based representation of a molecule can lead to significant changes to the underlying molecular structure or even invalidate it 34]. Additionally, partially generated molecular graphs can be interpreted as substructures, whereas partially generated text representations in many cases are not meaningful. As a result, we can perform chemical checks, such as valency checks, on a partially generated molecule when it is represented as a graph, but not when it is represented as a text sequence.[30]Reinforcement learning. A reinforcement learning approach to goal-directed molecule generation presents several advantages compared to learning a generative model over a dataset. Firstly, desired molecular properties such as drug-likeness [1, and molecule constraints such as valency are complex and non-differentiable, thus they cannot be directly incorporated into the objective function of graph generative models. In contrast, reinforcement learning is capable of directly representing hard constraints and desired properties through the design of environment dynamics and reward function. Secondly, reinforcement learning allows active exploration of the molecule space beyond samples in a dataset. Alternative deep generative model approaches 29][9,22,4, show promising results on reconstructing given molecules, but their exploration ability is restricted by the training dataset.16]Adversarial training. Incorporating prior knowledge specified by a dataset of example molecules is crucial for molecule generation. For example, a drug molecule is usually relatively stable in physiological conditions, non toxic, and possesses certain physiochemical properties . Although it is possible to hand code the rules or train a predictor for one of the properties, precisely representing the combination of these properties is extremely challenging. Adversarial training addresses the challenge through a learnable discriminator adversarially trained with a generator [28]. After the training converges, the discriminator implicitly incorporates the information of a given dataset and guides the training of the generator.[10]GCPN is designed as a reinforcement learning agent (RL agent) that operates within a chemistryaware graph generation environment. A molecule is successively constructed by either connecting a new substructure or an atom with an existing molecular graph or adding a bond to connect existing atoms. GCPN predicts the action of the bond addition, and is trained via policy gradient to optimize a reward composed of molecular property objectives and adversarial loss. The adversarial loss is provided by a graph convolutional network [20, based discriminator trained jointly on a dataset of example molecules. Overall, this approach allows direct optimization of application-specific objectives, while ensuring that the generated molecules are realistic and satisfy chemical rules.5]We evaluate GCPN in three distinct molecule generation tasks that are relevant to drug discovery and materials science: molecule property optimization, property targeting and conditional property optimization. We use the ZINC dataset  to provide GCPN with example molecules, and train the policy network to generate molecules with high property score, molecules with a pre-specified range of target property score, or molecules containing a specific substructure while having high property score. In all tasks, GCPN achieves state-of-the-art results. GCPN generates molecules with property scores 61% higher than the best baseline method, and outperforms the baseline models in the constrained optimization setting by 184% on average.[14]", "body": "Many important problems in drug discovery and material science are based on the principle of designing molecular structures with specific desired properties. However, this remains a challenging task due to the large size of chemical space. For example, the range of drug-like molecules has been estimated to be between 10 23 and 10 60 Recently, there has been significant advances in applying deep learning models to molecule generation Present Work. In this work, we propose Graph Convolutional Policy Network (GCPN), an approach to generate molecules where the generation process can be guided towards specified desired objectives, while restricting the output space based on underlying chemical rules. To address the challenge of goal-directed molecule generation, we utilize and extend three ideas, namely graph representation, reinforcement learning and adversarial training, and combine them in a single unified framework. Graph representation learning is used to obtain vector representations of the state of generated graphs, adversarial loss is used as reward to incorporate prior knowledge specified by a dataset of example molecules, and the entire model is trained end-to-end in the reinforcement learning framework.Graph representation. We represent molecules directly as molecular graphs, which are more robust than intermediate representations such as simplified molecular-input line-entry system (SMILES) Reinforcement learning. A reinforcement learning approach to goal-directed molecule generation presents several advantages compared to learning a generative model over a dataset. Firstly, desired molecular properties such as drug-likeness Adversarial training. Incorporating prior knowledge specified by a dataset of example molecules is crucial for molecule generation. For example, a drug molecule is usually relatively stable in physiological conditions, non toxic, and possesses certain physiochemical properties GCPN is designed as a reinforcement learning agent (RL agent) that operates within a chemistryaware graph generation environment. A molecule is successively constructed by either connecting a new substructure or an atom with an existing molecular graph or adding a bond to connect existing atoms. GCPN predicts the action of the bond addition, and is trained via policy gradient to optimize a reward composed of molecular property objectives and adversarial loss. The adversarial loss is provided by a graph convolutional network We evaluate GCPN in three distinct molecule generation tasks that are relevant to drug discovery and materials science: molecule property optimization, property targeting and conditional property optimization. We use the ZINC dataset Related WorkYang et al. Proposed MethodIn this section we formulate the problem of graph generation as learning an RL agent that iteratively adds substructures and edges to the molecular graph in a chemistry-aware environment. We describe the problem definition, the environment design, and the Graph Convolutional Policy Network that predicts a distribution of actions which are used to update the graph being generated.Problem DefinitionWe represent a graph G as (A, E, F ), where A \u2208 {0, 1} n\u00d7n is the adjacency matrix, and F \u2208 R n\u00d7d is the node feature matrix assuming each node has d features. We define E \u2208 {0, 1} b\u00d7n\u00d7n to be the (discrete) edge-conditioned adjacency tensor, assuming there are b possible edge types. E i,j,k = 1 if there exists an edge of type i between nodes j and k, and A = b i=1 E i . Our primary objective is to generate graphs that maximize a given property function S(G) \u2208 R, i.e., maximizewhere G is the generated graph, and S could be one or multiple domain-specific statistics of interest.It is also of practical importance to constrain our model with two main sources of prior knowledge.(1) Generated graphs need to satisfy a set of hard constraints. Graph Generation as Markov Decision ProcessA key task for building our model is to specify a generation procedure. We designed an iterative graph generation process and formulated it as a general decision process M = (S, A, P, R, \u03b3), where S = {s i } is the set of states that consists of all possible intermediate and final graphs, A = {a i } is the set of actions that describe the modification made to current graph at each time step, P is the transition dynamics that specifies the possible outcomes of carrying out an action, p(s t+1 |s t , ...s 0 , a t ). R(s t ) is a reward function that specifies the reward after reaching state s t , and \u03b3 is the discount factor. The procedure to generate a graph can then be described by a trajectory (s 0 , a 0 , r 0 , ..., s n , a n , r n ), where s n is the final generated graph. The modification of a graph at each time step can be viewed as a state transition distribution: p(s t+1 |s t , ..., s 0 ) = at p(a t |s t , ...s 0 )p(s t+1 |s t , ...s 0 , a t ), where p(a t |s t , ...s 0 ) is usually represented as a policy network \u03c0 \u03b8 .Recent graph generation models add nodes and edges based on the full trajectory (s t , ..., s 0 ) of the graph generation procedure Molecule Generation EnvironmentIn this section we discuss the setup of molecule generation environment. On a high level, the environment builds up a molecular graph step by step through a sequence of bond or substructure addition actions given by GCPN. Figure State Space. We define the state of the environment s t as the intermediate generated graph G t at time step t, which is fully observable by the RL agent. Figure Action Space. In our framework, we define a distinct, fixed-dimension and homogeneous action space amenable to reinforcement learning. We design an action analogous to link prediction, which is a well studied realm in network science. We first define a set of scaffold subgraphs {C 1 , . . . , C s } to be added during graph generation and the collection is defined as C = s i=1 C i . Given a graph G t at step t, we define the corresponding extended graph as G t \u222a C. Under this definition, an action can either correspond to connecting a new subgraph C i to a node in G t or connecting existing nodes within graph G t . Once an action is taken, the remaining disconnected scaffold subgraphs are removed. In our implementation, we adopt the most fine-grained version where C consists of all b different single node graphs, where b denotes the number of different atom types. Note that C can be extended to contain molecule substructure scaffolds State Transition Dynamics. Domain-specific rules are incorporated in the state transition dynamics. The environment carries out actions that obey the given rules. Infeasible actions proposed by the policy network are rejected and the state remains unchanged. For the task of molecule generation, the environment incorporates rules of chemistry. In Figure Reward design. Both intermediate rewards and final rewards are used to guide the behaviour of the RL agent. We define the final rewards as a sum over domain-specific rewards and adversarial rewards. The domain-specific rewards consist of the (combination of) final property scores, such as octanol-water partition coefficient (logP), druglikeness (QED) To ensure that the generated molecules resemble a given set of molecules, we employ the Generative Adversarial Network (GAN) framework where \u03c0 \u03b8 is the policy network, D \u03c6 is the discriminator network, x represents an input graph, p data is the underlying data distribution which defined either over final graphs (for final rewards) or intermediate graphs (for intermediate rewards). However, only D \u03c6 can be trained with stochastic gradient descent, as x is a graph object that is non-differentiable with respect to parameters \u03c6. Instead, we use \u2212V (\u03c0 \u03b8 , D \u03c6 ) as an additional reward together with other rewards, and optimize the total rewards with policy gradient methods Graph Convolutional Policy NetworkHaving illustrated the graph generation environment, we outline the architecture of GCPN, a policy network learned by the RL agent to act in the environment. GCPN takes the intermediate graph G t and the collection of scaffold subgraphs C as inputs, and outputs the action a t , which predicts a new link to be added, as described in Section 3.3.Computing node embeddings. In order to perform link prediction in G t \u222a C, our model first computes the node embeddings of an input graph using Graph Convolutional Networks (GCN) where E i is the i th slice of edge-conditioned adjacency tensor E, and \u1ebci = E i + I; Di = k \u1ebcijk . W (l) i is a trainable weight matrix for the i th edge type, and H (l) is the node representation learned in the l th layer. We use AGG(\u2022) to denote an aggregation function that could be one of {MEAN, MAX, SUM, CONCAT} Action prediction. The link prediction based action a t at time step t is a concatenation of four components: selection of two nodes, prediction of edge type, and prediction of termination. Concretely, each component is sampled according to a predicted distribution governed by Equation 3 and 4.We use m f to denote a Multilayer Perceptron (MLP) that maps Z 0:n \u2208 R n\u00d7k to a R n vector, which represents the probability distribution of selecting each node. The information from the first selected node a first is incorporated in the selection of the second node by concatenating its embedding Z a first with that of each node in G t \u222a C. The second MLP m s then maps the concatenated embedding to the probability distribution of each potential node to be selected as the second node. Note that when selecting two nodes to predict a link, the first node to select, a first , should always belong to the currently generated graph G t , whereas the second node to select, a second , can be either from G t (forming a cycle), or from C (adding a new substructure). To predict a link, m e takes Z a first and Z a second as inputs and maps to a categorical edge type using an MLP. Finally, the termination probability is computed by firstly aggregating the node embeddings into a graph embedding using an aggregation function AGG, and then mapping the graph embedding to a scalar using an MLP m t .Policy Gradient TrainingPolicy gradient based methods are widely adopted for optimizing policy networks. Here we adopt Proximal Policy Optimization (PPO) where r t (\u03b8) is the probability ratio that is clipped to the range of [1 \u2212 , 1 + ], making the L CLIP (\u03b8) a lower bound of the conservative policy iteration objective It is known that pretraining a policy network with expert policies if they are available leads to better training stability and performance ExperimentsTo demonstrate effectiveness of goal-directed search for molecules with desired properties, we compare our method with state-of-the-art molecule generation algorithms in the following tasks.Property Optimization. The task is to generate novel molecules whose specified molecular properties are optimized. This can be useful in many applications such as drug discovery and materials science, where the goal is to identify molecules with highly optimized properties of interest.Property Targeting. The task is to generate novel molecules whose specified molecular properties are as close to the target scores as possible. This is crucial in generating virtual libraries of molecules with properties that are generally suitable for a desired application. For example, a virtual molecule library for drug discovery should have high drug-likeness and synthesizability.Constrained Property Optimization. The task is to generate novel molecules whose specified molecular properties are optimized, while also containing a specified molecular substructure. This can be useful in lead optimization problems in drug discovery and materials science, where we want to make modifications to a promising lead molecule and improve its properties Experimental SetupWe outline our experimental setup in this section. Further details are provided in the appendix 1 .Dataset. For the molecule generation experiments, we utilize the ZINC250k molecule dataset Molecule environment. We set up the molecule environment as an OpenAI Gym environment Baselines. We compare our method with the following state-of-the-art baselines. Junction Tree VAE (JT-VAE) Molecule Generation ResultsProperty optimization. In this task, we focus on generating molecules with the highest possible penalized logP Compared with ORGAN, our model can achieve a perfect validity ratio due to the molecular graph representation that allows for step-wise chemical valency check. Compared to JT-VAE, our model can reach a much higher score owing to the fact that RL allows for direct optimization of a given property score and is able to easily extrapolate beyond the given dataset. Visualizations of generated molecules with optimized logP and QED scores are displayed in Figure Although most generated molecules are realistic, in some very rare cases, especially where we reduce the of the adversarial reward and expert pretraining components, our method can generate undesirable molecules with astonishingly high penalized logP predicted by the empirical model, such as the one on the bottom-right of Figure Property Targeting. In this task, we specify a target range for molecular weight (MW) and logP, and report the percentage of generated molecules with property scores within the range, as well as the diversity of generated molecules. The diversity of a set of molecules is defined as the average pairwise Tanimoto distance between the Morgan fingerprints Constrained Property Optimization. In this experiment, we optimize the penalized logP while constraining the generated molecules to contain one of the 800 ZINC molecules with low penalized logP, following the evaluation in JT-VAE. Since JT-VAE cannot constrain the generated molecule to have certain structure, we adopt their evaluation method where the constraint is relaxed such that the molecule similarity sim(G, G ) between the original and modified molecules is above a threshold \u03b4.We train a fixed GCPN in an environment whose initial state is randomly set to be one of the 800 ZINC molecules, then conduct the same training procedure as the property optimization task. Over the 800 molecules, the mean and standard deviation of the highest property score improvement and the corresponding similarity between the original and modified molecules are reported in Table Our model significantly outperforms JT-VAE with 184% higher penalized logP improvement on average, and consistently succeeds in discovering molecules with higher logP scores. Also note that JT-VAE performs optimization steps for each given molecule constraint. In contrast, GCPN can generalize well: it learns a general policy to improve property scores, and applies the same policy to all 800 molecules. Figure Conclusion", "conclusions": "We introduced GCPN, a graph generation policy network using graph state representation and adversarial training, and applied it to the task of goal-directed molecular graph generation. GCPN consistently outperforms other state-of-the-art approaches in the tasks of molecular property optimization and targeting, and at the same time, maintains 100% validity and resemblance to realistic molecules. Furthermore, the application of GCPN can extend well beyond molecule generation. The algorithm can be applied to generate graphs in many contexts, such as electric circuits, social networks, and explore graphs that can optimize certain domain specific properties.", "SDG": [3]}, "hidden_markov_models_based_on_symbolic_dynamics_for_statistical_modeling_of_cardiovascular_control_in_hypertensive_pregnancy_disorders": {"name": "Hidden Markov Models Based on Symbolic Dynamics for Statistical Modeling of Cardiovascular Control in Hypertensive Pregnancy Disorders", "abstract": " Discrete hidden Markov models (HMMs) were applied to classify pregnancy disorders. The observation sequence was generated by transforming RR and systolic blood pressure time series using symbolic dynamics. Time series were recorded from 15 women with pregnancy-induced hypertension, 34 with preeclampsia and 41 controls beyond 30th gestational week. HMMs with five to ten hidden states were found to be sufficient to characterize different blood pressure variability, whereas significant classification in RR-based HMMs was found using fifteen hidden states. Pregnancy disorders preeclampsia and pregnancy induced hypertension revealed different patho-physiological autonomous regulation supposing different etiology of both disorders.", "keywords": "Blood pressure variability,cardiovascular control,heart rate variability,hidden Markov model,preeclampsia,pregnancy induced hypertension", "introduction": "Hypertensive pregnancy disorders are a leading cause of fetal and maternal mortality . The etiology of this maladaptation of the cardiovascular system to pregnancy is unknown, but two mechanisms, the immune maladaptation and the genetic imprinting have been discussed [1], [2]. A new report of the Working Group on High Blood Pressure in Pregnancy classified the relevant hypertensive pregnancy disorders in: chronic hypertension, pregnancy-induced hypertension (PIH), and preeclampsia (PE), whereby it is discussed whether preeclampsia is pregnancy-induced hypertension plus proteinuria or is characterized by its own etiology [3].[4]Heart rate and blood pressure variability (HRV and BPV) are generated by the rhythmic actions of cardiovascular hormones and neuronal pathways on effector organs such as the heart, kidneys, and vessels  and are independent predictors for sudden cardiac death after acute myocardial infarction or chronic heart failure [5], [6]. Thus, they also might reflect different cardiovascular patho-physiologies in pregnancy, like PIH and PE. Several studies investigated HRV and BPV in normal pregnancy compared to nonpregnant women and women with PE [7], [8]. Since those studies were performed with parametric analyzes of HRV and BPV measures, we propose a more complex approach here, based on the modeling of the cardiovascular system with hidden Markov models (HMMs).[9]The theory of HMMs was introduced in the late 1960s by Baum and colleagues . They are statistical models where it is assumed that the system being modeled generates a Markov process, i.e., a stochastic process with the conditional probability distribution of future states depending only on current state. The observation is a probabilistic function of the state that is not observable (hidden), but can be observed through another set of stochastic processes that produce the sequence of observations (e.g., RR and blood pressure time series). This type of statistical modeling was applied to biomedical data, especially in speech recognition [10] or bioinformatics [11], but only a few applications are published for modeling the hidden dynamics of the cardiovascular system [12]- [13].[15]Assuming the mentioned pregnancy disorders cause a change in the dynamics of this biological system and consequently the physically measured observation sequences, the statistical (linear and nonlinear) properties have to be different. Therefore, we hypothesize that the mathematical structure of HMMs can be used to describe patho-physiological pregnancies and may be helpful in the discussion about a different etiology of PE compared with PIH.", "body": "Hypertensive pregnancy disorders are a leading cause of fetal and maternal mortality Heart rate and blood pressure variability (HRV and BPV) are generated by the rhythmic actions of cardiovascular hormones and neuronal pathways on effector organs such as the heart, kidneys, and vessels The theory of HMMs was introduced in the late 1960s by Baum and colleagues Assuming the mentioned pregnancy disorders cause a change in the dynamics of this biological system and consequently the physically measured observation sequences, the statistical (linear and nonlinear) properties have to be different. Therefore, we hypothesize that the mathematical structure of HMMs can be used to describe patho-physiological pregnancies and may be helpful in the discussion about a different etiology of PE compared with PIH.II. METHODSA. PatientsWe recruited 15 women with pregnancy-induced hypertension (PIH), 34 women with preeclampsia (PE) and 41 pregnant controls at the Department of Obstetrics and Gynecology, University of Leipzig, between June 2000 and December 2002. All diagnoses at admission were confirmed 6 weeks after delivery. The classification of the hypertensive disorders is according to the National High Blood Pressure Education Program Working Group on High Blood Pressure in Pregnancy B. Data Acquisition and PreprocessingContinuous blood pressure was recorded noninvasively via finger cuff (100 Hz, Portapres model 2). All measurements were performed over 30 minutes under standardized resting conditions between 8 AM and 12 AM as described before C. Symbolic DynamicsHRV and BPV analysis using symbolic dynamics has proven to be a powerful tool to assess cardiovascular control (1) Each time series in X is transformed in the symbol sequence Z (2) defined as Z = fzngn=0;1;...;s 2 0; 1; 2; 3 (2) using the following transformations:with as mean of the time series and threshold values a BBI = 0.5 and a SBP = 0.2. The choice of the threshold values is based on our experimental findings in previous studies. Small changes in the threshold values do not influence the results considerably Subsequently, Z is subdivided into short sequences of words O with a length of three symbols to assess cardiovascular short-term regulations. The length of words is limited due to the requirement of a statistically sufficient representation of each single word type.D. Statistical Modeling of Cardiovascular Control in PregnanciesThe word sequences represent main information of the two physically measured time series as observations of a system with an unknown number of regulatory sources. The theoretical description of such a system can be achieved by statistical modeling of the word sequences. For this purpose discrete HMMs of ergodic topology were developed for both pregnancy disorders PIH, PE and controls, respectively. In this type of HMM every state of the model can be reach in a single step from every other state of the model. The word sequences are given as observation sequences O = fO 1 ; O 2 ; 11 1;O T g of the cardiovascular system, where T is the number of words (observations).The main characteristics of the HMMs are as follows. The probability distribution of state transitions A = faijg where aij = P [qt+1 = Sjjqt = Si]; 1 i; j N:The observation (emission) symbol probability distribution B = fb j (k)g, in state j where b j (k) = P [V k at tjq t = S j ]; 1 j N; 1 k M: Thus, based on the different number of hidden states N = f5; 10; 15g and different time series X = fBBI; SBPg the statistical modeling resulted in six models for both disorders. For convenience, the compact notation (X; N) is used. Subsequently, each of the 62 observed word sequences (15 PIH and 34 PE) were classified into one of the two disorder models by 1) computing the probability of the observation sequence given the model P (Oj(X; N)), and 2) selecting the model with the highest probability.E. Statistical Modeling of Gaussian White NoiseTo evaluate the Markov models of cardiovascular control Gaussian white noise processes were simulated. To meet the 30 minutes recording length of the original data, ten realizations were generated each consisting of 2000 values. Subsequently, symbol sequences were computed as described above and used to 1) develop a separate Markov model (Gauss; N) for Gaussian white noise process, and 2) test the models of cardiovascular control developed with measured real data.III. RESULTSThe results of the classification procedure are presented as fourfold tables (Table As can be seen from Table I models (SBP; 5) based on the transformed SBP time series with five hidden states revealed a significant classification (p = 0.01; rejection of the null hypothesis). Here, 62% of all PIH and 80% of all PE were classified in the correct model. Classification was also significant for models (SBP; 10), at which 91% PIH but only 47% PE were correctly identified. In the models with a higher number of hidden states (SBP; 15) the differentiation between the models becomes worse and, therefore, all pregnancies are assigned to one model. The statistical modeling based on BBI reveals no appropriate HMMs using five or ten hidden states, respectively, ((BBI; 5) (BBI; 10)). Solely model (BBI; 15) was able to characterize differences in the regulatory systems of heart rate, because 77% PE and 60% PIH could be identified (tables are not presented for BBI-based models). The HMM best distinguishing between normal pregnancies and hypertensive pregnancy disorders was a model based on beat-to-beat interval time series with twelve hidden states. With this settings 57% pregnancy disorders and 74% normal pregnancies were correctly identified.The proposed model (SBP; 5) was also tested using surrogate data of PIH and PE, i.e., with the same power spectral densities of the original ones but with completely destroyed phase patterns. A significant difference in the probability of surrogate data from PE being classified into the one or the other model exists no longer, whereas the classification of surrogate data from PIH remains the same as with the original data. Therefore, it is suggested that nonlinear properties contribute to the differentiation between the pregnancy disorders.The HMMs modeled with the Gaussian white noise processes differed significantly from those of both pregnancy disorders. The ten simulated time series could be fully assigned to the respective models ((Gauss; 5) (Gauss; 10) and (Gauss; 15)). Neither were the Gaussian sequences classified in one of the two pregnancy disorder models, nor were a single word sequence from the pregnancy disorder patients assigned to the Gauss model.IV. DISCUSSION", "conclusions": "In order to evaluate the cardiovascular control in women with PIH in comparison to that of women with PE, we investigated HMMs of heart rate and blood pressure time series. Ergodic HMMs of transformed SBP time series with five to ten hidden states were found to be sufficient to characterize the different blood pressure variability of PE and PIH patients. The significant classification of these patients into different HMMs for blood pressure variability suggests a differently altered cardiovascular control, and therefore, a different patho-physiology. Models based on the transformation of BBI revealed sufficient classification using a higher number of hidden states, which seems to point at more complex variations in heart rate than in blood pressure signals. This agrees with the reported findings of different complexity between both time series in normal subjects  as well as in heart failure [30]. The tests with the models of Gaussian white noise processes proved that the physiological models of the pregnancy disorders have a deterministic structure, which is preserved through the transformation process (symbol coding).[31]Earlier analyzes by our group found a significantly increased peripheral blood pressure pulse  but no differences in HRV and BPV parameters except of mean systolic blood pressure that was increased in PE [32]. It could be shown that the vascular system in PE is altered due to the increased release of vasoconstrictive substances and an insensitivity to vasodilative hormones, leading to a smaller blood volume and increased peripheral resistance [33]. Since the HRV-based models of PIH and PE also resulted in a significantly different classification (higher number of hidden states), we suggest that the different pathophysiology mainly affects the vascular regulation (i.e., blood pressure control) and in consequence heart rate control. The classification results discriminating CON and combination of PIH + PE are less impressive but still significant. However, this phenomenon is in congruence with our findings [34]showing that HRV differs significantly between PIH and CON but not between PE and CON. Therefore, the global differentiation between CON and hypertensive disorders in pregnancy is of lower interest because of the different patho-physiological, regulatory, and compensatory mechanisms in PE and PIH.[34]Our approach is based on the assumption that BBI and SBP time series are quasi-stationary ergodic processes (ensured through measurement under standardized resting conditions) that can be used for statistical modeling using ergodic HMMs. We used words of three successive symbols to characterize the short-term regulation in the cardiovascular system, which results in 64 possible words (output symbols). For this approach the data length of about 2000 sample points is assumed to be sufficient for modeling purposes. Nevertheless, several regulatory mechanism, e.g., vasomotoric activity, last over a longer period of heart cycles. Therefore, further work requires either a longer period of data recording or a different approach for symbol coding. However, it might be problematic to increase the recording length because it is often not reasonable for pregnant woman beyond 30th gestational week to be measured for more then 30 min under resting conditions. It would also be conceivable to use a different type of HMM topology e.g., a left-right model in which the state index increases with time. But this should perform better if one is interested in modeling different stages of a disease.In conclusion, the pregnancy disorders PE and PIH seem to have a different pathophysiological blood pressure regulation that can be characterized by HMMs of BPV based on symbolic dynamics. Hence, the etiology of both disorders is assumed to be dissimilar.", "SDG": [3]}, "influence_maximization_for_social_network_based_substance_abuse_prevention": {"name": "Influence Maximization for Social Network Based Substance Abuse Prevention", "abstract": " Substance use and abuse is a significant public health problem in the United States. Group-based intervention programs offer a promising means of reducing substance abuse. While effective, inappropriate intervention groups can result in an increase in deviant behaviors among participants, a process known as deviancy training. In this paper, we present GUIDE, an AI-based decision aid that leverages social network information to optimize the structure of the intervention groups.", "keywords": "", "introduction": "Substance use and abuse is a significant public health problem among youth in the United States. According to the Monitoring the Future study , around 54 percent of high school students have tried at least one illicit substance. Interventions programs have successfully utilized social networks to disseminate and reinforce positive behavioral norms (e.g., (McCabe et al. 2014)). This is achieved through formation of subgroups where the individuals can talk, share experiences and engage in various constructive activities, and this way they form new social ties or abandon some of their existing relationships. Unfortunately, these social network-based efforts may also inadvertently increase the chances of exposure to negative social influence, as the social network of the youth changes. This is known as deviancy training and has been a problematic issue in these prevention programs. From an AI perspective, this problem can be viewed as a social network partitioning problem with the objective of maximizing positive influence and minimizing negative influence. However, to best of our knowledge no work has addressed such influence-based partitioning of networks with changing structures. To address this challenge, we propose an AI-based decision aid, called GUIDE (GroUp-based Intervention DEcision aid). GUIDE assists interventionists in substance abuse prevention, using a model for the group-based interventions that enables predicting, both the expected success of the intervention, and the possibility of harm, or deviancy training. We show that finding the optimal network partition is NP-hard and we use both a Mixed Integer Program (MIP) and a greedy-based  (Valente et al. 2003): Changes in tie strength post-intervention. The existing relationships, and the behavior of the individuals as well as their assignment to groups impacts the changes.1local search method that enables us to optimize for the network partitions.Tie Formation and Breakage. As a result of the interventions, the strength of the relationships is subject to change . For example, there is empirical evidence to suggest that the more similar two individuals are, the stronger their ties are (Centola and Macy 2007). Also, if two individuals are separated and at least one of them has \"user\" behavior, the intervention message will be to cut or weaken that tie. Therefore, based on behavioral theories, and observations in the previous interventions, we propose a model to explain how the network evolves during the course of the intervention which is summarized in Table (Aral and Walker 2014). In this table, the row labels are the behavior of the nodes, the column labels show their pre-intervention tie and the entries indicate the post-intervention tie.1Substance Abuse Prevention Influence Spread Model. Depending on how the network evolves, we evaluate the influence to predict the changes in the nodes' behaviors. We use a variant of the popular Linear Threshold model proposed in . Base on our model, each node selects a threshold value, uniformly at random, to represent his/her threshold to change behavior. If the incoming signal from the opposite behavior exceeds this threshold, the change happens with a fixed probability. (Borodin, Filmus, and Oren 2010)", "body": "Substance use and abuse is a significant public health problem among youth in the United States. According to the Monitoring the Future study local search method that enables us to optimize for the network partitions.Tie Formation and Breakage. As a result of the interventions, the strength of the relationships is subject to change Substance Abuse Prevention Influence Spread Model. Depending on how the network evolves, we evaluate the influence to predict the changes in the nodes' behaviors. We use a variant of the popular Linear Threshold model proposed in Mixed-Integer Programming FormulationWe present a Mixed Integer Linear Optimization (MIP) formulation for this problem and we use Gurobi solver to find the optimal partitioning.Local Neighborhood Search (LNS)We also use local search methods to optimize for the network partitions for scalability. LNS starts from a random feasible graph partition, and it improves the solution by searching in a space of candidate solutions. In this work, we restrict the search neighborhood to that created by random single swap of pairs of nodes. The search continues until no single swap can further improve the solution.Results and DiscussionBaselines. For evaluation, we compare three variations of our optimization approach (MIP, LNS and MIP+LNS which is MIP using LNS solution as warm-start) against three different baselines that either randomly assign the individuals, or let them decide based on their friendships, or finally a teacher nominated baseline, which uses a heuristic to divide up the participants. One the common heuristics is the even distribution of the \"users\" across groups. Solution Quality Metrics. Different solution strategies are compared based on a success metric, which we define as:The numerator is in fact the expected number of youth that have become \"non-users\" as the result of the intervention. The denominator is its maximum possible value which corresponds to the case where all \"users\" threshold are exceeded (they are surrounded by \"enough\" \"non-user\" friends). Solution Quality. Figures Conclusion", "conclusions": "Substance abuse is a very significant public health and social problem in the United States. We showed that by careful construction of the intervention groups, we can outperform the traditional strategies significantly. GUIDE is developed in collaboration with Urban Peak, a homeless-youth serving organization in Denver, CO, and is under preparation for deployment.", "SDG": [3]}, "junction_tree_variational_autoencoder_for_molecular_graph_generation": {"name": "Junction Tree Variational Autoencoder for Molecular Graph Generation", "abstract": " We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.", "keywords": "", "introduction": "The key challenge of drug discovery is to find target molecules with desired chemical properties. Currently, this task takes years of development and exploration by expert chemists and pharmacologists. Our ultimate goal is to automate this process. From a computational perspective, we decompose the challenge into two complementary subtasks: learning to represent molecules in a continuous manner that facilitates the prediction and optimization of their properties (encoding); and learning to map an optimized continuous representation back into a molecular graph with improved properties (decoding). While deep learning has been extensively investigated for molecular graph encoding (Duvenaud et al., 2015;Kearnes et al., 2016;, the harder combinatorial task of molecular graph generation from latent representation remains under-explored.  Prior work on drug design formulated the graph generation task as a string generation problem Gilmer et al., 2017)(G\u00f3mez-Bombarelli et al., 2016; in an attempt to side-step direct generation of graphs. Specifically, these models start by generating SMILES Kusner et al., 2017), a linear string notation used in chemistry to describe molecular structures. SMILES strings can be translated into graphs via deterministic mappings (e.g., using RDKit (Weininger, 1988)). However, this design has two critical limitations. First, the SMILES representation is not designed to capture molecular similarity. For instance, two molecules with similar chemical structures may be encoded into markedly different SMILES strings (e.g., Figure (Landrum, 2006)). This prevents generative models like variational autoencoders from learning smooth molecular embeddings. Second, essential chemical properties such as molecule validity are easier to express on graphs rather than linear SMILES representations. We hypothesize that operating directly on graphs improves generative modeling of valid chemical structures.1Our primary contribution is a new generative model of molecular graphs. While one could imagine solving the problem in a standard manner -generating graphs node by node -the approach is not ideal for molecules. This is because creating molecules atom by atom would force the model to generate chemically invalid intermediaries (see, e.g., Figure ), delaying validation until a complete graph is generated. Instead, we propose to generate molecular graphs in two phases by exploiting valid subgraphs as components. The overall generative approach, cast as a junction tree variational autoencoder, first generates a tree structured object (a junction tree) whose role is to represent the scaffold of subgraph components and their coarse relative arrangements. The components are valid chemical substructures automatically extracted from the training set using tree decomposition and are used as building blocks. In the sec- ond phase, the subgraphs (nodes in the tree) are assembled together into a coherent molecular graph.2We evaluate our model on multiple tasks ranging from molecular generation to optimization of a given molecule according to desired properties. As baselines, we utilize state-of-the-art SMILES-based generation approaches (Kusner et al., 2017;. We demonstrate that our model produces 100% valid molecules when sampled from a prior distribution, outperforming the top performing baseline by a significant margin. In addition, we show that our model excels in discovering molecules with desired properties, yielding a 30% relative gain over the baselines.Dai et al., 2018)", "body": "The key challenge of drug discovery is to find target molecules with desired chemical properties. Currently, this task takes years of development and exploration by expert chemists and pharmacologists. Our ultimate goal is to automate this process. From a computational perspective, we decompose the challenge into two complementary subtasks: learning to represent molecules in a continuous manner that facilitates the prediction and optimization of their properties (encoding); and learning to map an optimized continuous representation back into a molecular graph with improved properties (decoding). While deep learning has been extensively investigated for molecular graph encoding Our primary contribution is a new generative model of molecular graphs. While one could imagine solving the problem in a standard manner -generating graphs node by node -the approach is not ideal for molecules. This is because creating molecules atom by atom would force the model to generate chemically invalid intermediaries (see, e.g., Figure We evaluate our model on multiple tasks ranging from molecular generation to optimization of a given molecule according to desired properties. As baselines, we utilize state-of-the-art SMILES-based generation approaches Junction Tree Variational AutoencoderOur approach extends the variational autoencoder Our vocabulary of components, such as rings, bonds and individual atoms, is chosen to be large enough so that a given molecule can be covered by overlapping components or clusters of atoms. The clusters serve the role analogous to cliques in graphical models, as they are expressive enough that a molecule can be covered by overlapping clusters without forming cluster cycles. In this sense, the clusters serve as cliques in a (non-optimal) triangulation of the molecular graph. We form a junction tree of such clusters and use it as the tree representation of the molecule. Since our choice of cliques is constrained a priori, we cannot guarantee that a junction tree exists with such clusters for an arbitrary Figure To decode the molecule, we first reconstruct junction tree from zT , and then assemble nodes in the tree back to the original molecule. molecule. However, our clusters are built on the basis of the molecules in the training set to ensure that a corresponding junction tree can be found. Empirically, our clusters cover most of the molecules in the test set.The original molecular graph and its associated junction tree offer two complementary representations of a molecule. We therefore encode the molecule into a two-part latent representation z = [z T , z G ] where z T encodes the tree structure and what the clusters are in the tree without fully capturing how exactly the clusters are mutually connected. z G encodes the graph to capture the fine-grained connectivity. Both parts are created by tree and graph encoders q(z T |T ) and q(z G |G). The latent representation is then decoded back into a molecular graph in two stages. As illustrated in Figure Notation A molecular graph is defined as G = (V, E) where V is the set of atoms (vertices) and E the set of bonds (edges). Let N (x) be the neighbor of x. We denote sigmoid function as \u03c3(\u2022) and ReLU function as \u03c4 (\u2022). We use i, j, k for nodes in the tree and u, v, w for nodes in the graph.Junction TreeA tree decomposition maps a graph G into a junction tree by contracting certain vertices into a single node so that G becomes cycle-free. Formally, given a graph G, a junction tree T G = (V, E, X ) is a connected labeled tree whose node set issatisfying the following constraints:1. The union of all clusters equals G. That is, i V i = Vand i E i = E.2. Running intersection: For all clusters C i , C j and C k ,Viewing induced subgraphs as cluster labels, junction trees are labeled trees with label vocabulary X . By our molecule tree decomposition, X contains only cycles (rings) and single edges. Thus the vocabulary size is limited (|X | = 780 for a standard dataset with 250K molecules).Tree Decomposition of Molecules Here we present our tree decomposition algorithm tailored for molecules, which finds its root in chemistry Graph EncoderWe first encode the latent representation of G by a graph message passing network where \u03bduv is the message computed in t-th iteration, initialized with \u03bd (0) uv = 0. After T steps of iteration, we aggregate those messages as the latent vector of each vertex, which captures its local graphical structure:The final graph representation is hThe mean \u00b5 G and log variance log \u03c3 G of the variational posterior approximation are computed from h G with two separate affine layers. z G is sampled from a Gaussian N (\u00b5 G , \u03c3 G ).Tree EncoderWe similarly encode T G with a tree message passing network. Each cluster C i is represented by a one-hot encoding x i representing its label type. Each edge (C i , C j ) is associated with two message vectors m ij and m ji . We pick an arbitrary leaf node as the root and propagate messages in two phases. In the first bottom-up phase, messages are initiated from the leaf nodes and propagated iteratively towards root. In the top-down phase, messages are propagated from the root to all the leaf nodes. Message m ij is updated as:where GRU is a Gated Recurrent Unit The message passing follows the schedule where m ij is computed only when all its precursors {m ki | k \u2208 N (i)\\j} have been computed. This architectural design is motivated by the belief propagation algorithm over trees and is thus different from the graph encoder.After the message passing, we obtain the latent representation of each node h i by aggregating its inward messages:The final tree representation is h T G = h root , which encodes a rooted tree (T , root). Unlike the graph encoder, we do not apply node average pooling because it confuses the tree decoder which node to generate first. z T G is sampled in a similar way as in the graph encoder. For simplicity, we abbreviate z T G as z T from now on.This tree encoder plays two roles in our framework. First, it is used to compute z T , which only requires the bottom-up phase of the network. Second, after a tree T is decoded from z T , it is used to compute messages m ij over the entire T , to provide essential contexts of every node during graph decoding. This requires both top-down and bottom-up phases. We will elaborate this in section 2.5.Tree DecoderWe decode a junction tree T from its encoding z T with a tree structured decoder. The tree is constructed in a top-down fashion by generating one node at a time. As illustrated in Figure When a new child node is created, we predict its label and recurse this process. Recall that cluster labels represent subgraphs in a molecule. The decoder backtracks when a node has no more children to generate.At each time step, a node receives information from other nodes in the current tree for making those predictions. The information is propagated through message vectors h ij when trees are incrementally constructed. Formally, let, where m = 2|E| as each edge is traversed in both directions. The model visits node i t at time t. Let \u1ebct be the first t edges in \u1ebc. The message h it,jt is updated through previous messages:where GRU is the same recurrent unit as in the tree encoder.Topological Prediction When the model visits node i t , it makes a binary prediction on whether it still has children to be generated. We compute this probability by combining Set X i \u2190 all cluster labels that are chemically compatible with node i and its current neighbors.4:Set d t \u2190 expand with probability p t . Eq.( Create a node j and add it to tree T .7:Sample the label of node j from X i . Eq.( 8:SampleTree(j, t + 1)end if 10: end function z T , node features x it and inward messages h k,it via a one hidden layer network followed by a sigmoid function:Label Prediction When a child node j is generated from its parent i, we predict its node label withwhere q j is a distribution over label vocabulary X . When j is a root node, its parent i is a virtual node and h ij = 0.Learning The tree decoder aims to maximize the likelihood p(T |z T ). Let pt \u2208 {0, 1} and qj be the ground truth topological and label values, the decoder minimizes the following cross entropy loss: 1Similar to sequence generation, during training we perform teacher forcing: after topological and label prediction at each step, we replace them with their ground truth so that the model makes predictions given correct histories.Decoding & Feasibility Check Algorithm 1 shows how a tree is sampled from z T . The tree is constructed recursively guided by topological predictions without any external guidance used in training. To ensure the sampled tree could be realized into a valid molecule, we define set X i to be cluster labels that are chemically compatible with node i and its current neighbors. When a child node j is generated from node i, we sample its label from X i with a renormalized distribution q j over X i by masking out invalid labels.Graph DecoderThe final step of our model is to reproduce a molecular graph G that underlies the predicted junction tree T = ( V, E). Note that this step is not deterministic since there are potentially many molecules that correspond to the same junction tree. The underlying degree of freedom pertains to how neighboring clusters C i and C j are attached to each other as subgraphs. Our goal here is to assemble the subgraphs (nodes in the tree) together into the correct molecular graph.Let G(T ) be the set of graphs whose junction tree is T . Decoding graph \u011c from T = ( V, E) is a structured prediction:where f a is a scoring function over candidate graphs. We only consider scoring functions that decompose across the clusters and their neighbors. In other words, each term in the scoring function depends only on how a cluster C i is attached to its neighboring clusters C j , j \u2208 N T (i) in the tree T . The problem of finding the highest scoring graph \u011cthe assembly task -could be cast as a graphical model inference task in a model induced by the junction tree. However, for efficiency reasons, we will assemble the molecular graph one neighborhood at a time, following the order in which the tree itself was decoded. In other words, we start by sampling the assembly of the root and its neighbors according to their scores. Then we proceed to assemble the neighbors and their associated clusters (removing the degrees of freedom set by the root assembly), and so on.It remains to be specified how each neighborhood realization is scored. Let G i be the subgraph resulting from a particular merging of cluster C i in the tree with its neighbors C j , j \u2208 N T (i). We score G i as a candidate subgraph by first deriving a vector representation h Gi and then using f a i (G i ) = h Gi \u2022 z G as the subgraph score. To this end, let u, v specify atoms in the candidate subgraph G i and letThe indices \u03b1 v are used to mark the position of the atoms in the junction tree, and to retrieve messages m i,j summarizing the subtree under i along the edge (i, j) obtained by running the tree encoding algorithm. The neural messages pertaining to the atoms and bonds in subgraph G i are obtained and aggregated into h Gi , similarly to the encoding step, but with different (learned) parameters:The major difference from Eq. ( Learning The graph decoder parameters are learned to maximize the log-likelihood of predicting correct subgraphs G i of the ground true graph G at each tree node:where G i is the set of possible candidate subgraphs at tree node i. During training, we again apply teacher forcing, i.e. we feed the graph decoder with ground truth trees as input.Complexity By our tree decomposition, any two clusters share at most two atoms, so we only need to merge at most two atoms or one bond. By pruning chemically invalid subgraphs and merging isomorphic graphs, |G i | \u2248 4 on average when tested on a standard ZINC drug dataset. The computational complexity of JT-VAE is therefore linear in the number of clusters, scaling nicely to large graphs.ExperimentsOur evaluation efforts measure various aspects of molecular generation. The first two evaluations follow previously proposed tasks Below we describe the data, baselines and model configuration that are shared across the tasks. Additional setup details are provided in the task-specific sections.Data We use the ZINC molecule dataset from Baselines We compare our approach with SMILES-based baselines: 1) Character VAE (CVAE) Model Configuration To be comparable with the above baselines, we set the latent space dimension as 56, i.e., the tree and graph representation h T and h G have 28 dimensions each. Full training details and model configurations are provided in the appendix.Molecule Reconstruction and ValiditySetup The first task is to reconstruct and sample molecules from latent space. Since both encoding and decoding process are stochastic, we estimate reconstruction accuracy by Monte Carlo method used in To compute validity, we sample 1000 latent vectors from the prior distribution N (0, I), and decode each of these vectors 100 times. We report the percentage of decoded molecules that are chemically valid (checked by RDKit).For ablation study, we also report the validity of our model without validity check in decoding phase.Results Table Analysis We qualitatively examine the latent space of JT-VAE by visualizing the neighborhood of molecules. Given a molecule, we follow the method in Bayesian OptimizationSetup The second task is to produce novel molecules with desired properties. Following For comparison, we report 1) the predictive performance of SGP trained on latent encodings learned by different VAEs, measured by log-likelihood (LL) and root mean square error (RMSE) with 10-fold cross validation.2) The top-3 molecules found by BO under different models.  Results As shown in Table Constrained OptimizationSetup The third task is to perform molecule optimization in a constrained scenario. Given a molecule m, the task is to find a different molecule m that has the highest property value with the molecular similarity sim(m, m ) \u2265 \u03b4 for some threshold \u03b4. We use Tanimoto similarity with Morgan fingerprint A modification succeeds if one of the decoded molecules satisfies the constraint and is distinct from the original.To provide the greatest challenge, we selected 800 molecules with the lowest property score y(\u2022) from the test set. We report the success rate (how often a modification succeeds), and among success cases the average improvement y(m ) \u2212 y(m) and molecular similarity sim(m, m ) between the original and modified molecules m and m . In comparison, our method enforces chemical validity and is more efficient due to the coarse-to-fine generation.Graph-structured Encoders The neural network formulation on graphs was first proposed by Tree-structured Models Our tree encoder is related to recursive neural networks and tree-LSTM On the decoding side, tree generation naturally arises in natural language parsing Conclusion", "conclusions": "In this paper we present a junction tree variational autoencoder for generating molecular graphs. Our method significantly outperforms previous work in molecule generation and optimization. For future work, we attempt to generalize our method for general low-treewidth graphs.", "SDG": [3]}, "machine_learning_applications_in_cancer_prognosis_and_prediction": {"name": "Additionally, there has been considerable activity regarding the inte", "abstract": " Cancer has been characterized as a heterogeneous disease consisting of many different subtypes. The early diag-13 nosis and prognosis of a cancer type have become a necessity in cancer research, as it can facilitate the subsequent 14 clinical management of patients. The importance of classifying cancer patients into high or low risk groups has 15 led many research teams, from the biomedical and the bioinformatics field, to study the application of machine 16 learning (ML) methods. Therefore, these techniques have been utilized as an aim to model the progression and 17 treatment of cancerous conditions. In addition, the ability of ML tools to detect key features from complex 18 datasets reveals their importance. A variety of these techniques, including Artificial Neural Networks (ANNs), 19 Bayesian Networks (BNs), Support Vector Machines (SVMs) and Decision Trees (DTs) have been widely applied 20 in cancer research for the development of predictive models, resulting in effective and accurate decision making. 21 Even though it is evident that the use of ML methods can improve our understanding of cancer progression, an 22 appropriate level of validation is needed in order for these methods to be considered in the everyday clinical prac-23 tice. In this work, we present a review of recent ML approaches employed in the modeling of cancer progression. 24 The predictive models discussed here are based on various supervised ML techniques as well as on different input 25 features and data samples. Given the growing trend on the application of ML methods in cancer research, we 26 present here the most recent publications that employ these techniques as an aim to model cancer risk or patient 27 outcomes.", "keywords": "ML, Machine Learning,ANN, Artificial Neural Network,SVM, Support Vector Machine,DT, Decision Tree,BN, Bayesian Network,SSL, Semi-supervised Learning,TCGA, The Cancer Genome Atlas Research Network,HTT, High-throughput Technologies,OSCC, Oral Squamous Cell Carcinoma,CFS, Correlation based Feature Selection,AUC, Area Under Curve,ROC, Receiver Operating Characteristic,BCRSVM, Breast Cancer Support Vector Machine,PPI, Protein-Protein Interaction,GEO, Gene Expression Omnibus,LCS, Learning Classifying Systems,ES, Early Stopping algorithm,SEER, Surveillance, Epidemiology and End results Database,NSCLC, Non-small Cell Lung Cancer,NCI caArray, National Cancer Institute Array Data Management System", "introduction": "", "body": "Available online xxxx 11 12Cancer has been characterized as a heterogeneous disease consisting of many different subtypes. The early diag-13 nosis and prognosis of a cancer type have become a necessity in cancer research, as it can facilitate the subsequent 14 clinical management of patients. The importance of classifying cancer patients into high or low risk groups has 15 led many research teams, from the biomedical and the bioinformatics field, to study the application of machine 16 learning (ML) methods. Therefore, these techniques have been utilized as an aim to model the progression and 17 treatment of cancerous conditions. In addition, the ability of ML tools to detect key features from complex 18 datasets reveals their importance. A variety of these techniques, including Artificial Neural Networks (ANNs), 19 Bayesian Networks (BNs), Support Vector Machines (SVMs) and Decision Trees (DTs) have been widely applied 20 in cancer research for the development of predictive models, resulting in effective and accurate decision making.21 Even though it is evident that the use of ML methods can improve our understanding of cancer progression, an 22 appropriate level of validation is needed in order for these methods to be considered in the everyday clinical prac-23 tice. In this work, we present a review of recent ML approaches employed in the modeling of cancer progression.24 The predictive models discussed here are based on various supervised ML techniques as well as on different input 25 features and data samples. Given the growing trend on the application of ML methods in cancer research, we 26 present here the most recent publications that employ these techniques as an aim to model cancer risk or patient 27 outcomes.Over the past decades, a continuous evolution related to cancer research has been performed These techniques can discover and identify patterns and relationships between them, from complex datasets, while they are able to effectively predict future outcomes of a cancer type.Given the significance of personalized medicine and the growing trend on the application of ML techniques, we here present a review of studies that make use of these methods regarding the cancer prediction and prognosis. In these studies prognostic and predictive features are considered which may be independent of a certain treatment or are integrated in order to guide therapy for cancer patients, respectively An obvious trend in the proposed works includes the integration of mixed data, such as clinical and genomic. However, a common problem that we noticed in several works is the lack of external validation or testing regarding the predictive performance of their models. It is clear that the application of ML methods could improve the accuracy of cancer susceptibility, recurrence and survival prediction. Based on Several studies have been reported in the literature and are based on different strategies that could enable the early cancer diagnosis and prognosis In the present work only studies that employed ML techniques for modeling cancer diagnosis and prognosis are presented.ML techniquesML, a branch of Artificial Intelligence, relates the problem of learning from data samples to the general concept of inference (ii) unsupervised learning. In supervised learning a labeled set of training data is used to estimate or map the input data to the desired output.In contrast, under the unsupervised learning methods no labeled examples are provided and there is no notion of the output during the and the dangers of information leak recorded in published studies 171 The main objective of ML techniques is to produce a model which to the total number of correct predictions. On the contrary, AUC is a measure of the model's performance which is based on the ROC curve that plots the tradeoffs between sensitivity and 1-specificity (Fig. The predictive accuracy of the model is computed from the testing set which provides an estimation of the generalization errors. In order to obtain reliable results regarding the predicting performance of a model, training and testing samples should be sufficiently large and independent while the labels of the testing sets should be known. Among the most commonly used methods for evaluating the performance of a classifier by splitting the initial labeled data into subsets are: Fig. ML and cancer prediction/prognosisThe last two decades a variety of different ML techniques and feature selection algorithms have been widely applied to disease prognosis and prediction Survey of ML applications in cancerAn extensive search was conducted relevant to the use of ML techniques in cancer susceptibility, recurrence and survivability prediction.Two electronic databases were accessed namely PubMed, Scopus. Due to the vast number of articles returned by the search queries, further scrutinization was needed in order to maintain the most relevant articles. The relevance of each publication was assessed based on the keywords of the three predictive tasks found in their titles and abstracts.Specifically, after reading their titles and abstracts we only selected those publications that study one of the three foci of cancer prediction and included it in their titles. The majority of these studies use different types of input data: genomic, clinical, histological, imaging, demographic, epidemiological data or combination of these. Papers that focus on the prediction of cancer development by means of conventional statistical methods (e.g. chi-square, Cox regression) were excluded as were papers that use techniques for tumor classification or identification of predictive factors. According to Prediction of cancer recurrenceBased on our survey, we here present the most relevant and recent publications that proposed the use of ML techniques for cancer recurrence prediction. A work which studies the recurrence prediction of t1:1    scores with signatures based on immunohistochemistry Among the most common applied ML algorithms relevant to the prediction outcomes of cancer patients, we found that SVM and ANN classifiers were widely used. As mentioned to our introductory section, ANNs have been used extensively for nearly 30 years Concerning the future of cancer modeling new methods should be studied for overcoming the limitations discussed above. A better statistical analysis of the heterogeneous datasets used would provide more accurate results and would give reasoning to disease outcomes. Further research is required based on the construction of more public databases that would collect valid cancer dataset of all patients that have been diagnosed with the disease. Their exploitation by the researchers would facilitate their modeling studies resulting in more valid results and integrated clinical decision making.Conclusions", "conclusions": "In this review, we discussed the concepts of ML while we outlined their application in cancer prediction/prognosis. Most of the studies that have been proposed the last years and focus on the development of predictive models using supervised ML methods and classification algorithms aiming to predict valid disease outcomes. Based on the analysis of their results, it is evident that the integration of multidimensional heterogeneous data, combined with the application of different techniques for feature selection and classification can provide promising tools for inference in the cancer domain. ", "SDG": [3]}, "novel_biomarkers_for_pre_eclampsia_detected_using_metabolomics_and_machine_learning": {"name": "Novel biomarkers for pre-eclampsia detected using metabolomics and machine learning", "abstract": " Pre-eclampsia is a multi-system disorder of pregnancy with major maternal and perinatal implications. Emerging therapeutic strategies are most likely to be maximally effective if commenced weeks or even months prior to the clinical presentation of the disease. Although widespread plasma alterations precede the clinical onset of pre-eclampsia, no single plasma constituent has emerged as a sensitive or specific predictor of risk. Consequently, currently available methods of identifying the condition prior to clinical presentation are of limited clinical use. We have exploited genetic programming, a powerful data mining method, to identify patterns of metabolites that distinguish plasma from patients with pre-eclampsia from that taken from healthy, matched controls. High-resolution gas chromatography time-of-flight mass spectrometry (GC-tof-MS) was performed on 87 plasma samples from women with pre-eclampsia and 87 matched controls. Normalised peak intensity data were fed into the Genetic Programming (GP) system which was set up to produce a model that gave an output of 1 for patients and 0 for controls. The model was trained on 50% of the data generated and tested on a separate hold-out set of 50%. The model generated by GP from the GC-tof-MS data identified a metabolomic pattern that could be used to produce two simple rules that together discriminate pre-eclampsia from normal pregnant controls using just 3 of the metabolite peak variables, with a sensitivity of 100% and a specificity of 98%. Thus, preeclampsia can be diagnosed at the level of small-molecule metabolism in blood plasma. These findings justify a prospective assessment of metabolomic technology as a screening tool for pre-eclampsia, while identification of the metabolites involved may lead to an improved understanding of the aetiological basis of pre-eclampsia and thus the development of targeted therapies.", "keywords": "pre-eclampsia,mass spectrometry,GC-MS,metabolomics,machine,learning,genetic programming,prognosis,diagnosis,classification", "introduction": "Pre-eclampsia is an important cause of maternal morbidity and mortality. The World Health Organization estimates that worldwide over 100,000 women die from pre-eclampsia each year, and the condition has been the most important cause of maternal death in the UK over recent decades (Hibbard and Milner, 1994;. Recent CESDI reports cite 1 in 6 stillbirths and 1 in 6 sudden infant deaths as occurring in pregnancies complicated by maternal hypertension, and the condition is responsible for the occupancy of approximately 20% of special care baby unit cots Lewis, 2001).(CESDI, 1998)Although the precise aetiology of pre-eclampsia is poorly defined, there is accumulating evidence for a pathogenic model of pre-eclampsia, whereby inappropriate adaptation of the interface between the maternal vasculature and the developing placenta early in pregnancy leads to the development of a poorly perfused feto-placental unit (Pijnenborg et al., 1991;. In this model, continuing poor perfusion of the placenta is proposed to result in the secretion of a factor(s) into the maternal circulation. These factors are thought to cause ''activation'' of the vascular endothelium and the clinical syndrome of pre-eclampsia results from widespread changes in endothelial cell function in both small and large vessels Hayman et al., 1999)(Rodgers et al., 1988;Roberts et al., 1989;. Equivalently, or in addition, one might imagine that plasma normally contains a factor(s) that maintains standard endothelial function but which is absent in pre-eclampsia.Kenny et al., 2002)Thus, it is clear that as pre-eclampsia originates early in pregnancy, potential therapies are most likely to be maximally effective if commenced weeks or even months prior to the clinical presentation of the disease. There are currently several candidate pharmacological therapies under investigation. However, targeted intervention is impractical as currently available tests (such as Doppler ultrasound waveform analysis of the uterine arteries) that seek to identify the condition prior to clinical presentation have low sensitivities, and are thus of limited clinical use.Biomarkers, including surrogate markers, are wellrecognised to be of great value in human disease diagnosis (Lesko and Atkinson, 2001;, and functional studies at the level of gene expression (transcriptomics) and protein translation (proteomics) have recently enjoyed some success in the early detection and diagnosis of cancer and its subtypes Frank and Hargreaves, 2003)(Golub et al., 1999;.Petricoin et al., 2002)Candidate proteins have been investigated as risk determinants for pre-eclampsia, both in isolation and in combination with other markers, but have limited sensitivity and specificity . Preeclampsia is undoubtedly a multisystem disorder, and the manifestations of the disease seem unlikely to be related to a single protein. Consequently, analytical methods devised to detect specific changes miss a wide range of other substances which will be numerous and may be significantly more important as surrogate (or even aetiological) markers. Although the metabolome is certainly ''complementary'' to transcriptomics and proteomics, it might be seen to have special advantages. In particular, it is known from both the theory underlying metabolic control analysis (Hayman et al., 1999) and from experiment (Kell and Mendes, 2000) that, although changes in the quantities of individual enzymes might be expected to have little effect on metabolic fluxes, they can and do have significant effects on the concentrations of numerous individual metabolites. In addition, the metabolome is further down the line from gene to function and so reflects more closely the activities of the cell or organism at a functional level. Thus, as the ''downstream'' result of gene expression, changes in the metabolome are expected to be amplified relative to changes in the transcriptome and the proteome (Raamsdonk et al., 2001). In addition, metabolic fluxes are not regulated by gene expression alone, and metabolites are increasingly recognised as important signalling molecules (Urbanczyk-Wochniak et al., 2003). Given recent successes in disease diagnosis using NMR analyses of the metabolome (e.g., (Shi et al., 2003), it was therefore of interest to enquire as to whether a metabolomics approach Brindle et al., 2002)(Oliver et al., 1998;Harrigan and Goodacre, 2003;Bino et al., 2004;Goodacre et al., 2004;Kell, 2004Kell, , 2005;;van der Greef et al., 2004;Whitfield et al., 2004;Brown et al., 2005;, in which as many metabolites as possible are measured, might permit a distinction between plasma from women with preeclampsia and that from normal pregnant women.Kell et al., 2005)Gas Chromatography-Mass Spectrometry (GC-MS) provides the high resolution separation of metabolites by gas chromatography and sensitive detection by mass spectrometry that is most appropriate for complex bio-logical fluids (e.g., Jellum et al., 1981;Goodacre et al., 2004;Dunn and Ellis, 2005;. The variant used here employs a GC-tof-MS instrument Dunn et al., 2005) optimised using a closed-loop algorithm (Fiehn et al., 2000), allowing the detection, in a non-biased manner, of up to 900 metabolite peaks in some 20 min. This allows metabolites of interest (including disease biomarkers) to be detected and quantified without a priori knowledge of what they are and then to determine which, if any, are significant for the problem of interest (see (O'Hagan et al., 2005). In health-related fields GC-MS has been used for a number of years in a range of applications, including the diagnosis of inborn errors of metabolism Kell and Oliver, 2004).(Rashed, 2001)Fourier transform infrared (FT-IR) spectroscopy involves the observation of molecules that are excited by an infrared beam, resulting in an infrared absorbance spectrum which -as with most NMR approachesrepresents a ''fingerprint'' characteristic of any chemical or biochemical substance , and has been previously used in metabolome profiling (Ellis et al., 2002). Its main advantages are that it is very rapid (taking seconds), reagentless and nondestructive. FT-IR has been applied to a wide-range of biological studies including clinical ones (Harrigan and Goodacre, 2003). Results using FT-IR will be reported elsewhere.(Ellis et al., 2003)Profiles generated from these techniques can contain hundreds or even thousands of data points, necessitating sophisticated analytical tools. We aimed to use these high-dimensional GC-tof-MS data to define an optimum discriminatory metabolomic pattern that would distinguish plasma from women with a known diagnosis of pre-eclampsia from plasma taken from matched controls.", "body": "Pre-eclampsia is an important cause of maternal morbidity and mortality. The World Health Organization estimates that worldwide over 100,000 women die from pre-eclampsia each year, and the condition has been the most important cause of maternal death in the UK over recent decades Although the precise aetiology of pre-eclampsia is poorly defined, there is accumulating evidence for a pathogenic model of pre-eclampsia, whereby inappropriate adaptation of the interface between the maternal vasculature and the developing placenta early in pregnancy leads to the development of a poorly perfused feto-placental unit Thus, it is clear that as pre-eclampsia originates early in pregnancy, potential therapies are most likely to be maximally effective if commenced weeks or even months prior to the clinical presentation of the disease. There are currently several candidate pharmacological therapies under investigation. However, targeted intervention is impractical as currently available tests (such as Doppler ultrasound waveform analysis of the uterine arteries) that seek to identify the condition prior to clinical presentation have low sensitivities, and are thus of limited clinical use.Biomarkers, including surrogate markers, are wellrecognised to be of great value in human disease diagnosis Candidate proteins have been investigated as risk determinants for pre-eclampsia, both in isolation and in combination with other markers, but have limited sensitivity and specificity Gas Chromatography-Mass Spectrometry (GC-MS) provides the high resolution separation of metabolites by gas chromatography and sensitive detection by mass spectrometry that is most appropriate for complex bio-logical fluids (e.g., Fourier transform infrared (FT-IR) spectroscopy involves the observation of molecules that are excited by an infrared beam, resulting in an infrared absorbance spectrum which -as with most NMR approachesrepresents a ''fingerprint'' characteristic of any chemical or biochemical substance Profiles generated from these techniques can contain hundreds or even thousands of data points, necessitating sophisticated analytical tools. We aimed to use these high-dimensional GC-tof-MS data to define an optimum discriminatory metabolomic pattern that would distinguish plasma from women with a known diagnosis of pre-eclampsia from plasma taken from matched controls.MethodsParticipantsPlasma samples were obtained from the GOPEC archive. The GOPEC study was a British Heart Foundation-funded multi-centre collaborative study involving ten University Departments of Obstetrics and Gynaecology in the UK. Within this study, 1000 ''lowrisk'' Caucasian women who developed pre-eclampsia were recruited and sampled between 1999 and 2003. Specifically, women were included if they had a systolic blood pressure \u2021 140 mmHg and diastolic pressure \u202190 mmHg on two occasions after the 20th week of pregnancy and proteinuria >300 mg/L in a 24 h collection, or 500 mg/24 h. Women with chronic hypertension, a history of renal or cardiovascular disease, diabetes mellitus including gestational diabetes, three or more spontaneous abortions, a hydatidiform mole in the index or earlier pregnancy, or a multiple pregnancy, were excluded from the study. Eighty-seven women within the archive who had donated blood to the study antenatally (after diagnosis and within a week prior to delivery) were identified and were matched with 87 normal pregnant controls for maternal age, parity and BMI and for gestational age at sampling. Controls were obtained from antenatal clinics in Manchester and Dundee, and their plasma samples were only retained for this study if they subsequently experienced an uncomplicated pregnancy.Sample collectionBlood samples were taken at the time of recruitment. Samples were collected into pre-cooled glass tubes containing EDTA using the Vacutainer\u00d2 system and immediately centrifuged at 1500 g for 15 min at 4\u00b0C. Plasma was then removed and stored in aliquots at )80\u00b0C until required. The collection and storage conditions were identical for samples taken from both patients and controls.GC-tof-MSSample preparation for GC-MS analysis was performed as follows; 175 ll plasma was spiked with 50 ll internal standard solution (1.53 mg/ml succinic d 4 acid, 2.34 mg/ml malonic d 2 acid, 1.59 mg/ml glycine d 5 , 0.76 mg/ml glucose 13 C 6 ; Sigma-Aldrich, Gillingham, UK) and vortex-mixed for 15 s. Four hundred and fifty micro litres of acetonitrile (AR grade; Sigma-Aldrich, Gillingham, UK) were added followed by vortex mixing (15 s) and centrifugation (13,385 g, 15 min) to deproteinise the samples. The supernatant was transferred to an Eppendorf tube and freeze dried (HETO VR MAXI vacuum centrifuge attached to a HETO CT/DW 60E cooling trap; Thermo Life Sciences, Basingstoke, UK). Two-stage sample chemical derivatisation was performed on the dried sample. 80ll 20 mg/ml O-methylhydroxylamine solution was added and heated at 40\u00b0C for 90 min followed by addition of 80ll MSTFA (Nacetyl-N-[trimethylsilyl]-trifluoroacetamide) and heating at 40\u00b0C for 90 min. Twenty microlitres of a retention index solution (4 mg/ml n-decane, n-dodecane, n-pentadecane, n-nonadecane, n-docosane dissolved in hexane) was added and the samples were analysed using a Agilent 6890 N gas chromatograph and 7683 autosampler (Agilent Technologies, Stockport, UK) coupled to a LECO Pegasus III electron impact time-of-flight mass spectrometer (LECO Corporation, St Joseph, USA). Optimised instrumental conditions for serum have been described elsewhere Machine learning, statistical analyses and visualisationGC-tof-MS data, ratioed as above, were exported together with their class memberships (pre-eclampsia or normal) as an Excel table into the program The-gmax, bio-edition (Predictive Solutions Ltd, Aberystwyth; http://www.predictivesolutions.co.uk/). The program, which encodes rules as trees and evolves them according to the general principles described elsewhere ResultsPatientsA summary of the patient details for the two groups (diseased/control) are detailed in table 1.The pre-eclampsia group had significantly raised mean arterial pressure (p<0.0001), which was taken as the maximum recorded value in the 24 h immediately preceding delivery, and a significantly shorter gestation period than the normal pregnant group. Individual birthweight ratios were calculated for each pregnancy. These are dependent upon maternal ethnicity, height, weight, parity and gestation at delivery, in addition to fetal birthweight and sex Metabolome dataA typical GC-tof-MS trace of the plasma metabolome is shown in figure The main part of the figure shows a typical GC-tof-MS trace for the plasma from one of the diseased patients taken at random. The data were deconvolved using the Chroma-tof software and peaks were extracted into MS-Excel. Application of genetic programming using the program The-gmax suggested that three highly discriminatory variables were labeled peaks 403, 415 and 427, and it was clear from a 3D plot of these (figure A pair of rules that may be derived from inspection of figure since 415 is normally at a noticeably lower concentration than 427. Apart from the samples where 403>0.03 the concentrations of 403 and 415 are more or less collinear, suggesting a possible relationship between them. The concentration of 410 was also related to that of 415 and was only slightly less discriminatory. However, the disjoint nature of the metabolite data, requiring at least two rules to separate the classes, shows (i) that a unitary hypothesis for pre-eclampsia is inappropriate, consistent with its recognition (see above) as a multi-system disorder, and (ii) that machine learning methods (such as genetic programming) are much more suitable than is classical statistics -which tests the goodness of fit to an existing hypothesis Since (see table A number of other features of the data are of interest. First, there is no obvious difference regarding the three discriminating metabolites between the two control populations from Manchester and Dundee, indicating that specific demographic factors are not responsible for these peaks. Secondly, further evidence that the loss of 415 is potentially involved in the development of disease comes from the fact (not shown) that all patients with deliveries at early gestational ages (26-28 weeks), and with higher levels of urinary protein, both indicators of disease severity, had the lowest levels of 415. None of these metabolites was significantly different in patients receiving antihypertensive therapy (and correspondingly none of them represented the metabolic products of antihypertensive drugs).Discussion and conclusions", "conclusions": "Many diseases have an uncertain aetiology, and novel strategies are required to make progress in discovering how they develop. As in functional genomics, where thousands of genes of unknown function were uncovered following the systematic genome sequencing programmes, it is now common to use data-driven expression profiling strategies in which a more specific hypothesis is the result, not the starting point, of the cycle of investigation that links hypotheses with data .(Kell and Oliver, 2004)Pre-eclampsia is such a disease, in which while there are indications that circulating factors (or maybe the lack of them) are of potential aetiological and/or diagnostic importance, we have no knowledge of what these might be. Thus according to one model (Pijnenborg et al., 1991;, continuing poor perfusion of the placenta is proposed to result in the secretion of one or more factors into the maternal circulation. These factors are thought to cause ''activation'' of the vascular endothelium and the clinical syndrome of pre-eclampsia results from widespread changes in endothelial cell function in both large and small vessels Hayman et al., 1999)(Rodgers et al., 1988;Roberts et al., 1989;.Kenny et al., 2002)By using GC-tof-MS we were able to separate and detect several hundred metabolites from both control and diseased plasma samples, and the application of genetic programming to these data indicated that the pre-eclamptic plasma could be discriminated from the matched controls on the basis of just three metabolite peaks (two of which tended to be lower and one tended to be higher in the samples from women with preeclampsia, and to a certain extent this correlated with the severity of the disease). In this context it is worth commenting that the GP type of approach is to be Note: The differences for metabolites 415 and 427 were highly significant (using a Mann-Whitney U test). That for 403 was not, although by inspection and from rule 2 it clearly discriminated a subset of the samples.Figure . Lack of relationship between diastolic blood pressure and biomarker metabolite 427 in the plasma of pre-eclamptic women. In this plot the systolic blood pressure, which is fairly well correlated with the diastolic, is encoded in the size of the symbols.3preferred over other machine learning methods such as neural networks and support vector machines, as it allows one to understand the problem in terms of small subsets of input variables that it combines into rules.In the present case, it has not yet proved possible to identify these molecules chemically, and it is clear that the next stage of a subsequent investigation is to do so. While these substances that we have identified might simply be biomarkers, it is at least possible that they are among the circulating factors that are implicated in disease aetiology. We note that the fact that 415 is lowered (effectively absent) in the disease means that attempts to purify it from pre-eclamptic plasma are likely to prove unrewarding. It could be viewed as a possible protective factor against the development of pre-eclampsia.In the present case, only 10 each of the disease and control samples were taken at a gestational age of under 30 weeks, and a clear task for the future is to establish the extent to which these diagnostic rules apply earlier in pregnancy and thus are of greater prognostic value.In conclusion, however, this is the first study that has identified a small subset of small-MW metabolites that effectively detects pre-eclampsia in human plasma; the potential of such metabolomic strategies in medicine is clearly considerable.", "SDG": [3]}, "performance_evaluation_of_predictive_classifiers_for_pregnancy_care": {"name": "Performance Evaluation of Predictive Classifiers for Pregnancy Care", "abstract": " Hypertensive disorders are the leading cause of deaths during pregnancy. Risk pregnancy accompaniment is essential to reduce these complications. Decision support systems (DSS) are important tools to patients' accompaniment. These systems provide relevant information to health experts about clinical condition of the patient anywhere and anytime. In this paper, a model that uses the Na\u00efve Bayesian classifier is introduced and its performance is evaluated in comparison with the Data Mining (DM) classifier named J48 Decision Tree. This study includes the modeling, performance evaluation, and comparison between models that could be used to assess pregnancy complications. Evaluation analysis of the results is performed through the use of Confusion Matrix indicators. The founded results show that J48 decision tree classifier performs better for almost all the used indicators, confirming its promising accuracy for identifying hypertensive disorders on pregnancy.", "keywords": "e-Health,Hypertension,Decision support systems,Bayes methods,Decision trees,Data mining,Pregnancy", "introduction": "Although many efforts have been performed to reduce deaths during gestation, about more than 800 women die every day in pregnancy complications according to the World Health Organization (WHO). Hypertensive disorders are the most common cause of these complications occurring in about 2-3% of pregnancies. The causes of these diseases in pregnancy have not been well established. Research shows that there is an association with hypertension, which can be chronic or specific pregnancy. This disease is a risk factor for future development of other complications. Even women who had normalization of blood pressure after childbirth in a long term is four times higher the risk of developing chronic hypertension. One way to avoid the aggravation of these problems is the careful and systematic prenatal care during pregnancy.With the growing number of available data in healthcare, the DM techniques could be a very important tool in knowledge extraction of these data helping health experts in making decisions aimed at prevention and health promotion. DM is one of the most promising available technologies for information extraction in huge amount of data. This is due to the fact that companies spent lots of money in data collection and no useful information is identified. Before these techniques, the transformation process from data to information (and, after, into knowledge) was performed through manual processing by experts in order to produce reports for analysis. However, in most situations, due to the large volume of data, this process has become impractical. The knowledge discovery in databases (KDD) is an attempt to address this data overload.Wu et al. discuss the introduction of the Big Data concept and as it is rapidly growing in all science and engineering domains . A theorem that characterizes the Big Data expansion is based on DM perspectives. It involves the following four domains: demand aggregation, mining and analysis, modeling, and security and privacy. The open issues on this data-driven model and the growing topic Big Data are also analyzed. Mukhopadhyay et al. present the main characteristics of each DM technique used to build efficient predictive or descriptive models that use a large amount of data [1]. This paper presents a comprehensive survey addressing the recent developments of multiobjective evolutionary algorithms based on DM techniques, and tries to solve relative problems in a large amount of data. Some concepts related to optimization and DM for Internet of Things (IoT) are also proposed in [2]. It discusses the relationship between IoT and DM giving a brief review considering the features of these two concepts. Kalegele et al. discuss the DM usage for networks and systems managing during last forty years [3]. This research work discusses the perspective of the critical open issues for the effective application of DM in heterogeneous systems. DM presents itself as an efficient technique, responsive, reliable, and able to capture information, which is considered important hidden in large volumes of data.[4]A topic of greatest usage of decision support systems (DSS) is the healthcare, where these systems provide relevant medical information regarding the history/condition of patients. Wang et al. propose a learning framework to perform the mining of longitudinal heterogeneous event data . The effectiveness of the proposed algorithm is validated with a healthcare dataset and shows that this optimization method can learn event 978-1-5090-1328-9/16/$31.00 \u00a92016 IEEE patterns on a group. Tekin et al. propose a Web-based expert system to learn about the most relevant context to assign to each patient using DM [5]. This class of algorithms aims to discover the best clinic and expert given a patient's context. Results show that this proposed algorithm is capable to discover the optimal expert and clinic in a specific context. Yang and Kundakcioglu discuss novel opportunities to extract useful information from diverse and heterogeneous data sets in order to make better decisions in the medical field and improve the performance of systems on healthcare [6]. The Big Data concept is motivating a deep transformation on healthcare giving important opportunities for researchers in order to conduct innovative and transformative research, mainly, in DM techniques and health informatics [7]. This paper proposes a model that uses the Na\u00efve Bayesian classifier to evaluate pregnant disorders. This model is evaluated in comparison with the well-known Decision Tree classifier J48 through a real dataset and the results are very promising.[8]The rest of the paper is organized as follows. Sections II and III address the use of the Na\u00efve Bayes classifier and the J48 Decision Tree classifier. Section IV presents the performance evaluation study and results analysis considering the proposed methods. Finally, Section V provides the conclusion and suggestions for further works.", "body": "Although many efforts have been performed to reduce deaths during gestation, about more than 800 women die every day in pregnancy complications according to the World Health Organization (WHO). Hypertensive disorders are the most common cause of these complications occurring in about 2-3% of pregnancies. The causes of these diseases in pregnancy have not been well established. Research shows that there is an association with hypertension, which can be chronic or specific pregnancy. This disease is a risk factor for future development of other complications. Even women who had normalization of blood pressure after childbirth in a long term is four times higher the risk of developing chronic hypertension. One way to avoid the aggravation of these problems is the careful and systematic prenatal care during pregnancy.With the growing number of available data in healthcare, the DM techniques could be a very important tool in knowledge extraction of these data helping health experts in making decisions aimed at prevention and health promotion. DM is one of the most promising available technologies for information extraction in huge amount of data. This is due to the fact that companies spent lots of money in data collection and no useful information is identified. Before these techniques, the transformation process from data to information (and, after, into knowledge) was performed through manual processing by experts in order to produce reports for analysis. However, in most situations, due to the large volume of data, this process has become impractical. The knowledge discovery in databases (KDD) is an attempt to address this data overload.Wu et al. discuss the introduction of the Big Data concept and as it is rapidly growing in all science and engineering domains A topic of greatest usage of decision support systems (DSS) is the healthcare, where these systems provide relevant medical information regarding the history/condition of patients. Wang et al. propose a learning framework to perform the mining of longitudinal heterogeneous event data The rest of the paper is organized as follows. Sections II and III address the use of the Na\u00efve Bayes classifier and the J48 Decision Tree classifier. Section IV presents the performance evaluation study and results analysis considering the proposed methods. Finally, Section V provides the conclusion and suggestions for further works.II. TREE-BASED CLASSIFIERS ON HEALTHCAREA Decision Tree (DT) is defined as a data structure with leaf nodes that indicate a class or decision nodes, which contains a test on the value of an attribute. Kelarev et al. make a comparison study of several methods based on decision trees and propose a novel application of sensor data processing for diabetes patients The key factor for the large use of J48 algorithm in DM comes from the fact that it proves to be suitable for procedures involving continuous variables (data) and discrete qualitative, presented in several databases. The J48 algorithm is considered the one with the best results in the decision tree based approaches that use a set of training data. On this study, the J48 algorithm is used as it has a great accuracy rate. Algorithm I shows the proposed approach.The information gain of an attribute A is calculated by the equation ( (, where T represents a set of cases and T i (i=1 to s) are subsets of T and comprises distinct values for the attribute A. The term info(T) represents the entropy function described in Equation ( ( For choice of the attribute to experiment the current node, it is used the attribute that presents the highest gain information. This approach minimizes the expected number of necessary experiments to classify an object guaranteeing a simpler tree.III. BAYESIAN CLASSIFIERS ON HEALTHCAREThe Naive Bayes classifier is probably the most widely used classifier in Machine Learning. This classifier assumes that attributes are conditionally independent. Despite this method has been considered a simplistic premise, this classifier reports the best performance in various classification tasks. Shaikh et al. propose an electronic recording system for heart disease prediction that uses this DM modeling technique The Na\u00efve Bayesian algorithm IV. PERFORMANCE EVALUATIONA. Standard metric measurementsThe above-presented classifiers are evaluated with a healthcare dataset that includes discrete and categorical attributes. The dataset includes data gathered from experienced physicians organized in a database. In this research the experiments are executed considering the metrics of precision, recall, and F-measure for assess the classifiers performance. Performance metrics are calculated using a predictive classification table, known as Confusion Matrix. Performance evaluation of the classifiers used in the healthcare dataset is analyzed considering the confusion matrix. It uses the common standard indicators for measuring the performance classification of several models. Precision (Prec.) is the proportion of the predicted relevant data sets that were correct. Recall (Rec.) represents the proportion of data sets that were correctly identified. Finally, the F-measure derives from precision and recall values as shown in Equation ( (3) This indicator is very significant because it only produces a good result when the Precision and Recall are both equilibrated.In this research, a Receiver Operating Characteristic (ROC) analysis was also performed. It uses sensitivity and specific indicators. The ROC analysis is a comparison of two characteristics: True Positive Rate (TPR) and False Positive Rate (FPR). The TPR measures the number of relevant classifications that were correctly identified while the FPR measures the percentage of samples misclassified as positive among all real negative.B. Experimental ResultsThe healthcare dataset used for classification includes 25 cases of hypertension with four attributes. The categorical attributes considered for classification are shown in Table Figure Figure In this paper the Na\u00efve Bayes and the J48 Decision Tree classifiers were used on a real healthcare dataset to identify several hypertensive disorders. Comparison evaluation of these classifiers was performed, in detail, with confusion matrix and using predictive parameters. The performance evaluation of these two classifiers was analyzed from standard metric measures in order to classify the healthcare dataset following the International Statistical Classification of Diseases and Related Health Problems.The classification results analyzed in this study shows that J48 Decision Tree classifier is a more accurate technique than the Na\u00efve Bayes classifier. Although the results of these classifiers are very close, both are used as good predictors to decision-making problems. Therefore, the J48 Decision Tree classifier is proposed in medical field for knowledge discovery from the healthcare datasets to support physicians in problems that need more attention.Further works aim to improve classification accuracy through better and large datasets using other kinds of classifiers. Different types of Bayes-based and Tree-based algorithms will be considered. A study based on Multi-instance learning may also be considered. SVM and K-Neural Networks are also important tools to improve the accuracy of predictive models. Although the diagnosis of hypertensive disorders is complex, many efforts have been carried out in order to improve smart decision support systems that support health experts in uncertain moments (medical diagnosis). ACNOWLEDGEMENTSFig. 1 .Fig. 2 .Fig. 3 .Fig. 4 .", "conclusions": "", "SDG": [3]}, "prediction_of_major_complications_affecting_very_low_birth_weight_infants": {"name": "Prediction of major complications affecting very low birth weight infants", "abstract": " Bronchopulmonary dysplasia (BPD), necrotizing enterocolitis (NEC), and retinopathy of prematurity (ROP) are severe complications affecting Very Low Birth Weight (VLBW) infants. Our findings show that data gathered in the intensive care unit during the first 24 or 72 hours of care can be used to predict whether a VLBW infant is at risk of developing BPD. Using Gaussian process classification, we achieved classification results with areas under the receiver operator characteristic curve of 0.85 (standard error (SE) 0.05) for 24h and 0.87 (SE 0.06) for 72h BPD data. This compares favourably with results achieved using the clinical standard SNAP-II and SNAPPE-II scores. Sensitivity for BPD was 0.52 (SE 0.06). Sensitivity for NEC and ROP was close to zero, suggesting that NEC and ROP can not be reliably predicted with this approach from our data set.", "keywords": "biomedical time series analysis,Gaussian process classification,bronchopulmonary dysplasia,necrotizing enterocolitis,retinopathy of prematurity,neonatal intensive care,very low birth weight infants", "introduction": "Very Low Birth Weight (VLBW) infants are born with a birth weight less than 1500 g. VLBW infants require critical care in a neonatal intensive care unit (NICU) and are at a high risk of developing both acute and later serious health issues. Many of their later problems are thought to originate from early care phases.Three major complications affecting VLBW infants are bronchopulmonary dysplasia (BPD) , necrotizing enterocolitis (NEC) [1], and retinopathy of prematurity (ROP) [2]. The aim of the present study was to develop machine learning tools for early prediction of BPD, NEC, and ROP in VLBW infants using time series data. The contribution of this paper is to show that Gaussian process (GP) classification [3] can be used to predict whether a patient is in danger of developing BPD using time series data from the first 24 or 72 hours after a VLBW infant's admission to the NICU. Prediction of NEC and ROP using data from the same time frame gives results with close to zero sensitivity, which suggests that this approach does not work for these diagnoses.[4]In our previous work, NICU data from initial 24 hours has been used to predict in-hospital mortality  with GP classification. Prior to this, logistic regression [5], [6], [7], [8] and classifiers [9], [9] based on support vector machines (SVM) [10] have been used for BPD. An algorithm has been developed for predicting NEC using proteins in urine [11]. The prediction of sepsis, NEC, and in-hospital mortality using biosignals have also been considered [12], [13], [14].[15]", "body": "Very Low Birth Weight (VLBW) infants are born with a birth weight less than 1500 g. VLBW infants require critical care in a neonatal intensive care unit (NICU) and are at a high risk of developing both acute and later serious health issues. Many of their later problems are thought to originate from early care phases.Three major complications affecting VLBW infants are bronchopulmonary dysplasia (BPD) In our previous work, NICU data from initial 24 hours has been used to predict in-hospital mortality II. BACKGROUNDBPD, NEC, and ROP all manifest from days to weeks postnatally, but the development of all these diseases starts perinatally or during the early postnatal period BPD is a severe chronic pulmonary complication of preterm birth. This most common lung disease in preterm infants can be diagnosed starting from the age of four weeks (28 days) NEC is a critical illness in which segments of intestine undergo necrosis (tissue death). Severe NEC typically manifests during the first several weeks after birth, but its origins are thought be in the early phases of care. It is a life-threatening condition that often requires surgery and increases the risks for long term consequences, such as malnutrition, growth failure, BPD, ROP, and neurodevelopmental problems. Current treatments of NEC are not always effective Over 30% of VLBW infants weighing less than 1250 g develop ROP that can lead to severe vision problems and blindness in one or both eyes. Prematurity, low birth weight, and inappropriate oxygen levels are known risk factors of ROP SNAP-II and SNAPPE-II scores measure neonatal illness severity and predict risk for mortality. SNAP-II represents mortality risk from physiological problems, to which SNAPPE-II adds supplemental risk factors, such as birth weight and growth restriction III. METHODSA. DataOur data set contains data collected from 2059 VLBW infants treated between 1999 and 2013 in the NICU of Helsinki University Hospital's Children's Hospital. There are 416 (\u224820%) patients diagnosed with BPD, 65 (\u22483%) patients diagnosed with NEC, and 153 (\u22487%) patients diagnosed with ROP.For each patient, there are static values (SNAP-II and SNAPPE-II score, birth weight, and gestational age at birth) and time series data (systolic, mean, and diastolic arterial blood pressure, heart rate, and oxygen saturation). Time series data is averaged over 2 minute intervals.B. ClassifierThe patients were classified into two classes (likely/unlikely to get diagnosis) y i \u2208 {\u22121, 1} using GP classification with a probit measurement model (Eq. 1)and a kernel constructed as a linear combination of squared exponential, linear, and constant kernels (Eq. 2). This classifier has been previously used for VLBW infant in-hospital mortality prediction For training the classifier we used the GPstuff Toolbox Gestational age and birth weight were used as static features. In addition, we used time series data for the following five variables: systolic, mean, and diastolic arterial blood pressure, ECG heart rate, and oxygen saturation. These parameters were chosen because of their clinical and scientific importance. In the case of time series data, availability was also a consideration; for the chosen variables, reasonably complete time series data for the 24h and 72h periods being analyzed was available for >1000 patients.The classification results were validated by stratified 5fold crossvalidation which takes the class priors into account when forming the partitions.For comparison purposes, patients were classified using thresholding with SNAP-II and SNAPPE-II scores only. A reference classifier which assumes that no patient will get the diagnosis in question was also used to quantify the classification results.IV. RESULTSWe have compared the classification results using the area (AUC) under the receiver operator characteristic curve (ROC) Best AUC achieved without either gestational age or birth weight was 0.81 (SE 0.05). While markedly worse than the best overall AUC, even this result surpassed that of SNAP-II and SNAPPE-II.B. Necrotizing enterocolitisThe best AUC 0.74 (SE 0.02) was achieved with 72h data using all available features. However, sensitivity was close  0.97 (0.00) 0.40 (0.24) 0.00 (0.00) 0.99 (0.00) 0.72 (0.01) SNAPPE-II 0.98 (0.00) 0.80 (0.20) 0.00 (0.00) 1.00 (0.00) 0.69 (0.04) SNAP-II 0.98 (0.00) 0.80 (0.20) 0.00 (0.00) 1.00 (0.00) 0.68 (0.03) TS 24h 0.97 (0.00) 0.60 (0.24) 0.00 (0.00) 1.00 (0.00) 0.61 (0.01) Reference 0.98 (0.00) 1.00 (0.00) 0.00 (0.00) 1.00 (0.00) - C. Retinopathy of prematurityV. DISCUSSIONThe best predictive classification results were achieved for BPD, with AUC 0.85 (SE 0.05) for 24h and 0.87 (SE 0.06) for 72h data. This was in excess of the results for SNAP-II (AUC 0.70, SE 0.03) and SNAPPE-II (AUC 0.72, SE 0.04), indicating that GP classification using time series data is a better predictor of a patient's likelihood of developing BPD than either of these medical standard scores.ROP classification results were better than SNAP-II and SNAPPE-II thresholding, however, sensitivity (true positive rate) in the best predictions was close to zero. Factors predisposing to ROP may emerge later than the (up to) 3-day period of this study, and so its reliable prediction may require data from a longer time period. It is also quite possible that the variables chosen for this study are not optimal for ROP prediction.In the prediction of NEC, AUC for 24h data (0.72) was marginally better than SNAPPE-II (0.69). 72h data gave better results, with AUC 0.74 (SE 0.02). SNAP-II has not been found to predict later development of NEC A SVM-based approach has been used to achieve an accuracy of 0.832 for BPD prediction In another study There is a direct correlation between prediction accuracy and relative class size. Only 65 out of 2059 were diagnosed with NEC, making the negative class more than 30 times as large as the positive class.Gestational age and birth weight have been found earlier to be important variables in predicting BPD The classifier, as presented, can be used to predict BPD but would have to be revised for NEC and ROP, perhaps by using a sliding time window tracking recent changes in patient state instead of only data from the early stages of care. Feature selection is also an important factor. The data set used in this paper contains time series data for blood pressure, heart rate, and oxygen saturation. Using supplementary oxygen, its effect on oxygen saturation, and the rates of change of both could shed more light on these, especially in the case of ROP.VI. CONCLUSIONS", "conclusions": "Time series data from the initial hours of a VLBW infant's life can be used to predict the infant's susceptibility to major complications. These predictions will in general be more accurate than just using the medical standard SNAP-II or SNAPPE-II scores, which are established with data available from the first 12 hours in the NICU.In this study we looked at the predictive power of 24h and 72h data. As can be expected, classification results were improved when more data was available. However, 24h data already gives a good prediction of an infant's likelihood of developing BPD.In contrast with BPD, our findings show that GP classification can not reliably predict NEC nor ROP using early time series data for blood pressure, heart rate, and oxygen saturation.A classification tool based on this approach could assist care personnel in following the most important parameters in order to identify and predict patients most likely to develop complications and subsequently to develop personalized care for these patients at risk.Ethics approval: The study was approved by the Helsinki University Central Hospital Ethics Committee, decision number 115/13/03/00/14 dated 8 April 2014.", "SDG": [3]}, "recurrent_fully_convolutional_neural_networks_for_multi_slice_mri_cardiac_segmentation": {"name": "Recurrent Fully Convolutional Neural Networks for Multi-slice MRI Cardiac Segmentation", "abstract": " In cardiac magnetic resonance imaging, fully-automatic segmentation of the heart enables precise structural and functional measurements to be taken, e.g. from short-axis MR images of the left-ventricle. In this work we propose a recurrent fully-convolutional network (RFCN) that learns image representations from the full stack of 2D slices and has the ability to leverage inter-slice spatial dependences through internal memory units. RFCN combines anatomical detection and segmentation into a single architecture that is trained end-to-end thus significantly reducing computational time, simplifying the segmentation pipeline, and potentially enabling real-time applications. We report on an investigation of RFCN using two datasets, including the publicly available MICCAI 2009 Challenge dataset. Comparisons have been carried out between fully convolutional networks and deep restricted Boltzmann machines, including a recurrent version that leverages inter-slice spatial correlation. Our studies suggest that RFCN produces state-of-the-art results and can substantially improve the delineation of contours near the apex of the heart.", "keywords": "Recurrent fully convolutional networks,Recurrent restricted Boltzmann machine,Left ventricle segmentation", "introduction": "Cardiovascular disease is one of the major causes of death in the world. Physicians use imaging technologies such as magnetic resonance imaging (MRI) to estimate structural (e.g. volume) and functional (e.g. ejection fraction) cardiac parameters for both diagnosis and disease management. Fully-automated estimation of such parameters can facilitate early diagnosis of the disease and has the potential to remove the more mechanistic aspects of a radiologist's assessment. As such, lately there has been increasing interest in machine learning algorithms for fully automatic left-ventricle (LV) segmentation [1,8,10,12,. This is a challenging task due to the variability of LV shape across slices, cardiac phases, patients and scanning machines as well as weak boundaries of LV due to the presence of blood flow, papillary muscles and trabeculations. A review of LV segmentation methods in short-axis cardiac MR images can be found in 17].[20]The main image analysis approaches to LV segmentation can be grouped into three broad categories: active contour models, machine learning models, and hybrid versions that combine elements of the two approaches. Active contour models with either explicit  or implicit [13] contour representations minimize an energy function composed of internal and external constraints. The internal constraints represent continuity and smoothness of the contour and external constraints represent appearance and shape of the target object. However, designing appropriate energy functions that can handle all sources of variability is challenging. Also, the quality of the segmentations produced by these methods typically depends on the region-of-interest (ROI) used to initialise the algorithms. Machine learning approaches have been proposed to circumvent some of these issues [15][1,9,17, at the expense of collecting large training datasets with a sufficient number of examples. Investigating hybrid methods that combine some elements of both approaches is an active research area 18]. Current state-of-theart LV segmentation approaches rely on deep artificial neural networks [4][1,17,. Typically, these solutions consists of three distinct stages carried out sequentially. Initially, the LV is localised within each two-dimensional slice; then the LV is segmented, and finally the segmentation is further refined to improve its quality. For instance, a pipeline consisting of Deep Belief Networks (DBNs) for both localisation and segmentation, followed by a level-set methodology, has shown to generate high-quality segmentations 18]. In more recent work, a different pipeline has been proposed that consists of convolutional neural networks for initial LV detection, followed by a segmentation step deploying stacked autoencoders, and a fine-tuning strategy also based on level-sets methodology [17]. The latter approach has been proved to produce state-of-the-art results on the MIC-CAI 2009 LV segmentation challenge [1]. Both approaches share a number of common features. First, the segmentation is carried out using two-dimensional patches that are independently extracted from each MRI slice. Second, they use a separate architecture for the two tasks, localization and segmentation. Third, different neural network architectures are trained for cardiac MR slices containing the base and apex of the heart, due to the observed heterogeneity in local shape variability.[21]In this work we investigate a neural network architecture, trained end-to-end, that learns to detect and segment the LV jointly from the entire stack of shortaxis images rather than operating on individual slices. Recently, fully convolutional networks (FCN) have been proposed for the segmentation of 2D images . They take arbitrarily sized input images, and use feature pooling coupled with an upsampling step to produce same size outputs delivering the segmentation. Compared to more traditional sliding-window approaches, FCNs are more efficient. They have received increasing interest lately as they unify object localization and segmentation in a single process by extracting both global and local context effectively [16][16,. Applications of FCNs to medical imaging segmentation problems have also started to appear, for instance for the identification of neuronal structures in electron microscopic recordings 22]. In independent work, Valipour et al. [22] have recently adapted recurrent fully convolutional networks for video segmentation.[25]Here we propose an extension of FCNs, called Recurrent Fully-Convolutional Networks (RFCN), to directly address the segmentation problem in multi-slice MR images. We are motivated by the desire to exploit the spatial dependences that are observed across adjacent slices and learn image features that capture the global anatomical structure of the heart from the full image stack. We investigate whether exploiting this information is beneficial for accurate anatomical segmentation, especially for cardiac regions with weak boundaries, e.g. poor structural contrast due to the presence of blood flow, papillary muscles and trabeculations.", "body": "Cardiovascular disease is one of the major causes of death in the world. Physicians use imaging technologies such as magnetic resonance imaging (MRI) to estimate structural (e.g. volume) and functional (e.g. ejection fraction) cardiac parameters for both diagnosis and disease management. Fully-automated estimation of such parameters can facilitate early diagnosis of the disease and has the potential to remove the more mechanistic aspects of a radiologist's assessment. As such, lately there has been increasing interest in machine learning algorithms for fully automatic left-ventricle (LV) segmentation The main image analysis approaches to LV segmentation can be grouped into three broad categories: active contour models, machine learning models, and hybrid versions that combine elements of the two approaches. Active contour models with either explicit In this work we investigate a neural network architecture, trained end-to-end, that learns to detect and segment the LV jointly from the entire stack of shortaxis images rather than operating on individual slices. Recently, fully convolutional networks (FCN) have been proposed for the segmentation of 2D images Here we propose an extension of FCNs, called Recurrent Fully-Convolutional Networks (RFCN), to directly address the segmentation problem in multi-slice MR images. We are motivated by the desire to exploit the spatial dependences that are observed across adjacent slices and learn image features that capture the global anatomical structure of the heart from the full image stack. We investigate whether exploiting this information is beneficial for accurate anatomical segmentation, especially for cardiac regions with weak boundaries, e.g. poor structural contrast due to the presence of blood flow, papillary muscles and trabeculations.DatasetsOur experiments are based on two independent datasets consisting of short-axis cardiac MR images for which the endocardium has been manually segmented by expert radiologists in each axial slice. Further details are provided below.MICCAI DatasetThe MICCAI 2009 LV Segmentation Challenge PRETERM DatasetA second and larger dataset was used for an independent evaluation of all the cardiac segmentation algorithms. The dataset consists of 234 subjects used to study perinatal factors modifying the left ventricular parameter Recurrent Fully-Convolutional NetworksThe proposed recurrent fully-convolutional network (RFCN) is an extension of the architecture originally introduced in Three main building blocks characterise the proposed RFCN as illustrated in Fig. At the end of this contracting path the network has extracted the most compressed features carrying global context. The global feature component starts here with a (3 \u00d7 3) convolutional layer (with stride of 1) followed by a ReLU. We denote e s the output of this layer where s indicates the slice index, i.e. s \u2208 {1, . . . , S}. This output consists of (256 \u00d7 30 \u00d7 30) feature maps. In an attempt to extract global features that capture the spatial changes observed when moving from the base to the apex of the heart, we introduce a recurrent mechanism mapping e s into a new set of features, h s = \u03c6(h s\u22121 , e s ), where \u03c6(\u2022) is a non-linear function, and the size of h s is the same as the size of e s . Another (3 \u00d7 3) convolutional layer (with stride of 1) followed by a ReLU is then applied to complete the global-feature extraction block.Given that training recurrent architectures is particularly difficult due to the well-document vanishing gradient problem, several options were considered for the implementation of recurrent function \u03c6, including a Long Short-Term Memory (LSTM) For every slice, the dense feature maps that have been learned by the convolutional GRU module are then upsampled to compensate for the input size reduction caused by the max-pooling operations. The upsampled features are concatenated with a high resolution parallel layer aligned to the feature-extraction component, similarly to the U-net architecture Other Architectures and Model TrainingRecently, deep belief networks (DBNs) have been proposed for automatic LV detection and segmentation using short-axis MR images In order to further investigate whether modelling the dependence across slices typically yields improved performance, and motivated by the existing body of work on DBNs, we have also assessed the performance of a recurrent version of restricted Boltzmann machines (RRBM), originally proposed to learn human body motion The two convolutional architectures, FCN and RFCN, were trained by minimizing the cross-entropy objective function. FCN was trained using a stochastic gradient descent algorithm with momentum whereas RFCN was trained using a stochastic gradient descent algorithm with RMSProp During training, we performed translation (\u00b116 pixels) and rotation (\u00b140 \u2022 ) data augmentation, which was found to yield better performance.Experimental ResultsThis section presents an empirical evaluation of several LV endocardium segmentation algorithms using three performance metrics: good contours (GC) The PRETERM dataset was modelled using the same architectures, without further customisation. The results of this application are also summarised in Table In order to shed insights into the regional improvements introduced by RFCN, the Dice index was computed separately for different local regions of the LV, and the results are summarised in Table Conclusions", "conclusions": "In this paper we have investigated whether a single neural network architecture, trained end-to-end, can deliver a fully-automated and accurate segmentation of the left ventricle using a stack of MR short-axis images. The proposed architecture, RFCN, learns image features that are important for the localisation of the LV in a sequential manner, going from the base to the apex of the heart, through a recurrent modification of fully convolutional networks.Experimental findings obtained from two independent applications demonstrate that propagating information from adjacent slices can help extract improved context information with positive effect on the resulting segmentation quality. The hypothetical value of the large inter-slice correlation has been further tested by introducing a recurrent version of deep belief networks, and verified with our results showing that RDBNs generally outperform DBNs on the segmentation task, assuming the LV has already been localised. As expected, notable improvements can be seen in the delineation of cardiac contours around the apex, which are notoriously more difficult to identify.One surprising finding was to note that performance of RFCN in apical slices was better for MICCAI than for PRETERM cohort (0.85 vs. 0.76 Dice index in the most apical slice, see Table ), when one could expect the opposite: a regular and homogeneous cohort, PRETERM, should lead to a better performance when leveraging the inter-slice spatial dependence. This aspect will warrant further investigations.2Compared to other models, RFCN has the advantage of carrying out both LV detection and segmentation in a single architecture with clear computational benefits and the potential for real-time application. In future work, we are planning to investigate alternatives operations that can capture inter-slice correlations, such as 3D convolutions, and further extend RFCN by incorporating a bi-directional mechanism for the inclusion of an inverse path (from the apex to the base of the heart) as well as a temporal extension to handle all cardiac phases at once.", "SDG": [3]}, "recurrent_neural_networks_for_driver_activity_anticipation_via_sensory_fusion_architecture": {"name": "Recurrent Neural Networks for Driver Activity Anticipation via Sensory-Fusion Architecture", "abstract": " Anticipating the future actions of a human is a widely studied problem in robotics that requires spatiotemporal reasoning. In this work we propose a deep learning approach for anticipation in sensory-rich robotics applications. We introduce a sensory-fusion architecture which jointly learns to anticipate and fuse information from multiple sensory streams. Our architecture consists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM) units to capture long temporal dependencies. We train our architecture in a sequence-to-sequence prediction manner, and it explicitly learns to predict the future given only a partial temporal context. We further introduce a novel loss layer for anticipation which prevents over-fitting and encourages early anticipation. We use our architecture to anticipate driving maneuvers several seconds before they happen on a natural driving data set of 1180 miles. The context for maneuver anticipation comes from multiple sensors installed on the vehicle. Our approach shows significant improvement over the state-of-the-art in maneuver anticipation by increasing the precision from 77.4% to 90.5% and recall from 71.2% to 87.4%.", "keywords": "", "introduction": "Anticipating the future actions of a human is an important perception task and has many applications in robotics. It has enabled robots to navigate in a social manner and perform collaborative tasks with humans while avoiding conflicts [24,41,21,. In another application, anticipating driving maneuvers several seconds in advance 40][19,27,38, enables assistive cars to alert drivers before they make a dangerous maneuver. Maneuver anticipation complements existing Advance Driver Assistance Systems (ADAS) by giving drivers more time to react to road situations and thereby can prevent many accidents 36].[30]Activity anticipation is a challenging problem because it requires the prediction of future events from a limited temporal context. It is different from activity recognition , where the complete temporal context is available for prediction. Furthermore, in sensory-rich robotics settings, the context for anticipation comes from multiple sensors. In such scenarios the end performance of the application largely depends on how the information from different sensors are fused. Previous works on anticipation [40][20,21, usually deal with single-data modality and do not address anticipation for sensory-rich robotics applications. Additionally, they learn representations using shallow architectures 24][19,20,21, that cannot handle long temporal dependencies 24].[4]In order to address the anticipation problem more generally, we propose a Recurrent Neural Network (RNN) based architecture which learns rich representations for anticipation. We focus on sensory-rich robotics applications, and our architecture learns how to optimally fuse information from different sensors. Our approach captures temporal dependencies by using Long Short-Term Memory (LSTM) units. We train our architecture in a sequence-to-sequence prediction manner (Figure ) such that it explicitly learns to anticipate given a partial context, and we introduce a novel loss layer which helps anticipation by preventing over-fitting.1We evaluate our approach on the task of anticipating driving maneuvers several seconds before they happen [19,. The context (contextual information) for maneuver anticipation comes from multiple sensors installed on the vehicle such as cameras, GPS, vehicle dynamics, etc. Information from each of these sensory streams provides necessary cues for predicting future maneuvers. Our overall architecture models each sensory stream with an RNN and then nonlinearly combines the high-level representations from multiple RNNs to make a final prediction.27]We report results on 1180 miles of natural driving data collected from 10 drivers . The data set is challenging because of the variations in routes and traffic conditions, and the driving styles of the drivers (Figure [19]). On this data set, our deep learning approach improves the state-ofthe-art in maneuver anticipation by increasing the precision from 77.4% to 84.5% and recall from 71.2% to 77.1%. We further improved these results by extracting richer features from cameras such as the 3D head pose of the driver's face. Including these features into our architecture increases the precision and recall to 90.5% and 87.4% respectively. Key contributions of this paper are:2\u2022 A sensory-fusion RNN-LSTM architecture for anticipation in sensory-rich robotics applications. \u2022 A new vision pipeline with rich features (such as 3D head pose) for maneuver anticipation. \u2022 State-of-the-art performance on maneuver anticipation on 1180 miles of driving data .[19]II. RELATED WORK Our work is related to previous works on anticipating human activities, driver behavior understanding, and Recurrent Neural Networks (RNNs) for sequence prediction.Several works have studied human activity anticipation for human-robot collaboration and forecasting. Anticipating human activities has been shown to improve human-robot collaboration [40,21,25,. Similarly, forecasting human navigation trajectories has enabled robots to plan sociable trajectories around humans 10][20,6,. Feature matching techniques have been proposed for anticipating human activities from videos 24]. Approaches used in these works learn shallow architectures [31] that do not properly model temporal aspects of human activities. Furthermore, they deal with a single data modality and do not tackle the challenges of sensory-fusion. We propose a deep learning approach for anticipation which efficiently handles temporal dependencies and learns to fuse multiple sensory streams.[4]We demonstrate our approach on anticipating driving maneuvers several seconds before they happen. This is a sensor-rich application for alerting drivers several seconds before they make a dangerous maneuvering decision. Previous works have addressed maneuver anticipation [1,19,27,9, through sensory-fusion from multiple cameras, GPS, and vehicle dynamics. In particular, Morris et al. 37] and Trivedi et al. [27] used a Relevance Vector Machine (RVM) for intent prediction and performed sensory fusion by concatenating feature vectors.[37]More recently, Jain et al.  showed that concatenation of sensory streams does not capture the rich context for modeling maneuvers. They proposed an Autoregressive Input-Output Hidden Markov Model (AIO-HMM) which fuses sensory streams through a linear transformation of features and it performs better than feature concatenation [19]. In contrast, we learn an expressive architecture to combine information from multiple sensors. Our RNN-LSTM based sensory-fusion architecture captures long temporal dependencies through its memory cell and learns rich representations for anticipation through a hierarchy of non-linear transformations of input data. Our work is also related to works on driver behavior prediction with different sensors [27][17,14,13,, and vehicular controllers which act on these predictions 9][33,38,.11]Two building blocks of our architecture are Recurrent Neural Networks (RNNs)  and Long Short-Term Memory (LSTM) units [29]. Our work draws upon ideas from previous works on RNNs and LSTM from the language [16], speech [35], and vision [15] communities. Our approach to the joint training of multiple RNNs is related to the recent work on hierarchical RNNs [8]. We consider RNNs in multimodal setting, which is related to the recent use of RNNs in image-captioning [12]. Our contribution lies in formulating activity anticipation in a deep learning framework using RNNs with LSTM units. We focus on sensory-rich robotics applications, and our architecture extends previous works on sensory-fusion from feed-forward networks [8][28, to the fusion of temporal streams. Using our architecture we demonstrate state-of-the-art results on maneuver anticipation.34]", "body": "Anticipating the future actions of a human is an important perception task and has many applications in robotics. It has enabled robots to navigate in a social manner and perform collaborative tasks with humans while avoiding conflicts Activity anticipation is a challenging problem because it requires the prediction of future events from a limited temporal context. It is different from activity recognition In order to address the anticipation problem more generally, we propose a Recurrent Neural Network (RNN) based architecture which learns rich representations for anticipation. We focus on sensory-rich robotics applications, and our architecture learns how to optimally fuse information from different sensors. Our approach captures temporal dependencies by using Long Short-Term Memory (LSTM) units. We train our architecture in a sequence-to-sequence prediction manner (Figure We evaluate our approach on the task of anticipating driving maneuvers several seconds before they happen We report results on 1180 miles of natural driving data collected from 10 drivers \u2022 A sensory-fusion RNN-LSTM architecture for anticipation in sensory-rich robotics applications. \u2022 A new vision pipeline with rich features (such as 3D head pose) for maneuver anticipation. \u2022 State-of-the-art performance on maneuver anticipation on 1180 miles of driving data II. RELATED WORK Our work is related to previous works on anticipating human activities, driver behavior understanding, and Recurrent Neural Networks (RNNs) for sequence prediction.Several works have studied human activity anticipation for human-robot collaboration and forecasting. Anticipating human activities has been shown to improve human-robot collaboration We demonstrate our approach on anticipating driving maneuvers several seconds before they happen. This is a sensor-rich application for alerting drivers several seconds before they make a dangerous maneuvering decision. Previous works have addressed maneuver anticipation More recently, Jain et al. Two building blocks of our architecture are Recurrent Neural Networks (RNNs) III. PRELIMINARIESWe now formally define anticipation and then present our Recurrent Neural Network architecture. The goal of anticipation is to predict an event several seconds before it happens given the contextual information up to the present time. The future event can be one of multiple possibilities. At training time a set of temporal sequences of observations and events {(x 1 , x 2 , ..., x T ) j , y j } N j=1 is provided where x t is the observation at time t, y is the representation of the event (described below) that happens at the end of the sequence at t = T , and j is the sequence index. At test time, however, the algorithm receives an observation x t at each time step, and its goal is to predict the future event as early as possible, i.e. by observing only a partial sequence of observations {(x 1 , ..., x t )|t < T }. This differentiates anticipation from activity recognition In this work we propose a deep RNN architecture with Long Short-Term Memory (LSTM) units A. Recurrent Neural NetworksA standard RNN (2) where f is a non-linear function applied element-wise, and y t is the softmax probabilities of the events having seen the observations up to x t . W, H, b, W y , b y are the parameters that are learned. Matrices are denoted with bold, capital letters, and vectors are denoted with bold, lower-case letters. In a standard RNN a common choice for f is tanh or sigmoid. RNNs with this choice of f suffer from a well-studied problem of vanishing gradients B. Long-Short Term Memory CellsLSTM is a network of neurons that implements a memory cell LSTM consists of three gates -input gate i, output gate o, and forget gate f -and a memory cell c. See Figure where is an element-wise product and \u03c3 is the logistic function. \u03c3 and tanh are applied element-wise. W * , V * , U * , and b * are the parameters, further the matrices V * are daigonal. The input and forget gates of LSTM participate in updating the memory cell We now describe our RNN architecture with LSTM units for anticipation. Following which we will describe a particular instantiation of our architecture for maneuver anticipation where the observations x come from multiple sources.IV. NETWORK ARCHITECTURE FOR ANTICIPATIONIn order to anticipate, an algorithm must learn to predict the future given only a partial temporal context. This makes anticipation challenging and also differentiates it from activity recognition. Previous works treat anticipation as a recognition problem Furthermore, anticipation in robotics applications is challenging because the contextual information can come from multiple sensors with different data modalities. Examples include autonomous vehicles that reason from multiple sensors A. RNN with LSTM units for anticipationAt the time of training, we observe the complete temporal observation sequence and the event {(x 1 , x 2 , ..., x T ), y}. Our goal is to train a network which predicts the future event given a partial temporal observation sequence {(x 1 , x 2 , ..., x t )|t < T }. We do so by training an RNN in a sequence-to-sequence prediction manner. Given training examples {(x 1 , x 2 , ..., x T ) j , y j } N j=1 we train an RNN with LSTM units to map the sequence of observations (x 1 , x 2 , ..., x T ) to the sequence of events (y 1 , ..., y T ) such that y t = y, \u2200t, as shown in Fig. B. Fusion-RNN: Sensory fusion RNN for anticipationWe now present an instantiation of our RNN architecture for fusing two sensory streams: {(x 1 , ..., x T ), (z 1 , ..., z T )}. In Sections V and VI, we use the fusion architecture for maneuver anticipation.An obvious way to allow sensory fusion in the RNN is by concatenating the streams, i.e. using ([x 1 ; z 1 ], ..., [x T ; z T ]) as input to the RNN. However, we found that this sort of simple concatenation performs poorly. We instead learn a sensory fusion layer which combines the high-level representations of sensor data. Our proposed architecture first passes the two sensory streams {(x 1 , ..., x T ), (z 1 , ..., z T )} independently through separate RNNs () Sensory fusion:) where W * and b * are model parameters, and LSTM x and LSTM z process the sensory streams (x 1 , ..., x T ) and (z 1 , ..., z T ) respectively. The same framework can be extended to handle more sensory streams.C. Exponential loss-layer for anticipation.We propose a new loss layer which encourages the architecture to anticipate early while also ensuring that the 1 Driving maneuvers can take up to 6 seconds and the value of T can go up to 150 with a camera frame rate of 25 fps.architecture does not over-fit the training data early enough in time when there is not enough context for anticipation. When using the standard softmax loss, the architecture suffers a loss of \u2212 log(y k t ) for the mistakes it makes at each time step, where y k t is the probability of the ground truth event k computed by the architecture using Eq. This loss penalizes the RNN exponentially more for the mistakes it makes as it sees more observations. This encourages the model to fix mistakes as early as it can in time. The loss in equation 13 also penalizes the network less on mistakes made early in time when there is not enough context available. This way it acts like a regularizer and reduces the risk to over-fit very early in time.D. Model training and data augmentationOur architecture for maneuver anticipation has more than 25,000 parameters that need to be learned (Section V). With such a large number of parameters on a non-convex manifold, over-fitting becomes a major challenge. We therefore introduce redundancy in the training data which acts like a regularizer and reduces over-fitting On the augmented data set, we train the network described in Section IV-B. We use RMSprop gradients which have been shown to work well on training deep networks V. CONTEXT FOR MANEUVER ANTICIPATIONIn maneuver anticipation the goal is to anticipate the driver's future maneuver several seconds before it happens We improve the pipeline from Jain et al. anticipation. Figure A. Features for maneuver anticipationIn the vision pipeline of Jain et al. B. 3D head pose and facial landmark featuresWe now propose new features for maneuver anticipation which significantly improve upon the features from Jain et al. In Section VI we present results with the features from Jain et al. VI. EXPERIMENTSWe evaluate our proposed architecture on the task of maneuver anticipation We evaluate on the driving data set publicly released by Jain et al. We compare our deep RNN architecture with the following baseline algorithms:1) Chance: Uniformly randomly anticipates a maneuver. 2) Random-forest: A discriminative classifier that learns an ensemble of 150 decision trees. 3) SVM In order to study the effect of our design choices we also compare the following modifications of our architecture: 6) Simple-RNN (S-RNN): In this architecture sensor streams are fused by simple concatenation and then passed through a single RNN with LSTM units. 7) Fusion-RNN-Uniform-Loss (F-RNN-UL): In this architecture sensor streams are passed through separate RNNs, and the high-level representations from RNNs are then fused via a fully-connected layer. The loss at each time step takes the form \u2212 log(y k t ). 8) Fusion-RNN-Exp-Loss (F-RNN-EL): This architecture is similar to F-RNN-UL, except that the loss exponentially grows with time \u2212e \u2212(T \u2212t) log(y k t ). We use the RNN and LSTM implementations provided by Jain A. Evaluation setupWe follow an evaluation setup similar to Jain et al. Algorithm 1 Maneuver anticipationWe should note that driving straight maneuver is not included in evaluating precision B. ResultsWe evaluate anticipation algorithms on the maneuvers not seen during training with the following three prediction settings: (i) Lane change: algorithms only anticipate lane changes, i.e. M = {left lane change, right lane change, driving straight}. This setting is relevant for freeway driving; (ii) Turns: algorithms only anticipate turns, i.e. M = {left turn, right turn, driving straight}; and (iii) All maneuvers: algorithms anticipate all five maneuvers. Among these prediction settings, predicting all five maneuvers is the hardest.Table As shown in Table We study the effect of our improved features in Table The confusion matrix in Figure In Figure VII. CONCLUSION", "conclusions": "In this work we addressed the problem of anticipating maneuvers several seconds before they happen. This problem requires the modeling of long temporal dependencies and the fusion of multiple sensory streams. We proposed a novel deep learning architecture based on Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units for anticipation. Our architecture learns to fuse multiple sensory streams, and by training it in a sequence-to-sequence prediction manner, it explicitly learns to anticipate using only a partial temporal context. We also proposed a novel loss layer for anticipation which prevents over-fitting.Our deep learning architecture outperformed the previous state-of-the-art on 1180 miles of natural driving data set. It improved the precision from 78% to 84.5% and recall from 71.1% to 77.1%. We further showed that improving head tracking and including the driver's 3D head pose as a feature gives a significant boost in performance by increasing the precision to 90.5% and recall to 87.4%. We believe that our approach is widely applicable to many activity anticipation problems. As more anticipation data sets become publicly available, we expect to see a similar improvement in performance with our architecture.  ", "SDG": [3, 11]}, "suggested_criteria_for_successful_deployment_of_a_clinical_decision_support_system": {"name": "Suggested Criteria for Successful Deployment of a Clinical Decision Support System (CDSS)", "abstract": " Three criteria are suggested to help design a Clinical Decision Support System (CDSS) that would have a better chance of being successfully deployed in a clinical environment. These criteria have been successfully applied to a CDSS designed to estimate outcomes for neonatal intensive care unit (NICU) patients. The CDSS was deployed in a pilot study at the Children's Hospital of Eastern Ontario (CHEO)'s NICU. The results of the study showed that the accuracy was deemed acceptable by the physicians and the CDSS would meet their expectations when ready for deployment in a clinical environment.", "keywords": "clinical decision support system,successful deployment,neonatal intensive care,estimating outcomes", "introduction": "Clinical decision support systems (CDSSs) have been used for several decades and have the potential to significantly improve patient care and patient safety . Despite this potential, their successful deployment has been limited and in some cases has resulted in conflict between physicians and hospital administrators [1]. When developing a new CDSS, several factors need to be considered to increase the likelihood that it will be integrated into health care delivery. These factors need to be applied at all stages of the development life cycle of the CDSS.[2]", "body": "Clinical decision support systems (CDSSs) have been used for several decades and have the potential to significantly improve patient care and patient safety II. METHODA literature survey was conducted to determine the most important factors affecting the successful deployment of a CDSS. We discuss how these factors were applied to the development of a CDSS in the neonatal intensive care unit (NICU) at the Children's Hospital of Eastern Ontario (CHEO). This particular CDSS is intended to estimate a number of important clinical outcomes such as mortality, and predict resource utilisation such as expected duration of concentrated nursing care and duration of artificial ventilation. This development is described as a case study in the latter part of the article.The criteria for a successful deployment of a CDSS can be divided into three main areas: (i) The data entry and the decision algorithms; (ii) the human-computer interaction, including the data acquisition, and the manner in which information is requested from the system; (iii) the output of the CDSS, including the format and type of information supplied.A. Suggested criteria for successful deployment of a CDSS (i) Input to the CDSSThe data and information entry into the CDSS is one of the leading causes of failed CDSSs Another major issue is keeping the CDSS decision algorithm up to date. Since patient management changes from time to time, the CDSS can easily become obsolete. An issue that has been encountered in the past is that CDSSs are often created with soft funding. When the funding runs out, keeping the system up to date is a major challenge, so it is critical that systems be designed to have automated updating features. One way to do this is to have the CDSS retrain itself periodically and automatically (ii) Human-Computer InteractionThe human-computer interaction is a critical component of a successful CDSS; access to the system should be easy while being secure. Clinicians are busy and have many diverse tasks to perform. According to Bates et. al, speed is of utmost importance for physicians, therefore, the decision support system should be designed to use the least amount of physician time possible; this includes time to logon to the system and time to acquire the information desired Moreover, it is more convenient for physicians if the information can be obtained from a mobile CDSS, or one with many terminals, rather than from a single terminal that may be located far away (iii) OutputA CDSS should be of clinical value to physicians, improve the quality of care and decrease costs of health care delivery. The CDSS must fit into the physicians' workflow and provide them with useful information. The system output format and type are dependent on what physicians need. Each clinician has different work habits and thus may have different requirements for this function. This makes the development of an effective CDSS more complicated, but this is important for a successful deployment. The manner in which the information is provided should be simple and effective To achieve the goal of deployment with physicians using the system as part of their workflow, designers of CDSSs must work closely with the users at every step. Without this close partnership, it is more likely that a new system will not be used by physicians. Usability testing is also essential at every critical stage of development. Desirable attributes of CDSSs include smart information and smart alerts. When it comes to clinical decision alerts, there needs to be a balance between too many and too few. Without a sufficient number of alerts, it may be difficult to achieve the clinical and economic benefit of having the CDSS. However, too many alerts cause interruptions to physicians, which is undesirable especially if the alerts have a low specificity (high rate of false alarms). Some doctors prefer more alerts while others only desire to be notified for the most critical ones. This again emphasizes the need for close work with users during development Many of the failures of early CDSSs were due to the fact that the user had to filter the information and discard erroneous or useless information. This required a lot of user time and the user had to actively interact with the system rather than just be a passive recipient of the output III. A CASE STUDY: A CDSS FOR NEONATAL INTENSIVECAREPremature births are defined as those occurring at less than 37 weeks of gestation. These newborn infants frequently have serious health problems and make up 75% of the population in neonatal intensive care units Our research group has developed a CDSS that estimates mortality, common complications such as bronchopulmonary Dysplasia (BPD) which is a chronic lung disorder; severe Intraventricular Hemorrhage (IVH, grade III or IV, that is bleeding in the brain associated with long-term disability; Necrotizing Enterocolitis (NEC), which is a serious intestinal illness; and resource utilisation outcomes such as length of stay and duration of artificial ventilation. The project was conducted with CHEO neonatologists, followed by a pilot test in the NICU We are developing a Clinical Data Repository (CDR) that automatically collects data from NICU patients in real time and stores the data in a manner that can be easily retrieved for analysis; the data consists of vital signs from bedside monitors, ventilators, pulse oximeters, and laboratory results Our CDSS is a feedforward backpropagation artificial neural network with the weight-elimination algorithm, and hyperbolic tangent transfer function; it has one hidden layer with an optimal number of nodes. ANNs have the potential to model complex interactions between variables and the advantage over conventional statistics is that they can be trained to predict outcomes on any database, with multiple input parameters and possible outcomes The database used in this work was collected by the Canadian Neonatal Network (CNN), a group of multidisciplinary Canadian researchers focusing on neonatal and perinatal care. Data collected by the CNN between January 1996 and October 1997 contains information from 17 NICUs, which represents 75% of all tertiary-level beds in Canada IV. RESULTSThe neonatal CDSS system under development by our research group meets the requirements outlined by the three criteria for a successful CDSS. The first condition states that data input should require the least amount of physician time possible and that the system should be self updating to ensure the decision algorithm does not become out of date. The next version of our neonatal CDSS will use data automatically collected from patient monitors and ventilators, and automatically access laboratory and imaging tests; this will significantly decrease the time to input data into the CDSS. The majority of the data will be collected automatically in real time without manual input; however some manual input will still be required such as patient name, bed location, patient weight, gestational age, and Apgar 5 results. This data can be entered by the nurses or ward clerks. Additionally, we will ensure that the CDSS does not become obsolete by using an ANN that is able to self-train with new data being acquired.The second condition regarding human-computer interaction requires that the user-interface be simple and provide quick access to data. Our CDSS interface was designed to be simple, and easy to use, with minimal training. The CDSS is easy to navigate and users are able to get important information in a timely manner.For the third condition, regarding output format, we are working closely with the physicians to determine the best way to present the information they require. One improvement we made was the presentation of a risk level (low, medium, or high) of the outcome of interest, rather than presenting physicians with a percentage number, as was the case with our first model. The first prototype of the neonatal CDSS was tested on 60 patient cases at the CHEO NICU that had a mortality rate of 18.33%. The CDSS presented results as high, moderate, and low risk categories as requested by the neonatologists. Table 43/ 88%From Table The pilot study also included a short usability test with the neonatologists who used the system. The results showed that the system was easy to use, provided information in a timely manner, was thought to be useful. There was a high interest in the further development and deployment of the system among the participating neonatologists V. CONCLUSION", "conclusions": "In this work, we suggested criteria to enhance the probability that a CDSS will be deployed and used by health care personnel. In applying these criteria to the design of a CDSS for NICU patients, we conclude that they proved to be useful as our physician partners are very interested in the development and potential deployment of the system at the CHEO NICU in Ottawa and plan to use it when it is ready for a clinical environment. Quoting our physician partner at CHEO: \"This work is so important and is the way that all of medicine (adult and pediatric) is heading.\" -Dr. Erika Bariciak, Neonatologist.In our future work we will continue to add features to our CDSS that will help with the integration of this tool into the real clinical environment according to the three criteria for successful deployment. Work will be done to ensure that the CDSS will train itself periodically, especially when several new patient cases are added to the database. We will also be focusing on automating as much of the data collection, retrieval and analysis as possible.", "SDG": [3]}, "towards_detection_of_bus_driver_fatigue_based_on_robust_visual_analysis_of_eye_state": {"name": "Towards Detection of Bus Driver Fatigue Based on Robust Visual Analysis of Eye State", "abstract": " Driver's fatigue is one of the major causes of traffic accidents, particularly for drivers of large vehicles (such as buses and heavy trucks) due to prolonged driving periods and boredom in working conditions. In this paper, we propose a vision-based fatigue detection system for bus driver monitoring, which is easy and flexible for deployment in buses and large vehicles. The system consists of modules of head-shoulder detection, face detection, eye detection, eye openness estimation, fusion, drowsiness measure percentage of eyelid closure (PERCLOS) estimation, and fatigue level classification. The core innovative techniques are as follows: 1) an approach to estimate the continuous level of eye openness based on spectral regression; and 2) a fusion algorithm to estimate the eye state based on adaptive integration on the multimodel detections of both eyes. A robust measure of PERCLOS on the continuous level of eye openness is defined, and the driver states are classified on it. In experiments, systematic evaluations and analysis of proposed algorithms, as well as comparison with ground truth on PERCLOS measurements, are performed. The experimental results show the advantages of the system on accuracy and robustness for the challenging situations when a camera of an oblique viewing angle to the driver's face is used for driving state monitoring.", "keywords": "Driver monitoring,fatigue detection,fusion,machine learning,percentage of eyelid closure (PERCLOS),spectral regression,video analytics", "introduction": "F ATIGUE, drowsiness and sleepiness are often used synonymously in driving state description . Involving multiple human factors, it is multidimensional in nature that researchers have found difficult to define over past decades [1]- [2]. Despite the ambiguity surrounding fatigue, it is a critical factor for driving safety. Studies have shown that fatigue is one of the leading contributing factors in traffic accidents worldwide [5]. It is particularly critical for occupational drivers, such as drivers of buses and heavy trucks, due to the fact that they may have to work over a prolonged duration of the driving task, during the peak drowsiness periods (i.e., 2:00 A.M. to 6:00 A.M. and 2:00 P.M. to 4:00 P.M.), and under monotonous or boredom working conditions [6], [7].[8]Research to detect driver drowsiness can be classified into three categories: 1) vehicle-based approaches, 2) behavior-based approaches, and 3) physiological-signal based approaches (see , [7] for a good review). In physiological approaches, the physiological signals from a body, such as electroencephalogram (EEG) for brain activity, electrooculogram (EOG) for eye movement, and electrocardiogram (ECG) for heart rate, are evaluated to detect driver drowsiness [9]- [10]. Recent studies show that the methods using physiological signals (specially the EEG signal) can achieve better reliability and accuracy of driver drowsiness detection compared to other methods [15]. However, the intrusive nature of measuring physiological signals can hinder driving, especially for prolonged driving periods. Vehiclebased approaches collect signal data from sensors in vehicles to evaluate driver's performance. These methods monitor the variations of steering wheel angle, lane position, speed, acceleration, and braking to predict the driver fatigue [16]- [17]. It is convenient to collect vehicle signals. However, these approaches might be too slow to detect driver drowsiness [21].[7]Behavior-based approaches depend on vision analysis to monitor driver's behavior, including eye-closure, eye-blinking, yawning, head pose, hand gesture, etc., through a camera directed to driver's face - [22]. The driver is alerted if a drowsiness symptom is detected. The vision-based systems on behavior analysis are attractive to automobile industries since they are non-intrusive to the driver and the measures are effective and reliable to predict driver fatigue [27].[28]A drowsy driver displays a number of symptoms, including frequent eye-closure, rapid and constant blinking, nodding or swinging head, and frequent yawning . In the last decade, numerous vision systems have been developed to detect such behaviors of drowsiness for driving safety. Most of the existing systems require the installation of a camera directly toward the driver's face to capture high-resolution face images, and some of them employ specifically designed infra-red (IR) cameras [29], [23] or stereo cameras [30], [31]. The vision algorithms are designed for high-resolution front-view face and eye images (e.g., the height of the face is over 60% of image height in input images over 640 \u00d7 480 pixels). This configuration is not applicable for buses and large vehicles. A bus mostly has a large front glass window to let the driver have a wide-field-view of scene for safe driving since it is much wider than cars. Placing a camera on the front glass window is not practical, and that also blocks the drivers' view. If the camera is installed on the frame around the window, the camera is not able to capture the frontal view of driver's face, so that existing vision algorithms are not applicable. In this paper, we present a novel vision system for bus driver monitoring. It is designed for easy and flexible deployment on existing cameras in buses with no extra hardware cost required. In most existing buses, there are already many dome cameras installed for security purposes. One is mounted in the upperright or upper-left position with respect to the driver to record the driver behavior on duty, as shown in the left image in Fig. [32]. Since this camera is first installed for recording of driving behavior, a wide-view dome camera is used to capture the visible upper body of the driver, as shown in the rest images in Fig. 1. This configuration poses three challenges to visionbased driver monitoring:11) Oblique-view: In most normal driving poses, only oblique-view face images can be captured by the camera (e.g., 30 \u2022 \u223c 40 \u2022 ). In this case, the vision algorithms designed for a frontal view of faces are not applicable. 2) Low-resolution: From the images which capture the full visible part of the driver body by a wide-view dome camera, only low-resolution face and eye images can be obtained (in the raw input image, the height of the face is of 80\u223c110 pixels). In such case, the feature-point based approaches would often fail, and binary classification of eye state is not reliable. 3) Pose variation: The bus driver has to move and turn his upper body and head to look around when driving a large vehicle. In this case, the approaches based on face detection and tracking often fail to locate the head and face in the image.A novel vision system is proposed to deal with these challenges, which integrates upper-body detection, face detection, eye detection, eye openness estimation, fusion, and symptom measure estimation. The framework is illustrated in Fig. . The core innovative algorithms are eye-openness estimation and fusion. We propose a manifold learning algorithm which can learn a mapping from a low-resolution eye image (e.g., 32 \u00d7 24 pixels) to a 1-dimensional continuous level of eye-openness. There are two advantages of this approach. First, there is no need to detect eye feature points for symptom estimation that often fails on low-resolution face images. Second, it avoids classifying the ambiguous partially closed eyes into open or close state for binary state (0/1) classification. This improves the accuracy of identifying \"drowsy\" state between \"normal\" and \"sleepy\" states. To obtain an accurate and robust estimate of eye openness, a novel fusion algorithm is proposed which adaptively integrates the results of eye openness estimations on the Fig. 2. Framework of the proposed system. multi-model eye detections for both eyes. Based on the innovative techniques, the system achieves robust performance on the challenging scenarios where the existing approaches often fail.2The main contribution of this paper is a novel vision-based system for bus driver fatigue detection which is applicable to low-resolution face images captured from an oblique viewing angle to the driver's face, so that it can share a wide-view camera mounted for driver's full body behavior monitoring. The technological contributions can be summarized as follows:\u2022 A novel framework for vision-based driver fatigue detection which integrates head-shoulder detection, multi-pose face detection, multi-model eye detection, eye openness estimation, fusion, and PERCLOS estimation for driver fatigue detection; \u2022 A manifold learning algorithm to learn a mapping from a low-resolution eye image to a continuous level of eyeopenness; \u2022 A fusion algorithm to obtain an accurate and robust eye openness estimate based on adaptive integration on multimodel eye detections on both eyes; \u2022 A refined approach to compute PERCLOS measure based on the continuous levels of eye-openness.The next subsection discusses some related work. Section II presents the proposed approach and system, including an overview description and details of each module. Experimental results and evaluations are presented in Section III and conclusions are given in Section IV.", "body": "F ATIGUE, drowsiness and sleepiness are often used synonymously in driving state description Research to detect driver drowsiness can be classified into three categories: 1) vehicle-based approaches, 2) behavior-based approaches, and 3) physiological-signal based approaches (see Behavior-based approaches depend on vision analysis to monitor driver's behavior, including eye-closure, eye-blinking, yawning, head pose, hand gesture, etc., through a camera directed to driver's face A drowsy driver displays a number of symptoms, including frequent eye-closure, rapid and constant blinking, nodding or swinging head, and frequent yawning 1) Oblique-view: In most normal driving poses, only oblique-view face images can be captured by the camera (e.g., 30 \u2022 \u223c 40 \u2022 ). In this case, the vision algorithms designed for a frontal view of faces are not applicable. 2) Low-resolution: From the images which capture the full visible part of the driver body by a wide-view dome camera, only low-resolution face and eye images can be obtained (in the raw input image, the height of the face is of 80\u223c110 pixels). In such case, the feature-point based approaches would often fail, and binary classification of eye state is not reliable. 3) Pose variation: The bus driver has to move and turn his upper body and head to look around when driving a large vehicle. In this case, the approaches based on face detection and tracking often fail to locate the head and face in the image.A novel vision system is proposed to deal with these challenges, which integrates upper-body detection, face detection, eye detection, eye openness estimation, fusion, and symptom measure estimation. The framework is illustrated in Fig. The main contribution of this paper is a novel vision-based system for bus driver fatigue detection which is applicable to low-resolution face images captured from an oblique viewing angle to the driver's face, so that it can share a wide-view camera mounted for driver's full body behavior monitoring. The technological contributions can be summarized as follows:\u2022 A novel framework for vision-based driver fatigue detection which integrates head-shoulder detection, multi-pose face detection, multi-model eye detection, eye openness estimation, fusion, and PERCLOS estimation for driver fatigue detection; \u2022 A manifold learning algorithm to learn a mapping from a low-resolution eye image to a continuous level of eyeopenness; \u2022 A fusion algorithm to obtain an accurate and robust eye openness estimate based on adaptive integration on multimodel eye detections on both eyes; \u2022 A refined approach to compute PERCLOS measure based on the continuous levels of eye-openness.The next subsection discusses some related work. Section II presents the proposed approach and system, including an overview description and details of each module. Experimental results and evaluations are presented in Section III and conclusions are given in Section IV.A. Related WorkMany vision-based systems have been developed for driver fatigue detection based on driver face monitoring In Using visible light, numerous methods for fatigue detection based on facial feature point detection are reported The classification approaches train a binary classifier, e.g., SVM, Adaboost, nearest neighbor (NN), etc., to classify the high-resolution eye image into open or close state. In Finally, eye state is classified as open or close using SVM. Under binary eye state classification framework, the ambiguous partially closed eyes may cause either too many false alarms or a lower detection rate. To the best of our knowledge, there is no literary on estimating a continuous level of eye openness and applying it to compute PERCLOS measurement for driver fatigue detection.On the other hand, in computer vision, there are investigations on face detection Most vision-based methods infer the states of fatigue and sleepiness based on the symptoms extracted from the eye regions II. THE PROPOSED METHOD AND SYSTEMIn this Section, first, an overview of the system framework is presented, and then, each module is described. The details are focused on the innovative eye-openness estimation, fusion, and PERCLOS computing.A. System OverviewThe system framework is illustrated in Fig. B. Head-Shoulder Detector for Driver DetectionSince the camera observes the driver's face from an oblique viewing point, and the bus driver may change his body pose and head direction to check the surrounding situations when approaching an intersection or a bus stop, a system starting from face detection and tracking may fail to localize the driver's head in the input images. In this system, we build a headshoulder detector to detect the presence of a driver on seat and locate the driver's head position in the incoming image. A HOG (histogram of oriented gradients) descriptor C. Face DetectionOver the region of head, face detectors are applied to find a face looking towards the front of the bus. First, the OpenCV face detector D. Eye DetectionTwo eye detectors are applied to the rectangular region of the detected face. The OpenCV eye detector E. Eye Openness EstimationDrowsy driver may not simply close his/her eyes frequently. He/she may be struggling to keep his/her eyes partially open. Hence, to accurately characterize driver drowsiness based on eye images, it is desirable to measure the eye-openness continuously. As described above, in this application scenario, we can only obtain small, low-resolution images of eyes in images. Feature-based techniques for eye image analysis are not applicable. We develop a novel method for eye-openness estimation based on the full image of the eye so that there is no need to detect feature points and curves around eyes. This method is applicable for low-resolution images of eyes.From the result of eye detection, we obtain a normalized eye image of 32 \u00d7 24 pixels. We apply Spectral Regression (SR) Embedding Given m training samples {x i , u i } m i=1 (x i \u2282 R n and n = 32\u00d724), we wish to find a transformation A to map x into a d-dimensional manifold subspace z (d n) with SR Embedding (i.e., x \u2192 z = A T x), and then find a mapping z \u2192 u to estimate the level of eye openness.Let W be a m \u00d7 m symmetric similarity matrix. Its element W ij represents the similarity between samples x i and x j . Different from existing SR approaches which define the similarity weight based on either the distance between sample points (i.e., unsupervised learning on a continuous similarity weight) or sample class labels (i.e., supervised learning on a binary similarity weight), we define the similarity weight based on the ground truth of eye openness. The similarity weight is defined aswhere, if the eye-openness levels of two images are close, the similarity weight W ij is high. Hence, our approach can be considered as a supervised SR learning on a continuous similarity weight. Suppose y = [y 1 , y 2 , \u2022 \u2022 \u2022 , y m ] T is the map from the graph to the real line. The optimal y tries to minimize i,j T is the augmented feature vector so that the linear mapping function has not to be constrained to pass the center of the feature space. The mapping vectors {a k } d k=1 can be obtained as the solution of regularized least square problemwhere Then, the solution of A can be obtained aswhere I is the identity matrix. The new input sample x can be embedded into the d-dimensional manifold subspace byFinally, a linear model u = bz with z = [z T , 1] T is used to learn the mapping for eye openness measurement, which can be obtained by least square fitting. The shape model maps the 2D normalized eye image to a continuous 1D measurement of eye openness of which a small value (e.g., 0) means a closed eye while a large value (e.g., \u2265 0.5) indicates a wide open eye.The similarity weight defined in (2) indicates the advantage of using manifold learning for this problem. We want to evaluate a continuous measure of eye openness from an eye image, and to be robust to variations of inter-person differences, head poses, and lightings. According to (2), the eye images of close openness levels have high similarity weights, so that they would be mapped to close positions in the learned manifold feature subspace. However, if two eye images are similar on visual appearance (e.g., the brightness), but the eye-openness levels are different significantly, they would be mapped to well separated positions in the learned manifold feature subspace. Hence, due to the advantage of spectral regression In this application, due to the oblique viewing angle to the driver's face, the eye shapes of left and right eyes are quite different. Hence, two models are trained separately for the left and right eyes. A few examples of eye openness estimation for left and right eyes are shown in Fig. F. FusionSince two eye detectors (CV-ED and I2R-ED) are applied and each detector generates a pair of detection windows for the left and right eyes, four detection windows of eyes are obtained. Eye openness estimation is very accurate if the eye center is well localized. If the eye detector can locate the center of the detection box at the center of the pupil closely, the computed value of the eye openness level is very close to the ground truth value. It is observed that for the open eyes on the normal front-view and oblique-view faces, both OpenCV and I2R eye detectors can locate the two eyes quite well, among them the I2R eye detector performs slightly better to locate the centers of pupils due to the design of the model First, let us denote CV , y rc CV ): center of the right eye detected by CV-ED; where the detected left and right eyes in the face image correspond to driver's right and left eyes, respectively. We define the agreement measures of the two eye detectors for the left and right eyes aswhere \u03c3 x and \u03c3 y are the scale parameters which are determined by the scale of face window generated by the face detector. Specifically, let H F be the height of the detected face window, the parameters are set as \u03c3 x = r W H F and \u03c3 y = r H H F , where r W and r H are selected empirically based on face biometrics. Obviously, if an eye is accurately located by both eye detectors, the agreement measure for the eye is high (i.e., close to 1), otherwise, it is low (i.e., close to 0) which may be caused by poor head pose or closed eye. If the eye is detected just by one eye detector (i.e., CV-ED or I2R-ED), the agreement measure is set as 1.In normal cases, we can obtain four eye detections from one face. When applying the method of eye openness estimation, one can obtain four estimates of the eye openness. Let us denote them as The value of the estimated eye openness level varies in the range [0,1), where u = 0 indicates a closed eye. For a naturally open eye, u is within 0.3 to 0.5. If I2R-ED missed the eyes, mostly due to closed eyes or poor head poses, both u l I2R and u r I2R are set as 0. Also, if CV-ED missed the two eyes, u l CV and u r CV are set as 0. For an oblique-view face, the right eye on the face image (i.e., the driver's left eye) may be too close to the face boundary. CV-ED may miss the right eye, or locate both eyes overlapped on the left eye, or locate the right eye on the mouth. In these cases, u r CV is set to be 0. The latter two cases can be expressed asCV \u2212 y rc CV | > 2\u03c3 y ), THEN u r CV = 0. The last condition means that the detected two eyes are largely separated along vertical direction, mostly due to locating the right eye on mouth. Considering all the above situations, the final eye openness measure (eye state) can be obtained by adaptively integrating all the estimations on the eye detections. The fusion function is defined aswhere k l + k r = 1 and \u03b2 I + \u03b2 C = 1. These parameters are determined according to the reliability of each detector. In this work, the parameters are chosen empirically as k l = 0.67 and \u03b2 I = 0.6. This selection is consistent with the observation that from the oblique viewing angle as shown in the two middle images in Fig. G. Measurement of FatiguePERCLOS is an important symptom of driver fatigue detection. PERCLOS is defined as the percentage of eyelid closure over the pupil over a specified time period. Specifically, PERCLOS calculates the proportion of time within a specified time duration that the eyelid covers over 80% of the pupil Existing measures of PERCLOS are defined on hard thresholding on 80% of eye closure, which is an ambiguous measurement when \u0169i < 0.07; 1 \u2212 \u0169i \u22120.07 0.06 , when 0.07 \u2264 \u0169i \u2264 0.13; 0, when \u0169i > 0.13.(This weight measure indicates that if the eye openness level \u0169i is less than 0.1 (80% eyelid closure), there is high chance that the driver is fatigued. Now, the PERCLOS defined on eye openness can be computed asThe driving states can be classified as Normal and Fatigue on PERCLOS values aswhere the threshold 0.2 corresponds to 80% of eye closures III. EXPERIMENTS AND EVALUATIONSIn this paper, we focus on bus driver fatigue detection from an existing surveillance camera in a bus. The datasets used to evaluate existing vision-based approaches to monitor a driver's facial and head behaviors from a camera directly pointing to the driver's face in a car are not applicable for evaluation of the proposed method and system. On the other hand, it is not safe and ethical to make a drowsy person drive a bus on real roads. We have obtained real-world videos of bus drivers on duty from a local company, but it is not allowed to use them for open report. To evaluate the performance of the proposed techniques and system for bus driver fatigue detection under real-world conditions, we capture dozens of simulated bus driving videos using two wide-angle cameras from the viewing point similar to the dome cameras mounted in existing buses. The simulation scenarios of driving bus are performed by 23 people under varying lighting conditions. They are asked to simulate both normal and drowsy driving over ten minutes. The participants include young and middle aged male and female persons, among them 6 people wear transparent glasses. During the simulated bus driving periods, we switch on/off a different number of lights and lower/raise curtains in the lab to simulate lighting changes in real-world conditions. In addition, to evaluate the performance of our system in realworld conditions, we also captured 3 simulated driving videos in moving car.The system is developed and evaluated in two phases. In the first phase, we collect individual images to train and validate the eye openness estimation model. Images for training and validation are randomly selected from simulated driving sequences by 8 participants. This set of experiments is performed on individual images. In the second phase, we evaluate our system for driver fatigue detection according to PERCLOS measurement on novel sequences of simulated driving. This set of experiments is performed on whole video sequences of another 15 persons.A. Experiments on Eye Openness EstimationIn the first phase, from 8 image sequences, we first randomly select 1068 images to form a training set (TrainSet), and then, from the rest of the images, we randomly select 337 frames according to even distribution to form a test set (TestSet). In both sets, the eye positions and the 4 marks for each eye (as illustrated by the upper-right picture in Fig. When applying our model in the proposed system, the eye openness estimation is performed on the eye boxes automatically located by OpenCV and I2R eye detectors. There are shifts of eye center positions and variations of eye box scales. We also build a dataset (AutoSet) which consists of 402 eye samples successfully localized by either OpenCV or I2R eye detectors. The samples are randomly collected from the rest images of the 8 sequences. The average errors of our model on this dataset are also listed in Table The model for eye openness estimation is trained for large variations in lighting conditions. A few examples of training eye images which cover a large range of brightness are shown in Fig. The proposed fusion approach makes the final estimation much more reliable and stable due to the fact that the final result depends on more reliable eye detections. A few examples of eye openness estimations after fusion with respect to good and poor eye detections are shown in Fig. Under each image in Fig. B. Experiments on Accuracy of PERCLOS MeasurementIn the second phase, we evaluate the performance of our system for driver fatigue detection on whole video sequences. The test dataset consists of 15 long videos performed by another 15 different individuals. Each participant is told to simulate bus driving from normal to drowsy and then sleepy states gradually. Each video sequence lasts about 9 minutes.To compare with state-of-the-art technologies, we implement a baseline method. The baseline method consists of three steps. In the first step, PCA is applied to the raw eye image to reduce the feature dimensionality. In the second step, an SVM (Support Vector Machine) classifier is trained to classify the eye image into two states, i.e., open and closed eyes. The SVM classifier is trained using all samples in TrainSet and TestSet (in which the samples are obtained according to manual annotations). Finally, PERCLOS measures are computed according to the percentage of recognized closed eye within recent 1 minute. This baseline method is similar to the approaches reported in It is not safe and practical to collect data of drowsy bus driving on real-world roads. To evaluate the effectiveness of our system for drowsy driving detection, we compared the PERCLOS generated by our method with that computed on ground truths of eye openness levels, since the PERCLOS has  been evaluated as an effective measure of driver fatigue on visual signal The plots of the PERCLOS measurements against the number of frames and the corresponding classifications of driver states based on the PERCLOS scores for 4 typical sequences of the 15 test videos are shown in Fig. The baseline method fails to detect drowsy and sleepy driving states for the third and fourth persons due to a large number of classification errors for eye states. For the second person, it is too sensitive due to a large number of false detection errors of closed eyes. The blue PERCLOS curve of the first person seems to be able to capture the increases of eye closure durations, but the difference of it with green curve of ground truth is still quite large. The large error rate of SVM classification may be caused by low-resolution of eye images, large oblique angle to the faces, and ambiguity of partial open eyes, as well as the variations of center positions and scales of eye detection windows. From the figure, one can observe that the red and green plots match quite well, especially when the PERCLOS values are small which is critical to distinct normal and fatigue drivers. If we define a distance measure between the PERCLOS curves aswhere N is the number of images in Set2, P M (i) is the predicted PERCLOS value and P G (i) is the ground truth PERCLOS value for the ith image. The denominator is used for normalization (where the constant 0.2 corresponds to Fig. 80% of eye closure) since a relative large difference for large P G (i) value has less impact for state classification. Using this measure, the average distance between our method and ground truth is 0.1223 and that for the baseline approach is 0.5308. In the plots of the corresponding classifications of driver states based on PERCLOS scores, the \"blue\" plots represent the driving states obtained by the baseline method, the \"red\" plots represent the driving states generated by our method, and the \"green\" plots represent the driving states obtained from PERCLOS measures on ground truths. The state 1 represents normal state and the state 2 indicates drowsy and sleepy state. It can be observed that our method matches the ground truth well. If we define a matching rate for kth state aswhere S i is the predicted state and S g i is the ground truth state for the ith image in Set2, and B() is a Boolean logic function. The matching rates between our method and the ground truth for normal and fatigue states are 85.02% and 95.18%, respectively. The matching rates of baseline method and ground truth are 63.50% and 59.52% for normal and fatigue states, respectively.The simulated drowsiness may not be same as true drowsiness in driving. The participants were asked to perform eye opening, blinking, and eye closure with different frequencies and durations to simulate drowsiness and sleepy. From the curves in Fig. Our system detects the driver first using the head-shoulder detector, and then face detection and eye localization are performed on head regions. The head-shoulder detector is extended from pedestrian detector C. Computational ComplexityWe implemented the proposed system in C++ Visual Studio 2008 on a Windows 7. On a Dell desktop of Intel 2.80GHz CPU (single core) with 4 GB of memory, head shoulder detection takes about 20 ms, face detection takes about 49 ms, both the eye detections takes about 110 ms, fusion, PERCLOS measure estimation and fatigue state classification takes about 8 ms. Overall, the operation for a single frame takes about 0.19 second, that means our system can run at about 5 frames per second (fps). Since no tracking is required in our system, the computations on consecutive frames can be performed in parallel. Hence, using a common quad-core machine, our system can run at about 20fps. In addition, the delay of half or one second will not affect the PERCLOS based fatigue detection in real-world applications.IV. CONCLUSIONS AND FUTURE WORK", "conclusions": "In this paper, we presented a vision-based method and system towards bus driver fatigue detection using existing dome cameras in buses. Our approach starts with the detection of head-shoulders of the figure in the image, followed by face and eye detections and eye openness estimation. Finally, a multi-model fusion scheme is designed to infer eye state and a PERCLOS measure on the continuous measure of eye openness is computed to predict driver's attention state, i.e., normal or fatigue driving state. Experimental results show that our proposed method is able to distinguish the simulated drowsy and sleepy states from the normal state of driving on the lowresolution images of faces and eyes observed from an oblique viewing angle. Hence, our system might be able to effectively monitor bus driver's attention level without extra requirement for cameras. Our approach could extend the capability and applicability of existing vision-based techniques for driver fatigue detection. In the next work, we will investigate to apply it for other vehicles like car, van, minibus and lorry for easy and cheap deployment. One more issue for future work is how to exploit the fatigue detection to improve driver safety in the drowsiness situations - [34].[37]", "SDG": [3, 11]}, "user_adaptive_chatbot_for_mitigating_depression": {"name": "User Adaptive Chatbot for Mitigating Depression", "abstract": " The rate of depression is growing at an alarming rate. A study found that people are more likely to open up to a talking computer than a human. The aim of this paper is to motivate a person going through low phase of his life and to avoid ill-effects of depression. We propose a chatbot that can enable positivity boosting conversation with the user. The chatbot will personalize its replies as per user to keep the conversation engaging. It is made emotionally supportive by training it over a motivating dialogue corpus. The corpus is extended with the users historical chat data from social media platforms. Tensorflow framework and high power GPU is required by server for training. The advantage of such a system is that instead of reaching a phase requiring a visit to a psychiatrist, an online free service will reach many people, will mediate ill-effects of depression and contribute to the betterment of society.", "keywords": "Affective computing,Artificial intelligence,Chatbot,Cognitive computing,Depression", "introduction": "A lot of research has been done to make machines identify human emotions by using complex artificial intelligence systems. There are different types of systems used for different purposes like making decisions, robotics, expert systems, etc. The models are trained using deep learning techniques like recurrent neural network, convolutional neural network, attention network and so on. Some systems combine various classification models using hybrid models. Nowadays, hybrid methods and adaptive methods are also adopted to understand human natural language. Such systems can learn for themselves over a period of time by accessing all the data available online. They provide positive results for emotion recognition and a human user can ask questions to these systems in the same way as one would to another human . Some of the currently available chat bots for assistance are Apple Siri, Google Allo, Microsoft Cortana, etc. These services serve to user queries and provide useful answers but they use only Natural Language Processing(NLP) and cannot react to emotional questions. Some of the chats bots available in market which provide emotional assistance are Woebot, Pepper.ai, Wysa, Joy, Evei. In this paper, an online chat bot called Bot -Autonomous Emotional Support(BAES) is introduced which will help to uplift the mental state of depressed people. An advantage of BAES over other chat bots is that it will be open source and identify with users way of replying over a period of time. BAES will match the input from user with the data gathered about that user from various social networking websites. As it is an adaptive chat bot, the replies will get better as more data is collected. BAES can understand English and Hindi-English languages which makes it easier for Indian public to use.[1]Lately, many people are undergoing depression without them realizing it. Any mental condition is still not considered as an illness in various parts of the world and often goes unattended. People do not even understand that they are going through depression since it is somewhat of a taboo to talk about mental illness and so, people end up committing suicide. Also, the cost of counseling from experts like psychiatrists, psychologists is very high . To provide a cheap and effective way to treat this, open source chat bots need to be encouraged. People might start feeling lonely and isolated, and this is when they can easily access chat bot for comfort and counseling when they have no other person to go to. Thus, we have provided a solution which will tend to the mental state of the user and which will assist each one personally depending on the data of user. BAES will try to remind a depressed person of his/her achievements, which have been shared on social media or given directly to the bot, to pacify him/her and try to make him/her feel better about himself/herself. This will lead to improved mental health and mental satisfaction of the user and will provide a support whenever needed.[2]The rest of the paper is structured as follows: Section 2 describes the previous work that has been done in this particular domain. Section 3 discusses about the methodology and approach proposed in this paper to help people get over depression. Section 4 throws a light on the setup required to build and use this application and provides results of the partial work done. Section 5 gives an idea of this bot with its partial implementation. Finally, the paper is wrapped up with the benefits of BAES over other chat bots and its future scope for actual implementation.", "body": "A lot of research has been done to make machines identify human emotions by using complex artificial intelligence systems. There are different types of systems used for different purposes like making decisions, robotics, expert systems, etc. The models are trained using deep learning techniques like recurrent neural network, convolutional neural network, attention network and so on. Some systems combine various classification models using hybrid models. Nowadays, hybrid methods and adaptive methods are also adopted to understand human natural language. Such systems can learn for themselves over a period of time by accessing all the data available online. They provide positive results for emotion recognition and a human user can ask questions to these systems in the same way as one would to another human Lately, many people are undergoing depression without them realizing it. Any mental condition is still not considered as an illness in various parts of the world and often goes unattended. People do not even understand that they are going through depression since it is somewhat of a taboo to talk about mental illness and so, people end up committing suicide. Also, the cost of counseling from experts like psychiatrists, psychologists is very high The rest of the paper is structured as follows: Section 2 describes the previous work that has been done in this particular domain. Section 3 discusses about the methodology and approach proposed in this paper to help people get over depression. Section 4 throws a light on the setup required to build and use this application and provides results of the partial work done. Section 5 gives an idea of this bot with its partial implementation. Finally, the paper is wrapped up with the benefits of BAES over other chat bots and its future scope for actual implementation.Literature SurveyBayu Setiaji, Ferry Wahyu Wibowo have proposed in In The approach of a novel neural network model called RNN En-coderDecoder that consists of two recurrent neural networks (RNN) is proposed by Kyunghyun Cho et al. in The conversational service can provide personalized counseling service to individuals head-to-head. It is important to resolve the isolation of the patients who have a mental disorder such as depression and lethargy. One-to-one conversation can resolve the isolation effectively. Personal dialogues can also operate efficiently when a user needs urgent interventions. In the early study Proposed MethodologyChatbots tend to fail when encountered with an unknown statement. This can be solved by manually typing responses for all the possible statements. However, this would make the database bulky and its impossible to think of every possible statement. This gets solved by tensorflow Seq2Seq model which tries to generate response for every statement. The network architecture of tensorflow model is a standard encoder/decoder with 3 LSTM layers (hidden size of 256) Our approach has integrated both, tensorflow based responses and database based responses through ChatterBot library Experimental SetupHardware requirements are minimal for user. However, hardware requirements for server are higher end. Hardware Requirement (Server): CPU: Intel i5 or better, RAM: 4 GB or better, OS: Windows 7 or better, VIDEO CARD: NVIDIA Graphics Card with minimum 1 GB memory, FREE DISK SPACE: 3 GB.Software Requirement for user would involve only a browser but the requirements for server are mentioned as follows: Python 3.5.2, CUDA Toolkit 8.0, Chrome Webdriver, PhantomJS.Following libraries in python: setuptools, requests, django debug toolbar, Django, pack-aging, selenium, ChatterBot, django braces, beautifulsoup4, django model utils, uvloop, websockets, wheel, tensorflow, tensorflow-gpu, numpy, nltk, tqd.With the above requirements, the current setup starts with user being navigated to django local server URL. The homepage contains a login screen and if its first time login then user is asked if he/she wants to use his/her social media chat history data for BAES. If provided, the raw chat data is converted to sequence of question and answer format. Currently, whatsapp & facebook chat history data is used.ImplementationThe server starts learning with the inclusion of users chat data in its existing base dataset as shown in figure This database is indexed on text key for faster searching of database. The statement provided and texts stored in database is compared using levenshtein distance algorithm. Levenshtein distance (LD) is a measure of the similarity between two strings, which we will refer to as the source string (s) and the target string (t) The benefits of this system include: a lonely person having a smart phone can reach out and share their emotional state, anytime & anywhere, with the bot. It will be free of cost which will attract anyone to at least give it a try. The chat bot wont judge a person on the basis of his failures. A chat bot can make it easy for a person to lift a weight off his/her heart. The chat bot will have an implicit NDA and keep the chat log in database. It will help to remind a person for his/her own worth by revisiting some of the achievements of the user. It would be able to talk like the user based on provided chat history of user which would make it friendly & interesting.Conclusion", "conclusions": "Now-a-days chat bots are looked at from an assisting approach. This makes them unable to establish any friendly connection with the user. Also, purely AI chat bots do not have set responses for set statements. Our approach will provide a response based on users way of chatting as well as based on positively prepared responses. A study introduced a personalization framework using long-term memory . Another study used word2vec to generate natural sounding phrases [11]. This will help establish a friendly connection with the user as well as provide positive thoughts to user. If the user keeps giving negative response over time, the chat bot can have a functionality to alert the users close ones.[12]", "SDG": [3]}, "analyzing_the_log_patterns_of_adult_learners_in_lms_using_learning_analytics": {"name": "Analyzing the Log Patterns of Adult Learners in LMS Using Learning Analytics", "abstract": " In this paper, we describe a process of constructing proxy variables that represent adult learners' time management strategies in an online course. Based upon previous research, three values were selected from a data set. According to the result of empirical validation, an (ir)regularity of the learning interval was proven to be correlative with and predict learning performance. As indicated in previous research, regularity of learning is a strong indicator to explain learners' consistent endeavors. This study demonstrates the possibility of using learning analytics to address a learner's specific competence on the basis of a theoretical background. Implications for the learning analytics field seeking a pedagogical theory-driven approach are discussed.", "keywords": "K.3.1 [Computer and Education]: Computer Uses in Education -Distance Education Measurement,Human Factors Learning Analytics,Big-data mining,Log data,Time Management Strategy,Adult Education", "introduction": "There are high demands within e-learning for adult learners. Over the past years, there have been an increase in online course enrollment among adult learners in order to obtain knowledge or develop professional skills [6,17,. However, difficulties have also been posed by adult learners in taking online courses due to their lack of time management skills 38]. Time management strategies are increasingly required in the context of adult learning because they are usually involved in both their study and job at the same time; therefore, a successful completion of an online course depends on the efficient use of a given amount of time. To a considerable degree, it is reported that failure in an online course for adult learners results from poor time management [22][11,.19]By analyzing adult learners' online activity based on educational data mining, instructors can detect the status of their learning processes in an earlier stage. Given that most activities of learners enrolled in online courses occur in a Learning Management System (LMS), utilizing the log data within the LMS could provide crucial insight into the learning analytics field. If we can distinguish the learning patterns in the early stage of an online course, it will be conducive to encouraging or guiding learners by providing them with an appropriate instructional intervention .[4]Log data, which is saved as an unstructured data set, contains users' log information within online systems, and it can be used to represent how the learning processes occur on the web throughout the login duration. Furthermore, this information might be more genuine when compared to the data from surveys, which rely highly on learners' recall and subjective interpretations; thus, we do not have to consider the possibility of distortion or low reliability [1,. However, log data alone cannot be transferred to the learning processes without a sophisticated interpretation in regards to theoretical aspects. Our contribution is to suggest an effective way to convert users' log data into predictive indicators of learning performance based on theoretical background.12]The focuses of this study are twofold. First, we elicit \"candidates for proxy variables\" from the log data set that represent learners' time management strategies as conceptual constructs which have long been considered to be a vital factor to their performances [2,26,31,34,. Second, we determine whether the elicited proxy variables predict learner performance in terms of verifying the empirical validity. If so, the proxy variables can be used to detect the status of learners' time management and predict performance in other data sets from similar contexts.42]", "body": "There are high demands within e-learning for adult learners. Over the past years, there have been an increase in online course enrollment among adult learners in order to obtain knowledge or develop professional skills By analyzing adult learners' online activity based on educational data mining, instructors can detect the status of their learning processes in an earlier stage. Given that most activities of learners enrolled in online courses occur in a Learning Management System (LMS), utilizing the log data within the LMS could provide crucial insight into the learning analytics field. If we can distinguish the learning patterns in the early stage of an online course, it will be conducive to encouraging or guiding learners by providing them with an appropriate instructional intervention Log data, which is saved as an unstructured data set, contains users' log information within online systems, and it can be used to represent how the learning processes occur on the web throughout the login duration. Furthermore, this information might be more genuine when compared to the data from surveys, which rely highly on learners' recall and subjective interpretations; thus, we do not have to consider the possibility of distortion or low reliability The focuses of this study are twofold. First, we elicit \"candidates for proxy variables\" from the log data set that represent learners' time management strategies as conceptual constructs which have long been considered to be a vital factor to their performances PROXY VARIABLES TO REPRESENT TIME MANAGEMENT STRATEGY IN LMSConverting a gigantic data set into proxy variables involves the following steps (see Fig. The proxy variable, which is not identical to the targeted conceptual construct but is optimally processed, can be applied to other data sets. In this study, three variables are chosen on the basis of previous studies. Time management strategyA large body of research argued that time management strategy is deeply associated with the ability to prioritize tasks Fig. 2. Selection of three variables.Fig. Total login timeThe degree to which learners invest their time has been recognized as a powerful factor correlative with performance; moreover, much research has reported a strong relationship existent between the total studying time and performance In this study, \"total studying time\" is represented by the term \"total login time,\" to measure the actual learning time. To support the significance of the variable, Cotton and Savard's conception of learning time was adopted Figure 3. Three types of studying timeIn this study, the author argued that the three different types of time mentioned above are significantly correlated with learning performance. Of course, it is hard to regard login time as genuine academic learning time because merely logging into LMS does not necessarily denote meaningful learning in itself. However, we can easily assume that recorded login time belongs with the allocated time or time-on-task when considering that the greatest proportion of learning-related activities occur within LMS, such as observing lectures, gathering information, interacting with peers or submitting assignments.Therefore, we decided to use the login time to construct the proxy variable as an indicator standing for the allocated time, which is an extended concept in which two other types of time are inherent.Login frequencyHow frequently learners participate in an online course has been regarded as an important factor that predicts higher levels of performance. Piccoli, Ahmad, and Ives In this study, we assume that the more frequently learners log into LMS, the more newly updated and shared information they shall obtain, which is a factor that leads to their better understanding of the learning content as well as what they must prepare for classes. The login frequency was calculated by adding up the number of individual student's login time into LMS.In this study, we assume that the more frequently learners log into LMS, the more newly updated and shared information they shall obtain, a factor which shall lead to their better understanding of learning content and of what they must prepare for classes.(Ir)regularity of learning intervalRegularity of learning is defined as the extent to which learners regularly engage in learning, and has been recognized as one of the time management strategies In this study, data is calculated into a standard deviation of the login interval. Thus, it basically indicates the \"irregularity of learning interval.\" To be specific, in the following (see Fig. RESEARCH QUESTIONSThe specific research questions are as follows.R1: How can candidates for proxy variables regarding the learners' time management strategy be elicited from log data? R2: Do the suggested variables (total login time, login frequency and regularity of login interval) predict adult learners' performance?ANALYSIS AND RESULTSParticipants and research contextThe participants in this study consisted of 200 adult learners enrolled in a commercial e-learning course entitled \"Credit Derivative\" administered by a Korean e-learning company. All participants were engaged in the financial business field as their full-time job. This course is operated 100% online over a month. At the end of the course, all participants were required to take a test.Measures and VariablesSuggested independent variablesLog data was collected from the LMS by an automatic collection module embedded within the system. The Total login time, Login frequency and (ir)regularity of learning interval were extracted as independent variables.Dependent variableLearning performance, a dependent variable in this study, is defined as a score of the final test, which consisted of 20 multiple choice items. The scores from each question were collected and added together in order to obtain the total score. The total score was graded on a scale of one hundred points. Multiple Linear Regression AnalysisA multiple regression analysis was conducted in order to determine whether the three suggested values, which serve as proxy indicators of time management strategy, predict learning performance. The results are presented in Table It is shown that the suggested three variables account for 20.8% of the variance in learning performance (F=36.267, p < .01).Of these three proxy variables, only (ir)regularity of learning interval was found to predict learning performance (\u00df=-4.343, t=-10.115, p < .01). DISCUSSIONThe result reveals that only the (ir)regularity of learning interval factor is proven to predict learning performance.Indeed, some of the research reported a limited relation between studying time and learning performance. Ha and colleagues The regularity of the learning interval, meanwhile, can provide critical evidence as to the fact that learners who more steadily log into LMS from the beginning of a study to the end show better performance. This involves neither a temporal access at a certain point nor merely a one long time visit, but rather a wellintended and conscious learning over a relatively long term. As a matter of fact, several articles recognize regular participation as a vital key to success in learning CONCLUSION", "conclusions": "This study has made a contribution to learning analytics, within the context of an adult learner's time management strategy which has been considered to be an essential factor for successful learning in andragogy .[5]This study shows a process of converting complex log patterns into elaborated \"proxy variables\" based on both a ripe theoretical foundation and well-intended manipulation. It demonstrates a possibility of further research regarding the formation of more sophisticated proxy variables that represent certain conceptual constructs drawn from an enormous database. Until now, much research in the learning analytics field has been conducted in a data-driven way and frequently with scarce theoretical background . Recently, however, the learning analytics field has constantly maintained its emphasis on the learning and teaching areas as well in contrast to its strong root as a data-driven approach [13]. The social and pedagogical usage of learning analytics is being actively discussed now, as researchers search to define it as a separate field by which to improve learning opportunities away from business area [40][13,. Along this line, an abundant theoretical foundation is required for the extensive application of research findings into the real world context.24]This study has limitations as well. We could not track the specific time use of the learners. With log data which better mapped a variety of time use on different menus and web pages, a more accurate analysis to catch real studying time could have been made possible. If we can track the learner's specific time use in LMS and thus extract actual studying time from log data at every moment, it would be possible to more clearly investigate the relationship between genuine studying time and learning performance.", "SDG": [4]}, "exploratory_study_examining_the_at_home_feasibilit": {"name": "Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism", "abstract": " Although standard behavioral interventions for autism spectrum disorder (ASD) are effective therapies for social deficits, they face criticism for being time-intensive and overdependent on specialists. Earlier starting age of therapy is a strong predictor of later success, but waitlists for therapies can be 18 months long. To address these complications, we developed Superpower Glass, a machine-learning-assisted software system that runs on Google Glass and an Android smartphone, designed for use during social interactions. This pilot exploratory study examines our prototype tool's potential for social-affective learning for children with autism. We sent our tool home with 14 families and assessed changes from intake to conclusion through the Social Responsiveness Scale (SRS-2), a facial affect recognition task (EGG), and qualitative parent reports. A repeated-measures one-way ANOVA demonstrated a decrease in SRS-2 total scores by an average 7.14 points (F(1,13) = 33.20, p = <.001, higher scores indicate higher ASD severity). EGG scores also increased by an average 9.55 correct responses (F(1,10) = 11.89, p = <.01). Parents reported increased eye contact and greater social acuity. This feasibility study supports using mobile technologies for potential therapeutic purposes.", "keywords": "", "introduction": "Children with autism spectrum disorder (ASD) struggle to recognize facial expressions, make eye contact, and engage in social interactions. 1, An estimated 1 in 68 children have an ASD, and many can have dramatic improvements if social skills are taught intensively from an early age. 2[1][2][3][4] Children with ASD have demonstrated deficits in facial processing abilities, such as distinguishing fear from surprise and identifying subtler emotions. [5][6][7][8] Children also struggle with facial engagement and eye contact. [9]10, Teaching these skills to children with autism is important for social development and is closely linked with empathy. 11[12][13][14][15] Today's standard for treatment of these core ASD deficits focuses on a form of behavioral therapy known as applied behavioral analysis (ABA). [16]17, Although ABA therapy is effective in increasing IQ, improving eye contact, face-to-face gaze, and emotion recognition, children who receive ABA often struggle to generalize learned behaviors to natural interactions and are dependent on prompts. 1817, Therapies called naturalistic developmental behavioral interventions (NDBIs) promote better generalization of newly learned skills due to their integration into the child's natural everyday interactions. 1915,20, However, the delivery of behavioral interventions like ABA and NDBIs is bottlenecked by an increasing imbalance between the availability of behavioral therapists and the number of children who must receive care. 2122, One of the strongest predictors of greater treatment outcomes is younger age at entry into behavioral interventions 2324, , but delays in access to therapy leave many children untreated until after sensitive periods for language and cognitive development have already passed. 2526, Additionally, waitlists for access to therapies, such as ABA and NDBIs can be up to 18 months long. 2723, Consequently, many children with autism are unable to build such core social skills and subsequently regress towards a path of isolation. 2524, These issues have compounded into an urgent need for alternative, ubiquitous mobile methods of delivery 28 that can positively alter the healthcare system and scale to meet the growing population in need of early intervention.29To address the complications associated with accessing the clinical setting and to expedite children's access to therapy, we have begun development of a system to deliver therapy at home using a machine-learning-assisted software system that runs on Google Glass paired with an Android smartphone, designed for use in the child's natural environment during social interactions with friends and family members. [30][31] It recognizes eight emotions, as described in detail by Ekman et al.: happiness, sadness, anger, disgust, surprise, fear, neutral, and contempt (named \"meh\" in child-friendly terms), which are recognized as theoretically universal emotions. [32][33][34] The Glass provides audiovisual feedback to the wearer that corresponds to which of the eight emotions the Glass recognizes during social interactions through its outward-facing camera.[35]Using technology and software for children with autism may assist children struggling with social anxiety in social interactions. [36][37] In addition, incorporating visual, dynamic, and real-world stimuli can increase learning and greater generalizability to other in vivo interactions. [38]36, Studies such as those conducted by Madsen et al. 39 and Liu et al. 40 , among others 41 , have utilized mobile technologies like portable PCs and Google Glasses to assist children with autism during social interactions via software for facial recognition, eye tracking, and structured games. While more recent projects incorporate social interactions with the use of technology, an important improvement on the use of computer programs, the limitations of most of these technologybased intervention studies include potentially distracting software, 42[40][41] a limited range of participants' autism severities, [42] adolescent participant populations, 41 and highly structured games (rather than naturalistic interactions). 4041, We seek to improve upon these approaches with a wearable system that can seamlessly augment social interactions with social learning cues in an unobtrusive, naturalistic way. We hypothesize that our system's ability to provide continuous behavioral therapy outside of clinical settings will enable faster gains in social acuity, and that within a limited and self-directed period of use, will permit the child to engage in increasingly more complex social scenarios on his/her own. Our pilot research 429,30, established smart glasses as a practical and feasible platform to deliver audio-visual feedback to children with ASD. Part of the foundation of this pilot work included an iterative design aspect, where, throughout the study, we evaluated and compared various interfaces and games. The human-computer interaction lessons from our iterative design process have been documented in Washington et al. 43 We created robust facial expression software 4331,45, and implemented many design-focused iterations. 4632, The present study expands upon our previous work and sends our prototype system to the home environments of children with ASD. In this study, we evaluated the potential of the Superpower Glass prototype as a wearable therapy intervention that increases social skills, facial affect recognition, and eye contact for children with autism between the ages of 3 and 17 years. Additionally, this field study was designed to determine feasibility of the fit and form factor of the Glasses, the appropriate \"dosage\" as determined by family usage over several weeks, and the feasibility of sustained use of Superpower Glass in the home setting.47", "body": "Children with autism spectrum disorder (ASD) struggle to recognize facial expressions, make eye contact, and engage in social interactions. To address the complications associated with accessing the clinical setting and to expedite children's access to therapy, we have begun development of a system to deliver therapy at home using a machine-learning-assisted software system that runs on Google Glass paired with an Android smartphone, designed for use in the child's natural environment during social interactions with friends and family members. Using technology and software for children with autism may assist children struggling with social anxiety in social interactions. RESULTSBetween July 2016 and October 2016, 24 participants consented to participate and attended an intake appointment at Stanford University. Five families withdrew prior to using the Superpower Glass system at home or were unable to continue the treatment portion of the study due to conflicts in personal schedules. All five of these participants who withdrew prior to using the Superpower Glass system at home were male, with an average age of 7 years 6 months (SD = 2.51 years), and had an average ABIQ score of 95.8 (median = 106, SD = 27.95). Four of these participants were Caucasian and one was Asian. The research team excluded five additional families who did not comply with the at-home study procedures, which required families to complete three or more sessions with Superpower Glass per week, for 20 min per session. The five families that were excluded by the research team due to noncompliance with the required use of the device were also all male, with an average age of 8 years 6 months (SD = 4.04 years), and had an average ABIQ score of 74.8 (median = 76, SD = 10.08). Two of these participants were Caucasian and three were Asian. This yields a study compliance rate of 73.68%. The following results are based on the remaining 14 families. Refer to Table We report the clinical implications of the feedback families gave during the semi-structured interview. Detailed design and user experience feedback is reported in a previous publication. Interview questions to parentsResponded \"yes\"(1) Do you feel that additional games and/or more complex games would increase your engagement? N = 14 (100%)(2) Did you find charging the device to be burdensome or challenging? N = 14 (100%)(3) Did you use some sort of a reward system to get your child to use the system? N = 5 (35.7%) (4) Would you use the system more if the entire experience was gamified? N = 14 (100%)(5) During the sessions, did you ever change your facial expression to be more emotive as a result of the emotion recognition accuracy?Did you find that your child made increased eye contact when not wearing the device? N = 12 (85.7%) (7) Did you find that your child made increased social interaction when not wearing the device? N = 7 (50%) (8) Did you find that your child exhibited increased emotional recognition when not wearing the device? N = 11 (78.6%) (9) Did you find that your child increased spontaneous conversation when not wearing the device? N = 2 (14.3%) (10) Did you find that your child showed increased patience when not wearing the device? N = 4 (28.6%) (11) Did you find that your child showed increased empathy when not wearing the device? N = 6 (42.9%) (12) Did you find that using the device resulted in an overall increase in quality family time? N = 13 (92.9%)Interview questions to children What was your preferred method of feedback: audio feedback only?summary, families found the system to be engaging, useful, and fun based on feedback from their conclusion interviews (Table The mean total SRS-2 score during the intake appointments was 80.07 (SD = 9.53, SEM = 2.55); the mean total SRS-2 score during the conclusion appointments was 72.93 (SD = 10.29, SEM = 2.75). Children's total SRS-2 scores decreased an average of 7.38 points over the course of the study (F(1,13) = 33.20, p < .001, a higher score indicates a higher severity of ASD) and there was no significant correlation between the decrease in SRS-2 scores to the number of days the device was at home or to the ASD severity category of ABIQ (Table In addition, the SRS-2 data showed significant changes pre-Glass and post-Glass usage on sub-domain questions, including changes in recognizing intent, social initiation, social interaction, and eye contact. The Wilcoxon Rank Sum test run on the SRS-2 65 item-level questions showed a nominally significant change from intake to conclusion for five items among the 65 (Table Due to our iterative design platform for this pilot work, the first three study participants did not receive the EGG evaluation at intake. The remaining 11 of the 14 participants completed the EGG at intake and conclusion. The 11 participants' EGG scores yielded a significant increase in emotion labeling accuracy (F(1,10) = 11.893, p = .006) (Table DISCUSSION", "conclusions": "Significant decreases in SRS-2 total scores and subscores, concomitant increases in emotion recognition measured by EGG, and responses to semi-structured interviews support the hypothesis that the use of Superpower Glass may be an effective and practical wearable therapy intervention for children with autism that can increase social skills, facial affect recognition, and eye contact. Since neither the number of days with Glass at home nor the child's ABIQ score were significantly correlated with improvements on both outcome measures from Pearson's correlation tests, this initial finding may suggest that the system was equally effective for all children in our study, irrespective of length of time with Superpower Glass at home (between 4 and 19 weeks), and ABIQ score. However, the significant change demonstrated by participants from the SRS-2 must be treated with caution, as we did not include a comparison control group for comparison.Twelve of the 14 families commented during the semistructured interview that they observed an increase in eye contact from intake to conclusion. This is also supported by one of the significant SRS-2 question items focused on eye contact, which was included in the Wilcoxon Rank Sum test analysis for changes from intake to conclusion. The implications of this finding suggest that Superpower Glass may improve eye contact among children with autism, although these findings were not compared to a control group and thus none of the results are conclusive. This will be examined in future studies using a control group and using more quantitative approaches.", "SDG": [4]}, "exploring_the_impact_of_artificial_intelligence_on": {"name": "Exploring the impact of artificial intelligence on teaching and learning in higher education", "abstract": " This paper explores the phenomena of the emergence of the use of artificial intelligence in teaching and learning in higher education. It investigates educational implications of emerging technologies on the way students learn and how institutions teach and evolve. Recent technological advancements and the increasing speed of adopting new technologies in higher education are explored in order to predict the future nature of higher education in a world where artificial intelligence is part of the fabric of our universities. We pinpoint some challenges for institutions of higher education and student learning in the adoption of these technologies for teaching, learning, student support, and administration and explore further directions for research.", "keywords": "Higher education,Artificial intelligence,Teacherbots,Augmentation,Machine learning,Teaching,Graduate attributes", "introduction": "The future of higher education is intrinsically linked with developments on new technologies and computing capacities of the new intelligent machines. In this field, advances in artificial intelligence open to new possibilities and challenges for teaching and learning in higher education, with the potential to fundamentally change governance and the internal architecture of institutions of higher education. With answers to the question of 'what is artificial intelligence' shaped by philosophical positions taken since Aristotle, there is little agreement on an ultimate definition.In 1950s, Alan Turing proposed a solution to the question of when a system designed by a human is 'intelligent.' Turing proposed the imitation game, a test that involves the capacity of a human listener to make the distinction of a conversation with a machine or another human; if this distinction is not detected, we can admit that we have an intelligent system, or artificial intelligence (AI). It is worth remembering that the focus on AI solutions goes back to 1950s; in 1956 John McCarthy offered one of the first and most influential definitions: \"The study [of artificial intelligence] is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\" .(Russell and Norvig 2010)Since 1956, we find various theoretical understandings of artificial intelligence that are influenced by chemistry, biology, linguistics, mathematics, and the advancements of AI solutions. However, the variety of definitions and understandings remains widely disputed. Most approaches focus on limited perspectives on cognition or simply ignore the political, psychological, and philosophical aspects of the concept of intelligence. For the purpose of our analysis of the impact of artificial intelligence in teaching and learning in higher education, we propose a basic definition informed by the literature review of some previous definitions on this field. Thus, we can define artificial intelligence (AI) as computing systems that are able to engage in human-like processes such as learning, adapting, synthesizing, self-correction and use of data for complex processing tasks.Artificial intelligence is currently progressing at an accelerated pace, and this already impacts on the profound nature of services within higher education. For example, universities already use an incipient form of artificial intelligence, IBM's supercomputer Watson. This solution provides student advice for Deakin University in Australia at any time of day throughout 365 days of the year (Deakin University 2014). Even if it is based on algorithms suitable to fulfill repetitive and relatively predictable tasks, Watson's use is an example of the future impact of AI on the administrative workforce profile in higher education. This is changing the structure for the quality of services, the dynamic of time within the university, and the structure of its workforce. A supercomputer able to provide bespoke feedback at any hour is reducing the need to employ the same number of administrative staff previously serving this function. In this context, it is also important to note that 'machine learning' is a promising field of artificial intelligence. While some AI solutions remain dependent on programming, some have an inbuilt capacity to learn patterns and make predictions. An example is AlphaGo-a software developed by DeepMind, the AI branch of Google's-that was able to defeat the world's best player at Go, a very complex board game . We define 'machine learning' as a subfield of artificial intelligence that includes software able to recognize patterns, make predictions, and apply the newly discovered patterns to situations that were not included or covered by their initial design.(Gibney 2017)", "body": "The future of higher education is intrinsically linked with developments on new technologies and computing capacities of the new intelligent machines. In this field, advances in artificial intelligence open to new possibilities and challenges for teaching and learning in higher education, with the potential to fundamentally change governance and the internal architecture of institutions of higher education. With answers to the question of 'what is artificial intelligence' shaped by philosophical positions taken since Aristotle, there is little agreement on an ultimate definition.In 1950s, Alan Turing proposed a solution to the question of when a system designed by a human is 'intelligent.' Turing proposed the imitation game, a test that involves the capacity of a human listener to make the distinction of a conversation with a machine or another human; if this distinction is not detected, we can admit that we have an intelligent system, or artificial intelligence (AI). It is worth remembering that the focus on AI solutions goes back to 1950s; in 1956 John McCarthy offered one of the first and most influential definitions: \"The study [of artificial intelligence] is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\" Since 1956, we find various theoretical understandings of artificial intelligence that are influenced by chemistry, biology, linguistics, mathematics, and the advancements of AI solutions. However, the variety of definitions and understandings remains widely disputed. Most approaches focus on limited perspectives on cognition or simply ignore the political, psychological, and philosophical aspects of the concept of intelligence. For the purpose of our analysis of the impact of artificial intelligence in teaching and learning in higher education, we propose a basic definition informed by the literature review of some previous definitions on this field. Thus, we can define artificial intelligence (AI) as computing systems that are able to engage in human-like processes such as learning, adapting, synthesizing, self-correction and use of data for complex processing tasks.Artificial intelligence is currently progressing at an accelerated pace, and this already impacts on the profound nature of services within higher education. For example, universities already use an incipient form of artificial intelligence, IBM's supercomputer Watson. This solution provides student advice for Deakin University in Australia at any time of day throughout 365 days of the year (Deakin University 2014). Even if it is based on algorithms suitable to fulfill repetitive and relatively predictable tasks, Watson's use is an example of the future impact of AI on the administrative workforce profile in higher education. This is changing the structure for the quality of services, the dynamic of time within the university, and the structure of its workforce. A supercomputer able to provide bespoke feedback at any hour is reducing the need to employ the same number of administrative staff previously serving this function. In this context, it is also important to note that 'machine learning' is a promising field of artificial intelligence. While some AI solutions remain dependent on programming, some have an inbuilt capacity to learn patterns and make predictions. An example is AlphaGo-a software developed by DeepMind, the AI branch of Google's-that was able to defeat the world's best player at Go, a very complex board game Results and discussionAs AI solutions have the potential to structurally change university administrative services, the realm of teaching and learning in higher education presents a very different set of challenges. Artificial intelligence solutions relate to tasks that can be automated, but cannot be yet envisaged as a solution for more complex tasks of higher learning. The difficulty of supercomputers to detect irony, sarcasm, and humor is marked by various attempts that are reduced to superficial solutions based on algorithms that can search factors such as a repetitive use of punctuations marks, use of capital letters or key phrases For example, we can remember that the enthusiastic and unquestioned trust in the AI capabilities of a revolutionary new car led on May 2016 to the death of the driver, when the car set on 'autopilot' went underneath a tractor-trailer that was not detected by the software There is consistent evidence-some presented in this paper-that AI solutions open a new horizon of possibilities for teaching and learning in higher education. However, it is important to admit the current limits of technology and admit that AI is not (yet) ready to replace teachers, but is presenting the real possibility to augment them. We are now seeing computing algorithms impacting on the most mundane aspects of daily life, from individuals' credit scores to employability. Higher education is placed at the center of this profound change, which brings with it both extraordinary opportunities and risks. This important crossroad requires careful consideration and analysis from an academic perspective, especially as we can find tendencies to look at technological progress as a solution or replacement for sound pedagogical solutions or good teaching. The real potential of technology in higher education is-when properly used-to extend human capabilities and possibilities of teaching, learning, and research. The purpose of this paper is to kindle scholarly discussions on the evolving field of artificial intelligence in higher education. This stays aligned with some of the most ambitious research agendas in the field, such as the \"National Artificial Intelligence Research and Development Strategic Plan,\" released by the US President Barack Obama in October 2016. The Report states that \"the walls between humans and AI systems are slowly beginning to erode, with AI systems augmenting and enhancing human capabilities. Fundamental research is needed to develop effective methods for human-AI interaction and collaboration\" (U.S. National Science and Technology Council 2016).As we note that significant advances in machine learning and artificial intelligence open new possibilities and challenges for higher education, it is important to observe that education is eminently a human-centric endeavor, not a technology centric solution. Despite rapid advancements in AI, the idea that we can solely rely on technology is a dangerous path, and it is important to maintain focus on the idea that humans should identify problems, critique, identify risks, and ask important questions that can start from issues such as privacy, power structures, and control to the requirement of nurturing creativity and leaving an open door to serendipity and unexpected paths in teaching and learning. The hype on AI can lead to an unquestioned panacea that can leave many who are on their path to higher learning under the wheels of reality, such as that tragic event of the driver led under a truck by what was considered to be a matchless software. Maintaining academic skepticism on this issue is especially important in education, as this is an act that can be reduced to information delivery and recollection; we need to maintain its aim to build educated minds and responsible citizens that are attached to general values of humanism.The role of technology in higher learning is to enhance human thinking and to augment the educational process, not to reduce it to a set of procedures for content delivery, control, and assessment. With the rise of AI solutions, it is increasingly important for educational institutions to stay alert and see if the power of control over hidden algorithms that run them is not monopolized by tech-lords. Frank Pasquale notes in his seminal book 'The Black Box Society' that \"Decisions that used to be based on human reflection are now made automatically. Software encodes thousands of rules and instructions computed in a fraction of a second\" At the same time, the rapid advancements of AI are doubled by the effort of defunded universities to find economic solutions to balance depleted budgets. AI already presents the capability to replace a large number of administrative staff and teaching assistants in higher education. It is therefore important to explore the effects of these factors on learning in higher education, especially in the context of an increasing demand for initiative, creativity, and 'entrepreneurial spirit' for graduates. This paper opens an inquiry into the influence of artificial intelligence (AI) on teaching and learning and higher education. It also operates as an exploratory analysis of literature and recent studies on how AI can change not only how students learn in universities, but also on the entire architecture of higher education.The rise of artificial intelligence and augmentation in higher educationThe introduction and adoption of new technologies in learning and teaching has rapidly evolved over the past 30 years. Looking through the current lens, it is easy to forget the debates that have raged in our institutions over students being allowed to use what are now regarded as rudimentary technologies. In a longitudinal study of accommodations for students with a disability conducted between 1993 and 2005 in the USA, authors remind us of how contentious the debate was surrounding the use of the calculators and spell check programs for students with a disability none-the-less the general student body Moreover, artificial intelligence (AI) is now enhancing tools and instruments used day by day in cities and campuses around the world. From Internet search engines, smartphone features and apps, to public transport and household appliances. For example, the complex set of algorithms and software that power iPhone's Siri is a typical example of artificial intelligence solutions that became part of everyday experiences Personalized solutions are also closer than we imagined: 'new scientist' presented at the end of 2015 the initiative of Talkspace and IBM's Watson to use artificial intelligence in psychotherapy Students are placed now at the forefront of a vast array of possibilities and challenges for learning and teaching in higher education. Solutions for human-AI interaction and collaboration are already available to help people with disabilities. They can inspire educators to apply them in education to augment learners and teachers for a more engaging process. Carl Mitcham describes in his Encyclopedia of Science, Technology and Ethics a cyborg as \"a crossbreed of a human and a machine\" This type of human-machine interface presents the immediate potential to change the way we learn, memorize, access, and create information. The question of how long it will take to use this type of interface to enhance human memory and cognition is one which we are currently unable to answer. It may turn to reality beyond the end of this century, as the MIT scholar suggests or much sooner when we consider the pace of change in the technologies used in teaching and learning since 2007 when the first iPhone was launched. Since then, not only has the iPhone integrated breakthrough technologies that seemed impossible just a few years ago to how we access and use information (such as fingerprint identification and the 'intelligent' Siri assistant), but this technology has introduced a significant cultural shift that impacts on our everyday lives. Either way, if we shift the focus of 'cyborgs' from science-fiction to the idea of computer augmented capacity for teachers and students alike, it is not unrealistic to consider that cyborgs-or 'crossbreeds' of human and machines-will soon be a reality in teaching and research in universities of the near future.The impact of artificial intelligence is already visible in the world economy and has captured the attention of many analysts. The largest investment ever made by Google in the European Union is the acquisition in 2014 of DeepMind technologies, with $400 million. DeepMind Technologies, now named Google DeepMind, is a London-based artificial intelligence startup specialized in machine learning and advanced algorithms. Notably, Google also made significant investments in the German Research Centre for Artificial Intelligence (DFKI GmbH), which is, according to their website, \"the biggest research center worldwide in the area of Artificial Intelligence and its application, in terms of number of employees and the volume of external funds\" (DFKI 2015). Tech giants like Apple, Google, Microsoft, and Facebook currently compete in the field of artificial intelligence and are investing heavily in new applications and research. Google announced in December 2015 that the company's quantum computer called D-Wave 2X will be used for complex operations of AI, generically referred to as optimization problems This wave of interest and investments in artificial intelligence will soon impact on universities. Most likely, financial pressures related to the large numbers of students currently undertaking higher education driven by the goal of democratization of higher education, and the international student market will stand as a compelling reason to seek out AI solutions. The 'outsourcing' of the academic workforce, in terms of numbers of academics employed and tenured positions, is now open to a massive takeover by intelligent machines As examples presented in previous page show, the \"crossbreed\" of the human brain and a machine is already possible, and this will essentially challenge teachers to find new dimensions, functions, and radically new pedagogies for a different context for learning and teaching. For example, brain-computer interfaces (BCIs), that captured the imagination of researchers across the world, are currently recording significant advances. Using brain signals with various recording and analysis methods, along with innovative technological approaches for new computing systems, specialists in the field now provide feasible solutions to remotely control software with a brain-computer interface Past lessons, possibilities, and challenges of AI solutionsWidening participation in higher education and the continuous increase in the number of students, class sizes, staff costs, and broader financial pressures on universities makes the use of technology or teacherbots a very attractive solution. This became evident when massive open online courses (MOOCs) enlightened the imagination of many university administrators. The understanding of \"open courses\" is that no entry requirements or fees were required, and online students could enroll and participate from any country in the world with internet access. Both of these factors enabled universities to market globally for students, resulting in massive enrolment numbers. The promise was generous, but it soon became evident that one of the problems created for teachers was their human capacity to actively engage with massive numbers of diverse students studying globally from different time zones, at different rates of progress and with different frames of reference and foundational skills for the course that they are studying. Assisting students in large classes to progress effectively through their learning experience to achieve desired outcomes, conduct assessments, and provide constructive personalized feedback remained unsolved issues. Sian Bayne makes the observation in Teacherbot: Interventions in Automated Teaching, that the current perspective of using automated methods in teaching \"are driven by a productivity-oriented solutionism,\" not by pedagogical or charitable reasoning, so we need to re-explore a humanistic perspective for mass education to replace the \"cold technocratic imperative\" The lesson of MOOCs is important and deserves attention. Popenici and Kerr observed that MOOCs were first used in 2008 and since then: \"\u2026we have been hearing the promise of a tsunami of change that is coming over higher education. It is not uncommon with a tsunami to see people enticed by the retreat of the waters going to collect shells, thinking that this is the change that is upon them. Tragically, the real change is going to come in the form of a massive wave under which they will perish as they play on the shores. Similarly, we need to take care that we are not deluded to confuse MOOCs, which are figuratively just shells on the seabed, with the massive wall of real change coming our way\" There are solid arguments-some cited above in this paper-to state that it is more realistic to consider the impact of machine learning in higher education as the real wave of change. In effect, lessons of the past show why it is so important to avoid the same mistakes revealed by the past fads or to succumb to a convenient complacency that is serving only the agenda of companies that are in search of new (or bigger) markets. Online learning proved very often the potential to successfully help institutions of higher education reach some of the most ambitious goals in learning, teaching, and research. However, the lesson of MOOCs is also that a limited focus on one technology solution without sufficient evidence-based arguments can become a distraction for education and a perilous pathway for the financial sustainability of these institutions.Higher education is now taking its first steps into the unchartered territory of the possibilities opened by AI in teaching, learning, and higher education organization and governance. Implications and possibilities of these technological advances can already be seen. By way of example, recent advancements in non-invasive brain-computer interfaces and artificial intelligence are opening new possibilities to rethink the role of the teacher, or make steps towards the replacement of teachers with teacher-robots, virtual \"teacherbots\" This enlightened the imaginations of many, reaching international news across the world and respected media outlets such as The New Your Times or The Washington Post. However, we must be careful when we see the temptation to equate education with solutions provided by algorithms. There are widespread implications for the advancement of AI to the point where a computer can serve as a personalized tutor able to guide and manage students' learning and engagement. This opens to the worrying possibility to see a superficial, but profitable, approach where teaching is replaced by AI automated solutions. Especially as we are at a point where we need to find a new pedagogical philosophy that can help students achieve the set of skills required in the twenty-first century for a balanced civic, economic, and social life. We have a new world that is based on uncertainty and challenges that change at a rapid pace, and all this requires creativity, flexibility, the capacity to use and adapt to uncertain contexts. Graduates have to act in a world of value conflicts, information limitations, vast registers of risks, and radical uncertainty. All this, along with the ongoing possibility of staying within personal and group 'bubbles' of and being exposed to vast operations of manipulation require a new thinking about the use of technology in education and a new set of graduate attributes. As advanced as AI solutions may become we cannot yet envisage a future where algorithms can really replace the complexity of human mind. For certain, current developments show that it is highly unlikely to happen in the next decade, despite a shared excessive optimism. The AI hype is not yet double by results; for example, Ruchir Puri, the Chief Architect of Watson, IBM's AI supercomputer, recently noted that \"There is a lot of hype around AI, but what it can't do is very big right now. What it can do is very small.\"This reality may encourage policy-makers and experts to reimagine institutions of higher education in an entirely new paradigm, much more focused on imagination, creativity, and civic engagement. With the capacity to guide learning, monitor participation, and student engagement with the content, AI can customize the 'feed' of information and materials into the course according to learner's needs, provide feedback and encouragement. However, teachers can use this to prepare students to a world of hyper-complexity where the future is not reduced to the simple aim of 'employability.' Teacherbots are already presenting as a disruptive alternative to traditional teaching staff, but it is very important to inquire at this point how do we use them for the benefit of students in the context of a profound rethink of what is currently labeled as 'graduate attributes' Even if in 2017 we find little and exploration of what is a teacherbot and what their capabilities are possible now and in a predictable future, AI technology has slipped into the backdoor of all our lives and this is imposing a much more focused research in higher education. AI solutions are currently monitoring our choices, preferences, movements, measuring strengths, and weaknesses, providing feedback, encouragement, badges, comparative analytics, customized news feeds, alerts, predictive text, so they are project managing our lives. At this point, we can see a teacherbot as a complex algorithmic interface able to use artificial intelligence for personalized education, able to provide bespoke content, supervision, and guidance for students and help for teachers. Teacherbots are defined as any machine-based software or hardware that assumes the role traditionally performed by a teacher assistant in organizing information and providing fast answers to a wide set of predictable questions; it can be facilitating, monitoring, assessing, and managing student learning within the online learning space. These solutions are closer than many academics may think. Tinkering with the old system of transmitting information to passive students, in class or in front of computers, is open to disruption from a highly personalized, scaleable, and affordable alternative AI solutions, such as 'Jill Watson.' While contact time and personal guidance by faculty may be should be retained not only in some elite institutions of higher education, as this will define the quality of education, but intelligent machines can be used by all to meet the learning and support needs of massive numbers of students.Conclusion", "conclusions": "The rise of AI makes it impossible to ignore a serious debate about its future role of teaching and learning in higher education and what type of choices universities will make in regard to this issue. The fast pace of technology innovation and the associated  job displacement, acknowledged widely by experts in the field (source), implies that teaching in higher education requires a reconsideration of teachers' role and pedagogies. The current use of technological solutions such as 'learning management systems' or IT solutions to detect plagiarism already raise the question of who sets the agenda for teaching and learning: corporate ventures or institutions of higher education? The rise of techlords and the quasi-monopoly of few tech giants also come with questions regarding the importance of privacy and the possibility of a dystopian future. These issues deserve a special attention as universities should include this set of risks when thinking about a sustainable future.Popenici and Kerr Research and Practice in Technology Enhanced Learning (2017) 12:22Moreover, many sets of tasks that are currently placed at the core of teaching practice in higher education will be replaced by AI software based on complex algorithms designed by programmers that can transmit their own biases or agendas in operating systems. An ongoing critique and inquiry in proposed solutions stay critical to guarantee that universities remain institutions able to maintain civilization, promote, and develop knowledge and wisdom.In effect, now is the time for universities to rethink their function and pedagogical models and their future relation with AI solutions and their owners. Furthermore, institutions of higher education see ahead the vast register of possibilities and challenges opened by the opportunity to embrace AI in teaching and learning. These solutions present new openings for education for all, while fostering lifelong learning in a strengthened model that can preserve the integrity of core values and the purpose of higher education.We consider that there is a need for research on the ethical implications of the current control on developments of AI and the possibility to wither the richness of human knowledge and perspectives with the monopoly of few entities. We also believe that it is important to focus further research on the new roles of teachers on new learning pathways for higher degree students, with a new set of graduate attributes, with a focus on imagination, creativity, and innovation; the set of abilities and skills that can hardly be ever replicated by machines.", "SDG": [4]}, "improving_social_skills_in_children_with_asd_using_a_long_term_in_home_social_robot": {"name": "Improving social skills in children with ASD using a long-term, in-home social robot", "abstract": " Social robots can offer tremendous possibilities for autism spectrum disorder (ASD) interventions. To date, most studies with this population have used short, isolated encounters in controlled laboratory settings. Our study focused on a 1-month, home-based intervention for increasing social communication skills of 12 children with ASD between 6 and 12 years old using an autonomous social robot. The children engaged in a triadic interaction with a caregiver and the robot for 30 min every day to complete activities on emotional storytelling, perspective-taking, and sequencing. The robot encouraged engagement, adapted the difficulty of the activities to the child's past performance, and modeled positive social skills. The system maintained engagement over the 1-month deployment, and children showed improvement on joint attention skills with adults when not in the presence of the robot. These results were also consistent with caregiver questionnaires. Caregivers reported less prompting over time and overall increased communication.", "keywords": "", "introduction": "Autism spectrum disorder (ASD) is a neurodevelopmental condition characterized by social interaction and communication deficits and the presence of restricted, repetitive patterns of behavior . Children and adults with ASD often have difficulty in responding to social overtures, recognizing the emotional states of others from visual or auditory cues, and understanding the importance of gaze as a social cue (1). Therapies are diverse, but are typically time-, resource-, and laborintensive, and can put substantial strain on families and caregivers (2).(3)Technology-based interventions, and robotics in particular, for ASD have been seen as a potential approach for augmenting the efforts of families and clinicians to provide on-demand, personalized, social skills training . The robots envisioned by these efforts are part of a new field called socially assistive robotics, which aims to construct systems that support social and cognitive growth by using social rather than physical means (4)(5)(6). These robots share characteristics of educational robots, which attempt to convey information typically via a tutor-student relationship (7), and rehabilitation robots, which provide structured physical therapy for deficits such as stroke and paralysis (8).(9)Exploratory studies from dozens of research groups have shown that many individuals with ASD enjoy interacting with robots and, in many cases, even demonstrate more appropriate social behaviors with robots than they do with peers or caregivers (10,. These initial exploratory studies focused on short interactions, spanning tens of minutes or less, under controlled laboratory or clinical conditions, often involving sample sizes of five children or fewer, and exclusively on robot-directed behavior 11). Although these studies generated considerable excitement, they held little clinical value.(7)Results tended to fade with repeated exposures and may have been the result of novelty, appropriate control conditions were rarely considered, and experiments failed to demonstrate learning that generalized to human-directed actions . A few studies did examine longer-term interactions (12) or demonstrated improved adult-directed social behavior (13), but none was able to demonstrate skill acquisition that could be considered clinically meaningful that generalized beyond the specific robot encounter.(14)We report here a demonstration of directly assessed improvements in social skills in children with ASD after an in-home, 1-month intervention in which daily social skills games were conducted by an autonomous, socially assistive robot (Fig. ). This study differs from previous work in this domain in four important aspects. First, this study used a fully autonomous robot system that operates for a 1-month deployment duration with no adjustments made by clinical or research staff. Many socially assistive robots still operate under teleoperative control, because autonomous operation for this duration is a substantial challenge in the robotics community even when static program requirements are used throughout the deployment 1. Second, unlike previous work where predefined protocols are followed explicitly (6), the system used here must adapt to the strengths and weaknesses of the individual child by changing the difficulty of individual tasks based on the child's preferences and performance. Because individuals with ASD have substantial individual differences in the type and severity of their social skill deficits, the need to adapt to an individual child is essential to enabling a positive learning outcome. Further, the interaction between the need for autonomy and the need for adaptation creates additional technical challenges. Third, this study provided therapy directly in homes with a fully autonomous robot. Whereas clinical and laboratory spaces represent known environmental conditions that can be controlled or explicitly planned for, the unconstrained home environment requires more complex sensing and behavioral routines to deal with greater variation in environmental conditions. Last, this study focused primarily on demonstrations of clinically meaningful measures of performance using standard evaluation metrics that are conducted by an independent assessor away from the robot. This represents a challenging evaluation standard, because a child must not only learn a skill while practicing with the robot but also be capable of generalizing that skill to interactions with an adult in an environment that differs from the practice games used by the robot.(15)", "body": "Autism spectrum disorder (ASD) is a neurodevelopmental condition characterized by social interaction and communication deficits and the presence of restricted, repetitive patterns of behavior Technology-based interventions, and robotics in particular, for ASD have been seen as a potential approach for augmenting the efforts of families and clinicians to provide on-demand, personalized, social skills training Exploratory studies from dozens of research groups have shown that many individuals with ASD enjoy interacting with robots and, in many cases, even demonstrate more appropriate social behaviors with robots than they do with peers or caregivers Results tended to fade with repeated exposures and may have been the result of novelty, appropriate control conditions were rarely considered, and experiments failed to demonstrate learning that generalized to human-directed actions We report here a demonstration of directly assessed improvements in social skills in children with ASD after an in-home, 1-month intervention in which daily social skills games were conducted by an autonomous, socially assistive robot (Fig. RESULTSOur robot-assisted intervention included a 30-min session every day for 30 days and involved triadic interactions among the social robot, the child, and the caregiver, providing opportunities for the child to interact and share experiences with the caregiver (Fig. Participant informationFourteen families with a child with ASD enrolled in this study. Two families withdrew, one due to unrelated health problems of a care-giver and one due to technical difficulties with the robot installation. Among the 12 families who finished the study, five of the children with ASD were females and seven were males. Caregivers reported that all 12 children identified racially as white and 4 of the 12 as Hispanic or Latino. These participants' age ranged from 6 to 12 years old [mean (M) = 9.02, SD = 1.41]. All had nonverbal intelligent quotient scores of \u226570 as determined by the Differential Ability Scales (DAS; M = 94.17, SD = 20.06) All participants were recruited from a large database of children with ASD who have either participated in previous research studies with our laboratory or expressed interest in participation. Eligible families were contacted via email to inquire about their interest in participating. Given the scope of the project, the first 12 eligible families were enrolled. Inclusionary criteria were (i) age between 4 and 12 years old, (ii) good medical health, (iii) cooperative with testing, (iv) English is a language spoken in the family, and (v) having been diagnosed with ASD and meet the characterization cutoffs described above. Exclusionary criteria were (i) a fragile health status and (ii) suspected or diagnosed hearing loss or visual impairment or diagnosed neurological abnormality significantly affecting visual or auditory acuity.All children in the study were enrolled in school programming full time and received intensive special education services as consistent with the state standards for educating children with ASD. Because the scope and form of these services and therapies varied substantially across participants based not only on their individual needs but also on family preferences and local resource availability, we used a single-subject withdrawal design (ABA) that allowed each child to serve as their own control (see Materials and Methods for details). Caregivers were instructed to maintain consistent intervention services during their participation in the study.Engagement and skills performanceA total of 127 hours of data was collected from the interaction between the 12 children, the robot, and their caregivers. These data included video and audio data, head orientation of both child and caregiver, interaction logs containing the robot utterances and actions, game logs for the tablet-based games, and caregiver survey responses. Because our primary study design was focused on showing the efficacy of this intervention, we focus in this paper on the analysis of child social performance as measured by game performance, caregiver reports, and clinical measures.  The robot adapted the difficulty of each individual game based on the child's history of performance in each skill set. On the emotion-understanding game \"Story,\" 86% of children reached the most difficult level of the game by the last session. On the perspective-taking games, 58 and 100% of children reached the highest level on \"Rocket\" and \"House,\" respectively. On the sequencing and ordering game \"Train,\" 67% of the children reached the highest level. The \"Spaceship\" and \"Traveler\" games used only a single difficulty level and were excluded from this analysis.Binomial generalized linear mixed models (Fig. Joint attentionPerformance on the joint attention probe was measured and recorded at four time points: (i) T 0 , 30 days before intervention began; (ii) T 1 , on the first day of robot intervention; (iii) T 2 , on the last day of intervention; and (iv) T 3 , 30 days after the end of the intervention. The difference between time points T 0 and T 1 was computed to measure change in joint attention during a period of time with no robot intervention and is denoted as the pretest. The difference between time points T 1 and T 2 was calculated to measure joint attention changes resulting from the robot-administered intervention and is denoted as the test phase. Last, the difference between time points T 1 and T 3 was evaluated to measure the stability of any changes recorded during the robot-administered intervention and is denoted as the posttest.Two participants were excluded for lack of data at one or more time points. Another participant was excluded for being out of the age range in which the task was normed (7 to 12 years of age). Group means were as follows: for T 0 , M = 16.89 and SD = 4.46; for T 1 , M = 15.67 and SD = 3.81; for T 2 , M = 20.89 and SD = 3.79; and for T 3 , M = 18.22 and SD = 5.02. A linear mixed model with compound symmetry repeated covariance effects indicated a significant time point effect [F(3,24) = 5.03, P = 0.008]. Planned comparisons showed that, although no pretest or posttest effect was observed (T 1 \u2212 T 0 , P = 0.395; T 3 \u2212 T 1 , P = 0.083), joint attention improvements occurred in the test phase (T 2 \u2212 T 1 , P = 0.001; Fig. Fig. 3. Robot-initiated joint attention.The robot models appropriate social gaze behavior by demonstrating context-contingent gaze and facilitates mutual gaze and experience sharing between the child and the caregiver. When the child is engaged with the robot (A), the robot directs the child's attention to relevant task content on the screen (B). As the child's attention shifts to the robot-directed focus on the screen, the robot then attempts to redirect gaze to the caregiver (C) in the hope of redirecting the child's visual attention to the caregiver (D). (These demonstration images were recreated in the laboratory to show both robot and child behavior because this perspective was not recorded by the deployed system.) Caregivers' surveyCaregivers completed an on-screen survey immediately after each day's intervention session during the test phase. In all but one family, these interactions were conducted with the same caregiver (one father, one grandmother, and nine mothers).The survey consisted of five-point Likert scale ratings. The questions were grouped into two categories: questions on how children interacted with caregivers during the past 24 hours, parallel questions about interactions with other people, and one final question regarding engagement. We compared the ratings scored by the caregivers on the first day and the last day of interventions with paired sample t tests. All 12 caregivers' responses were included in the analysis.Caregivers reported increased social skill performance between their child and themselves, including more eye contact [t( DISCUSSION", "conclusions": "The potential benefit of a socially assistive robot lies in the ability to provide personalized, on-demand, and structured cognitive or social support to augment the efforts of clinicians, teachers, and families. In the ideal case, robots could provide personalized support, whenever and wherever needed, and could be capable of producing lasting enhancements in social and communicative skills not only in humanrobot interactions but also in human-human interactions (4). The system presented here takes steps in this direction beyond the current state of the art but also does not yet live up to all of these grand visions. We focus our discussion around the points in which the current work makes substantial improvements and also describe the limitations and areas requiring continued focus as this field progresses.", "SDG": [4]}, "international_information_management_corporation": {"name": "Ambient Learning -Knowledge as a Service Model: Towards the Achievement of Sustainable Development Goal Four", "abstract": " Four is yet to be achieved. This paper presents an artefact named \"Ambient learning-Knowledge as a Service model\" for describing how actionable knowledge can be extracted from ambient learning systems to support improvement and consequently facilitate the achievement of Sustainable Development Goal Four. A creative process was adopted to guide the development of the model. The process involved carrying out problem analysis through literature review, designing the model by combining ambient learning and Knowledge as a Service concepts and demonstrating its application by developing a prototype. Evaluation results revealed that C4.5 algorithm that is implemented in Waikato Environment for Knowledge Analysis (WEKA) software is suitable for extracting knowledge from ambient learning systems while Swi-prolog software can be applied to create a tool for knowledge delivery.", "keywords": "Knowledge as Service,knowledge extraction,ambient learning,knowledge consumers", "introduction": "The 2030 agenda for Sustainable Development which was adopted by the United Nations General Assembly on 25 September 2015 seeks to leave no one behind as it aspires to transform the world we live in . Among the 17 goals that were specified in the agenda, Goal four seeks to ensure inclusive and quality education for all and to promote lifelong learning. However, studies reveal that this goal is yet to be fully realized. For instance, results from Kenya National Adult Literacy Survey show that only 61.5% of the adult and out-of-school youth above 15 years have attained minimum literacy level leaving 38.5% (7.8 million) adults illiterate. A majority of these are individuals from less fortunate backgrounds who have a limited chance of attaining quality education owing to lack of reading material and other resources. There also exists gender disparity in literacy levels with men rated at 64.1% and women at 58.9% [1].[2]Ambient learning promises to bridge this gap by allowing personalized access to high quality learning content from anywhere, anytime and anyhow . Nevertheless, the existing ambient learning models do not describe how knowledge from ambient learning systems can be extracted to support improvement of such systems. Therefore, there is need to initiate new mechanisms and paradigms for describing how knowledge can be retrieved and applied to enhance ambient learning systems.[3]Knowledge as a Service (KaaS) has been hailed as the new paradigm for acquiring knowledge through the cloud to support knowledge management . Old knowledge management technology cannot cut it in this new age of open data and big data environments. The potential capability of KaaS paradigm presents a unique opportunity for combining it with ambient approaches to continuously improve quality of education services offered by ambient learning systems. Despite such an opportunity, little or no research has been undertaken to explore appropriate model(s) that can be derived from both KaaS and ambient learning concepts in order to enhance the achievement of sustainable development goal four (SDG 4).[4]", "body": "The 2030 agenda for Sustainable Development which was adopted by the United Nations General Assembly on 25 September 2015 seeks to leave no one behind as it aspires to transform the world we live in Ambient learning promises to bridge this gap by allowing personalized access to high quality learning content from anywhere, anytime and anyhow Knowledge as a Service (KaaS) has been hailed as the new paradigm for acquiring knowledge through the cloud to support knowledge management ObjectivesThe main purpose of our study was to establish an appropriate model that describes how knowledge can be extracted from ambient learning systems and be used for improving such systems towards achieving SDG 4. Specific objectives of this work included the following: 1. To establish an appropriate algorithm for extracting knowledge from a typical ambient learning system. 2. To demonstrate how extracted knowledge can be delivered to the relevant knowledge consumers.Research MethodologyTo achieve the main objective of our study, the \"creative process\" Technology DescriptionAmbient Learning OverviewAccording to Open Mobile Ambient Learning (OMAL) However, there are two knowledge gaps in the reviewed literature. First, system architectures for illustrating how FIAL and HIAL can be implemented are lacking. Second, OMAL system architecture does not describe how knowledge can be extracted from the context database for the purpose of continuous improvement of ambient learning system. Therefore, there is need for innovative models that can be used to address the two gaps.Knowledge as a Service (KaaS) Paradigm.Traditional stand-alone platforms of client and server architecture are too rigid to keep pace with changes in knowledge management but cloud computing solves this challenge efficiently and effectively Figure Knowledge Extraction AlgorithmsIn recent years, many algorithms in the literature have been created for extracting knowledge in the data. Examples are 10 artificial intelligence algorithms that have been identified that have been identified by the IEEE International Conference on Data Mining  The OMAL system architectue. Adopted from DevelopmentsTo address this gap, we propose an ambient learning-KaaS model that can be used to continually enhance ambient learning systems. The framework is derived from (a) Data owners access data from ambient learning system through context database.They can freely utilize the data and are responsible for ensuring its security. An example is the learning institution that provides the data storage infrastructure. (b) KaaS provider is made up of two sub-components. First, a knowledge extractor in the form of a data mining algorithm that processes and transforms the data from the Knowledge server ambient learning system context database and finds the patterns hidden in the data. Second, a knowledge server that is used to deliver knowledge to knowledge consumers e.g. knowledge based systems (KBS). (c) Knowledge consumer is any entity that consumes the services offered by the KaaS provider under specific service level agreements (SLA). In order to demonstrate feasibility of ambient learning-KaaS model, proof of concept was developed. KCA University, which acted as data owner provided data collected from ambient learning system that was piloted in 2014 by ResultsAmong the identified 10 ten data mining algorithms, C4.5 constructs decision trees that can readily be grasped by non-specialists like some of the knowledge consumers in ambient learning environment Evaluation results showed that, out of 34 instances covered by this rule, none of them was misclassified.Having extracted knowledge from ambient learning system, the next step was to create knowledge server (KS) for delivering knowledge to knowledge consumers. For the purpose of our study, prolog was used to demonstrate how stand alone knowledge server can be implemented. The server was made up of the following three main components: 1. Inference engine that is already inbuilt within prolog. 2. Knowledgebase component that was used for storing rules and facts extracted from ambient learning system. Figure BenefitsThe adoption of the proposed model is expected to support continuous enhancement of existing ambient learning systems towards the realization of SDG goal 4. Therefore, the model will not only help revolutionize learning but also bring ambient learning to the forefront as one of the most efficient tools for providing inclusive and quality education for all. Additionally, incorporating an interactive component in KAAS can help to crowd source knowledge that may be useful in size or time units to knowledge consumers in ambient learning environments.Conclusions and Future Work", "conclusions": "This study proposed a new model that was derived from a combination of ambient learning concept and KaaS with an aim of supporting the achievement of United Nations SDG 4.Results revealed that the model can be implemented using an appropriate data mining tool to extract knowledge from ambient learning system and using swi-prolog to create stand knowledge server. The extracted knowledge can then be used by the ambient learning system experts to improve an existing or new ambient learning system.However, the study has three limitations. First, the proposed model was evaluated using only data collected from a case of MIAL approach. That is, the OMAL system that was piloted in KCA University. Second, among the identified top ten knowledge extraction algorithms, only C4.5 was used to demonstrate the application of the proposed model. Third, implementation of knowledge server was demonstrated using only one artificial intelligence tool. That is, swi-prolog that use predicate logic to represent knowledge.Therefore, future research activities that are aimed to extend ate extending this work can focus on three main areas. That is, (i) Evaluating the model using data from instantiations FIAL and HIAL, (ii) Exploring other data mining algorithms that can handle different sets of data for faster and easier dissemination of knowledge, and (iii) Explore other programming tools in the field of artificial intelligence that can facilitate implementation of a knowledge server with a more user friendly interface.", "SDG": [4]}, "is_there_a_gender_difference_in_interacting_with_intelligent_tutoring_system_can_bayesian_knowledge_tracing_and_learning_curve_analysis_models_answe": {"name": "Is there a gender difference in interacting with intelligent tutoring system? Can Bayesian Knowledge Tracing and Learning Curve Analysis Models answer this question?", "abstract": " Multiple studies have been conducted on Project LISTEN, an intelligent tutoring system (ITS) used to analyze educational learning through case analysis of students' interactions with ITS. Studies have defined the phenomenon by exploring 'what happens when/if' questions and analyzing these in the context of the specified phenomenon occurrence. While ITS often focus on student decisions regarding when and how to use the system's resources, we suggest further analysis and monitoring are needed to get the best results from these systems. In this study, we argue that boys interact differently with ITS than girls. This finding is evident in results from both the Bayesian Knowledge Tracing and Learning Curve Analysis models.", "keywords": "Intelligent,tutoring,systems,Bayesian,knowledge,tracing,Learning,curve,analysis", "introduction": "For almost a decade, faculty members from Western Kentucky University (WKU 1 ) have been designing a variety of intelligent systems, such as CaseGrader  and Hyper-ManyMedia (Crews & Murphy, 2007). In CaseGrader, (Zhuhadar & Nasraoui, 2008) used intelligent methods to provide personalized automated scoring to students based on their performance in solving mathematical or business problems within Microsoft Excel. On the other hand, the HyperManyMedia 2 platform provided recommendations to students based on their previous browsing activities; these recommendations were based on artificial intelligence algorithms where ontology is defined and semantic web is utilized to provide the most accurate recommendations to students based on their level within the course. Prior research describes this process in more detail Crews & Murphy (2007)(Zhuhadar, 2015;Zhuhadar, Carson, Daday, & Nasraoui, 2015;Zhuhadar & Nasraoui, ;Zhuhadar, Nasraoui, & Wyatt, 2007;Zhuhadar, Nasraoui, & Wyatt, 2009a, b;Zhuhadar, Nasraoui, Wyatt, & Romero, 2009;Zhuhadar, Nasraoui, Wyatt, & Yang, 2010;.Zhuhadar & Yang, 2012)However, in this research we utilize a platform (Project LISTEN 3 ) designed and developed by researchers at Carnegie Mellon University. 4 Many studies have been conducted on Project LISTEN (Huang & Mostow, 2015a, b;Mostow & Prieditis, 2014;, an intelligent tutoring system (ITS) used to analyze educational learning through case analysis of students' interactions. Many of these studies required specific details such as student choices, timing intervals, student outcomes (predictions), classifiers, and types of help given. The complexity of these details can make them laborious to browse and gather. Yuan, Chang, Taylor, & Mostow, 2014) address three simple techniques to make data mining easier to interpret, stating researchers can directly store interactions and index them into a database, which allows ease of access without the need to browse log files. (Beck, Chang, Mostow, & Corbett, 2008) also addressed a method to identify a tutorial event by linking the student, computer, and time interval together, as well as, restraining time intervals to define tutorial interactions within a hierarchical structure. The efficiency, generality, usability, and utility of a session browser aids in the successful facilitation of data mining efforts and will continue to be used in future research. Beck et al. (2008) looked at improving the analysis of intelligent learning systems by focusing on the cognitive model, a set of predefined rules that influence the process of student problem solving tailored to provide helpful feedback and hints while increasing difficulty to improve student learning and knowledge. In their research, three questions were asked regarding ITS: 1) how can researchers describe learning behavior in existing cognitive models; 2) can a learning rate be established for the student; and 3) how can the cognitive model be improved inexpensively by defining measures of complexity to improve the curriculum for various learning styles Cen, Koedinger, & Junker (2006). The researchers proposed the Learning Factor Analysis, a semiautomated method used in Java that combines statistics, human experience, and a combinatorial search (heuristic guidance) to add to tutor development, giving better insight into the analysis of data and log files through a knowledge-tracing algorithm (Cen et al., 2006).(Cen et al., 2006)While ITS often focus on learner control, the power of use is given to the student. Therefore, the student decides when and how to use the system's resources, essentially self-monitoring and judging when/if they can benefit from the help provided. , using the PACT geometry tutor, suggested that students often times do not have the required cognitive skills to take advantage of the resources available through the tutor. They argue for a meta-cognitive help-seeking model that can monitor the student's strategies in using resources provided from the tutor to lend the most support for on-request help or glossary access (Aleven & Koedinger, 2000). Results indicated that students used the tutor's intelligent help facilities (hints) more frequently than the non-intelligent resources (glossary). (Aleven & Koedinger, 2000) argue that the meta-cognitive helpseeking model could be implemented as a production rule model and could be used for model tracing, taking into account the student model information and whether a student might be overusing or under-using resources. With the meta-cognitive strategy, the tutor would make greater use of the glossary, a low cost resource, to find relevant information and apply it to the current problem. The tutor would also initiate intelligent help after two errors, thereby reducing the overall number of errors and the time required. They state that \"in order for an intelligent tutor system to be adaptive\", meta-cognitive skills must be taken into consideration to produce better learners Aleven & Koedinger (2000).(Aleven & Koedinger, 2000)Regarding response intervals,  further addresses the issue of time as an indicator and predictor of how much a student learns. Previous research conducted by Joseph (2005) indicated that students do not always try their best in solving problems; therefore, Aleven and Koedinger (2000) proposed an engagement tracing model that would better model student engagement by primarily focusing on disengagement. This approach would analyze the response times and correctness of responses to model overall engagement while using an intelligent tutor. The method is inexpensive and sensitive enough to detect temporal changes during the student's interactions with the tutor Joseph (2005). By modeling a student's engagement, research can predict how much an individual will benefit from using intelligent tutors, while allowing for modifications that adapt to student interactions, for greater learning efficiency.(Joseph, 2005)Lall e,  noted the challenge in evaluating student models by their impact on the success of an intelligent tutor's decision about which type of help to offer students. Individualized help can have a strong impact on learning; therefore, the better the tutor can adapt its help to the student and situation, the more likely the student will learn from it. Using logs of randomized tutorial decisions and ensuing student performance, Lall e et al. ( Mostow, Luengo, & Guin (2013)) trained a classifier to predict tutor decision outcomes (success or failure) based on situational features, such as student and task. Using historical data to simulate a policy by extrapolating its effects from the subset of randomized decisions that happened to follow the policy, the authors tested the method on data logged by Project LISTEN's reading tutor, which randomly chooses what type of help to give 2013. They also compared the impact of student models (knowledge-tracing model, constraint based model, and control based approach) on the expected success of tutorial decisions (greatest probability) to offer help. Using the learner policies to pick which type of help yields the greatest probability of success, taking into consideration the types of help available, student features, domain features and the student model, the measure has greater utility for measuring student learning. (Lall e et al., 2013) found that all learned policies tested improved the reading tutor's expected success compared to its original randomized decisions. Yet, this only applies to tutors that make decisions based on multiple types of available help. Furthermore, (Lall e et al., 2013) assessed learning decomposition to examine how much students learn from instruction. Learning decomposition determines the relative adequacy of different types of learning opportunities using a generalization of the learning curve analysis with non-linear regression. The authors suggested that students learn words better when they read a wide selection of stories rather than reading the same story multiple times (Beck & Mostow, 2008). Reading new stories, thereby expanding the exposure to words, is good for long-term learning.(Beck & Mostow, 2008)Beck and Mostow's model further indicated that when students reread words, the effectiveness of learning that word decreased, supporting the argument that students benefit less from mass practice . Individuals who benefited from mass practice and repeated reading were older, less proficient readers who were tagged as requiring learning support. As (2008) indicated, learning factor analysis, as noted earlier by (Beck & Mostow, 2008), is used to create better fitting learning curves; and learning decomposition (focused on determining impact of practice) is concerned with greater understanding of student learning potential. Gonz alez-Brenes & Mostow (2011) assessed the prediction value of models by using a regularized logistic regression, arguing that conventional classifier learners require large amounts of data to avoid over-fitting and do not generalize well to unseen examples (predictions). Using regularized logistic regression makes it feasible to classify dialogues in a high dimensional space and to demonstrate on real data from Project LISTEN's Reading Tutor (Gonz alez-Brenes & Mostow, 2011). One classifier predicts task completion to characterize differences in the behavior of children when they choose the story they read (71% accuracy), while another classifier (73.6% accuracy) infers who chose the story based on dialogue. Their approach solved two problems in classifying children's oral reading dialogue, predicting which stories they would finish and characterizing the student behavior according to who chose the stories. They achieved a 71.1% and a 73% cross validated classification accuracy on a balanced set of data from unseen students, indicating that regularized logistic regression is the best for assessing these problems in prediction (Gonz alez-Brenes Cen et al. (2006).& Mostow, 2011)As previous research has shown, several models have been tested to assess how and when students learn and whether or not tutor help is effective in increasing student learning. In assessing the findings, some researchers have called for a unified framework that simultaneously allows both the skills and impact of practice to vary . Some researchers have addressed the cost of analyzing ITS data (Beck & Mostow, 2008), suggesting an inexpensive EEG model that can assess information about comprehension, but noting that future work needs to increase prediction accuracy by assessing other dimensions of knowledge and applying those assessments to improve learning outcomes. Other research has examined modeling dialogue, arguing that high dimensional space opens the door from small manually generated sets of features to richer, automatically generated sets of features, thus improving the ability to assess student learning outcomes with regard to ITS (Gonz alez-Brenes (Yuan et al., 2014).& Mostow, 2011)Lall e et al. ( ) compared the impact of student models (knowledge tracing model, constraint based model, and control based approach) on the expected success of tutorial decisions to offer help through learner policies. These policies are vulnerable to under-covering and over-fitting; therefore, more accurate student models such as LR-DBN and stronger classifiers such as Support Vector Machines (SVM) or Random Forests should be used to improve the prediction accuracy of successful help in ITS. While there is still debate on what method works best, there seems to be consensus on the Bayesian Evaluation and Assessment approach 2013, which assesses both student and tutorial interventions, allowing students to transfer knowledge gained to later problems as a model for predicting learner outcomes and learner factor analysis (Beck et al., 2008). Furthermore, Beck et al.'s research on the two effects of help, scaffolding immediate performance and boosting actual learning, argues that evaluations of synthetic data are the most promising (Cen et al., 2006).(2008)During The 10th Annual LearnLab Summer Research School, 5 at Pittsburg Science of Learning Center, we acquired a large dataset from Project LISTEN. 6 This dataset consisted of students' knowledge tracing while interacting with the system. This data provided information about 90,000 student-word encounters. Each \"word\" is considered a skill that can be known, but that knowledge is hidden from us. We used Bayes Net Toolbox for Student Modeling (BNT-SM 7 ) to facilitate the use of dynamic Bayes nets in the student modeling community. In this study, we argue that boys interact with Project LISTEN's Intelligent Tutoring System differently than girls. This finding was not only evident by using Bayesian Knowledge Tracing but also by using Learning Curve Analysis. In the following paragraphs, we discuss our findings in detail.", "body": "For almost a decade, faculty members from Western Kentucky University (WKU 1 ) have been designing a variety of intelligent systems, such as CaseGrader However, in this research we utilize a platform (Project LISTEN 3 ) designed and developed by researchers at Carnegie Mellon University. 4 Many studies have been conducted on Project LISTEN While ITS often focus on learner control, the power of use is given to the student. Therefore, the student decides when and how to use the system's resources, essentially self-monitoring and judging when/if they can benefit from the help provided. Regarding response intervals, Lall e, Beck and Mostow's model further indicated that when students reread words, the effectiveness of learning that word decreased, supporting the argument that students benefit less from mass practice As previous research has shown, several models have been tested to assess how and when students learn and whether or not tutor help is effective in increasing student learning. In assessing the findings, some researchers have called for a unified framework that simultaneously allows both the skills and impact of practice to vary Lall e et al. ( During The 10th Annual LearnLab Summer Research School, 5 at Pittsburg Science of Learning Center, we acquired a large dataset from Project LISTEN. 6 This dataset consisted of students' knowledge tracing while interacting with the system. This data provided information about 90,000 student-word encounters. Each \"word\" is considered a skill that can be known, but that knowledge is hidden from us. We used Bayes Net Toolbox for Student Modeling (BNT-SM 7 ) to facilitate the use of dynamic Bayes nets in the student modeling community. In this study, we argue that boys interact with Project LISTEN's Intelligent Tutoring System differently than girls. This finding was not only evident by using Bayesian Knowledge Tracing but also by using Learning Curve Analysis. In the following paragraphs, we discuss our findings in detail.Comparative studyA challenge to evaluating ITS is to evaluate student models by their impact on the success of help given. Which type of help do students really need? Is the help better addressed on an individualized or mass practice basis? Prior research was conducted on Project LISTEN In our study, we use the same methods proposed by Gonz alez-Brenes & Mostow (2011) to assess student learning outcomes with regard to ITS. We also look at three approaches for evaluating this issue, including experimental trials, learning decomposition, and Bayesian Evaluation and Assessment; however, our goal is to examine if there is a significant difference in the way students interact with the tutoring system. More specifically, is there a gender difference in these interactions? If so, does this support the idea of gender-specific tutoring system designs?The sections that follow provide details of our framework, our research questions, our findings, and our future research.Proposed research framework and data analysisProject LISTEN 8 logs its interactions with children directly into a database. These interactions are archived at multiple grain sizes ranging from sessions to stories, from sentences to utterances, from individual words to mouse clicks and key presses. The Reading Tutor administers within-subject randomized controlled trials by selecting randomly among alternative tutorial actions; the resulting experiments have had as many as 180,000 trials. In addition, the logged data includes text and speech, with some gaze and EEG as well.We used the Session Browser to explore individual interactions and MySQL queries to aggregate them. Finally, we used tools such as Matlab and SAS to run statistical analysis and machine learning algorithms on the resulting data. We used various approaches to predict whether a child would finish a story or not. For instance, we analyzed the relative value of different types of reading practice for oral reading fluency; and we compared the efficacy of different types of help being used by each child on hard words.Dynamic Bayes Nets (DBNs) provide a powerful way to represent and reason uncertainty in time series data and are, therefore, well-suited to model a student's changing knowledge state during skill acquisition Generated outputEvidence.xls consists of Bayes Net data for each skill from all children. Evidence data are comprehensive. Hidden variables and missing observations are marked with NULL. For discrete variables, the values cannot be 0 because Matlab uses array subscripting (starting with 1). Therefore, we often increment the discrete variables by 1. In the case of a binary variable, 1 is used for 0 or false values, whereas 2 is used for 1 or true values, as shown in Table Param_table.xls consists of BNT-SM estimates for skill-specific models where a Bayes Net is trained for each skill in the training dataset, as shown in Table Inference_result.xls has a format identical to that of evidence.xls, except that BNT-SM performs inferences on the hidden variables and estimates their values as follows:For binary hidden variable X, the estimated value will be the probability of X \u00bc 1 (represented X \u00bc \u00bc 2 in Matlab). For discrete hidden variables Y with values greater than 2, the probability of all values will be output in the form of [p 1 ; p 2 ; \u2026 p n ;] By default, BNT-SM infers posterior probability (after observing the evidence) If instead, you want to infer prior probability (before observing the evidence, e.g. classic Knowledge Tracing), you can make a switch in RunBnet.m when calling inference_bnet.m, as shown in Table Proposed research frameworkFig. Research question?Is there a difference between girls (female) and boys (male) in their ways of interacting with the intelligent tutoring system?ExperimentsData setWe queried a dataset of student-word encounters (a balanced dataset of easy words and hard words). For complete results, refer to these links (male results, 10 female results 11 ).Building modelsWe used BNT-SM to generate these four training parameters for each student-word encounter: P(Know/Learn), P(already Knew), P(Guess), and P(Slip).Table We used the Wilcoxon signed-rank test instead of t-test since our observations are not independent. The Wilcoxon signed-rank test is a nonparametric test used to compare two sets of scores that come from the same participants. This can occur when we wish to investigate any change in scores from one point in time to another, or when individuals are subjected to more than one condition. For more information, refer to the resource below. 12 The Wilcoxon Scores of P(Know/Learn), P(already Knew), P(Guess), and P(Slip) are reported in Tables Table FindingsBayesian Knowledge TracingDespite the observation of females encountering more words, we found that the means and standard deviations do not appear to differ between the gender-specific model and the aggregated model. However, a difference is seen across genders within the gender-specific model, as shown in Table On the other hand, by looking at Wilcoxon Scores, seen in Table 1. The P_already Knew, P_Guess, P_Know/Learn for girls (female) are significantly higher than boys (male), and; 2. The P_Slip for boys (male) is significantly higher than girls (female).Learning curve analysisIn addition to investigating Bayesian Knowledge Tracing, we compared the Learning Curve Analysis for a specific word \"Different\" between male and female, as shown in Figs. We would assume that a boy would be more interested in reading a story about car races, whereas a girl would be more interested in reading princess stories. Knowing there is a difference does not mean that girls are better than boys regarding reading comprehension. Rather, our findings suggest that there is a difference in their way of learning; and we should embrace this difference to effectively promote greater reading comprehension by providing reading materials that would be of interest to them.Conclusion and future works", "conclusions": "The LearnLab Summer Research School 15 at the Pittsburgh Science of Learning Center provides researchers with opportunities to access large datasets from various projects dealing with, but limited to: cognitive psychology or educational psychology, computersupported collaborative learning, development of technologyenhanced course content, and analysis of student data or educational data mining. In this research, we used a large dataset generated by students using Project LISTEN. 16 This dataset consisted of students' knowledge tracing while interacting with the system. This data provided information about 90,000 student-word encounters. We found that boys (males) interact with Project LISTEN's ITS differently than girls (females). This finding was not only evident by using the Bayesian Knowledge Tracing but also by using Learning Curve Analysis. While ITS often focus on the student decisions regarding when and how to use the system's resources, we suggest further analysis and monitoring are required to get the best results from these systems. Considering gender differences in the way students learn, in addition to the context of the reading materials presented, is essential, especially for boys. In this study, we argue that boys learn vocabulary differently than girls. This finding was evident in both Bayesian Knowledge Tracing and in Learning Curve Analysis. Our future work will include an extension of the learning curve analysis considering Linear Mixed Effects Models.", "SDG": [4]}, "kim_baylor2016_article_research_baseddesignofpedagogi": {"name": "Research-Based Design of Pedagogical Agent Roles: a Review, Progress, and Recommendations", "abstract": " In this paper we review the contribution of our original work titled BSimulating Instructional Roles Through Pedagogical Agents^published in the International Journal of Artificial Intelligence and Education (Baylor and Kim in Computers and Human Behavior, 25(2), 450-457, 2005). Our original work operationalized three instructional roles as simulated through pedagogical agents, and demonstrated the effectiveness of these agent roles on learning and motivation. Since the publication of our work, pedagogical agent research has expanded its scope from the provision of intelligent guidance to a broad interest in agents' social and affective support for learners. We discuss current progress in pedagogical agent roles and capabilities, and speculate about the future of agent role design. We expect that optimizing the roles of artificial beings including on-screen agents and robots will continue to interest the educational technology community as these technologies continue to evolve. Pedagogical agents . Experimental research . Virtual humans . Interface design Overview of Original Paper Motivation In the early 2000s, there was some empirical support that the presence of a pedagogical agent facilitated deeper learning and enhanced motivation (Atkinson 2002; Moreno", "keywords": "", "introduction": "", "body": "illustrates how we operationalized the three roles with respect to appearance (image), nonverbal communication (animation), voice, affect and messaging (script).For an anthropomorphized agent to be effective, it was important that learners first perceive the agent as intended. We examined the effectiveness of these role instantiations in Experiment 1, with 78 undergraduate students learning computer literacy skills. Each student was randomly assigned to one of the three agent roles (Expert, Motivator, Mentor). In Experiment 2, we further examined the effectiveness of each of the three roles on actual motivation and learning outcomes, with 71 undergraduate pre-service teachers learning instructional design skills. Detailed descriptions of the methodology and dependent measures can be found in the original paper.For both studies, agent gender, lip synchronization, and script length were controlled to eliminate confounding effects. Additionally, the agent-learner interactions were designed so that each learner received all of the agent guidance. While this limited the Bintelligence^of the agents, it ensured that each learner had a similar experience and was necessary for the experimental approach.The Expert AgentThe Expert was designed to exhibit mastery in the field (e.g., The Motivator AgentThe Motivator was designed based on social modeling research that indicated the importance of learners' efficacy beliefs The Mentor AgentThe Mentor agent was designed to represent a guide or coach with advanced experience and knowledge to work collaboratively with the learners to achieve goals. As a mentor, this agent was designed to demonstrate competence to the learner while simultaneously developing a social relationship to motivate the learner FindingsOverall, the results of the two experiments revealed that the agent instructional roles were not only perceived by the students to reflect their intended purposes (Experiment 1), but also led to significant changes in learning and motivation, as designed (Experiment 2). Specifically, the motivational agents (Motivator and Mentor) were perceived as more human-like and led to improved learner self-efficacy. However, the affective encouragement and support were not sufficient for the learners to achieve the learning goals (i.e., to facilitate learning). The agents with expertise (Expert and Mentor) led to improved learning outcomes and were perceived as more credible and more facilitating learning. Of the two agents with expertise, the Mentor was perceived to be more engaging and also led to improved self-efficacy, thus having the overall best impact on learning and motivation. This finding paralleled desirable human instructors, in that both students and teachers desired good human instructors to be both supportive and knowledgeable in the domain Contributions, Impact, and LimitationsCore ContributionsWe believe that our original work made three important contributions: 1) providing an example of how three agent instructional roles could be systematically constructed and validated; 2) showing that non-intelligent agents could significantly impact the desired learning and motivational goals; and 3) confirming that learners perceived an agent as a whole persona and responded to the agent socially. Prior to our work, a number of pedagogical agents were employed in intelligent systems without systematic assessment of agent features on specific educational outcomes Often, being intelligent was essential in the definition of pedagogical agents in the research pioneered by the AI community Lastly, we intended to create the agents as whole beings that could approximate qualities of human instructors, as best as we could, within the limit of technologies at that time. We often observed that researchers and designers paid attention solely to media features of pedagogical agents (i.e., image, voice, animation, and non-verbal communication). They compared the impact of each feature, with the intent to single out the most effective media feature. Through our work, it was clear that the agents embodied with a role and persona successfully instilled a sense of human-like instructional presence and elicited social responses from college students, which may not be possible for a single media feature.Practical ImpactSince the publication of the original paper, researchers have increasingly implemented pedagogical agents with corresponding interest in how the agents can enact human-like roles within a learning system. Building upon this initial work, we found the splitpersona effect, indicating that splitting agent functionality into two distinct agent roles (i.e., Expert and Motivator) is preferable to combining into one agent role (Mentor) In addition, without a priori assumptions about agent roles from the perspective of grounded theory Next, given the effectiveness of the peer-like motivator, the use of a peer metaphor in agent design became popular, designing agents as learning companions or virtual peers. In subsequent research, we emphasized the motivating role of agent peers for students who were learning challenging topics In addition, the media features we used to define the agent roles gained attention from the agent research community. When carefully coordinated, these features might increase an agent's believability and naturalness. Several studies have investigated to what degree each of the media features could contribute to improved learning and motivation. For example, if the agent's appearance and voice were perceived as likable, this positively contributed to motivation and transfer of learning Progress and RecommendationsCurrent ProgressPedagogical agent research has made great progress, expanding its scope from the focus on agents' provision of expert guidance to a broader interest in agents' social and affective capabilities to support learners With the growing understanding of the integral relationship between emotions and cognition, some researchers have been striving to provide a supportive environment by equipping tutoring systems with affective capabilities The examination of agent impact in terms of learner characteristics is also noteworthy. We found, in general, that females across age groups were more in favor of agentbased interactive learning environments than males Future RecommendationsOver the last decade, agent design tools and AI technologies have significantly advanced. The technologies for designing media features and emotional interactions enable designers and researchers to create more believable agents. Natural language processing coupled with speech recognition technology allows for a more natural tutorial dialogue between learner and agent and better simulates human-to-human conversations-for example, refer to the virtual humans developed by the Institute for Creative Technologies (http://ict.usc.edu) and Project Listen (http://www. projectlisten.org). The instructional and motivational messages that were pre-scripted and controlled in our study now can be more easily adapted to students' instructional needs in real-time. Along this line, revisiting our study with agents that have more natural and interactive speech is warranted. Such research could take into account the needs of the learners, particularly with diverse student populations who might have unique challenges in motivation and learning.We expect that designing the optimal instructional role for artificial beings will continue to be of interest in the AI research community and more broadly the educational technology community as well. As multi-agent systems are implemented more easily, the definition of agent roles has become fine-tuned. Agents have taken on a range of roles, from proactively teaching and guiding, through collaborating and colearning, to even existing subtly in the background (e.g., in multi-agent educational games and combat training simulations). Also, with the advances in online technology, learning platforms have become increasingly personalized, and a growing number of people choose online learning over classrooms. The personalized service provided by pedagogical agents might enrich these learning experiences both socially and cognitively. Lastly, pedagogical agents have taken the form of on-screen characters in many recent studies. It is very plausible that the agents will expand in their representations, such as humanoid robots Table 1Table 1", "conclusions": "", "SDG": [4]}, "progressing_toward_digital_equity": {"name": "Progressing Toward Digital Equity", "abstract": " In the computer domain, the objective to be reached in order to give opportunities equality to all persons is defined as Digital Equity. However, different international organizations are working to reach this goal. The analysis of the key points for reaching this objective and how international organizations (International Telecommunications Union, United Nations, International Federation for Information Processing) are working for this goal will be presented in the paper.", "keywords": "Digital equity,Building the infrastructure,Education levels,ITU WSIS,UN SDG,IFIP", "introduction": "ICT is a set of technologies that well used should allow the progress and the wellbeing of people everywhere in the world. However as there are important differences in the current situation of people, depending on its general level of use in each country, the use of ICT to reach the progress and the wellbeing cannot be the same in all countries. To evaluate this influence of the ICT we will use the concept of Digital Equity.Digital equity  is the social-justice goal of ensuring that everyone in our society has equal access to technology tools, computers and the Internet. Even more, it is when all individuals have the knowledge and skills to access and use technology tools, computers and the Internet. A simple definition of digital equity can be a state in which both the digital divide and the participation gap are bridged.[1]Digital equity ensures that everyone has equal opportunities to use the tools and resources needed to fully participate as a citizen in today's digitally-powered world. Lacking these opportunities causes people to encounter educational, economic and social limitations that negatively impact their quality of life. The progress towards the Digital Equity, as it is established in the IFIP strategic action, should be achieved by:\u2022 Promoting accessibility to ICT, \u2022 Promoting good practices, \u2022 Promoting and enhancing appropriate access to knowledge and experiences, \u2022 Organizing and contributing to activities aimed at achieving the goals of the World Summit on the Information Society (ITU-WSIS) ,[2]\u2022 Organizing and contributing to activities aimed at achieving the UN Sustainable Development Goals (SDGs) , as we will analyze in Sect. [3].4To analyze in depth the points suggested in the previous paragraphs, this paper will be organized as follows: in Sect. 2 the concept of Digital Equity will be detailed; Sect. 3 will be devoted to the relations of Digital Equity with the ITU-WSIS Action Lines; the relation of ITU-WSIS Action Lines with the UN Sustainable Development Goals will be the topic of Sect. 4. In Sect. 5 it is analyze the differences of Digital Equity in developed and developing countries and how IFIP  can contribute to reach these goals. Finally Sect. 6 gives some conclusions about the coming future.[4]", "body": "ICT is a set of technologies that well used should allow the progress and the wellbeing of people everywhere in the world. However as there are important differences in the current situation of people, depending on its general level of use in each country, the use of ICT to reach the progress and the wellbeing cannot be the same in all countries. To evaluate this influence of the ICT we will use the concept of Digital Equity.Digital equity Digital equity ensures that everyone has equal opportunities to use the tools and resources needed to fully participate as a citizen in today's digitally-powered world. Lacking these opportunities causes people to encounter educational, economic and social limitations that negatively impact their quality of life. The progress towards the Digital Equity, as it is established in the IFIP strategic action, should be achieved by:\u2022 Promoting accessibility to ICT, \u2022 Promoting good practices, \u2022 Promoting and enhancing appropriate access to knowledge and experiences, \u2022 Organizing and contributing to activities aimed at achieving the goals of the World Summit on the Information Society (ITU-WSIS) \u2022 Organizing and contributing to activities aimed at achieving the UN Sustainable Development Goals (SDGs) To analyze in depth the points suggested in the previous paragraphs, this paper will be organized as follows: in Sect. 2 the concept of Digital Equity will be detailed; Sect. 3 will be devoted to the relations of Digital Equity with the ITU-WSIS Action Lines; the relation of ITU-WSIS Action Lines with the UN Sustainable Development Goals will be the topic of Sect. 4. In Sect. 5 it is analyze the differences of Digital Equity in developed and developing countries and how IFIP Concept of Digital Equity 2.1 What Is Digital Equity?As it has been stated in the previous section, Digital equity is the social-justice goal of ensuring that everyone in our society has equal access to technology tools, computers and the Internet. Even more, it is when all individuals have the knowledge and skills to access and use technology tools, computers and the Internet. Digital equity can be defined as the state in which both the digital divide and the participation gap are both bridged. Digital equity ensures that everyone has equal opportunities to use the tools and resources needed to fully participate as a citizen in today's digitally-powered world. Lacking these opportunities causes people to encounter educational, economic and social limitations that negatively impact their quality of life.Technology is so commonplace in our lives that it may be hard to believe there are still many people with limited access to and knowledge of the resources that are available online today. Children still stand in lines at libraries for a brief stint on a computer and parents have difficulty completing online employment applications; a clear and important difference between the new digital generation and the previous ones. While cell phones may have helped to close the gap to some degree, there are still important activities that are not well-suited for small mobile devices.The Five Dimensions of Digital EquityThese dimensions have been chosen as fundamental categories by educators and professionals working in the field. If you are just beginning to learn about this field then these categories should help you address your basic needs:\u2022 Access to technology resources (hardware, software, wiring and connectivity): possibility to have access to the technological resources allowing us to access the information existing in the network. \u2022 Access to high quality digital content; if the information available in the network is not of good quality, people will not be attracted to access it. \u2022 Access to high quality, culturally relevant content; the available information should not only be of high quality but adapted to the context in which each community is leaving. \u2022 Educators skilled in using these resources effectively for teaching and learning; it Is obvious that for approaching the digital equity it is necessary to educate users but in many environments there are not enough people with appropriate skills for learning end users in all the needed categories (e.g. basic user, advanced user, expert in installation and maintenance, hardware and software developer and builder, ICT research). \u2022 Opportunities for learners and educators to create their own content; a way to increase the capacity of learners and educators is to offer them some tools to create contents accessible to the appropriate end users.Digital Divide ImpactsIt is difficult to have progress toward Digital Equity homogeneously in its five dimensions and, in consequence the risk to fall in Digital Divide exists, at least in someone of the five dimensions. How to avoid this inconvenient situation? What are the initial steps to progress toward Digital Equity?\u2022 First it is necessary to allow that people can obtain the education allowing to conveniently using computers and networks in a large sense that is that users need to acquire a convenient knowledge for a correct use of computers and networks depending on the people desired education level: basic user, advanced user, expert in installation and maintenance, hardware and software developer and builder, ICT research, etc.. \u2022 And second people should be able to easily and deeply use both computers and networks for accessing and using all kind of information. Without infrastructure the other dimensions have no sense. But the investment in a new infrastructure has to be done only if the possibility of maintaining it is ensured; otherwise there is an important risk of wasting the funds of this investment.In summary, the first steps to reach Digital Equity should be education and infrastructure. However the progress towards Digital Equity cannot be reached through individual efforts but by the coordinate effect of actions promoted by governments and governmental organizations. The global organizations to consider will be the International Telecommunication Union (ITU), the United Nations (UN) and the International Federation for Information Processing (IFIP).3 Relation Between Digital Equity and the ITU-WSIS Action Lines Relation Between WSIS and Digital EquityThe action lines proposed by the ITU have as global goal the implementation of the Digital Equity in order to avoid the Digital Divide. The description of these Action Lines can be found in Example 1. Action Line 2: Information and Communication Infrastructure. The vision of this action line, as it is defined in Digital Equity should be supported by a convenient infrastructure. So the infrastructure suggested in this Action Line has to be able to allow all citizens the correct access and use of ICT, covering one of the five dimensions of the Digital Equity.Example 2: Action Line 3: Access to Information and Knowledge. The vision of this action line, as it is defined in To succeed with the Digital Equity the government has to create and offer information and knowledge to all citizens or to establish the appropriate mechanisms allowing the creation and offering of information and knowledge accessible for all citizens. Otherwise there is the risk of provoking the Digital Divide between the citizens of the same country or between the citizens of the country with the citizens of other countries.Example 3: Action Line 8: Cultural Diversity and Identity, Linguistic Diversity and Local Content. The vision of this action line, as it is defined in If the global goal is to attain the Digital Equity it is necessary to respect the cultural diversity and identity giving the convenient importance to the linguistic diversity and local content. Otherwise there will be people without interest in accessing the information existing in the network and avoiding the attainment of the Digital Equity.ITU-WSIS Action Lines and the UN SustainableDevelopment Goals Each one of these goals is divided in Relations Between the UN SDG and the WSIS Action LinesWSIS has prepared a matrix indicating the effect of each one of the Action Lines on the different Sustainable Development Goals If all WSIS Action Lines have some impact in the UN SDGs, the actions conducting to the Digital Equity will contribute at some level to approach the UN SDGs. So we can say that Digital Equity establishes the UN SDG in the specific domain of ICT that is WSIS Action Lines represent the application of SDG in the ICT domain. In some sense the dimensions of Digital Equity are included in one or several SDG and actions going toward Digital Equity contribute to the UN SDGs.Digital Equity in Different Groups or EnvironmentsIs Digital Equity in identical situation in all countries? Is Digital Equity in identical situation for the different genders?It would be interesting to see the situation of Digital Equity with respect to different criteria like developed versus developing countries, like gender issue, etc. but let us concentrate in this article in the differences between developed and developing countries. \u2022 Access to technology resources (hardware, software, wiring and connectivity): possibility to have access to the technological resources allowing us to access the information existing in the network: Obviously the situation of the technological resources in developed countries is more comfortable than in developing ones, mainly because the extension of the network coverage and its capacity in developed countries are much more higher, faster and reliable than in developing ones. Also because the availability of devices in developed countries is greater, with higher novelty of models and at prices economically cheaper taking into account the economic level of the country. And finally the possibility for the end users of receiving the appropriate education is also higher; and this is true at all levels of education: basic user, advanced user, expert in installation and maintenance, hardware and software developer and builder, ICT research, etc.; it is easy to find the necessary education at reasonable prices for all type of education. Also in many countries there are strong differences in the access possibilities for the different genders. \u2022 Access to high quality digital content; if the information available in the network is not of good quality, people will not be attracted to access it. Developed countries have an infrastructure allowing the access to all information existing in Internet by both the quality of the physical connection and the freedom for accessing all kind of information. Developing countries can have limitations by one or both of these aspects; in some cases the network has not enough capacity for accessing heavy information, and in other cases the access is limited by political reasons. \u2022 Access to high quality, culturally relevant content; the available information should not only be of high quality but adapted to the context in which each community is living. Most of the web pages are written in English (or at least is the greatest minority) making difficult the access of people not fluent in reading this language. Also the topics contained in these pages are thought with Anglo-Saxon parameters making the access by people of other cultures of low interest. Consequence: not all pages are equally interesting for all people in the world, increasing the disadvantages of people of developing countries. \u2022 Educators skilled in using these resources for effectively teaching and learning; it Is obvious that for approaching the digital equity it is necessary to educate users but in many environments there are not enough people with appropriate skills for learning end users in all the needed categories. Education is a key point for reaching Digital Equity. However to deliver education it is necessary to have educators of all the needed profiles. The number and quality of the education centres dedicated to the educators are higher in developed than in developing countries.\u2022 Opportunities for learners and educators to create their own content; a way to increase the capacity of learners and educators is to offer them some tool to create contents accessible to the appropriate end users. The hardware and software infrastructure is much more solid in developed than in developing countries.Contribution of IFIP to Digital EquityThe International Federation for Information Processing is a society created in 1960 under the auspices of UNESCO grouping computer societies under the base of one member society per country. From the technical point of view IFIP is composed of a number of Technical Committees (TC) each one dedicated to a specific aspect of computer science, engineering and applications. Each TC is composed of a number of Working Groups (WG), each one devoted to a specific aspect of the TC domain.Currently there are 13 TCs and 120 WGs grouping thousands of computer professionals. IFIP has decided that Digital Equity is one of its Strategic Activity Lines. In general we can consider that the work of all these bodies contribute to the Digital Equity. However there are some of them are more specifically oriented to tackle the Digital Divide and empowering Digital Equity.From the previous introduction let us review the different TCs and analyze what is their involvement in some aspect of the Digital Equity. The IFIP TCs are:\u2022 TC1 on Foundations of Computer Science, whose aims are: to support the development of theoretical computer science as a fundamental science; and to support the development and exploration of fundamental concepts, models, theories, systems, and and the understanding of laws, limits, and possibilities of information processing. Its influence on Digital Equity is generic but not directly implied. \u2022 TC2 on Software, Theory and Practice, whose aim is to obtain a deeper understanding of programming concepts in order to improve the quality of software by studying all aspects of the software development process, both theoretical and practical. So, building good software products has a positive influence on Digital Equity mainly promoting good practices in the creation of software products. \u2022 TC3 on Education whose aims are: to provide an international forum for educators to discuss research and practice in: teaching informatics; and educational uses of communication and information technologies (ICT); to establish models for informatics curricula, training programs, and teaching methodologies; to consider the relationship of informatics in other curriculum areas; to promote the ongoing education of ICT professionals and those in the workforce whose employment involves the use of information and communication technologies; to examine the impact of information and communication technologies on the whole educational environment: teaching and learning; administration and management of the educational enterprise; and local, national and regional policy-making and collaboration. As education is a key factor for reaching Digital Equity, we can see that the aims of this TC are fully in line with the dimensions of Digital Equity. \u2022 TC5 on Information Technology Applications whose aim is to promote research and development of fundamental concepts, models, and theories to support applications of ICT. So, it has a generic interest for Digital Equity; only the promotion of applications appropriate for developing countries can help to attain Digital Equity. \u2022 TC6 on Communication Systems, whose aims are to promote the international exchange of information related to communication systems; to bridge gaps existing between users, telecommunication operators, service providers and computer and equipment manufacturers; and to establish working contacts with international bodies concerned with data communication, such as ITU, ETSI, ISO, IEEE, IETF, ITC and ATM Forum. The aims of this TC are fully in line with a dimension of the Digital Equity (creation of a convenient infrastructure). In particular in this TC there is a WG devoted to developing countries so dedicated to bridge the gap between developed and developing countries with respect to Digital Equity. This is the WG6.9 on Communications Systems for Developing Countries, whose aims are: to identify and study technical problems related to the access to, understanding of and application of network and telecommunications technology in developing countries or regions; to encourage cross-fertilization of concepts and techniques among developing countries, and between developing countries and developed countries; to promote activities oriented to the diffusion of the methods and techniques for accessing computer networks in developing countries or regions. \u2022 TC7 on System Modelling and Optimization, whose aims are: to provide an international clearing house for computational (as well as related theoretical) aspects of optimization problems; to promote the development of necessary theory to meet the needs of complex optimization problems and cooperate with the International Mathematics Union; and to foster interdisciplinary activity on optimization problems spanning the areas such as Economics, Biomedicine, Meteorology, etc., in cooperation with associated international bodies. So, it has not a specific interest for Digital Equity. \u2022 TC8 on Information Systems whose aim is to promote and encourage interactions among professionals from practice and research and advancement of investigation of concepts, methods, techniques, tools, and issues related to information systems in organizations. So, it has a generic interest for Digital Equity specially proposing good practices for the information systems. \u2022 TC9 on ICT and Society whose aims are: to develop understanding of how ICT progress is associated with change in society; and to influence the shaping of socially responsible and ethical policies and professional practices. The aims of this TC are fully in line with several dimensions of the Digital Equity. In particular it has a WG devoted to Developing countries dedicated to bridge the gap between developed and developing countries. It is the WG9.4 on Social Implications of Computers in Developing Countries, whose aims are: to collect, exchange and disseminate experiences of ICT implementation in developing countries; to develop a consciousness amongst professionals, policy makers and public on social implications of ICT in developing nations; to develop criteria, theory, methods, and guidelines for design and implementation of culturally adapted information systems; and to create a greater interest in professionals from industrialized countries to focus on issues of special relevance to developing countries.\u2022 TC10 on Computer Systems Technology, whose aim is the promotion of the State-of-the-Art and the coordination of information exchange on concepts, methodologies, and tools in the stages in the life cycle of computer systems. So, it has a generic but not specific interest for Digital Equity. \u2022 TC11 on Security and Privacy Protection in Information Processing Systems whose aim is to increase the trustworthiness and general confidence in information processing and to act as a forum for security and privacy protection experts and others professionally active in the field. Obviously the topics concerned by this TC are of high importance for reaching Digital Equity, ensuring the needed security and privacy protection to the information and to the end users. \u2022 TC12 on Artificial Intelligence whose aims are: to foster the development and understanding of Artificial Intelligence and its applications worldwide; to promote interdisciplinary exchanges between Artificial Intelligence and other fields of information processing; and to contribute to the overall aims and objectives and further development of IFIP as the international body for Information Processing. The aims of this TC are not of specific interest for Digital Equity. \u2022 TC13 on Human-Computer Interaction whose aims are: to encourage empirical research (using valid and reliable methodology, with studies of the methods themselves where necessary); to promote the use of knowledge and methods from the human sciences in both design and evaluation of computer systems; to promote better understanding of the relation between formal design methods and system usability and acceptability; to develop guidelines, models and methods by which designers may be able to provide better human-oriented computer systems; and to co-operate with other groups, inside and outside IFIP, so as to promote userorientation and \"humani-zation\" in system design. Obviously the topics concerned by this TC are important for reaching Digital Equity, because a good interface to the searched information helps a lot to its usability by the end users. \u2022 TC14 on Entertainment Computing whose aims are: to enhance algorithmic research on board and card games; to promote a new type of entertainment using information technologies; to encourage hardware technology research and development to facilitate implementing entertainment systems; and to encourage non-traditional human interface technologies for entertainment. Maybe the topics concerned by this TC are marginal for reaching Digital Equity, but it works on topics making the access to information more attractive to the end users.So IFIP has convenient bodies to tackle the problems derived from and related to Digital Equity in general and specifically in developing countries. The recent creation of a Standing Committee in Digital Equity will increase and coordinate the action to promote the Digital Equity.Conclusions", "conclusions": "We have analyzed the concept of Digital Equity and the different aspect it includes. Also we have seen how several international organizations; the Action Lines of the World Summit in Information Society, the United Nations Sustainable Development Goals and the Technical Committees of the International Federation for Information Processing have a strong overlapping with the goals proposed by Digital Equity. Maybe these international organizations have not an exact coincidence on their goals, but an important overlap between them has been clearly stated.", "SDG": [4]}, "teaching_training_in_a_mixed_reality_integrated_learning_environment": {"name": "Teaching training in a mixed-reality integrated learning environment", "abstract": " In this mixed-method study, we examined the design and potential impact of a mixed-reality integrated learning environment (MILE) in providing the simulated and immersive teaching practice for university teaching assistants. A virtual-reality-based learning platform integrating a Kinect-enabled sensorimotor interface was developed and used by twenty three university teaching assistants. Qualitative and quantitative data on the participants' participation behaviors, engagement, and perceptions were collected via video/screen recording, interview, surveys on teaching self-efficacy and sense of presence, and eye tracking. Results indicated that the MILE reinforced sense of presence and supported the performance of an ample range of virtual teaching tasks/actions with avatar-embodied live gesturing. The environmental fidelity in the mixed-reality learning spaces, the design and arrangement of virtual agents and avatars, and the affordance of embodied gesturing and walking are salient MILE design features that affected participants' sense of presence and their virtual teaching performance.", "keywords": "Sense,of,presence,Virtual,reality,Mixed,reality,Immersive,learning,Teaching,training", "introduction": "The recent development of computer hardware and software has made it feasible to incorporate Internet-based, 3D virtual reality (VR) in innovative applications of teaching, learning, and training (Abulrub, Attridge, & Williams, 2011;Gregory et al., 2013;. Virtual reality (VR) or virtual world is a computer-generated 3D representation of real-life environments. A user can autonomously navigate around a VR (in the form of avatars) and interact with simulated objects and other avatars in real time at the same pace one would experience events in the real world Jou & Wang, 2013). In comparison with other computerized programs, virtual reality supports realistic, immersive simulation to enable the transfer of skills between taught and real contexts, and provides a multi-user, embodied, and interactive space for real time active learning (Mitchell, Parsons, & Leonard, 2007). It is speculated that VR can act as a promising tool for training skill application and complex problem solving that requires weighing multiple variables and situational decision making (Cheng & Wang, 2011)(Bertram, Moskaliuk, & Cress, 2015;Cheng & Wang, 2011;Dede, 2005.Dede, , 2009))The emerging 3D body sensory technology, such as Microsoft Kinect, can be used as an intuitive interface to create a naturalistic, augmented interaction between users and a VR-based simulation, thus merging real and virtual worlds to support the mixed reality . A Kinect captures a user's body gestures and replicates them in a computer application, which offers new ways of human-computer interaction in educational settings. In a Kinectenabled, mixed-reality environment, a user can use physical body movements to interact with the virtual characters and objects in the virtual reality. Kinect-based applications in a VR setting may help to enhance the sense of presence and immersion, and facilitate embodied and situated cognition of learners.(Ohta & Tamura, 2014)The use of a mixed-reality integrated learning environment (MILE) is just emerging (Hayes, Straub, Dieker, Hughes, & Hynes, 2013;Liarokapis & Anderson, 2010;, and is in need of empirical research on its design, implementation, and educational effectiveness. In this study, we examined the design and application of a mixed-reality integrated learning environment e a virtual reality learning platform that integrates a Kinect-enabled sensorimotor interface e in the setting of teaching training for university teaching assistants. Teaching is a complex problem solving task that requires contextualized and adaptive implementation of content representation and (both verbal and embodied) interpersonal interaction. Learning to teach is a challenging and important area of inquiry for educational practice and research Staub, Dieker, Hynes, & Hughes, 2014). Examining the implementation of MILE for teaching training will help to inform the role of this emerging platform in providing embodied, immersive interactions for learning a complex task, and to test its implementation feasibility via the lens of future teachers. Specifically, this exploratory study aimed to address two research questions: (1) What is the effectiveness of the mixed-reality integrated learning environment for teaching training? (2) What features of this learning environment influence participants' perceptions and teaching-training experiences?(Quintana & Fern andez, 2015)", "body": "The recent development of computer hardware and software has made it feasible to incorporate Internet-based, 3D virtual reality (VR) in innovative applications of teaching, learning, and training The emerging 3D body sensory technology, such as Microsoft Kinect, can be used as an intuitive interface to create a naturalistic, augmented interaction between users and a VR-based simulation, thus merging real and virtual worlds to support the mixed reality The use of a mixed-reality integrated learning environment (MILE) is just emerging Theoretical frameworkSense of presence and immersion in a virtual learning environmentSense of presence refers to \"a state of consciousness\" and the psychological sense of being in the virtual place A design construct closely related to sense of presence is immersion e the extent of \"the subjective impression that one is participating in a comprehensive, realistic experience\" and \"the semi-voluntary experience of being transported into an alternate context for an extended duration\" The recent development of computer hardware and software has made it feasible to incorporate Internet-based, 3D virtual reality (VR) to create an immersive learning environment Embodied and situated cognitionSituated and embodied cognition theories hold that cognition is not abstract or centralized but a situated activity that takes place in active and continuous interactions with the environment, vastly via perceptual and motor activities Mixed-reality integrated teaching training for graduate teaching assistantsTeaching is a complex problem-solving task that requires weighing many variables and adaptively implementing principles of instruction, communication, and content representation in a highly situated context Teaching training for GTAs, then, is critical for supporting curriculum reforms, bolstering college teaching and learning, and training future faculty members The research on GTA training is limited and fragmentary The recent development of 3D body sensory technology, such as Microsoft Kinect, can be used as an intuitive interface to create a naturalistic, augmented interaction between users and a VR-based simulation. Specifically, in a Kinect-integrated VR environment, a GTA can use physical body movements to control an instructoravatar to perform augmented and active teaching practice in a simulated class, or he/she can be embodied as a student in the class to observe, critique, and learn from the teaching performance of another GTA.The use of mixed reality for simulated teaching and training is just emerging, but there is preliminary evidence of its instructional effectiveness MethodDue to the lack of systematic investigation, and hence, the lack of design foundation and conceptual distinctions of the mixed reality integrated learning environments (MILE) for the teaching training with GTAs, this study was structured using the exploratory research framework. This mixed-method case study Participants and mixed-reality integrated learning environmentTwenty three university teaching assistants were recruited from the disciplines of education (n \u00bc 10), engineering (n \u00bc 5), business (n \u00bc 2), and arts and science (n \u00bc 6) in a land-grant university in United States. All these participants were international graduate students from 6 different countries, including 12 females and 11 males. Out of the 23 participants, 7 had less than one-year teaching experience, 6 had both online and face-to-face teaching experience, and all were non-native English speakers.Delivered via OpenSimulator (an open-source VR platform), a virtual class simulation was designed to simulate the daily classroom setting at the university. Each participant was requested to practice teaching in this virtual class, including lecturing, mentoring, and classroom management. The simulated audience in the virtual class were a mixture of non-player characters (NPCs) controlled by the artificial intelligent (AI) script and student avatars played by peer trainees. Using Kinect and a middleware that connected and interfaced Kinect with the Opensimulator platform, this 3D virtual class (see Fig. Data collectionProcedureAll participants were surveyed on their teaching self-efficacy, demographics, academic background, and teaching experience before the intervention. They were then provided a pre-study orientation on the mixed-reality integrated learning environment (MILE), and requested to participate in two one-hour MILE teaching-training sessions.In one session, each participant was requested to teach a selfchosen, domain-specific content topic to a virtual class of students. In the other session, the participant would sit in the virtual class, act as a student along with other avatars and agents, and interact with the virtual instructor played by another peer participant. It was speculated that acting as a student and observing how peers teach a novel topic would award the participant an opportunity to experience, observe, and reflect on varied teaching actions and strategies, and potentially facilitate an interactive learning experience by having participants challenging and critiquing each other during the virtual class. The sequence of participating in the two sessions was selected by each participant and randomized across all. Right after the first session, every participant received a survey on the perceived sense of presence in the virtual class and a second survey on their teaching self-efficacy. At the end of the second session, participants were surveyed again on their sense of presence and teaching self-efficacy.Every MILE intervention session was screen and video recorded; the recordings captured both online and offline behaviors of participants when they interacted with the VR simulation via the Kinect interface. Every participant received a 30-min, videostimulated, semi-structured recall interview at the end of each intervention session. The interview focused on exploring participants' perceptions and experiences of the MILE. Six randomly selected individual participants were also eye-tracked using the kit of ASL Mobile Eye Tracking during the MILE-based teaching process. Eye-tracking data would contribute supplementary data, in addition to the observed behaviors of participation, to provide information about the level of engagement and hence sense of immersion exhibited by the program participants.InstrumentsPre-, during-, and post-intervention teaching self-efficacy were measured via a customized version of the Teaching Self Efficacy Scale (Tschannen-Moran & Woolfolk Hoy, 2001; 22 items, 9-point scale, a \u00bc 0.89 in this study). Two individual items in the original scale questioned on the teaching practice that was not applicable in the current study (e.g., \"How much can you assist families in helping their children do well in school?\"), and hence were removed. In certain survey items, the phrase of \"in school\" in the original scale was customized to \"in class\" to reflect the targeted teaching setting in this study. A customized version of the presence Questionnaire Data analysisDescriptive statistics and paired t-tests were conducted with the survey results to examine whether the sense of presence was supported and sustained in the VR-based, Kinect-integrated learning environment, and to investigate the potential changes in participants' teaching self-efficacy. We conducted a behavior analysis with the recorded participants' performance in the intervention sessions. Systematically, we coded the recorded virtualparticipation behaviors per 30 seconds and categorized the attributes, contexts, and frequency of salient actions to explore the nature of the MILE-based lecturing, gesturing, interaction, and classroom management. We then performed a qualitative thematic analysis with the interview data, focusing on learners' perceptions of the MILE for the teaching practice. We triangulated the survey, behavioral analysis, and thematic analysis findings to gather the descriptive evidence on the effects of the MILE and develop propositions on the association between its design features and learning effectiveness. Eye-tracking data were analyzed to identify patterns of engagement and potential evidence of sense of presence of participants during the MILE-based teaching training.ResultsSense of presence and immersionThe paired t-test comparing the sense-of-presence survey response at the end of the first intervention session (M 1 \u00bc 108.1 out of a total of 168, SD 1 \u00bc 18.4) with that at the end of the second session (M 2 \u00bc 105.1, SD 2 \u00bc 22.6) did not indicate a statistically significant difference, t(22) \u00bc 0.73, p \u00bc 0.47. Both had a mean value that was higher than 60% of the maximum scale score. This result suggested that the sense of presence was fostered and sustained during MILE-based teaching training and there was no differential effect of acting as a teacher or a student in the virtual world on the sense of presence. During the interview when participants were asked to rate the degree of immersion in the MILE, only 2 out of all 23 participants gave an estimate lower than 60%. Specifically, participants' responses to the survey items on live gesturing in VR indicated a general endorsement of Kinect-based embodied presence, M \u00bc 8.4 (60% out of 14), SD \u00bc 3.4.Via the behavior analysis of participants' actions and reactions in MILE sessions, we further explored the affordance of the non-player characters (NPCs), Kinect-enabled virtual gesturing, and the VRbased teaching simulation for fostering learner immersion in the MILE. The result indicated that around 56% of the NPC-related participant behaviors portrayed a positive experience of immersion. Particularly, it was found that participants tried to interact with an NPC in the same earnest manner as that with a real student, \"Mario (an NPC student), is that your leg on the table? What is that? Put your leg DOWN please!\" Yet the low voice volume, occasional out-of-context inquiries (e.g., requesting the extension of assignment deadlines when the instructor was still giving a lecture), and the lack of reciprocal reactions of NPCs (e.g., Mario did not put down the leg as requested) led to the lessening of realism in NPC-initiated interactions. In the current MILE, NPCs in the virtual class delivered mainly pre-timed, pre-set behaviors and utterances; they could not reciprocally interacted with an instructor or a student avatar. Around 43% of participants reported that in the virtual class they gradually recognized that these characters, different from avatars, were actually \"non-human.\" Consequently, it was observed that those participants attended to lecturing more than student interaction or classroom management in the later part of virtual teaching. One of them commented, \"I just don't care anymore.\"In general, around 78% of observed user interactions evidenced an immersive engagement of participants in the virtual, simulated teaching environment. In the remaining 22% enactments, participants were found to interact with items in their physical environment (e.g., self-brought lecturing handouts) or question the observing researcher about the virtual class simulation. This observation indicated that the environmental distractions were not fully suppressed and the experience of immersion in the MILE might have been interrupted.All study participants performed a succession of avatarembodied live gesturing during virtual teaching. More than 90% of gesturing actions portrayed the experience of embodied presence, which comprised beat gestures in conjunction with the rhythm of speech to emphasize certain phrases, pointing gestures (i.e., act of pointing as a way of referring to an object or event), and representational gestures (i.e., gestures used to explain the form or nature of a concept). The remaining 10% of gesturing and bodymovement behaviors demonstrated insufficient virtual presence. Particularly, the Kinect equipment captured a fixed physical area. The embodied gesturing would be interrupted when a participant accidentally stepped out of the capturing zone during lecturing.Engagement with the virtual teaching simulationThe eye-tracking data in this study (Table A further analysis of the eye-tracking and video/screen recording data indicated that participants' gazing out of the AOI (the VR display of the virtual class) occurred mainly in the following situations: (1) deliberating in reaction to a challenging student behavior or inquiry, (2) trying to retrieve materials and find help from an offline lecturing aid, (3) being distracted by the unexpected environmental noise (e.g., coughing or sneezing of the observing researcher), (4) pondering before answering a question or continuing the instruction. Among these four situations, the first and last ones were responses and signs of cognitive engagement (e.g., head tilting or lowering as a sign of uncertainty and thinking). The second and third situations were indicative of physicalenvironment disruptions formed by a conflicting object of attention (e.g., the offline lecturing aids in addition to the virtual ones) and the lack of exclusiveness in the simulated learning space (e.g., the onsite presence of an observing researcher as a diversion from the virtual presence of VR students).Teaching self-efficacyThe paired t-tests with the pre, mid, and post-survey results of the teaching self-efficacy did not indicate statistically significant differences in the teaching self-efficacy before, in between, and after the two intervention sessions, t(pre-post) \u00bc 1.50, p \u00bc 0.15; t(pre-mid) \u00bc \u00c00.05, p \u00bc 0.96. A ceiling effect on teaching selfefficacy may have occurred, due to the finding that before the intervention study participants had reported an average of 76% teaching self-efficacy and a median of 17.5-month teaching experience.There was actually a numerical decline in teaching self-efficacy from the pre-intervention (M pre \u00bc 150.9 out of a total of 198, SD pre \u00bc 28.3) and the mid-intervention conditions (M mid \u00bc 151.0, SD mid \u00bc 29.9) to the post-intervention condition (M post \u00bc 132.5, SD post \u00bc 44.9). An explanation of such a trend, based on the qualitative data, is that the two-hour MILE intervention helped participants develop a better understanding of the challenges in managing a heterogeneous learner group during lecturing and hence increased their awareness of the lack of competence in classroom management. For the VR-based class simulation, we purposefully gathered and replicated a variety of disruptive student scenarios e complaints (e.g., \"I don't understand you!\" or \"This is boring!\"), irrelevant questions, non-compliant behaviors (e.g., putting legs onto the table, leaving class to pick up a cell phone), and verbal challenges (e.g, \"Do you speak English?\"). Embedded as incessant disturbances during one's lecturing in the virtual class, these disruptive student scenarios appeared to challenge most study participants. They unanimously reported the lack of opportunities to experience and practice on those scenarios in their conventional teaching practice. As observed, participants frequently showed disorientation when encountering disruptive students during virtual lecturing. Some participants ignored or reacted heatedly towards students' disruptive behaviors, or appeared defensive with the interruptive, insolent comments made by NPC students. More participants expressed that the experience of handling difficult students were thought-provoking, as one of them explained, \"Negative comment is more effective, because I am curious and cautious when getting negative comments.\" Participants generally expressed discontent with their failure to complete an intended lecture due to the intermittent classroom-managing encounters. Frequent comments were, \"It's hard,\" \"I forgot what I 1172.17 1172.17 intended to say,\" \"I was busy looking at the classroom,\" and \"The experience made me see the gap (between the expected and current teaching performance).\"VR-based teaching practiceDuring interviewing, 22 out of 23 participants stated that the MILE should be very helpful for training novice teaching assistants. When asked to rate their virtual teaching experience, participants reported an average of 78% degree of satisfaction. The behavior analysis with participants' virtual teaching processes showed a comprehensive portray of the targeted teaching performance by the participants, supporting the affordance of the VR-based, Kinectintegrated learning environment as an alternative platform for active teaching practice. Salient teaching tasks, teaching actions, and componential strategies performed by the participants during virtual teaching sessions, along with performance frequencies congregated across all participants, are outlined in Table Among the teaching actions performed by the study participants, 66% were for instruction (or lecturing), 19% were for classroom management, and only 15% was contributed to student interaction. Pertinently, more than 80% of participants expressed a teaching belief that focuses on instruction rather than student interaction or management, as highlighted by a participant comment, \"The importance is to prepare the lecture, not the classroom.\" The reduced involvement in interacting with students may also relate to the difficulty of performing reciprocal and dynamic interactions with NPC students and hence a reduced sense of presence with the instructor-student interaction. Among the instructor-student interactions, 66% were contributed to question answering. Notably, participants had managed to verbally and nonverbally connect to individual students (21% of the instructorstudent interactions) and greet them (13%), demonstrating social engagement with their virtual students to some extent.All study participants were able to deliver a comprehensive and multi-step instruction during the virtual teaching sessions (as shown by Fig. Such a finding further affirmed the potential effectiveness of a VRbased, Kinect-integrated learning environment in promoting an authentic teaching rehearsal.Another observation of the Kinect-integrated learning environment was that study participants were able to naturally convey live gesturing via their avatars during the process of virtual teaching. Among the teaching gestures performed (a total of 663 gestures coded across participants), 29% were representational gestures that were used to embody and represent a concept, 18% were pointing gestures that aimed to obtain or guide the audience's attention, and 53% were beating gestures that were used with the rhythm of speech. This finding suggested that integrating Kinect in the virtual-reality-based training environment enabled the participants to actively practice both verbal and nonverbal teaching strategies in an integrative manner.Salient design features of the MILEThe MILE in this study simulated a conventional teaching environment and a heterogeneous student group with disruptive behaviors/requests. The environmental fidelity of the virtual class, the selection of domain-specific archetypical teaching scenarios/ tasks, the arrangement of virtual agents and avatars, and the affordance of Kinect-based embodied gesturing all appeared to influence participants' sense of presence and the externalization of their teaching techniques and beliefs.Fidelity and design of the mixed-reality learning spaceThe virtual class, along with the building where it is situated, was designed to fully simulate a conventional classroom in the building of college of education at the sampled university. Participants of education and arts and science majors who were used to such type of classroom all commented the 3D virtual space as \"real\" or \"authentic.\" Yet those of other majors criticized that the size, seating arrangement, or interior design of the classroom were \"different\" from what they experienced or expected, as the following quotes from participants indicated.\"This class was a little small.\" \"I don't think the current seating arrangement will work for group activities.\"\"I was used to a lab than a teaching station.\"\"The colors of the carpet and wall are too dark.\"Those comments indicated the need to customize the classroom design based on varied teaching settings and disciplinary cultures.A related observation was the importance of selecting and designing domain-specific, archetypical teaching scenarios and tasks. Lecturer-led direct instruction was depicted as the major teaching method in the intervention. Yet multiple participants of science, engineering, and business majors reported that in their academic disciplines, graduate teaching assistants mainly participated in laboratory and recitation teaching that comprised more discussion facilitation or lab-activity mentoring than lecturing.Unanimously, participants complained about the difficulty of establishing common reference during virtual lecturing. Although the shared media boards in the virtual world enabled media sharing/viewing among the virtual instructor and students, embodied pointing and gazing onto a reference object (e.g., a line within a virtual PowerPoint slide) to achieve joint attention and shared focus during the virtual lecturing was tricky. Particularly, the instructor had to mirror Kinect-enabled pointing or gazing to an object in the physical space onto the corresponding virtual reference in the virtual space (as indicated in Fig. Another critical design requirement for integrating the virtual and physical learning spaces to create an immersive MILE is the suppression of environmental distractions from the physical space.In this current study, the MILE was set up in a conventional conference room to better evaluate the implementation of such a learning environment in a low-cost, daily school setting. Researchers stayed in the same room to conduct onsite observation. Thus the exclusiveness of the space could be comprised. Even with the headsets, participants might still be distracted by the presence of the onsite observers. Additionally, some study participants admitted that they did not fully prepare their lectures before the teaching session, and had intermittently looked at the lecturing aids (e.g., papers and slides) in their hands rather than focusing on their virtual students in the class.Differential roles of virtual agents and avatarsStudents in the virtual class comprised both non-player characters (NPCs) controlled by the artificial intelligent (AI) script and avatars played by peer trainees. As observed, NPC agents presented initiative prompts that had stimulated reactive teaching behaviors with emotional experience of participants. \"Nervous,\" \"uncomfortable,\" \"upset,\" and \"embarrassed\" described emotional reactions of around 65% of participants toward these NPC prompts. The prompts also managed to externalize participants' internal teaching beliefs or preferences. For example, one commented, \"I saw him (an NPC student), but I just didn't care. The importance is to prepare the lecture, not the classroom.\" Another echoed, \"I am kind of shy, so whenever I am dealing with my lecture, I tried to look at my lecture slides.\" Correspondingly, participants were found to be either active or passive in managing student behaviors, portraying different student-management actions that were normative, coercive, retreating, or remunerative (as described in Table The lack of reciprocity and contextualization in NPC-initiated interactions necessitated the inclusion of avatars to facilitate the lecture-related instructor-student interaction. For example, a participant commented on the lack of content-specific queries from an NPC, \"He said he didn't understand, but I would need more details as to why and what he didn't understand.\" When acting as virtual students, participants were found to be attentive and proactive in questioning their peers on the content lectured. Viewing others' teaching performance also stimulated the intent to selfevaluate one's own teaching. Multiple participants had requested the feedback and the review of their teaching archives after viewing others' teaching performance.Embodied gesturing and walkingKinect-enabled gesturing was well observed and documented during participants' virtual teaching process (as reported in the previous sections). Another frequently-mentioned component for embodied presence in the MILE was walking, which can be illustrated by a participant quote -\"I like to interact with each individual student. I will walk to them.\" Walking toward and among students while talking was reported as a preferred interactive teaching style by multiple study participants. Yet the scope and distance of walking was somewhat constrained by the limits of the Kinect sensor. A walking zone was hence marked out on the ground of the conference room; participants had to check this physical boundary while walking and talking. Free movement, thus, was not a natural occurrence.Another salient design feature related to the embodied presence in the MILE is the point of view. In the virtual world, participants were able to shift between a zoom-in, first person view and a zoomout, third-person perspective. Some preferred the former because they would like to have a close-up view of their virtual students, to \"have (simulated) eye contact with students,\" and to avoid seeing their own gesturing \"because it is distractive.\" Others preferred the latter so they could better track the whole class. The shifting between the first-person and third-person views, however, was controlled via an external Bluetooth mouse, which created another impediment for the embodied presence.Discussion and conclusions", "conclusions": "", "SDG": [4]}, "active_search_of_connections_for_case_building_and_combating_human_trafficking": {"name": "Active Search of Connections for Case Building and Combating Human Trafficking", "abstract": " How can we help an investigator to efficiently connect the dots and uncover the network of individuals involved in a criminal activity based on the evidence of their connections, such as visiting the same address, or transacting with the same bank account? We formulate this problem as Active Search of Connections, which finds target entities that share evidence of different types with a given lead, where their relevance to the case is queried interactively from the investigator. We present RedThread, an efficient solution for inferring related and relevant nodes while incorporating the user's feedback to guide the inference. Our experiments focus on case building for combating human trafficking, where the investigator follows leads to expose organized activities, i.e. different escort advertisements that are connected and possibly orchestrated. RedThread is a local algorithm and enables online case building when mining millions of ads posted in one of the largest classified advertising websites. The results of RedThread are interpretable, as they explain how the results are connected to the initial lead. We experimentally show that RedThread learns the importance of the different types and different pieces of evidence, while the former could be transferred between cases.", "keywords": "Active Learning,Graph Construction,Link Inference", "introduction": "", "body": "We introduce the problem of Active Search of Connections, i.e. to infer the connections between entities from the evidence available in data, where the user is able to provide feedback and guide the network inference. Active Search of Connections is closely related to local clustering, active search and active exploration on graphs. However, the solutions for those problems can not directly be applied in this setting. Local clustering on graphs Active Search of Connections is potentially useful in a wide range of domains, wherever connections between entities are not given a priori, in particular for mapping out the covert web of entities in fraud detection, counter-terrorism, or tracking online sales of illegal goods (weapons, drugs, etc.). We are particularly motivated by its application to support law enforcement for case building in counter human trafficking operations. This paper complements the recent efforts to develop data-driven techniques for tracking online human trafficking RedThread is designed to find organized activities in online escort ads, i.e. ads marketing different potential victims which are linked by different types of evidence, e.g. phone numbers, catchphrases, misspelling and other text patterns, images with the same background, or other evidence of connection. An example case built by RedThread is illustrated in Fig. We have applied RedThread to find similar cases in millions of escort ads posted on Backpage.com for cities across the U.S. and Canada posted between 2013 to 2017. We model this data as a k-partite graph, in which ads are connected to various types of evidence they share, e.g. phone numbers, urls, images, names, bigrams. The user's feedback is then used to guide the navigation through the graph when finding ads related to the given seed. RedThread learns the relative importance of each type of evidence (e.g. phone numbers would become stronger indicators than bigrams), as well as the relevance of specific pieces of evidence (e.g. some phone numbers are relevant to the current seed). The main intuition of RedThread is that the candidates for inclusion into the case graph are explored in order of how well-connected they are to the labeled nodes already explored, whereas the importances of these connections are updated incrementally based on the user's feedback.In our experiments, we also use two publicly available datasets not related to escort ads to show RedThread's general applicability: (i) a music record dataset where RedThread retrieves records of the same artist, and (ii) a news dataset where RedThread finds memes originating from the same source. In all our datasets, RedThread achieves significant improvements over the baselines, including a random-walk which restarts given negative feedbacks. To summarize, our main contributions are twofold: first, we introduce the novel problem of Active Search of Connections motivated by its application in case building; second, we present RedThread method as an efficient, local and interpretable solution. For reproducibility, our source code is made publicly available at: https: //github.com/rabbanyk/RedThread.RELATED WORKRelated work in Section 2.1 overviews the application-related works on analyzing online escort advertisements, and Section 2.2 covers theory-related algorithms to Active Search of Connections.Analyzing Online Escort AdvertisementsOnline classified advertising and social networking websites provide easy to use and low-risk platforms for traffickers which give them a sense of anonymity and enable wide geographic reach Given the scale and importance of the problem, there have been multiple recent efforts to i) capture and extract information from the web mainly in form of knowledge graphs In our experiments, we use hard identifiers such as phone numbers as a proxy for a human expert for evaluation purposes since using an actual expert is not possible for experiments. In practice, the phone numbers will be part of the evidence set and the labels will be queried actively from the user. This adaptability is not available with the current classifier based techniques which also use the hard identifiers -phone numbers-as the training labels, e.g. Active Search, Exploration and ClusteringSelective labeling in active learning enables analyzing data where labels are scarce. Here a given number of labels, determined by the query budget, can be queried from the user/oracle during the learning. In this setting, the task of recovering only the relevant portion of the data is known as active search RedThread uses a similar learning framework but provides a local and efficient solution which draws inspiration from the developed optimal policy of these methods, i.e. sampling highly correlated regions Local clustering algorithms, a.k.a. community detection, are generally applied to when data is large scale. While global clustering algorithms partition a given graph, the local methods retrieve a cluster by expanding from a given seed node. Although the relevant entities in Active Search of Connections are assumed to be highly connected, the objective is different from the clustering algorithms, as we are interested in finding entities with positive labels which reflect the user's interest. There are several local graph clustering algorithms which also incorporate attributes for nodes Finally, RedThread is also related to measuring proximity in graphs PROBLEM DEFINITIONConsider n datapoints D = {d 1 , d 2 . . . d n } connected to k different types of evidence (e.g. phone number, image, bi-grams) which we refer to as modalities. Let evidence setdenote the set of indicator matrices for these k modalities, i.e.where c m is the cardinality (number of unique pieces of evidence) of modality m (e.g. number of unique phone numbers). Each column of X m shows a set of datapoints that share the corresponding evidence (e.g. datapoints that all share a particular phone number), and each row of X m shows the pieces of evidence associated to the corresponding datapoint (all the phone numbers mentioned in a particular datapoint). Shared evidence across modalities for two datapoints i and j can be derived as:Given the evidence set E, Active Search of Connections finds datapoints of interest which are related to the given seed i through shared evidence; whereas being of interest is queried from the user or oracle. More precisely, Definition 3.1 (Active Search of Connections). Given seed i \u2208 D, evidence set E (Eq. ( where we assume the unknown label y j \u2208 {\u22121, 1} can be queried from the oracle for each j \u2208 {1..n}.We emphasize that in Active Search of Connections, labels are local and depend on the given seed, since they indicate whether other datapoints are related to the current case. This is different than the usual active search framework where the labels are global and positive labels might not be connected through the structure, for instance in active search on graphs METHODOLOGYWe can consider each indicator matrix in the evidence set E represents the incidence matrix of a hypergraph, or the feature matrix of the datapoints, or the biadjacency matrix of a 2-partite graph. We will adopt the latter in the rest of the paper, from which all the modalities form a k-partite graph representation for the data. We refer to this as k-modal evidence (KME) graph. More specifically, Definition 4.1 (k-modal evidence graph). Given evidence set E construct a heterogeneous graph with vertex setwhere {u m 1 , u m 2 , . . . u m c m } are c m nodes corresponding to the pieces of evidence of type m, i.e. the unique values of modality m (e.g. one node per each phone number); and {v 1 , v 2 . . . v n } show the nodes corresponding to the n given datapoints ( e.g. one node per each advertisement). Edge set of this representation is then defined as:which considers an edge between each data point and every evidence it is associated with, e.g. edges would be formed from the node corresponding to the i t h advertisement, v i , to the pieces of evidence in that advertisement, phone numbers, images, etc.Given this k-modal evidence graph and seed node of interest v i , the Active Search of Connections translates to finding nodes in V D that are (tightly) connected to node v i with even length paths (i.e. through shared evidence) and are relevant to the seed (i.e. have positive labels).In the next section we propose RedThread which navigates through this graph to efficiently find these nodes while learning the importance of each modality and each piece of evidence from the labels (user's feedbacks) obtained while expanding. Before that, we start with describing the general learning framework and baselines.Basic BaselinesThe baseline algorithms as well as the RedThread have an iterative nature. The algorithms start from the given seed, and in each iteration picks another datapoint that it deems related to the seed, then queries the user to see if this is correct i.e. observes the label. Algorithm 1 outlines this framework. // pick a datapoint j 5:if j L then 6:L[j] \u2190 or acl e (j ) // query user if j is relevantend if 9: end while 4.1.1 Random (Rand). The most naive baseline picks a datapoint at random, while ignoring both the given evidence and user's feedbacks. In other words, the function in f er () in algorithm 1 simply chooses j from {1 . . . n} uniformly at random.To make use of the available evidence, the following algorithms construct the k-modal evidence graph of definition 4.1 (KME), and navigate through it. From here on we use i and v i interchangeably to denote the i t h datapoint and its corresponding node in the graph.Random Walk (RW ). This baseline expands from the seed by randomly walking through its neighbors, querying labels from the expert on each encounter of an unlabeled node. In more details, given seed node v, N (v) = {u |(v, u) \u2208 E} gives the pieces of evidence connected to v, from which an evidence node u is selected uniformly at random to expand from, that is by randomly choosing the next node from the neighbors of u which are not previously labeled as negative, i.e. N (u) \\ L \u2212 , where L \u2212 = {j \u2208 L|y j < 0}. This process repeats until budget is exhausted and restarts from the seed whenever it gets a negative feedback. In this way, the random walk is taking into account the feedbacks by only expanding on positives. We are not, however, learning what caused the algorithm to reach to a positive or negative instance. The aspect missing here is present in the heart of RedThread, which learns the importance of different pieces of evidence and types of evidence as it acquires labels. Before moving to the description of the learning mechanism, we discuss one more baseline which uses a weighting scheme to adjust the importance of different pieces of evidence.Random WalkAdjusted by Inverse Degree (wRW ). Considering rare pieces of evidence are more telling, here we pick the evidence nodes proportional to the inverse of their degree, i.e. probability of u being selected iswhere d u denotes the degree of evidence u, d u = |N (u)|. Since the random walk still picks the neighbors of the evidence nodes uniformly at random, the transition probability from node v i to v j through evidence u would be 1/d 2 u . This is analogous to reweighting the edges connected to the evidence nodes by their inverse degree, which RedThread also uses. This weighting scheme resembles the popular inverse document frequency commonly used in information retrieval.RedThreadRedThread considers weights for different modalities (evidence types), assuming that some evidence are stronger than others, e.g. sharing a phone number is a stronger indicator of two ads being related than advertising persons with the same names. Moreover, given the current seed node, specific pieces of evidence become more and more relevant as they point to more positive instances, therefore the weight of evidence should also be adjusted as we get more labels. In the following, we first describe the parameters of RedThread, then explain how RedThread adaptively learns these parameters as labels are acquired, and applies them to infer the best node to query next from the user for labeling.Weighing the Modalities.RedThread considers a weight for each partition of the KME graph to enforce the importance of its corresponding modality in the given evidence set, i.e.Here \u03b8 m denotes the weight/importance of modality m. Correspondingly, RedThread considers the evidence flow coming from each partition separately, i.e. let s j m show the evidence support for v j from modality m, then we consider a tie-strength vector for each expansion candidate v j as:4.2.2 Computing the Evidence Flow. We can consider s j m simply as the number of (length two) paths that go through evidence nodes in partition m to reach v j . To enforce the hypothesis that rare pieces of evidence are more important, RedThread further weighs down the pieces of evidence by their prevalence (the number of datapoints they are associated with) measured as their degree in the graph. In more detail, if node v i and v j are connected through evidence u, this evidence contributes to their tie strength by 1/d 2 u , as each edge is down weighted by the degree. Moreover, the tie strength score of reaching v j is summed from all the currently explored (labeled) positive nodes aswhere L + = {j \u2208 L|y j > 0}, and4.2.3Inferring from the model and learning from feedback. To infer the next node, RedThread chooses the node with highest overall evidence support, as 1 :Now let node v j denote this last queried node, for which we observe the label y j . RedThread adjusts the modality which most supported the selection of node j, i.e. it first determines the support modality,Then it adjust the importance/credit of the modality m * as:where \u03b4 \u2208 (0, 1) is the learning rate. This penalizes or rewards the supporting modality based on the positive and negative labels. We note that this learning mechanism is inspired by the weighted majority voting and minimum regret learning.Initialization.Prior information on the importance of different modalities (e.g. phone number more important than unigrams) could be easily incorporated into the model as the initial values for \u0398. When no such information is available, \u0398 is initialized uniformly. In applications where cases are similar across different seeds, i.e. same evidence types are always important, e.g. phone numbers in our case building, the modality weights learned from one seed could be transfered to the next seed as initial parameters.4.2.5Re-Weighing the evidence. Given the observed negative labels, one can adjust the weights of different pieces of evidence, assuming some pieces of evidence (within or across different modalities) point to more relevant nodes. To enforce this, we re-weigh the pieces of evidence proportional to how many positive v.s. negative instances they point to. In more detail, instead of Eq. ( where \u03b4 (j) = 1 if y j > 0 and \u03b4 (j) = \u22121 when y j < 0.Computational Efficiency and Implementation.RedThread uses local computation similar to local clustering methods. It maintains a priority queue to keep track of the top candidate for expansions (i.e. for computing Eq. ( Decompositional BaselinesTo better evaluate the performance of the RedThread and the effect of different its components, we consider four more baselines descried as follow.RedThread without Feedback (NF). This baseline uses the scoring scheme described in Section 4.2.2 to expands from the node with the current maximum score, using Eq. ( RedThread without Parameters (NP). This baseline consider feedback by only expanding from positive nodes, but does not have any parameter learning.RedThread without Modality Parameters (NM). This baseline uses the feedback to re-weigh the pieces of evidence using the procedure described in Section 4.2.5. However the importance of different modalities is not adjusted by the feedback.RedThread without Evidence Weights (NE).This baseline uses the feedback to learn the importance of different modalities but skips the re-weighing of the evidence (Section 4.2.5).In the next section, we present a selected set of experiments to showcase the effectiveness of the proposed RedThread.EXPERIMENTSHere we first describe our datasets, then compare the performance of the RedThread with different baselines introduced in Section 4.DatasetsOur data consists of advertisements scraped from escort section of Backpage.com for cities across the United States of America, Canada, and their territories from August 2013 to January 2017. For each advertisement, we have access to its unstructured text (title and body), attached images, date and location posted. Overall, we have about 40 million advertisements. We use a publicly available regular expression extractor, which is developed for the same data previously For the evaluation of the algorithms, and since labels are not available in these datasets, we keep out of the graph some modalities that strongly indicate relations between ads to derive labels. In more detail, we construct labels by connected components formed when only using url, email, and phone numbers. This means that two ads are assumed 'truly' related only if they share at least one of these hard identifiers 3 . Here, the RedThread will only use the first eight evidence types reported in Table We also include two publicly available datasets in our experiments. This further shows the generality of RedThread when applied in different domains. The first dataset is the Discogs from KONECT the release, artists (primary and extra) involved, record labels, track information, companies involved in the production of the release, etc. Full list of entities and their frequencies (number of unique values) is reported in Table Performance Comparison with BaselinesWe compare the general performance of RedThread against different baselines discussed in Section 4.1. In particular, Fig. In Fig. an unsupervised local clustering algorithm. Moreover, performance is only slightly better for the random walk variations which use the feedback to restart (RW and wRW ) but ignore the importance of different modalities and don't remember the factors that resulted in reaching a negative. The variations of RedThread which only consider weights on evidence (NM) or modalities (NE) are also not doing as well as when incorporating both in RedThread. We further see that in general transferring weights improves the performance of RedThread in these datasets. The choice to transfer or reset depends on whether the modalities have the same importance across cases or not, which depends on the applications, For the two public datasets we observe a better performance when reseting between seeds, which is reported in Fig. Filtering NearDuplicates in Escort Advertisements. In the human trafficking datasets it is common to have a large number of near duplicates, i.e. the same advertisement posted repeatedly over time. We filter out these duplicates in the results reported in Fig. We consider two ads to be near duplicates if they share images, have a very similar text, and are advertising the same person(s). In more detail, the inferred advertisement j is compared pairwise with all of the ads the algorithm has returned thus far excluding those that were labeled negative, i \u2208 L \u2212 , and is detected as a near duplicate if it duplicates any of them: j duplicates i, iff it uses the exact set of names as i, shares at least one image with i (when they both contain images), and at least 95% of their bigrams (used in  title or body) overlaps. A more sophisticated duplicate matcher or entity resolution method could be plugged in here, but this is out of the scope of current paper.Skipping High ConfidenceQueries. In the same manner as above, there are ads which the algorithms reach which are very similar to the currently positive set, but are not quite duplicates. Given the limited query budget, it would be better to skip querying the user with these high confidence matches. Note that we are not trying to select the queries for the most information gain, and are still mainly focused on maximizing the number or relevant entities found given the fixed query budget. This number would be boosted significantly if we skip querying those we are almost sure would be positive. Hence we consider a second post-processing procedure which filters queries to user. Similar to the near-duplicate detector, this process performs a pairwise comparison with all of the ads that the algorithm has returned thus far excluding members of L \u2212 , and skips querying a selected ad if it shares more than 90% of its quad-grams with one of the positive labeled ads. This procedure is applied to all the baselines and RedThread in similar fashion to the near duplicator discussed in Section 5.2.1.Figure Precision and Recall.When the query budget is fixed and we are not skipping the high confidence matches, i.e. in Fig. Constructing Graphs from Inferred LinksFor a given set of positive labeled set, we can track and plot in which order and with what score the advertisements were inferred, to derive a graph of how advertisements are connected. Based on this, we can construct i) a heterogeneous graph with evidence and advertisement nodes, where length two paths show the ads that are connected and how; and ii) a multi-edge graph in which advertisement nodes are connected by edges of different type (evidence); or iii) a simple graph where advertisements are connected by aggregate edges of all the pieces of evidence they share. Figure CONCLUSION AND FUTURE WORK", "conclusions": "In this paper, we defined Active Search of Connections based on its use-case for spotting organized activities in escort advertisements. Active Search of Connections finds related and relevant datapoints to a given lead through their shared evidence. We presented RedThread as an efficient solution which searches locally by expanding from the lead and learns interactively by querying labels.RedThread considers a heterogeneous structure to account for different evidence types. We experimentally compared RedThread performance with different baselines on five different datasets, two of which are publicly available. 8 The code for RedThread is released publicly.There are multiple lines of future work which we enumerate throughout the paper. This includes improving the post-processing procedures to further enhance recall of RedThread and baselines, and interactive visualization to present the results of RedThread in an accessible form to facilitate the collection of feedback from domain expert investigators. From a research perspective, we also envision the following future objectives: i) flagging suspicious seeds automatically, to generate leads for RedThread, using unsupervised pattern and anomaly detection methods. ii) incorporating additional databases as evidence modalities, e.g. bitcoin wallets, social media information, etc; iii) incorporating features on specific nodes in the inference, e.g. indicators on advertisement's textual component: third person style, authorship, etc; and iv) using graph databases to allow fast online storage and analysis of the data.", "SDG": [5]}, "a_comparative_study_of_fairness_enhancing_interventions_in_machine_learning": {"name": "A comparative study of fairness-enhancing interventions in machine learning *", "abstract": " Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption. We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought. \u2022 Computing methodologies \u2192 Machine learning; \u2022 Software and its engineering \u2192 Software libraries and repositories;", "keywords": "Fairness-aware,machine,learning,,benchmarks", "introduction": "As the use of machine learning to make decisions about people has increased, so has the drive to make fairness-aware machine learning algorithms. A considerable body of research over the past ten years has produced algorithms for accurate yet fair decisions, under varying definitions of fair, for goals such as non-discriminatory hiring, risk assessment for sentencing guidance, and loan allocation. And yet we have not yet seen extensive deployment of these algorithms in the pertinent domains. The primary technical obstacle appears to be our ability to compare methods effectively across different evaluation measures and different data sets with consistent data preprocessing and testing methodologies. Such comparisons would not just reveal \"best-in-class\" methods; they would also suggest which measures are robust and how different algorithms are sensitive to different kinds of preprocessing. As pointed out by Lehr and Ohm , such considerations of the data processing pipeline are not just important for efficient implementation but also have legal ramifications for the resulting automated decision-making process.[24]In this paper, we present a test-bed to facilitate direct comparisons of algorithms with respect to measures on a variety of datasets. Our open-source framework allows for the easy addition of new methods, measures and data for the purpose of evaluation. We show how to use our test-bed for determining not only which specific algorithm has the best performance under a fairness or accuracy measure, but what types of algorithmic interventions 1 tend to be the most effective. In addition to the impact of these algorithmic choices, we examine the impact of different preprocessing techniques and different measures for accuracy and fairness that have an important, and previously obscured, impact on the results of these algorithms. Our goal is to provide a comprehensive comparative analysis of existing approaches that is currently lacking in the literature.", "body": "As the use of machine learning to make decisions about people has increased, so has the drive to make fairness-aware machine learning algorithms. A considerable body of research over the past ten years has produced algorithms for accurate yet fair decisions, under varying definitions of fair, for goals such as non-discriminatory hiring, risk assessment for sentencing guidance, and loan allocation. And yet we have not yet seen extensive deployment of these algorithms in the pertinent domains. The primary technical obstacle appears to be our ability to compare methods effectively across different evaluation measures and different data sets with consistent data preprocessing and testing methodologies. Such comparisons would not just reveal \"best-in-class\" methods; they would also suggest which measures are robust and how different algorithms are sensitive to different kinds of preprocessing. As pointed out by Lehr and Ohm In this paper, we present a test-bed to facilitate direct comparisons of algorithms with respect to measures on a variety of datasets. Our open-source framework allows for the easy addition of new methods, measures and data for the purpose of evaluation. We show how to use our test-bed for determining not only which specific algorithm has the best performance under a fairness or accuracy measure, but what types of algorithmic interventions 1 tend to be the most effective. In addition to the impact of these algorithmic choices, we examine the impact of different preprocessing techniques and different measures for accuracy and fairness that have an important, and previously obscured, impact on the results of these algorithms. Our goal is to provide a comprehensive comparative analysis of existing approaches that is currently lacking in the literature.Our resultsOur evaluation yields the following major findings.Fairness-accuracy tradeoffs depend on preprocessing (Section 5). Different algorithms tend to have slightly different requirements in terms of input: how are sensitive attributes encoded? Are multiple sensitive attributes supported? Does the algorithm directly support categorical attributes or are attribute transformations required? Choices for these requirements directly affect the accuracy and fairness of a fairness-aware classifier. This is significant because prior formal studies of fairness-accuracy tradeoffs typically focused on hyperparameter tuning, rather than preprocessing.Measures of discrimination correlate with each other (Section 6).Even though there has been a proliferation of measures designed to highlight discrimination instances by machine learning algorithms, we find that a large number of these measures tend to strongly correlate with one another. As a result, techniques optimizing for one measure could perform well for a different measure (and similarly for poor performance).Algorithms make significantly different fairness-accuracy tradeoffs (Section 7). The specific mechanisms that different algorithms employ to increase fairness are quite varied, but surprisingly, the actual predictions made by these algorithms tend to vary significantly as well. As a result, no algorithm's performance (as of the latest state of our benchmark) appears to dominate, either in accuracy or fairness measures.Algorithms are fragile: they are sensitive to variations in the input (Section 7). We find surprising variability in fairness measures arising from variations in training-test splits; this appears to not have been previously mentioned in the literature.BACKGROUNDFairness-aware machine learning algorithms seek to provide methods under which the predicted outcome of a classifier operating on data about people is fair or non-discriminatory for people based on their protected class status such as race, sex, religion, etc., also known as a sensitive attribute. Broadly, fairness-aware machine learning algorithms have been categorized as those preprocessing techniques designed to modify the input data so that the outcome of any machine learning algorithm applied to that data will be fair, those algorithm modification techniques that modify an existing algorithm or create a new one that will be fair under any inputs, and those postprocessing techniques that take the output of any model and modify that output to be fair Algorithm modifications. Modifications to specific learning algorithms, e.g., in the form of additional constraints, have been by far the most common approach. We study three such methods in this paper. Kamishima et al. Another method that combines preprocessing and algorithm modification is the work by Zemel et al. Postprocessing techniques. A third approach to building fairness into algorithm design is by modifying the results of a previously trained classifier to achieve the desired results on different groups. Kamiran et al. In this paper we focus on group fairness approaches that aim to ensure non-discrimination across protected groups where the goal is to optimize metrics such as disparate impact. Another line of thought, known as individual fairness, is detailed in Related WorkThree prior efforts are relevant to our work. FairTest There are other software packages that audit black box software to determine the influence of individual variables. We omit a detailed description of these approaches as they are out of the scope of the investigation presented here. For more information, the reader is referred to the excellent new survey on explainability by Guidotti et al. BENCHMARK STRUCTUREIn order to provide a platform for clear comparison of results across fairness-aware machine learning algorithms, we separate each stage of the learning and analysis process (see Figure In order to encourage easy adoption of this codebase as a platform for future algorithmic analysis, each of these choices is modularized so that adding new datasets, measures, and/or algorithms to the pipeline is as easy as creating a new object. The pipeline will then ensure that all existing algorithms are evaluated under the new dataset and measure. More details and instructions for adding to the code base can be found at the repository. 4   DATAWe perform all experiments based on five real-world data sets that have been previously considered in the fairness-aware machine learning literature and preprocess each consistently depending on the needs of the algorithm. 5 The real-world datasets come from some of the domains impacted by questions of fairness in machine learning: hiring and promotion, credit-worthiness and loans, and recidivism prediction.Ricci. The Ricci dataset comes from the case of Ricci v. DeStefano Adult Income. The Adult Income dataset German. The German Credit dataset ProPublica recidivism. The ProPublica data includes data collected about the use of the COMPAS risk assessment tool in Broward County, Florida ProPublica violent recidivism. The violent recidivism version of the ProPublica dataPREPROCESSINGEach algorithm we will analyze has certain requirements for the type of data it will operate over, and these necessitate different preprocessing techniques. However, in order to provide a consistent comparison across algorithms, it's important that each algorithm receive the same input. We reconcile these needs by creating types of inputs that multiple algorithms can handle. Algorithms that handle the same input can be directly compared to each other. Algorithms can also be compared across different preprocessing strategies for the same dataset, even though in this setting conclusions are less clear, since the two sources of variability might interfere with one another.Our first preprocessing step is to modify the input data according to any data-specific needs: removing features that should not be used for classification, removing or imputing any missing data, and potentially removing items or adding derived features. In order to allow the analysis of fairness based on multiple sensitive attributes (e.g., not just ensuring fairness based on race or sex alone, but based on both someone's race and sex) we also add a combined sensitive attribute (e.g., attribute \"race-sex\" with values like \"White-Woman\") to each dataset that contains multiple sensitive attributes. All algorithms will receive versions of the dataset with this same preprocessing applied.While some algorithms are able to handle the datasets for training with only the described initial preprocessing (we'll call this version of the processed data original), most algorithms considered here have additional constraints. 6 For algorithms that can only handle numerical training data as input, we modify the data to include one-hot encoded versions of each categorical variable and call this version of the data numerical. Some algorithms additionally require that the sensitive attributes be binary (e.g., \"White\" and \"not White\" instead of handling multiple racial categorizations)for this version of the data (numerical+binary) we modify the given privileged group to be 1 and all other values to be 0. The three data tags should be interpreted as indicating constraints on the algorithms that use them.AnalysisWith these preprocessed versions of each data set in place, we can compare how a single algorithm performs relative to all versions of the dataset on which it can run. The most common form of input for the algorithms we consider here is numerical, and all these algorithms can additionally handle the numerical+binary version of the dataset. This gives an opportunity to determine the effect, per algorithm and per dataset, of allowing an algorithm access to full information about sensitive attribute categorization or only a binary summary.Figure We can do a similar analysis on the fairness achieved by the methods, as seen in the right side of Figure MEASURESThere are many ways to evaluate the accuracy and fairness of a model. Rather than be exhaustive, 7 we will focus on representative measures for each aspect. Let D = (X, S, Y ) be a dataset where X is the data subset that can be used for training (whether categorical or numerical), S is the sensitive attribute where 1 is the privileged class, and Y is the binary classification label where 1 is the positive outcome and 0 is the negative outcome. Let \u0176 be the predicted outcomes of some algorithm. We can define accuracy and fairness measures in terms of conditional probabilities of outcome variables (Y , \u0176 ) with respect to variables like \u0176 and S.Accuracy measuresWe consider the standard accuracy measures: the (uniform) accuracy (P[ \u0176 = Y ]), the true positive rate (TPR) (P[ \u0176 = 1 | Y = 1]), and the true negative rate (TNR) (P[ \u0176 = 0 | Y = 0]). We also consider All of these measures lie in the range [0, 1].Fairness measuresFairness measures can be divided into three broad categories, in all cases conditioned on values of the sensitive attribute S. In what follows, we normalize measures to make comparisons easier. In all cases, the measures lie in the range [0, \u221e) or [0, 2] where in both cases perfect fairness is achieved at 1. We note that some of measures have appeared in the literature not as something to be optimized (to be close to 1) but as a constraint to be satisfied (i.e, that the appropriate value must equal 1).Measures based on base rates.Definition 6.2 (Disparate Impact (DI) This measure is inspired by one of the two tests for disparate impact in the legal literature in the United States This measure is the same as DI, but where the difference is taken instead of the ratio; such a measure has been used for example to measure discrimination in the United Kingdom Measures based on group-conditioned accuracy.In general, we can think of fairness measures based on group-conditioned accuracy as asking whether the error rates for each group are similar. This yields the following definitions. Definition 6.4. (Group-conditioned accuracy measures.)We note that these measures have been studied under different names. For example, error rate balance In each of these cases, as we saw above, the unprivileged sensitive values could be grouped together or handled separately in the ratio or difference. For example, consider a dataset where race is the sensitive attribute and which has been preprocessed so that the sensitive attribute takes binary values. In this case, the accuracy conditioned on having a sensitive value of 1 (e.g., \"White\") is denoted as the 1\u2212accuracy. We will denote the average of the 1\u2212accuracy and 0\u2212accuracy in this case as the race-accuracy (or in general as the sensitive-accuracy) and the mean of the perrace differences, i.e., s \u2208S [1 \u2212 (1-accuracy \u2212 s-accuracy)]/|S |, as the comparative-race-accuracy (or in general as the comparativesensitive-accuracy). We'll use the same naming scheme for other accuracy measures and other sensitive attributes.  Calibration has been introduced previously with the goal of equalizing across sensitive value AnalysisMost of the algorithms considered here (discussed in more detail in Section 7) were analyzed with respect to the single fairness measure being introduced in the paper, or with respect to a subset of the measures. Incorporating all of the above accuracy and fairness measure variations into our software framework allows us to examine measure trends across multiple measures and multiple algorithms. While these measures are often presented as opposing, here we are interested in analyzing the extent to which this is true in practice.There are many variations on these and other measures, but we find many of these are correlated on these algorithms and datasets. This is not entirely surprising as these measures are definitionally related. For example, DI takes the ratio of two probabilities while CV takes the difference. However, by analyzing resulting measures across many algorithms, we find correlations that are less obvious. In fact, it appears that there are a few main clusters of measures.In Figure To determine the extent how this clustering was impacted by the skew in the data (in terms of both class and sensitive attribute), the datasets were downsampled uniformly with replacement to contain 1000 items that were balanced so that 500 have the positive classification and 500 the negative, and within each of those 250 have the privileged sensitive attribute and 250 have an unprivileged value. The bottom of Figure Within this clustering there are some clear patterns. Pairs of accuracy measures and their sensitive counterparts (e.g., accuracy and sensitive-accuracy, TPR and sensitive-TPR, and TNR and sensitive-TNR) are always clustered together. Recall that sensitive-accuracy is the average of the accuracy on the privileged group (the 1-accuracy) and the accuracy on the unprivileged groups (the 0-accuracy), so it makes sense that improving the overall accuracy would improve this average as well. Perhaps more surprisingly, the 0-accuracy and 1-accuracy are also strongly positively correlated, i.e., improving the accuracy on the privileged group also improves the accuracy on the unprivileged groups on these algorithms and datasets.A caveat to the strength of these clusterings is that these results only consider the measures when assessed on these algorithms. Algorithms might exist or be created that focus on optimizing one specific measure that change these clusterings, especially in cases where the rationale for the clusterings is less obvious (e.g., the clustering of accuracy and TNR together). But this experiment does allow us to assess in practice how optimizing for one fairness measure affects other fairness measures.The Calders, Feldman, Kamishima, and Zafar algorithms were all designed to optimize DI, CV, or similarly motivated measures. Since DI and CV are analytically closely related to each other, optimizing for one can be reasonably expected to optimize for the other. But does optimizing for these base rate focused fairness measures also optimize for the group-conditioned accuracy focused fairness measures? When considering only these fairness-aware algorithms, the clusterings presented in Figure Additionally, in some cases we expect would be tradeoffs between measures. Assuming unequal base rates across populations, impossibility results show it is impossible to achieve both calibration and error rate balance (both the same false positive rate and the same false negative rates across groups) ALGORITHMSWe choose a selection of existing fairness-aware algorithms to assess, based on availability of source code and diversity of fairness interventions (e.g., preprocessing versus algorithm modification). Each algorithm is run on each dataset and each metric is calculated on the predicted results. 9 Synthesis statistics (such as stability) are then calculated and comparison graphs are produced. 10 We analyze the following algorithms along with non-fairness-aware algorithms chosen for a baseline comparison: SVM, decision trees, Gaussian naive Bayes, and logistic regression (LR). Calders and VerwerFeldman et al. values of \u03bb at increments of 0.05 in [0, 1] are included when the algorithm is optimized using a grid search over the parameters. The implementation comes from Feldman et al. Zafar et al. Figure StabilityWhen analyzing algorithms, we are additionally concerned with stability -will the algorithm still perform well if the training data is slightly different? To assess this, we considered the standard deviation of each metric over 10 random splits, where for each split the algorithm is trained and evaluated on a different train/test partition. The results are shown in Figure ParametersMany fairness-aware learning algorithms provide a parameter to allow manually trading off fairness and accuracy. We automate the search for this balance and present results for all algorithms optimizing accuracy or fairness. This provides an additional means of testing the algorithm, as well as the possibility for further optimizing the tradeoff between the two. Figure Multiple sensitive attributesFigure While there are still few fairness-aware algorithms that can explicitly handle multiple sensitive attributes ( DISCUSSION", "conclusions": "Besides providing a central point of access to existing fairnessenhancing interventions and classification algorithms, our benchmark also highlights a number of gaps in the current practice and reporting of fairness issues in machine learning. We conclude with the following recommendations for future contributions to the area: Emphasize preprocessing requirements. If there are multiple plausible ways in which a dataset can be processed to generate training data for an algorithm, provide performance metrics for more than one of the possible choices. If algorithms are being compared to each other, ensure they are compared based on the same preprocessing.Avoid proliferation of measures. New fairness measures should only be introduced if they behave fundamentally differently from existing metrics. Our study indicates that a combination of groupconditioned accuracy and either DI or CV is a good minimal set.Account for training instability. Showing the performance of an algorithm in a single training-test split appears to be insufficient. We recommend reporting algorithm success and stability based on a moderate number of randomized training-test splits.One limitation of our benchmark is the number of methods it currently provides implementation for. We hope other researchers will contribute their implementations to the repository. It would be particularly interesting to see how our conclusions above evolve as the number and variety of methods increases.Additionally, while we frame some of the differences in algorithm performance as fairness versus accuracy tradeoffs, this can be misleading since it makes many assumptions about the data and social context, including, e.g., that the labels represent desired outcomes. We leave the examination of how the algorithmic choices interact with the social context for other work.", "SDG": [5]}, "a_non_parametric_learning_approach_to_id": {"name": "A Non-Parametric Learning Approach to Identify Online Human Trafficking", "abstract": " Human trafficking is among the most challenging law enforcement problems which demands persistent fight against from all over the globe. In this study, we leverage readily available data from the website \"Backpage\"-used for classified advertisement-to discern potential patterns of human trafficking activities which manifest online and identify most likely trafficking related advertisements. Due to the lack of ground truth, we rely on two human analysts -one human trafficking victim survivor and one from law enforcement, for hand-labeling the small portion of the crawled data. We then present a semisupervised learning approach that is trained on the available labeled and unlabeled data and evaluated on unseen data with further verification of experts.", "keywords": "", "introduction": "Human trafficking has received increased national and societal concern over the past decade . According to the United Nation [1], human trafficking is defined as the modern slavery or the trade of humans mostly for the purpose of sexual exploiting and forced labor, via different improper ways including force, fraud and deception. Human trafficking is among the challenging problems facing the law enforcementit is difficult to identify victims and counter traffickers.[2]Before the advent of the Internet, pimps were under the risks of being arrested by law enforcement, while advertising their victims on the streets . However, the move to the Internet, has made it easier and less dangerous for both sex buyers and sellers, especially for the pimps [3] as they no longer needed to advertise on the streets. There are now plethora of websites that host and provide sexual services, under categories of escort, adult entertainment, massage services, etc., which help pimps, traffickers and sex buyers (a.k.a. \"johns\"), maintain their anonymity. Though some services such as Craiglist's adult section and myredbook.com were shut down recently, still there are many websites such as Backpage.com that provide such services and many new are frequently created. Traffickers even use dating and social networking websites, including Twitter, Facebook, Instagram and Tinder to reach out to the johns and their other followers. Although Internet has presented new trafficking related challenges for law enforcement, it has also provided readily and publicly available rich source of information which could be gleaned from online sex advertisements for fighting this crime [4].[5]Although, the Internet is being used for many other activities including attracting the victims, communicating with costumers and rating the escort services, here we only focus on the online advertisements. In this study, we use data crawled from the adult entertainment section of the website Backpage.com and propose a non-parametric learning approach to identify the most likely human trafficking related online advertisements out of the escort advertisements. To the best of our knowledge, this is the first study that employs both data mining and semi-supervised machine learning techniques to identify the potential human trafficking related advertisements given only a small portion of labeled data. We thus make the following contributions.1) We collected real posts from the U.S. cities represented on Backpage.com. The data was then preprocessed and cleaned. 2) Based on the literature, we created different groups of features that capture the characteristics of potential human trafficking activities. The less likely human trafficking related posts were then filtered out using these features. 3) Due to the lack of ground truth, we relied on human analysts for hand-labeling small portion of the filtered data. 4) We trained a semi-supervised learner on labeled and unlabeled data and sent back the identified highly human trafficking related advertisements to the experts for further verification. We then validated our approach on unseen data with further verification of experts. The rest of the paper is organized as follows. In Section II, we briefly provide the background of the problem of human trafficking. Next, we review the prior studies on human trafficking in Section III. Then in Section IV, we explain our data preparation and feature extraction scheme. Our unsupervised filtering and expert assisted labeling are explained in Sections V and VI, respectively. We detail our non-parametric learning approach in Sections VII. We conclude the paper by providing future research directions in Section VIII.", "body": "Human trafficking has received increased national and societal concern over the past decade Before the advent of the Internet, pimps were under the risks of being arrested by law enforcement, while advertising their victims on the streets Although, the Internet is being used for many other activities including attracting the victims, communicating with costumers and rating the escort services, here we only focus on the online advertisements. In this study, we use data crawled from the adult entertainment section of the website Backpage.com and propose a non-parametric learning approach to identify the most likely human trafficking related online advertisements out of the escort advertisements. To the best of our knowledge, this is the first study that employs both data mining and semi-supervised machine learning techniques to identify the potential human trafficking related advertisements given only a small portion of labeled data. We thus make the following contributions.1) We collected real posts from the U.S. cities represented on Backpage.com. The data was then preprocessed and cleaned. 2) Based on the literature, we created different groups of features that capture the characteristics of potential human trafficking activities. The less likely human trafficking related posts were then filtered out using these features. 3) Due to the lack of ground truth, we relied on human analysts for hand-labeling small portion of the filtered data. 4) We trained a semi-supervised learner on labeled and unlabeled data and sent back the identified highly human trafficking related advertisements to the experts for further verification. We then validated our approach on unseen data with further verification of experts. The rest of the paper is organized as follows. In Section II, we briefly provide the background of the problem of human trafficking. Next, we review the prior studies on human trafficking in Section III. Then in Section IV, we explain our data preparation and feature extraction scheme. Our unsupervised filtering and expert assisted labeling are explained in Sections V and VI, respectively. We detail our non-parametric learning approach in Sections VII. We conclude the paper by providing future research directions in Section VIII.II. BACKGROUNDThe United States' Trafficking Victim Protection Act of 2000 (TVPA 2000) The Find Me Group (FMG) was founded by retired DEA Special Agent Jerry \"Kelly\" Snyder in 2002 primarily to locate missing persons. The natural evolution of the group in locating missing persons was to allocate resources for locating victims in human trafficking, as well as identifying the persons responsible and reporting these organizations to law enforcement. The FMG consists of current and retired law enforcement agents and officers with a wide-range of investigative expertise, including but not limited to linguistics, handwriting analysis, body language, missing persons and homicide. The search and rescue component of the FMG is also comprised of current and retired law enforcement officers and agents with 28 years of field management skills in locating missing persons. The FMG has an additional advantage by using trained experts/sources that provide detailed location information of human trafficking victims.The ultimate goal of the current project is to identify missing persons which are connected to human trafficking organizations. This can be done by identifying their locations, utilizing logistical methodology with an additional focus on their financial status and reporting assets to worldwide law enforcement.III. RELATED WORKRecently, several studies have examined the role of the Internet and related technology in facilitating human trafficking One of the earliest works which leveraged data mining techniques for online human trafficking was Beyond these works, the work of IV. DATA COLLECTION EFFORTWe collected about 20K publicly available listings from the U.S. posted on Backpage.com in March, 2016. Each post includes a title, description, time stamp, the poster's age, poster's ID, location, image, video and sometimes audio. The description usually lists the attributes of the individual(s) and contact phone numbers. In this work we only focus on the textual component of the data. This free-text data required significant cleaning due to a variety of issues common to textual analytics (i.e. misspellings, format of phone numbers, etc.). We also acknowledge that the information in data could be intentionally inaccurate, such as the poster's name, age and even physical appearance (i.e. bra cup size, weight). Figure A. Feature EngineeringThough many advertisements on Backpage.com are posted by posters selling their own services without coercion and intervention of traffickers, some do exhibit many common trafficking triggers. For example, in contrast to the previous advertisements, Figure Inspired from literature, we define and extract 6 groups of features from advertisements, shown in Table 1) Advertisement Language Pattern:The first group consists of different language related features. For the first and second features, we identify posts which has third person language (more likely to be written by someone other than the escort) and posts which contain first person plural pronouns such as 'we' and 'our' (more likely to be an organization) To ensure their anonymity, traffickers would deploy techniques to generate diverse information and hence make their posts look more complicated. They usually do this to avoid being identified by either human analyst or automated programs. Thus, to obtain the third feature, we take an approach from complexity theory, namely Kolmogorov complexity which is defined as the length of the shortest program to reproduce the advertisement content on a universal machine such as Turing Machine We expect higher values of the Entropy correspond to human trafficking. Finally, we discretize the result by using the threshold of 4 which was found empirically in our experiments.Next, we use word-level n-grams to find the common language patterns of the advertisements, as the character-level n-grams have already shown to be useful in detecting unwanted content for Spam detection 2) Words and Phrases of Interest: Despite the fact that advertisements on Backpage.com do not directly mention sex with children, costumers who prefer children, know to look for words and phrases such as \"sweet, candy, fresh, new in town, new to the game\" 3) Countries of Interest:We identify if the individual being escorted is coming from other countries such as those in Southeast Asia (especially from China, Vietnam, Korea and Thailand, as we observed in our data) 4)Multiple Victims Advertised: Some advertisements advertise for multiple women at the same time. We consider the presence of more than one girl as a potential evidence of organized human trafficking 5) Victim Weight:We take into account weight of the individual being escorted as a feature (if it is available). This information is particularly useful assuming that for the most part, lower body weights (under 110 lbs) correlate with smaller and underage girls 6) Reference to Website or Spa Massage Therapy:The presence of a link in the advertisement, either referencing to an outside website (especially infamous ones) or spa massage therapy, could be an indicator of more elaborate organization In order to extract these features, we first clean the original data and conduct preprocessing. Then we draw 999 instances out of our dataset for further analysis, as they might be evidence of human trafficking-this is described in the next section.V. UNSUPERVISED FILTERING Having detailed our feature set, we now construct feature vectors for each instance by creating a vector of 15 binary features that correspond to the important characteristics of human trafficking related posts.We obtain 999 instances from our dataset by filtering out samples that do not posses any of the binary features. We will refer to this as our filtered dataset. In Figure Moreover, since we lack ground truth for our data, we rely on human analysts (experts) for labeling the listings as either human trafficking or not. In the next section, we select a smaller yet finer grain subset of this data to be sent to the experts. This alleviates the burden of the tedious work of handlabeling.VI. EXPERT ASSISTED LABELINGWe first obtain a sample of 150 listings from the filtered dataset. This set of listings was labeled by two human experts: a previous human trafficking victim and a law enforcement officer who specialized in this type of crime. From this subset, a law enforcement professional and human trafficking victim identified 38 and 139 instances (respectively) to be human trafficking related instances. Among them, there were 31 records for which both experts agreed were highly related to human trafficking. Thus, we now have 31 confirmed positive samples, but still have large amounts of unlabeled examples (849 instances) in our dataset. We summarize the data statistics in Table VII. NON-PARAMETRIC LEARNINGWe use the Python package scikit-learn As we see from this table, out of 849 unlabeled data, our learner with RBF and KNN kernels assigned positive labels to the 147 and 188 instances, respectively. Next, we pass the identified positive labels to the experts for further verification. Our approach with RBF and KNN correctly identified 134 and 170 labels out of 145 and 188 positive instances and achieved precision of 92.41% and 90.42%, respectively. We further demonstrate the word clouds for the positive instances assigned by RBF and KNN, in Figure VIII. CONCLUSION", "conclusions": "Readily available online data from escort advertisements could be leveraged in favor of fighting against the human trafficking. In this study, having focused on textual information from available data crawled from Backpage.com, we identified if an escort advertisement can be reflective of human trafficking activities. More specifically, we first propose an unsupervised filtering approach to filter out the data which are more likely involved in trafficking. We then trained a semi-supervised learner on small portion of such data, handlabeled by human trafficking experts, to identify the labels for unseen data. The results suggest our non-parametric approach is successful at identifying the potential human trafficking related advertisements.In future work we seek to extract the underlying network of the data to find interesting patterns such as the most influential nodes as they might indicate the known pimps and traffickers. We would also like to replicate the study by integrating more features, especially those supported by the criminology literature.", "SDG": [5]}, "forecasting_domestic_violence_a_machine_learning_approach_to_help_inform_arraignment_decisions": {"name": null, "abstract": " We appreciate the help of Michael Gallagher, a retired police officer, who currently works with a domestic violence agency in the jurisdiction studied. He provided important information on the meaning of some of our variables and on related law enforcement procedures. We also received important assistance on the prosecutorial context of domestic violence from Marian G. Braccia, Deputy District Attorney in the District Attorney's Office of the jurisdiction. Thanks also go to the anonymous reviewers of this article.", "keywords": "", "introduction": "In this article, we address empirically the potential role of domestic violence forecasts when, at an arraignment, a judge, commissioner, or magistrate decides whether an offender can be released awaiting a formal hearing on the charges. In the past, forecasts of \"future dangerousness\" have sometimes been used at arraignments (Goldkamp &  Gottfredson 1985; VanNostran & Keebler 2009; Arnold Foundation 2013), but there have been to our knowledge no regularized applications of domestic violence risk assessment procedures in this setting. Our forecasting efforts are part of a larger pretrial reform initiative in a major metropolitan area.  A combination of machine learning and routine electronic information normally available at arraignment might be able to provide timely and useful domestic violence forecasts of risk. There are examples of successful forecasting in other criminal justice settings and for other kinds of crimes (Berk 2012). Moreover, machine learning forecasts can be delivered within a real time of several seconds. However, a major question is whether the information routinely available electronically prior to an arraignment is sufficiently rich to produce usefully accurate forecasts. We address this question using data on over 28,000 domestic violence arraignments. The performance of the forecasts is the empirical focus of this article.1", "body": "In this article, we address empirically the potential role of domestic violence forecasts when, at an arraignment, a judge, commissioner, or magistrate decides whether an offender can be released awaiting a formal hearing on the charges. In the past, forecasts of \"future dangerousness\" have sometimes been used at arraignments (Goldkamp &  Gottfredson 1985; VanNostran & Keebler 2009; Arnold Foundation 2013), but there have been to our knowledge no regularized applications of domestic violence risk assessment procedures in this setting. Our forecasting efforts are part of a larger pretrial reform initiative in a major metropolitan area. II. BACKGROUNDAlthough the details vary across jurisdictions, shortly after an arrest or an apprehension through a summons, there is an arraignment at which the offender receives a written copy of the charges alleged by the police. These charges typically have been reviewed by a representative from the district attorney's office and revised as needed. At the arraignment, a court official, variously called a judge, commissioner, or magistrate, decides whether to detain the offender in jail until the hearing or to release the offender, sometimes on bond or subject to certain conditions, with the requirement that the offender return to court on the hearing date. 2The arraignment is sometimes called a \"preliminary arraignment,\" in part because prosecutors have the option to unilaterally revise the charges subsequently.3In this article, we will use the title \"magistrate\" rather than judge or commissioner. Regardless of title, the tasks performed at a preliminary arraignment are essentially the same. and nearly instantaneous judgments (Reitler et al. 2013). Forecasts of flight risks are being addressed in other work. Here, we consider forecasts of domestic violence (DV). Domestic violence is among the more common charges heard at arraignments. Definitions vary, but in the jurisdiction for which our analysis is undertaken, \"domestic\" means an intimate relationship including dating, or a familial or blood relationship. Some equate domestic violence solely with intimate partner violence. There is a growing literature on ways to improve the processes and outcomes associated with arraignments (e.g., Bock & Frazier 1977; Frazier et al. 1980; Goldkamp &  Gottfredson 1985; Bridges et al. 1987; Demuth 2003; Devers 2011), even some studies based on randomized experiments (Goldkamp & White 2006; McElroy 2011; Bornstein  et al. 2013). Risk assessments have of late figured very significantly in these studies ( Van-Nostrand & Keebler 2009; Arnold Foundation 2013), but the risks being forecasted are typically defined as an arrest for any crime.It might seem that there is an easy fix: at the arraignment, use one of the better intimate partner risk assessment tools. However, even if one ignores troubling methodological 4 The U.S. Department of Justice defines domestic violence as \"as a pattern of abusive behavior in any relationship that is used by one partner to gain or maintain power and control over another intimate partner . . .\" (U.S. Department of Justice 2014).5A list of the crimes included can be provided upon request. There are several hundred such crimes. For example, there are about 50 kinds of sexual crimes. concerns (Farrington & Tarling 2003; Berk & Sorenson 2005; Gottfredson & Moriarty 2006;  Berk & Bleich 2013; Ridgeway 2013), popular intimate partner risk assessment tools typically do not consider the wide range of crimes that are folded into statutory definitions of domestic violence and require data that currently cannot be routinely obtained within the very short interval between arrest and arraignment (Roehl et al. 2005). That interval is typically less than 48 hours. Alternatively, one might consider whether machine learning procedures applied to routinely available criminal justice data lead to practical and sufficiently accurate forecasts. Data that typically are available include whatever is maintained on rap sheets, on records of past contacts with the courts, and some basic biographical information recorded when an arrest is made. These are no doubt slim pickings if the goal is to forecast domestic violence. For example, there often is no information on how the perpetrator and victim are related beyond what may be inferred from the charge of domestic violence, nothing about a household's economic circumstances, and no information on the offender's behavior and life circumstances more generally. Nevertheless, the inputs to most machine learning procedures are little more than raw material. Machine learning algorithms can transform, combine, and reconstruct a relatively small number of inputs into hundreds of predictors that may have little apparent relation to the inputs initially provided, but earn their keep by improving forecasting accuracy (Hastie et al.  2009; Berk 2012; Berk & Bleich 2013; Jordan & Mitchell 2015).For example, an offender's age may be, in effect, reconfigured as a set of indicator variables so that an empirically determined, nonlinear relationship with domestic violence is built. For violent street crimes, the ages of highest risk are the late teens and early 20s, after which the risk of perpetration drops dramatically (Berk & Bleich  2013). For domestic violence, one might anticipate a high risk well into the 30s and 40s. In both cases, age serves as a proxy for physical aging, life course events, and evolving relationships with others. There are no direct measures of such processes but perhaps appropriate reconstructions of age can suffice. Moreover, because a very large number of predictors can be exploited, one is not limited to predictors that are strongly related to the outcome being forecasted. A large number of weak predictors, which would ordinarily be dismissed, can in the aggregate dramatically improve forecasting accuracy.But all this comes at a price: machine learning procedures are \"algorithmic\" in nature (Breiman 2001b). There is no model in the usual statistical sense, and how the inputs are related to the outputs is not fully apparent. One is working with black-box procedures whose goal is accurate forecasts. Explanations of why the inputs are related to 6 It might be possible to construct different forecasting tools for different kinds of domestic violence, but the limitations of data available at arraignment remain. For example, time spent in jail after arraignment is plausibly related to the chances of a new domestic violence arrest and could be especially important were the goal to understand why some offenders fail. However, post-arraignment time spent in jail is unknowable at arraignment. outputs in any particular manner are a secondary concern and perhaps even unknowable (Berk & Bleich 2013).Readers already familiar with machine learning applications in criminal justice might wonder what is novel about the analysis to follow. To the best of our knowledge, this is the first machine learning application to forecasts of domestic violence, defined broadly as in the governing statute, to inform release decisions at a preliminary arraignment. The forecasting exercise is part of a local pretrial reform initiative. Past machine learning forecasting applications in criminal justice have addressed intimate partner violence to support police decisions when they arrive at the scene (Berk et al. 2005), misconduct in prison to help prison administrators assign inmates to appropriate security levels (Berk et al. 2006), homicides committed by probationers or parolees to identify individuals who pose a very serious threat to public safety (Berk et al. 2009), arrests for three different categories of crime committed while on probation or parole to rationalize the intensity of supervision and the kinds of services offered (Berk et al. 2010), and future violence to help judges make post-conviction sentencing decisions (Berk &  Bleich 2013, 2014; Berk & Hyatt 2015). The application reported in this article is unique and addresses offenses of great concern in many courtrooms across the country.III. DATA AND METHODSThe data used in this study were compiled as part of a larger effort to reform pretrial procedures and outcomes in a large metropolitan area. Here, we analyze 28,646 domestic violence arraignments leading to official charges and a release between The research design called for a two-year followup for each arraigned case through the end of October 2013. A two-year followup might seem to be a strange policy choice. One might assume that a magistrate's primary concern at a preliminary arraignment is what could happen between the time of a release and the time of the next court appearance. In most cases, that would be less than two years. However, local stakeholders made clear that a two-year followup is actually responsive to their decision-making needs because the two-year followup captures most of the cases they care about.It is possible for people to be detained while in pretrial posture after bail magistrates release them at arraignment. For example, if they fail to appear, or incur new charges or there is some issue, a judge can order detention at any phase of the trial. Traditionally, for measurement purposes we have looked at any pretrial misconduct from the point of release, through the adjudication of the case. Until the case is adjudicated, it is under the purview of Pretrial Services. The goal has always been to get offenders through trial successfully without any misconduct, not just to get them to their first court appearance. It would be fair to say this sentiment is shared by Pretrial Services, the Judiciary, and all stakeholders.\" (personal communication) 8These priorities are fully consistent with 1984 Bail Reform Act, which allows for the revocations and modifications of release conditions for wide range of stated reasons (Federal Judicial Center 1993:5-6), even if the release conditions have not been violated.Inputs used for forecasting were taken from electronic information available at arraignment. A list of inputs is provided in Table The post-arraignment outcomes to be forecasted were determined by stakeholders, which for this initiative included individuals on the oversight committee of the local Pre-Trial Reform Project. There were three domestic violence outcome classes defined to be consistent with our discussion above:1. DV0-no arrest for a domestic violence offense; 2. DV1-a domestic violence arrest not involving physical injury, an attempt to cause physical injury, or the threat of physical injury; and 3. DV2-a domestic violence arrest involving physical injury (including rape), an attempt to cause physical injury, or the threat of physical injury.The most pressing goal was to find a subset of offenders who could be released with no conditions and who were good bets not to be rearrested for domestic violence. Should a substantial number of such individuals be found, attention would then turn to the remaining offenders, who might be detained or released under a variety of imposed conditions, depending on the seriousness of domestic violence predicted. The mix of possible interventions is described briefly later.Stakeholders readily understood that the absence of a new arrest for domestic violence did not mean that no such crimes had occurred. Such crimes are underreported and, even when reported, often do not lead to an arrest. It is common, for instance, for police to arrive at the scene after the perpetrator has fled. The difference between a domestic violence incident and a domestic violence arrest has important policy implications that we address below.No effort was made to pare down the list of inputs, and there is no doubt substantial overlap in what is being measured. The machine learning procedure we favor has none of the problems conventional regression analysis would have with so many correlated variables and can even work with more forecasting inputs than observations. Consistent with practice in machine learning, the primary goal is not to determine which inputs are important and which are not, but to use all the inputs as a group to arrive at accurate forecasts.Our machine learning method of choice is random forests (Breiman 2001a), which is essentially a large ensemble of classification or regression trees. An outline of the algorithm is provided in the Appendix. There are several published examples of very successful criminal justice forecasting exercises using random forests, although some other machine learning procedures can forecast about as well (Berk 2012). IV. RESULTSThe mix of offenders at arraignment can be very different from the mix of offenders at other criminal justice decision points. Offenders at arraignment have been arrested, but have yet to be officially charged. For many offenders, prosecutors will choose not to proceed. If a decision is made to prosecute, some will be found not guilty at trial, and a much larger number will plead guilty to a much less serious offense. In short, offenders at arraignment can look somewhat different from those who commit crimes but who are not arrested and from those convicted of a crime who are then sentenced.At the same time, individuals at arraignment charged with domestic violence on the average look much like individuals at arraignment charged with other crimes who are released. We have information on all offenders arraigned, charged, and released between January 1, 2007 and October 31, 2011, not just those charged with domestic violence. Table The 15 percent figure for domestic violence priors might seem surprisingly low. However, as already noted, a domestic violence prior requires a domestic vio-10 All these methods will forecast as least as well as traditional regression approaches, and usually substantially better (Berk & Bleich 2013). Like all forecasting procedures, however, they assume that there is reasonable stability over the medium term in the processes that lead to domestic violence arrests. In this case, there were no major changes in statutes, administrative practices, or domestic violence interventions that would undermine the forecasting procedures. lence arrest. Someone must report a crime consistent with the statutory requirements for domestic violence, the police must respond and find that a crime has been committed consistent with that definition, and then the police must make an arrest.Consistent with the figure of 15 percent, a little less than 19 percent of the offenders are arrested for a new domestic violence offense during the two-year follow-up period. Only 1.7 percent are arrested for incidents in which there was no physical injury, no attempts to cause physical injury, and no threat of physical injury (DV1) compared to about 17 percent who are arrested for incidents involving physical injury, an attempt to cause physical injury, or a threat of physical injury (DV2). The relative absence of DV1 incidents is perhaps counterintuitive, but police officers may be disinclined to make an arrest unless there is visible evidence of simple assault, aggravated assault, or rape. We have anecdotal information to this effect. The rows and the columns address different questions. The rows provide information on how well a random forests application classifies known outcomes. Classification performance is used diagnostically to help determine the values of tuning parameters. The columns provide estimates of how well a random forests application will forecast when in practice the outcomes are not known. In the discussion to follow, the distinction between classification accuracy and forecasting accuracy is important and sometimes unappreciated.The performance assessments in Table A key factor affecting the entries in Table In this application, stakeholders believed that forecasting a DV0 when there was subsequently a DV2 was the worst possible false negative. They also believed that forecasting a DV2 when subsequently there was a DV0 was the worse possible false positive. The cost ratio of these kinds of false negatives to these kinds of false positives was set provisionally at 10 to 1.It is usually not possible with real data and an outcome with three classes to hit desired cost ratios exactly. The problem is partly mathematical. The most effective way to introduce asymmetric relative costs allows only for three cost-weighting tuning parameters. Yet, the cost ratios depend on the six off-diagonal cells. A complicating factor is that the data used to construct a confusion table are based on the randomly selected out-of-bag data, which necessarily introduces some noise. Nevertheless, with some trial and error, one can often come quite close. Consider the 2 3 2 subtable that includes only DV0 and DV2 cases. For this analysis, we approximate the 10 to 1 cost ratio reasonably well (12,936/1,206 5 10.7 to 1). Similar reasoning for relative costs can be applied to comparisons between any two outcome categories. Consider the 2 3 2 subtable that includes only DV0 and DV1 cases. Forecasting a case as DV0 when it is subsequently a DV1 is 3.2 times worse than forecasting that a case is a DV1 when it is subsequently a DV0 (272/85). Consider the 2 3 2 subtable that includes only DV1 and DV2 cases. Forecasting a case as a DV1 when it is subsequently a DV2 is 4.6 times worse than forecasting a case as DV2 when it is subsequently a DV1 (358/77). All cost ratios can affect the forecasts made because they shape 14The difference between an actual cost ratio of 10 to 1 compared to an actual cost ratio of 10.7 to 1 makes no practical difference for this analysis.the various tradeoffs built into the table. In this instance, the cost ratios in Table In short, the empirical cost ratios computed from confusion tables are not findings. They are empirical realizations of the cost ratios determined by stakeholders. But as such, they affect forecasting accuracy. Findings are interpretations of that forecasting accuracy.A closely related cost issue is that with more individuals released awaiting trial, more money is saved. From a fiscal perspective, the number of cases forecasted as DV0 ideally would be large. In principle, this is built into the cost ratios just discussed. For example, more cases will be forecasted as DV0 if the target cost ratio of 10 to 1 was decreased to, say, 5 to 1. That would also mean there will be fewer DV2 false positives and forecasts of DV2 would be correct more often. However, that would also lead to a smaller fraction of the DV2 cases being correctly classified. These matters will be revisited as the proposed pretrial reforms are clarified over the next year to 18 months.With these complexities, it is important to provide policymakers with some reasonable, if only tentative, bottom line. Currently, about 20 percent of the individuals arraigned in domestic violence cases are arrested for new domestic violence offenses within two year of release. It follows that if one continued current practices and forecasted no new domestic violence arrests for each offender released, that forecast would be correct approximately 80 percent of the time. The 80 percent figure sets a very high accuracy standard with no use of any predictors whatsoever. Achieving better than 80 percent accuracy is a major challenge, especially given the weak set of forecasting inputs available at arraignment.In fact, it is possible to do substantially better. In Table Stated more programmatically, under current practices, about 20 percent of those released to await a hearing fail (i.e., have a new domestic violence arrest). If magistrates were to release only those offenders forecasted to not be rearrested, the failure rate for such offenders would be only about 10 percent. Were one to introduce some criminal justice reform that legitimately cut a domestic violence rearrest rate in half, it would likely be considered a major success.It also makes good policy sense to consider forecasts for all three outcomes, and that is precisely what Table A. Input Contributions to Forecasting AccuracyAlthough there is no ability or intent to formally identify risk factors as such, it can be helpful to consider how each input is related to the outcomes being forecasted. Stakeholders will be more inclined to adopt the random forests forecasting procedure if the inputs are related to the outcomes in a sensible fashion, especially if the relationships are consistent with stakeholder preconceptions.Figure Forecasting importance plots could be reported for the two other outcome classes, and the order of the inputs would be at least somewhat different. One reason is that 16The random forest grown is unchanged. However, each predictor in turn is shuffled at random so that on the average it is unrelated to the outcome and cannot on the average contribute to forecasting accuracy. The shuffling does not change the random forest itself. 17 For all inputs, \"priors\" following a kind of crime refers to prior charges associated with an arrest. \"Count\" following a kind of crime refers to the number of counts associated with the charges read at the arraignment. \"High Crime Zip Code\" is a categorical variable with 31 classes, one for each zip code characterized as a high crime area. \"Prior Sentences,\" \"Prior Abscondings,\" \"Prior FTAs,\" \"Prior Jail Days,\" and \"Prior Jail Stays\" refer to the number of times each event occurred.  NOTE:The values on the horizontal axis show the reduction in forecasting accuracy for no new domestic violence arrests when a given variable is randomly shuffled. Various kinds of priors and age make the largest contributions to forecasting accuracy.the reference categories would differ for each plot. For Figure B. Relationships Between Inputs of the Outcomes to be ForecastedForecasting importance plots do not indicate the direction of an association between an input and the outcome being forecasted. That information is contained in partial dependence plots. In effect, such plots show the nature of the association between a given input and the outcome being forecasted, all other inputs held constant. These are not covariance adjusted effects as one would have in a conventional regression analysis. They are more akin to matching procedures. That algorithm is outlined in the Appendix and the details can be found elsewhere (see Hastie et al. 2009:Sec. 10.13.2; Berk  2008:Ch. 5).Figure The order of the variables also could differ somewhat for other more technical reasons (Hastie et al. 2009:Sec.  15.3.2; Berk 2008:Ch. 5). Thus, the importance plot for DV2 will not be the same as the importance plot for DV0 in reverse order.19The number of prior charges is generally less than 40. But, on occasion the number is very large because a substantial number of charges can be associated with a single arrest.violent crimes and then level off. Transforming the centered logits into probabilities, the large falloff means a difference in probabilities of about 0.09. 21It might seem preferable to transform the entire vertical axis in this manner, but when there are more than two outcome categories, there are significant technical complications. Among them is the reference category issue raised for importance plots (Hastie et al. 2009:370). Other than arbitrarily selecting one of the outcome categories as the reference, centering makes the mean logit the reference (analogizing to analysis of variance). example, male offenders are more likely be arrested for new domestic violence incidents. However, the quantitative inputs generally have highly nonlinear associations with the outcome in which the strongest associations are found for smaller values of the input. Thus, individuals whose first adult charge is at an older age are less likely to be rearrested for domestic violence, but the difference between a first adult charge at 16 versus 25 matters a lot. The difference between a first adult charge at 36 versus 45 matters hardly at all.There is perhaps one major surprise. In contrast to decades of research on street crime, the impact of the age of the offender falls off gradually. The reason seems to be, according to these data, that unlike street violence, domestic violence perpetration is nearly as common among individuals in their 30s and 40s as among individuals in their 20s.V. SUMMARY, SOME POLICY IMPLICATIONS, AND CONCLUSIONS", "conclusions": "Under current arraignment practices in the jurisdiction studied, arraignments are held within 48 hours of an arrest and are completed very quickly. Within a matter of several minutes, a magistrate reads the charges to the offender and decides whether the offender can be released until a subsequent formal hearing. If the offender is released, there can be release conditions, and a bond may be required that is forfeited if the offender fails to appear. The release decision is guided by two primary concerns: the risk of flight and the threat to public safety. Procedurally, domestic violence cases are handled at arraignment much like all other cases.Release decisions in domestic violence cases could perhaps be improved if sufficiently accurate forecasts of repeat domestic violence arrests could be made. However, existing threat assessment tools for domestic violence typically concentrate exclusively on intimate partner violence and the needs of victims. Forecasting accuracy, even when properly accessed, can be subverted in service of other goals. In contrast, the setting in which arraignments are held requires a focus on offenders and statutory definitions of domestic violence. Accurate forecasts are meant to help inform decisions made by magistrates. To be sure, it is important to document a victim's needs, but that is another task to be undertaken in very different settings.We are able to provide promising forecasting procedures even with data that are far less than ideal. Under current practice, about 20 percent of the individuals released after arraignment are arrested for domestic violence within two years. If magistrates only released offenders our forecasts identified as good bets, approximately 10 percent of those offenders would be arrested for domestic violence within two years. Failures could be cut in half. One would likely be pleased with any feasible intervention that performed as well. In the jurisdiction studied, the reduction in the percentage who fail translates into well over 1,000 fewer domestic violence arrests per year.There is, of course, the question of what many fewer domestic violence arrests means for domestic violence incidents not reported to the police. One might argue that because domestic violence arrests are made in response to domestic violence incidents, the number of incidents is also substantially reduced. But perhaps domestic violence offenders released after arraignment are more likely to threaten their victims with retaliation if the police are called. Arrests may be reduced without a meaningful drop in domestic violence incidents. A significant reduction in domestic violence arrests can lead to important practical and fiscal benefits for police, courts, prosecutors, and public defenders, but may or may not improve public safety.However, the forecasting inputs contributing most to forecasting accuracy suggest that we are identifying for possible release offenders who are actually less inclined to reoffend. Note that several measures of prior arrests, including priors for domestic violence, all forecast a greater likelihood of new domestic violence arrests. If arraignments simply increased offenders' incentives to thwart calls to the police, one would find that, all else equal, offenders with longer prior records would have fewer postarraignment domestic violence arrests because the domestic violence would be less likely to be reported. Nevertheless, we are exploring ways to collect better data on these issues.  Still to be addressed is what should be done with offenders who are forecasted to be rearrested for domestic violence, especially domestic violence in which there are physical injuries. A variety of strategies is being considered. As noted above, cost is a very significant constraint, so jail time will have to be used sparingly. One of the least costly alternatives might be to inform the offender that the court will be making regular phone calls to the victim to ask \"how things are going.\" Much the same protocol is part of many batterer intervention programs (BIPs). Another option might be probation-like supervision at several levels of intensity, some of which would include home visits. Diversion out of the court system is yet another possibility, and a diversion program for a small number of domestic violence offenders has actually been launched in the jurisdiction. After the arraignment, there is an initial screening for possible diversion. Those selected can be referred to a BIP that includes group counseling coupled with regular monitoring to help enforce attendance and improve victim safety. A failure to \"buy in\" can mean a return to court, where the usual options remain. The chances of success might be greater if only those forecasted as DV1 were diverted. 22 Almost regardless of which interventions are being considered, it could be productive to impose a hold of up to a week on offenders forecasted to fail, during which better data could be collected to make better forecasts. Those forecasts would almost certainly find another group of low-risk offenders appropriate for release. Equally important, one might use the data to better anticipate which kinds of 22 There could be two competing processes. On the one hand, offenders with longer prior records might have a greater proclivity to commit acts of domestic violence that through any of several mechanisms come to the attention of the criminal justice system. On the other hand, offenders with longer prior records might be more inclined to threaten their victims should the domestic violence be reported, and the threats could work. In our data, the first process seems to dominate.23", "SDG": [5]}, "investigating_the_impact_of_gender_on_rank_in_resume_search_engines": {"name": "Investigating the Impact of Gender on Rank in Resume Search Engines", "abstract": " In this work we investigate gender-based inequalities in the context of resume search engines, which are tools that allow recruiters to proactively search for candidates based on keywords and filters. If these ranking algorithms take demographic features into account (directly or indirectly), they may produce rankings that disadvantage some candidates. We collect search results from Indeed, Monster, and CareerBuilder based on 35 job titles in 20 U. S. cities, resulting in data on 855K job candidates. Using statistical tests, we examine whether these search engines produce rankings that exhibit two types of indirect discrimination: individual and group unfairness. Furthermore, we use controlled experiments to show that these websites do not use inferred gender of candidates as explicit features in their ranking algorithms.", "keywords": "H.3.5 Online Information Services: Web-based services,J.4 Social and Behavioral Sciences: Sociology,K.4.2 Social Issues: Employment information retrieval,algorithm auditing,discrimination", "introduction": "The internet is fundamentally changing the labor economy. Millions of people use services like LinkedIn, Indeed, Monster, and CareerBuilder to find employment [42,71,. These online services offer innovative mechanisms for recruiting and organizing employment, often driven by algorithmic systems that rate, sort, and recommend workers and employers.13]There is potential for online labor markets to mitigate some of the mechanisms that cause discrimination in traditional labor markets. In online contexts, workers' demographics may be less clear or even anonymized, which limits the potential for cognitive biases to skew recruiting decisions. For example, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Indeed, Monster, and CareerBuilder do not ask job seekers to input their demographics, or upload a profile image.Yet, evidence indicates that inequalities persist in many different online labor contexts. Scholars have uncovered cases of unequal opportunities presented to women in online ads ; biases in social feedback for gig-economy workers based on gender and race [18]; and discrimination against online customers based on socioeconomics [37]. In 2017, the Illinois Attorney General sent letters to six major hiring websites after users complained about age discrimination [87]. Although there are policies and best practices that employers may adopt to address biases in traditional hiring contexts, detecting and mitigating these issues in online, algorithmically-driven contexts remains an open challenge [63][4,.57]Our goal in this work is to investigate gender-based inequalities in the ranking algorithms used by three major hiring websites: Indeed, Monster, and CareerBuilder. A common feature offered by these (and many other) hiring websites is a resume search engine, which allows recruiters to proactively search for candidates based on keywords and filters. Like any search engine, these tools algorithmically rank candidates, with those at the top being more likely to be seen and clicked on by recruiters [79,17,. However, if the ranking algorithm takes demographic features into account (explicitly or inadvertently through a proxy feature), it may produce rankings that systematically disadvantage some candidates. Although candidates on hiring websites rarely self-report their gender, gender can be inferred with high-accuracy from other information, such as their first name 56][26,86,64,54,.93]First, we examine indirect discrimination, which is defined as correlations between the output of a system and sensitive user features (e.g., gender), even if those features are not explicitly used by the system [76,20,. A ranking algorithm that exhibits indirect discrimination may cause disparate impact on the individuals being ranked. To facilitate our investigation, we ran queries on each site's resume search engine for 35 job titles across 20 American cities between May and October 2016, and recorded the search results. Our final dataset includes over 855K job candidates. Intuitively, our data corresponds to a recruiter's perspective of these sites, including the candidates' profiles and their rank in the search results.95]Using this dataset, we leverage statistical tests to quantify whether the resume search engines exhibit individual fair-ness (which is defined as placing candidates with similar features, excluding gender, at similar ranks) and group fairness (which is defined as assigning similar distributions of ranks men and women) [20,95,. 1 We use two measures of fairness because they correspond to different assumptions about the world, and consequently have different normative consequences 94]. If we assume that candidates' personal profiles and resumes accurately reflect their intrinsic skills, then individual fairness is the appropriate accountability standard for search engine output. However, if we assume that candidates' data is impacted by structural inequalities in society, then the raw data is not an accurate reflection of intrinsic skills, and therefore group fairness is a more appropriate standard. We make no assumptions about how structural inequalities may impact our dataset, and thus we evaluate both types of fairness.[27]Our statistical tests reveal a complicated picture with respect to gender fairness on the three hiring websites:\u2022 Individual fairness: By fitting mixed linear models, we find that inferred gender is a significant, negative feature on all three websites (p \u2264 0.05), indicating that feminine candidates appear at lower ranks 2 than masculine candidates, even when controlling for all visible candidate features. However, the size of the gender effect is small: on Career-Builder, for example, at rank 30 men appear 1.4 ranks above equally qualified women on average (95% CI: [0.73, 2.13]). We demonstrate that these findings are robust by replicating them using matched subsets of candidates , varying subsets of the top k candidates, and candidate populations that include search filters. \u2022 Group fairness: Using the Mann-Whitney U test, we find that 8.5-13.2% of job title/city pairs exhibit significant group unfairness (p \u2264 0.05). In 12 of the 35 job titles, the search results consistently favor masculine candidates; Bartender is the only job title that favors women.[39]Second, we examine direct discrimination on these resume search engines, which is defined as the explicit use of inferred gender as a feature when ranking candidates . We performed controlled tests using resumes uploaded by us to test whether the ranking algorithms use features extracted directly from the resumes to rank candidates, including inferred gender, almae mater, and unemployment status.[76]Overall, our examination of resume search engines leads to mixed conclusions. On the positive side, we find that the ranking algorithms used by all three hiring sites do not use candidates' inferred gender as a feature. Furthermore, our regressions demonstrate that the three ranking algorithms are, for the most part, individually fair with respect to gender. The small, significant gender effects that we observe are likely caused by some ranking feature that serves as a weak proxy for gender. On the negative side, however, we do observe significant and consistent group unfairness against feminine candidates in roughly 1/3 of the job titles we examine. This may be of particular concern in technical professions like Electrical, Mechanical, Network, and Software Engineering that are known to be gender-imbalanced .[72]Whether the hiring websites should adopt ranking algorithms that strive for group fairness is a fraught question. Our analysis conclusively shows that these ranking algorithms do not \"create\" group unfairness with respect to gender: the algorithms are mostly \"gender blind.\" Rather, the algorithms are likely reflecting structural gender inequalities that are embedded in the raw data. Ultimately, we hope that our work furthers the dialog about the adoption of algorithmic affirmative action policies that benefit marginalized populations.", "body": "The internet is fundamentally changing the labor economy. Millions of people use services like LinkedIn, Indeed, Monster, and CareerBuilder to find employment There is potential for online labor markets to mitigate some of the mechanisms that cause discrimination in traditional labor markets. In online contexts, workers' demographics may be less clear or even anonymized, which limits the potential for cognitive biases to skew recruiting decisions. For example, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Indeed, Monster, and CareerBuilder do not ask job seekers to input their demographics, or upload a profile image.Yet, evidence indicates that inequalities persist in many different online labor contexts. Scholars have uncovered cases of unequal opportunities presented to women in online ads Our goal in this work is to investigate gender-based inequalities in the ranking algorithms used by three major hiring websites: Indeed, Monster, and CareerBuilder. A common feature offered by these (and many other) hiring websites is a resume search engine, which allows recruiters to proactively search for candidates based on keywords and filters. Like any search engine, these tools algorithmically rank candidates, with those at the top being more likely to be seen and clicked on by recruiters First, we examine indirect discrimination, which is defined as correlations between the output of a system and sensitive user features (e.g., gender), even if those features are not explicitly used by the system Using this dataset, we leverage statistical tests to quantify whether the resume search engines exhibit individual fair-ness (which is defined as placing candidates with similar features, excluding gender, at similar ranks) and group fairness (which is defined as assigning similar distributions of ranks men and women) Our statistical tests reveal a complicated picture with respect to gender fairness on the three hiring websites:\u2022 Individual fairness: By fitting mixed linear models, we find that inferred gender is a significant, negative feature on all three websites (p \u2264 0.05), indicating that feminine candidates appear at lower ranks 2 than masculine candidates, even when controlling for all visible candidate features. However, the size of the gender effect is small: on Career-Builder, for example, at rank 30 men appear 1.4 ranks above equally qualified women on average (95% CI: [0.73, 2.13]). We demonstrate that these findings are robust by replicating them using matched subsets of candidates Second, we examine direct discrimination on these resume search engines, which is defined as the explicit use of inferred gender as a feature when ranking candidates Overall, our examination of resume search engines leads to mixed conclusions. On the positive side, we find that the ranking algorithms used by all three hiring sites do not use candidates' inferred gender as a feature. Furthermore, our regressions demonstrate that the three ranking algorithms are, for the most part, individually fair with respect to gender. The small, significant gender effects that we observe are likely caused by some ranking feature that serves as a weak proxy for gender. On the negative side, however, we do observe significant and consistent group unfairness against feminine candidates in roughly 1/3 of the job titles we examine. This may be of particular concern in technical professions like Electrical, Mechanical, Network, and Software Engineering that are known to be gender-imbalanced Whether the hiring websites should adopt ranking algorithms that strive for group fairness is a fraught question. Our analysis conclusively shows that these ranking algorithms do not \"create\" group unfairness with respect to gender: the algorithms are mostly \"gender blind.\" Rather, the algorithms are likely reflecting structural gender inequalities that are embedded in the raw data. Ultimately, we hope that our work furthers the dialog about the adoption of algorithmic affirmative action policies that benefit marginalized populations.Limitations.There are several limitations of our work. First, there are no user studies that quantify how recruiters use resume search engines, including how many results they view and click on, or how they construct queries and filters. We attempt to address this in our analysis by examining gender effects under a large variety of use cases, e.g., 35 different job titles, search results lists of differing lengths, and queries with and without filters. Furthermore, it is unknown what fraction of online recruiting is active (using resume search engines) or passive (using advertisements for open positions). We leave user studies of recruiters as future work.Second, the nature of our dataset restricts us to inferring binary gender labels for candidates. This is a common limitation of work that uses observational datasets to examine gender biases in online contexts RELATED WORKLabor discrimination is a long standing, troubling aspect of society that may impact workers' wages or opportunities for advancement. In this paper we specifically focus on hiring discrimination, which occurs when discrimination impacts the candidates that are selected to fill open positions. Hiring discrimination still impacts the modern job market One of the key tools used to study hiring discrimination is the audit or correspondence study. In this methodology, researchers probe the hiring practices of a target by submitting carefully crafted resumes, or by sending human participants in for interviews Scholars and regulators have begun to focus on the ways that big data and algorithms can create hiring discrimination. Pauline T. Kim thoroughly catalogs how data-driven systems that evaluate job seekers may introduce new forms of discrimination against members of protected classes Defining FairnessDefining and operationalizing \"fair\" and \"non-discriminatory\" algorithms is an active area of research. Pedreshi et al. defined direct and indirect discrimination, where the former refers to algorithms that explicitly take sensitive features as input Dwork et al. introduced two types of fairness to address indirect discrimination: individual fairness states that similar individuals should be treated similarly, while group fairness states that demographic subsets of the population should be treated the same as the entire population Friedler et al. present a framework for grappling with the assumptions that underly individual and group fairness in algorithmic scenarios Biases in Online SystemsResearchers have begun to empirically investigate whether algorithmic systems may (inadvertently) cause harm to users. The process of examining a black-box computer system has become known as an algorithm audit, as the methodology draws inspiration from classic audit studies There are many causes for bias in online systems. Some cases are direct manifestations of societal biases by users, e.g., gender biases on Wikipedia Closely related to our work are studies that have uncovered discrimination on \"gig-\" and \"sharing-economy\" services. Studies have found examples of workers/service providers discriminating against customers on TaskRabbit The Importance of Rank.In this study, we examine three websites that present job seekers in ranked lists in response to queries from recruiters. If these ranking algorithms systematically elevate candidates with specific demographic attributes, this may recreate real-world social inequality in an online context because the top items of ranked lists are much more likely to be clicked by users Search Engines.Our work falls within the literature that examines social harms that can occur when search engines present misleading or biased information. Researchers have examined misinformation about vaccines Three studies have specifically examined demographic biases on search engines. Hann\u00e1k et al. found negative correlations between race and rank on the gig-economy website TaskRabbit, even after controlling for all other worker-related features Two studies have proposed metrics for quantifying bias in search results. Kulshrestha et al. separately quantify the amount of bias in a search engine's corpus and output BACKGROUNDIn this section, we introduce the three websites that are the focus of our study. We discuss how recruiters use these resume search engines, and specific details about their user interfaces. We also briefly discuss how candidates use these websites.Hiring WebsitesWe chose three hiring sites to examine in this study: Indeed, Monster, and CareerBuilder. We chose these sites for three reasons. First, they are three of the most popular employment websites in the U. S. (along with LinkedIn and Glassdoor) Each of these websites claims to serve millions of unique visitors and job queries per month Third, as we discuss next, all three sites have similar user interfaces for candidates and recruiters. This makes the sites roughly comparable in terms of usability and features, which allows us to contrast our results across the sites.Recruiter's PerspectiveIn this study, we examine the resume search engines provided by Indeed, Monster, and CareerBuilder. All three sites offer similar search engines that are designed to help recruiters identify and recruit new employees. The corpus of each resume search engine contains resumes and personal profiles uploaded by candidates seeking employment. Recruiters query the corpus by entering a free-text job title, a geographic location, and (optional) filters to refine the results (e.g., years of experience, minimum educational attainment, etc.). None of the sites allow recruiters to filter or order search results by demographics (e.g., gender, ethnicity), but proxy variables exist in some cases (e.g., years of experience as an indicator of age).The resume search engines use rankings algorithms to determine which candidates are relevant to a given query and their ordering, subject to any specified filters. By default, only candidates within 20-50 miles of the specified location are deemed relevant, and the search results are sorted by opaque metrics (e.g., \"most relevant\"), although the recruiter may re-sort the list by objective metrics like years of experience.  Scope.In this work, we focus on the ranking of candidates, rather than the composition of search results (i.e., which candidates are deemed relevant). Although examination of the overall candidate pool would be interesting, we cannot do so because there is no way for us to enumerate all candidates in a hiring website's corpus.Candidate's PerspectiveIndeed, Monster, and CareerBuilder are very similar from a candidate's perspective. Candidates must register for a free account, and possibly fill out a personal profile and upload a resume. The amount of profile information that is mandatory varies across the sites; on Monster, users must provide their name, location, educational attainment, and previous job, while CareerBuilder allows users to leave their profile empty. However, all three sites remind users to upload more information, especially a resume, since the job recommendation functionality on the sites depends on this information. Once a candidate has created a profile, they can browse open positions and respond to solications from recruiters.DATA COLLECTIONIn this section, we describe our data collection methodology, and the specific variables we extract from the data.Crawl.To collect data for this study, we use an automated web browser to search for candidates on Indeed, Monster, and CareerBuilder. Intuitively, our crawler is designed to emulate how a recruiter would search for candidates on these hiring websites. We ran queries for 35 job titles in 20 U. S. cities (described below) on all three sites, and recorded the resulting lists of candidates. We also queried for a subset of 490, 700, and 700 job title/city pairs with one, two, and three search filters, on Monster, Indeed, and CareerBuilder, respectively. For binary filters (e.g., willingness to relocate), we queried with both options; for non-binary filters (e.g., minimum years of experience), we set three different values for the filter, chosen such that the values select from 0-33%, 33-66%, and 66-100% of the candidate population. 3    Candidate Features.Next, we extract information about candidates in the search results. We focus on three types of features: 1) profile data (e.g., experience, education, etc.); 2) inferred gender; and 3) rank in search results. Table Inferring Gender.Since our goal is to examine resume search engines with respect to gender, we need to label each candidate's gender. However, none of the sites we focus on collect this information. Instead, we infer each candidate's gender based on their first name, which is a common method to infer gender in Western societies In this work, we rely on the U. S. baby name dataset Figure Limitations.There are two limitations of our dataset and labeling methodology that are worth noting. First, the candidate attributes that we extract from search results may not match a candidates' true attributes. Fortunately, this limitation does not impact our analysis, since the ranking algorithms we are auditing, as well as recruiters that rely on resume search engines, base their decisions on candidates' reported attributes. 5 Thus, throughout this paper, when we compare the capabilities of candidates, we are referring to their reported attributes, rather than their true attributes.Second, we do not know candidates' true genders. As above, this limitation does not impact our analysis, since recruiters and ranking algorithms must also rely on inferred gender when making decisions (if they use this information at all). Throughout this paper when we refer to gender, we are referring to inferred gender based on first name. Furthermore, as noted in the introduction, we are limited to analyzing binary genders.Ethics.We were careful to conduct our study in an ethical manner. This study was approved under Northeastern IRB #16-01-19. To protect the contextual privacy of candidates, we will not be releasing our crawled data. Furthermore, we limited the impact of our crawler on the hiring sites by restricting it to one query every 30 seconds, and at no point did we contact candidates. In our controlled experiments (detailed in the next section), we only uploaded two resumes at a time to the hiring sites, meaning we decreased the rank of other candidates by at most two. Although all three sites prohibit crawling in their terms of service, we believe our study is acceptable under the norms that have been established by prior algorithm audit studies ANALYSISIn this section, we investigate the rankings of candidates produced by resume search engines with respect to inferred gender. We organize our analysis around three questions: (1) Do the resume search engines exhibit individual fairness? (2) Do they exhibit group fairness? (3) Do they explicitly rank candidates based on inferred gender?Individual FairnessIn this section, we investigate whether the rankings of candidates produced by resume search engines are individually fair with respect to inferred gender. Recall that to be individually fair, the ranking algorithms must rank candidates with similar features (excluding gender) at similar ranks To investigate individual fairness, we use regression tests. Our goal is to examine the effect of inferred gender on candidate's rank while controlling for all other observable candidate features. If the gender feature is significant and has a non-zero coefficient in the fitted model, this indicates that the ranking algorithm in question is not individually fair, as candidates with equivalent features but different inferred genders are not assigned the same rank.Throughout this section, we focus on the top 100 candidates returned in search results, since recruiters are unlikely to browse to candidates at lower ranks Model Specification.We adopt a Mixed Linear Model (MLM) for our regressions. We regress on individual candidates, specifying the model as y = X\u03b2 \u03b2 \u03b2 + \u00b5 \u00b5 \u00b5 + . y is a vector of the responses (log 2 (rank) of each candidate, explained below). X is the design matrix, and the predictors include the features from Table We choose a model with fixed and random effects because it agrees with two fundamental assumptions about our data:1. Each hiring website has a single ranking algorithm with features and weights do not vary by query term or location. This corresponds to fixed effects (\u03b2) regardless of job title and location. This assumption is reasonable because it is impractical for a hiring website to implement different algorithms or feature weight vectors for an unbounded number of free-text job titles and locations. 2. Candidates with identical features that appear in different search result lists may be assigned dramatically different ranks. This is true because the population of candidates, and their relative qualifications, varies across locations and professions. This corresponds to random-effects group intercepts (\u00b5 \u00b5 \u00b5) that vary by job title and location.We use log 2 (rank) as the dependent variable in our regressions for two reasons:\u2022 It prioritizes top-ranked candidates by giving them higher weight, while decaying the importance for lower-ranked candidates. \u2022 The log() function is monotonically increasing, which maintains the ordering of candidates after the transformation.Logarithms have been found to be widely applicable in the IR literature. Empirical studies Model Fitting.We fit three MLMs, one for each hiring website on the top 100 candidates in each search result list. Negative coefficients signify effects with higher rank. We conduct the Variance Inflation Factors (VIF) test to remove multicollinearity before fitting the models; all the variables remain after the test. The correlation matrix is available in the supplementary materials.To assess how well the MLMs fit for our data, we evaluate their predictive power. To do this, we treat the each model as a ranking algorithm: we input the ground-truth feature vectors of candidates from a given search result list R into a fit model, which then outputs a predicted log ranking \u0177 (corresponding to a predicted ranking R). Next, we use the nDCG metric to compute the similarity between R and the original ranking R. nDCG is a standard metric used in the IR literature to compare ranked lists where g(r i ) is the \"gain\" or score assigned to result r i . nDCG is defined as DCG( R)/DCG(R), where R is the ideal ranking of the items in R. In our case, R is the original ranking produced by a hiring site (i.e., we treat the original ranking as the baseline), and R is a ranking predicted by the model. An    Figure Results.Table Lastly, we observe that the Probability of Being Masculine feature is significant (p \u2264 0.05 in all cases) and negative in all three models, meaning that overall, men rank higher than women with equivalent features.Figure For very high ranks, the increase in rank for men is negligible: for example, on Monster at rank 10 the mean increase in rank is 0.2 (95% CI: [0.02, 0.36]). However, by rank 50 on Monster, ties between men and women are consistently broken in favor of men (mean increase 0.96, 95% CI: [0.08, 1.82]), and by rank 80 the increase is large enough to push men across pagination boundaries (mean increase 1.54, 95% CI: [0.14, 2.91]). Indeed and CareerBuilder both exhibit larger effects than Monster.Robustness.In addition to the MLMs we fit to the top 100 candidates in our dataset, we fit hundreds of other MLMs to sub-and supersets of our candidate population to guage the robustness of our models. Specifically, we fit models to: (1) the top k candidates in search results as k is varied from 10 to 1000 (chosen because it is the maximum number of candidates returned by Monster); (2) the top 100 and 1000 candidates in a matched subset of the population (propensity score matching is a technique for reducing selection bias in observational datasets The \"TDRC\" columns show the total difference in area under the recall curves. Negative (positive) TDRC indicates that feminine (masculine) candidates are ranked higher overall. \"-\" marks instances where there are no cities with sufficiently large populations to test. We also remove the 8% of candidates with ambiguous genders. Job titles with significant unfairness in a constant direction are bolded.Overall, these models exhibit the same significance, sign, and effect sizes for the Probability of Being Masculine feature as the top 100, unmatched, non-filtered models that we examine in Table Group FairnessNext, we investigate whether the resume search engines exhibit group fairness with respect to inferred gender. To be group fair, the ranking algorithms should assign a similar distribution of ranks to masculine and feminine candidates Metrics.To examine group fairness, we use two metrics. First, we use the Mann-Whitney U (M-W U ) test to compare the distribution of ranks for men and women in a given list of search results Table The disadvantage of M-W U is that it does not tell us the direction or magnitude of group unfairness. To answer these questions, we calculate the area under recall curves. To generate recall, we iterate from the first candidate (i.e., rank 1) to the last candidate in a given search result list R. At rank i, we calculate a tuplewhere |R| is the total number of candidates in the list, |R f | is the total number of feminine candidates in R, and |R f,i | is the number of feminine candidates observed between ranks 1 and i.As an illustrative example, Figure Finally, we calculate the area under the recall curves and subtract the area under the diagonal; the Difference between Green check marks denote cases where the feature is taken into account by the ranking algorithm.the Recall Curves (DRC) exists in the range [\u22120.5, 0.5], with negative (positive) values indicating that feminine (masculine) candidates are favored in the rankings. Table Results.Table Direct Discrimination And Hidden FeaturesUp to now, our analysis has focused on candidate features that are directly observable in the search results. However, there is an element of each candidates' profile that we cannot observe (on Monster and CareerBuilder), but that may be taken into account by the ranking algorithm: resumes. For example, Monster and CareerBuilder ask candidates to enter their education level into their profile, but actual almae matres are likely stated in each candidate's resume. The ranking algorithm could parse this additional information from the PDF-format resume and use it when ranking candidates. Parsing resumes makes sense from a design standpoint, in that it allows the websites to collect detailed information about candidates without having to ask them to enter it manually, which can be tedious.To test if resume content influences ranking, we conducted controlled experiments using resumes created and uploaded by us. We create two user accounts, A and B, in that temporal order, with identical profile information and resumes. We then verify that the two accounts appear directly adjacent in search results in the order B, A. Next, we delete the old resumes, upload two new resumes (starting with A) that differ by exactly one feature, then query for our users again. 6 In each treatment, we assign A the \"stronger\" value for the feature, e.g., A attended an Ivy League school while B attended community college. If user A appears before user B, it means the treatment variable in the resumes has flipped the rank ordering, thus revealing that the algorithm takes that particular resume feature into account. We repeat this process on all three hiring websites, with the different treatment features shown in Table Table Limitations.Hiring websites may extract features from resumes that are not covered by our treatments in Table CONCLUDING DISCUSSION", "conclusions": "In this study, we examine gender inequality on the resume search engines provided by Indeed, Monster, and Career-Builder. We crawled search results for 35 job titles across 20 U. S. cities; these contain data on 855K candidates. Using statistical tests, we examine two types of algorithmic fairness with respect to inferred gender:\u2022 Individual fairness: We find statistically significant (p \u2264 0.05), negative correlations between rank and inferred gender in our dataset. This means that even when controlling for all other visible candidate features, there is a slight penalty against feminine candidates. These results are robust under a variety of conditions. However, the effect size is small: only by rank 30-50 (depending on the website) is the gender effect large enough that masculine candidates receive a substantive increase in rank. \u2022 Group fairness: We observe that 8.5-13.2% of job title/city pairs show statistically significant group unfairness. In 12 of 35 job titles, the unfairness benefits men.Using controlled experiments, we find that none of the hiring sites are directly discriminatory with respect to inferred gender. This concurs with the design of these websites, which do not ask candidates to input their gender. However, we see that until our users appear. Thus, it is unlikely that our users receive many clicks from recruiters before we measure their ranks. This is important, because clicks may be a feature used to rank candidates. 7 E.g. https://www.jobscan.co/ other hidden features (unemployment and alma mater) are taken into account.Why Is There Unfairness?One unsatisfying aspect of our study is that we are not able to say definitively why there is unfairness with respect to inferred gender on these resume search engines. This is a common criticism of algorithm audits that rely on observational data .[37]We hypothesize that there are two potential causes for the slight individual unfairness we observe. First, the ranking algorithms may rely on a hidden feature that is extracted from resumes that is (weakly) correlated with gender. Our controlled experiments rule out direct discrimination as a cause, and our regressions control for indirect discrimination that might be caused by visible candidate features. Unfortunately, we cannot isolate the hidden feature(s) that may be causing individual unfairness because we do not have access to all candidate resumes on Monster and CareerBuilder.A second possibility is that small amounts of individual unfairness occur because the algorithms adjust the rank of candidates based on the volume of clicks they receive from recruiters (a so-called learning-to-rank approach [40,19,). If recruiters are biased, they may generate more clicks on candidates with desirable demographic traits. Testing this hypothesis is challenging, since it would require uploading many resumes with varying features and then waiting weeks in the hope of collecting sufficient clicks to trigger changes in rank.45]With respect to group unfairness, the likely cause is structural inequality. It is unlikely to be a coincidence that the job titles where we observe the largest magnitudes of group unfairness include technical professions (e.g., Software Engineer), truck driver, and laborer, i.e., all professions that are historically gendered. Thus, it is fair to say that the ranking algorithms on these hiring sites are not increasing group unfairness on top of what already exists at large in society; rather, they reflect an unfortunate status quo that persists in many professions.", "SDG": [5]}, "man_is_to_computer_programmer_as_woman_is_to_homemaker_debiasingword_embeddings": {"name": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "abstract": " The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.", "keywords": "", "introduction": "Research on word embeddings has drawn significant interest in machine learning and natural language processing. There have been hundreds of papers written about word embeddings and their applications, from Web search  to parsing Curriculum Vitae [22]. However, none of these papers have recognized how blatantly sexist the embeddings are and hence risk introducing biases of various types into real-world systems.[12]A word embedding, trained on word co-occurrence in text corpora, represents each word (or common phrase) w as a d-dimensional word vector w 2 R d . It serves as a dictionary of sorts for computer programs that would like to use word meaning. First, words with similar semantic meanings tend to have vectors that are close together. Second, the vector differences between words in embeddings have been shown to represent relationships between words [27,. For example given an analogy puzzle, \"man is to king as woman is to x\" (denoted as man:king :: woman:x), simple arithmetic of the embedding vectors finds that x=queen is the best answer because ! man ! woman \u21e1 ! king ! queen. Similarly, x=Japan is returned for Paris:France :: Tokyo:x. It is surprising that a simple vector arithmetic can simultaneously capture a variety of relationships. It has also excited practitioners because such a tool could be useful across applications involving natural language. Indeed, they are being studied and used in a variety of downstream applications (e.g., document ranking 21], sentiment analysis [22], and question retrieval [14]).[17]However, the embeddings also pinpoint sexism implicit in text. For instance, it is also the case that: ! man ! woman \u21e1 ! computer programmer ! homemaker. In other words, the same system that solved the above reasonable analogies will offensively answer \"man is to computer programmer as woman is to x\" with x=homemaker. Similarly, it outputs that a Figure : Left The most extreme occupations as projected on to the she he gender direction on w2vNEWS. Occupations such as businesswoman, where gender is suggested by the orthography, were excluded. Right Automatically generated analogies for the pair she-he using the procedure described in text. Each automatically generated analogy is evaluated by 10 crowd-workers to whether or not it reflects gender stereotype.1father is to a doctor as a mother is to a nurse. The primary embedding studied in this paper is the popular publicly-available word2vec [19, 300 dimensional embedding trained on a corpus of Google News texts consisting of 3 million English words, which we refer to here as the w2vNEWS.20]One might have hoped that the Google News embedding would exhibit little gender bias because many of its authors are professional journalists. We also analyze other publicly available embeddings trained via other algorithms and find similar biases (Appendix B).In this paper, we quantitatively demonstrate that word-embeddings contain biases in their geometry that reflect gender stereotypes present in broader society. 1 Due to their wide-spread usage as basic features, word embeddings not only reflect such stereotypes but can also amplify them. This poses a significant risk and challenge for machine learning and its applications. The analogies generated from these embeddings spell out the bias implicit in the data on which they were trained. Hence, word embeddings may serve as a means to extract implicit gender associations from a large text corpus similar to how Implicit Association Tests  detect automatic gender associations possessed by people, which often do not align with self reports.[11]To quantify bias, we will compare a word vector to the vectors of a pair of gender-specific words. For instance, the fact that ! nurse is close to ! woman is not in itself necessarily biased(it is also somewhat close to ! man -all are humans), but the fact that these distances are unequal suggests bias. To make this rigorous, consider the distinction between gender specific words that are associated with a gender by definition, and the remaining gender neutral words. Standard examples of gender specific words include brother, sister, businessman and businesswoman. We will use the gender specific words to learn a gender subspace in the embedding, and our debiasing algorithm removes the bias only from the gender neutral words while respecting the definitions of these gender specific words.We propose approaches to reduce gender biases in the word embedding while preserving the useful properties of the embedding. Surprisingly, not only does the embedding capture bias, but it also contains sufficient information to reduce this bias.We will leverage the fact that there exists a low dimensional subspace in the embedding that empirically captures much of the gender bias.", "body": "Research on word embeddings has drawn significant interest in machine learning and natural language processing. There have been hundreds of papers written about word embeddings and their applications, from Web search A word embedding, trained on word co-occurrence in text corpora, represents each word (or common phrase) w as a d-dimensional word vector w 2 R d . It serves as a dictionary of sorts for computer programs that would like to use word meaning. First, words with similar semantic meanings tend to have vectors that are close together. Second, the vector differences between words in embeddings have been shown to represent relationships between words However, the embeddings also pinpoint sexism implicit in text. For instance, it is also the case that: ! man ! woman \u21e1 ! computer programmer ! homemaker. In other words, the same system that solved the above reasonable analogies will offensively answer \"man is to computer programmer as woman is to x\" with x=homemaker. Similarly, it outputs that a Figure father is to a doctor as a mother is to a nurse. The primary embedding studied in this paper is the popular publicly-available word2vec One might have hoped that the Google News embedding would exhibit little gender bias because many of its authors are professional journalists. We also analyze other publicly available embeddings trained via other algorithms and find similar biases (Appendix B).In this paper, we quantitatively demonstrate that word-embeddings contain biases in their geometry that reflect gender stereotypes present in broader society. 1 Due to their wide-spread usage as basic features, word embeddings not only reflect such stereotypes but can also amplify them. This poses a significant risk and challenge for machine learning and its applications. The analogies generated from these embeddings spell out the bias implicit in the data on which they were trained. Hence, word embeddings may serve as a means to extract implicit gender associations from a large text corpus similar to how Implicit Association Tests To quantify bias, we will compare a word vector to the vectors of a pair of gender-specific words. For instance, the fact that ! nurse is close to ! woman is not in itself necessarily biased(it is also somewhat close to ! man -all are humans), but the fact that these distances are unequal suggests bias. To make this rigorous, consider the distinction between gender specific words that are associated with a gender by definition, and the remaining gender neutral words. Standard examples of gender specific words include brother, sister, businessman and businesswoman. We will use the gender specific words to learn a gender subspace in the embedding, and our debiasing algorithm removes the bias only from the gender neutral words while respecting the definitions of these gender specific words.We propose approaches to reduce gender biases in the word embedding while preserving the useful properties of the embedding. Surprisingly, not only does the embedding capture bias, but it also contains sufficient information to reduce this bias.We will leverage the fact that there exists a low dimensional subspace in the embedding that empirically captures much of the gender bias.Related work and PreliminaryGender bias and stereotype in English. It is important to quantify and understand bias in languages as such biases can reinforce the psychological status of different groups actor are, by default, associated with the dominant class Bias within algorithms. A number of online systems have been shown to exhibit various biases, such as racial discrimination and gender bias in the ads presented to users Within machine learning, a body of notable work has focused on \"fair\" binary classification in particular. A definition of fairness based on legal traditions is presented by Barocas and Selbst Word embedding. An embedding consists of a unit vector w 2 R d , with k wk = 1, for each word (or term) w 2 W . We assume there is a set of gender neutral words N \u21e2 W , such as flight attendant or shoes, which, by definition, are not specific to any gender. We denote the size of a set S by |S|. We also assume we are given a set of F-M gender pairs P \u21e2 W \u21e5 W , such as she-he or mother-father whose definitions differ mainly in gender. Section 5 discusses how N and P can be found within the embedding itself, but until then we take them as given. As is common, similarity between two vectors u and v can be measured by their cosine similarity : cos(u, v) = u\u2022v kukkvk . This normalized similarity between vectors u and v is the cosine of the angle between the two vectors. Since words are normalized cos( w1 , w2 ) = w1 \u2022 w2 . 2   Unless otherwise stated, the embedding we refer to is the aforementioned w2vNEWS embedding, a d = 300-dimensional word2vec 3 https://code.google.com/archive/p/word2vec/ 4 All human experiments were performed on the Amazon Mechanical Turk platform. We selected for U.S.-based workers to maintain homogeneity and reproducibility to the extent possible with crowdsourcing.Geometry of Gender and Bias in Word EmbeddingsOur first task is to understand the biases present in the word-embedding (i.e. which words are closer to she than to he, etc.) and the extent to which these geometric biases agree with human notion of gender stereotypes. We use two simple methods to approach this problem: 1) evaluate whether the embedding has stereotypes on occupation words and 2) evaluate whether the embedding produces analogies that are judged to reflect stereotypes by humans. The exploratory analysis of this section will motivate the more rigorous metrics used in the next two sections.Occupational stereotypes. Figure Analogies exhibiting stereotypes. Analogies are a useful way to both evaluate the quality of a word embedding and also its stereotypes. We first briefly describe how the embedding generate analogies and then discuss how we use analogies to quantify gender stereotype in the embedding. A more detailed discussion of our algorithm and prior analogy solvers is given in Appendix C.In the standard analogy tasks, we are given three words, for example he, she, king, and look for the 4th word to solve he to king is as she to x. Here we modify the analogy task so that given two words, e.g. he, she, we want to generate a pair of words, x and y, such that he to x as she to y is a good analogy. This modification allows us to systematically generate pairs of words that the embedding believes it analogous to he, she (or any other pair of seed words). The input into our analogy generator is a seed pair of words (a, b) determining a seed direction \u00e3 b corresponding to the normalized difference between the two seed words. In the task below, we use (a, b) = (she, he). We then score all pairs of words x, y by the following metric:where is a threshold for similarity. The intuition of the scoring metric is that we want a good analogy pair to be close to parallel to the seed direction while the two words are not too far apart in order to be semantically coherent. The parameter sets the threshold for semantic similarity. In all the experiments, we take = 1 as we find that this choice often works well in practice. Since all embeddings are normalized, this threshold corresponds to an angle \uf8ff \u21e1/3, indicating that the two words are closer to each other than they are to the origin. In practice, it means that the two words forming the analogy are significantly closer together than two random embedding vectors. Given the embedding and seed words, we output the top analogous pairs with the largest positive S (a,b) scores.To reduce redundancy, we do not output multiple analogies sharing the same word x.We employed U.S. based crowd-workers to evaluate the analogies output by the aforementioned algorithm. For each analogy, we asked the workers two yes/no questions: (a) whether the pairing makes sense as an analogy, and (b) whether it reflects a gender stereotype. Overall, 72 out of 150 analogies were rated as gender-appropriate by five or more out of 10 crowd-workers, and 29 analogies were rated as exhibiting gender stereotype by five or more crowd-workers (Figure Identifying the gender subspace. Next, we study the bias present in the embedding geometrically, identifying the gender direction and quantifying the bias independent of the extent to which it is aligned with the crowd bias. Language use is \"messy\" and therefore individual word pairs do not always behave as expected. For instance, the word man has several different usages: it may be used as an exclamation as in oh man! or to refer to people of either gender or as a verb, e.g., man the station. To more robustly estimate bias, we shall aggregate across multiple paired comparisons. By combining several directions, such as ! she ! he and ! woman ! man, we identify a gender direction g 2 R d that largely captures gender in the embedding. This direction helps us to quantify direct and indirect biases in words and associations.In English as in many languages, there are numerous gender pair terms, and for each we can consider the difference between their embeddings. Before looking at the data, one might imagine  The table shows performance of the original w2vNEWS embedding (\"before\") and the debiased w2vNEWS on standard evaluation metrics measuring coherence and analogy-solving abilities: RG that they all had roughly the same vector differences, as in the following caricature:! gal ! guy = g However, gender pair differences are not parallel in practice, for multiple reasons. First, there are different biases associated with with different gender pairs. Second is polysemy, as mentioned, which in this case occurs due to the other use of grandfather as in to grandfather a regulation. Finally, randomness in the word counts in any finite sample will also lead to differences. Figure , where c is a parameter that determines how strict do we want to in measuring bias. If c is 0, then |cos( w g)| c = 0 only if w has no overlap with g and otherwise it is 1. Such strict measurement of bias might be desirable in settings such as the college admissions example from the Introduction, where it would be unacceptable for the embedding to introduce a slight preference for one candidate over another by gender. A more gradual bias would be setting c = 1. The presentation we have chosen favors simplicity -it would be natural to extend our definitions to weight words by frequency. For example, in w2vNEWS, if we take N to be the set of 327 occupations, then DirectBias 1 = 0.08, which confirms that many occupation words have substantial component along the gender direction.Debiasing algorithmsThe debiasing algorithms are defined in terms of sets of words rather than just pairs, for generality, so that we can consider other biases such as racial or religious biases. We also assume that we have a set of words to neutralize, which can come from a list or from the embedding as described in Section 5. (In many cases it may be easier to list the gender specific words not to neutralize as this set can be much smaller.) x is a projection onto the difference between the embeddings of the words he and she, and y is a direction learned in the embedding that captures gender neutrality, with gender neutral words above the line and gender specific words below the line. Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this figure, the words above the horizontal line would all be collapsed to the vertical line.The first step, called Identify gender subspace, is to identify a direction (or, more generally, a subspace) of the embedding that captures the bias. For the second step, we define two options: Neutralize and Equalize or Soften. Neutralize ensures that gender neutral words are zero in the gender subspace. Equalize perfectly equalizes sets of words outside the subspace and thereby enforces the property that any neutral word is equidistant to all words in each equality set. For instance, if {grandmother, grandfather} and {guy, gal} were two equality sets, then after equalization babysit would be equidistant to grandmother and grandfather and also equidistant to gal and guy, but presumably closer to the grandparents and further from the gal and guy. This is suitable for applications where one does not want any such pair to display any bias with respect to neutral words.The disadvantage of Equalize is that it removes certain distinctions that are valuable in certain applications. For instance, one may wish a language model to assign a higher probability to the phrase to grandfather a regulation) than to grandmother a regulation since grandfather has a meaning that grandmother does not -equalizing the two removes this distinction. The Soften algorithm reduces the differences between these sets while maintaining as much similarity to the original embedding as possible, with a parameter that controls this trade-off.To define the algorithms, it will be convenient to introduce some further notation. A subspace B is defined by k orthogonal unit vectors B = {b 1 , . . . , b k } \u21e2 R d . In the case k = 1, the subspace is simply a direction. We denote the projection of a vector v onto B by, vThis also means that v v B is the projection onto the orthogonal subspace. Step 2a: Hard de-biasing (neutralize and equalize). Finally, output the subspace B and the new embedding w 2 R d w2W . Equalize equates each set of words outside of B to their simple average \u232b and then adjusts vectors so that they are unit length. It is perhaps easiest to understand by thinking separately of the two components wB and w?B = w wB . The latter w?B are all simply equated to their average. Within B, they are centered (moved to mean 0) and then scaled so that each w is unit length. To motivate why we center, beyond the fact that it is common in machine learning, consider the bias direction being the gender direction (k = 1) and a gender pair such as E = {male, female}. As discussed, it so happens that both words are positive (female) in the gender direction, though female has a greater projection. One can only speculate as to why this is the case, e.g., perhaps the frequency of text such as male nurse or male escort or she was assaulted by the male. However, because female has a greater gender component, after centering the two will be symmetrically balanced across the origin. If instead, we simply scaled each vector's component in the bias direciton without centering, male and female would have exactly the same embedding and we would lose analogies such as father:male :: mother:female. We note that Neutralizing and Equalizing completely remove pair bias. Step 2b: Soft bias correction. Overloading the notation, we let W 2 R d\u21e5|vocab| denote the matrix of all embedding vectors and N denote the matrix of the embedding vectors corresponding to gender neutral words. W and N are learned from some corpus and are inputs to the algorithm. The desired debiasing transformation T 2 R d\u21e5d is a linear transformation that seeks to preserve pairwise inner products between all the word vectors while minimizing the projection of the gender neutral words onto the gender subspace. This can be formalized as min, where B is the gender subspace learned in Step 1 and is a tuning parameter that balances the objective of preserving the original embedding inner products with the goal of reducing gender bias. For large, T would remove the projection onto B from all the vectors in N , which corresponds exactly to Step 2a. In the experiment, we use = 0.2. The optimization problem is a semi-definite program and can be solved efficiently. The output embedding is normalized to have unit length, \u0174 = {T w/kT wk 2 , w 2 W }.Determining gender neutral wordsFor practical purposes, since there are many fewer gender specific words, it is more efficient to enumerate the set of gender specific words S and take the gender neutral words to be the compliment, N = W \\ S. Using dictionary definitions, we derive a subset S 0 of 218 words out of the words in w2vNEWS. Recall that this embedding is a subset of 26,377 words out of the full 3 million words in the embedding, as described in Section 2. This base list S 0 is given in Appendix F. Note that the choice of words is subjective and ideally should be customized to the application at hand. We generalize this list to the entire 3 million words in the Google News embedding using a linear classifier, resulting in the set S of 6,449 gender-specific words. More specifically, we trained a linear Support Vector Machine (SVM) with regularization parameter of C = 1.0. We then ran this classifier on the remaining words, taking S = S 0 [ S 1 , where S 1 are the words labeled as gender specific by our classifier among the words in the entire embedding that are not in the 26,377 words of w2vNEWS. Using 10-fold cross-validation to evaluate the accuracy, we find an F -score of .627 \u00b1 .102.Figure Debiasing resultsWe evaluated our debiasing algorithms to ensure that they preserve the desirable properties of the original embedding while reducing both direct and indirect gender biases. First we used the same analogy generation task as before: for both the hard-debiased and the soft-debiased embeddings, we automatically generated pairs of words that are analogous to she-he and asked crowd-workers to evaluate whether these pairs reflect gender stereotypes. Figure As an example, consider the analogy puzzle, he to doctor is as she to X. The original embedding returns X = nurse while the hard-debiased embedding finds X = physician. Moreover the harddebiasing algorithm preserved gender appropriate analogies such as she to ovarian cancer is as he to prostate cancer. This demonstrates that the hard-debiasing has effectively reduced the gender stereotypes in the word embedding. Figure Discussion", "conclusions": "Word embeddings help us further our understanding of bias in language. We find a single direction that largely captures gender, that helps us capture associations between gender neutral words and gender as well as indirect inequality. The projection of gender neutral words on this direction enables us to quantify their degree of female-or male-bias.To reduce the bias in an embedding, we change the embeddings of gender neutral words, by removing their gender associations. For instance, nurse is moved to to be equally male and female in the direction g. In addition, we find that gender-specific words have additional biases beyond g. For instance, grandmother and grandfather are both closer to wisdom than gal and guy are, which does not reflect a gender difference. On the other hand, the fact that babysit is so much closer to grandmother than grandfather (more than for other gender pairs) is a gender bias specific to grandmother. By equating grandmother and grandfather outside of gender, and since we've removed g from babysit, both grandmother and grandfather and equally close to babysit after debiasing. By retaining the gender component for gender-specific words, we maintain analogies such as she:grandmother :: he:grandfather. Through empirical evaluations, we show that our hard-debiasing algorithm significantly reduces both direct and indirect gender bias while preserving the utility of the embedding. We have also developed a soft-embedding algorithm which balances reducing bias with preserving the original distances, and could be appropriate in specific settings.One perspective on bias in word embeddings is that it merely reflects bias in society, and therefore one should attempt to debias society rather than word embeddings. However, by reducing the bias in today's computer systems (or at least not amplifying the bias), which is increasingly reliant on word embeddings, in a small way debiased word embeddings can hopefully contribute to reducing gender bias in society. At the very least, machine learning should not be used to inadvertently amplify these biases, as we have seen can naturally happen.In specific applications, one might argue that gender biases in the embedding (e.g. computer programmer is closer to he) could capture useful statistics and that, in these special cases, the original biased embeddings could be used. However given the potential risk of having machine learning algorithms that amplify gender stereotypes and discriminations, we recommend that we should err on the side of neutrality and use the debiased embeddings provided here as much as possible.", "SDG": [5]}, "technology_facilitated_sexual_violence_a_literature_review_of_empirical_research": {"name": "Technology-Facilitated Sexual Violence: A Literature Review of Empirical Research", "abstract": " Technology-facilitated sexual violence (TFSV) refers to a range of behaviors where digital technologies are used to facilitate both virtual and face-to-face sexually based harms. Such behaviors include online sexual harassment, gender-and sexuality-based harassment, cyberstalking, image-based sexual exploitation, and the use of a carriage service to coerce a victim into an unwanted sexual act. This article reviews the current state of knowledge on these different dimensions, drawing on existing empirical studies. While there is a growing body of research into technology-facilitated harms perpetrated against children and adolescents, there is a dearth of qualitative and quantitative research on TFSV against adults. Moreover, few of the existing studies provide reliable data on the nature, scope, and impacts of TFSV. Preliminary studies, however, indicate that some harms, much like sexual violence more broadly, may be predominantly gender-, sexuality-, and age-based, with young women being overrepresented as victims in some categories. This review collects the empirical evidence to date regarding the prevalence and gender-based nature of TFSV against adults and discusses the implications for policy and programs, as well as suggestions for future research.", "keywords": "technology,sexual violence,online sexual harassment,Internet communication,online victimization,revenge pornography", "introduction": "Sexual violence and harassment are widely recognized as globally significant human rights problems. 1 According to estimates by the World Health Organization (WHO, 2013), 35% of women worldwide report having experienced either physical or sexual violence by a partner, or sexual violence by a friend, family member, acquaintance, or stranger. National studies and police data indicate the highly gendered pattern to sexual violence, with women continuing to represent the majority of victims and men overwhelmingly, although not exclusively, the perpetrators. Young women aged 16 to 24 years are widely recognized as being at greatest risk of experiencing sexual assault, and most often at the hands of a known man, such as a boyfriend, friend, or acquaintance, rather than a stranger (for prevalence studies, see, e.g., Australian Bureau of Statistics, 2013; Bureau of Justice Statistics [US], 2013; Office for National Statistics [UK], 2014). Further studies indicate that sexual harassment is likewise a persistent problem. National surveys in Australia indicate that 1 in 3 women experience sexual harassment in their lifetime compared to 1 in 10 men . The international literature thus demonstrates the persistent nature and prevalence of sexual violence and harassment and that these harms 2 are disturbingly common, highly gendered, and most often relational.(Australian Human Rights Commission, 2012)With the rapid uptake of Internet-enabled devices, such as computers, laptops, mobile phones, and tablets, and online communication services, such as social media networks and social applications, it is perhaps unsurprising that digital technologies might also be used as tools to facilitate sexually based harms. While empirical studies have highlighted perpetrators' use of technology to facilitate domestic violence (see Burke, Wallen, Vail-Smith, & Knox, 2011;, dating abuse (see Dimond, Fiesler, & Bruckman, 2011)Stonard, Bowen, Lawrence, & Price, 2014;, cyberstalking (see Zweig, Dank, Yahner, & Lachman, 2013)Sheridan & Grant, 2007;, and sexual exploitation of children (see Spitzberg & Hoobler, 2002)Martin & Alaggia, 2013;, there is a dearth of empirical research examining the varied nature and prevalence of behaviors involving technology. Similar to the broader, umbrella term of ''sexual violence'' that is widely used in the research literature, we use a broad term to capture an array of abusive behaviors involving technology; what we call technology-facilitated sexual violence (hereinafter, TFSV). This conceptualization refers to a range of criminal, civil, or otherwise harmful sexually aggressive and harassing behaviors that are perpetrated with the aid or use of communication technologies.Mitchell, Jones, Finkelhor, & Wolak, 2011)To date, empirical studies of TFSV have almost exclusively focused on children and adolescents, and very little research has examined adult victimization or perpetration. 3 This article provides a literature review of the few existing empirical studies concerning the nature and/or prevalence of various forms of TFSV against adults. The findings of this review are organized according to five different, but interconnected, dimensions, which include (1) online sexual harassment, (2) gender-and sexuality-based harassment, (3) cyberstalking, (4) image-based sexual exploitation, and (5) the use of a carriage service to perpetrate a sexual assault or coerce an unwanted sexual experience. 4 The first section justifies the paper's terminology and inclusion of dimensions. The second section describes the method for the literature review. The third section then presents the key findings from existing empirical studies based on this literature review, and the final section discusses the implications for policy and practice.", "body": "Sexual violence and harassment are widely recognized as globally significant human rights problems. 1 According to estimates by the World Health Organization (WHO, 2013), 35% of women worldwide report having experienced either physical or sexual violence by a partner, or sexual violence by a friend, family member, acquaintance, or stranger. National studies and police data indicate the highly gendered pattern to sexual violence, with women continuing to represent the majority of victims and men overwhelmingly, although not exclusively, the perpetrators. Young women aged 16 to 24 years are widely recognized as being at greatest risk of experiencing sexual assault, and most often at the hands of a known man, such as a boyfriend, friend, or acquaintance, rather than a stranger (for prevalence studies, see, e.g., Australian Bureau of Statistics, 2013; Bureau of Justice Statistics [US], 2013; Office for National Statistics [UK], 2014). Further studies indicate that sexual harassment is likewise a persistent problem. National surveys in Australia indicate that 1 in 3 women experience sexual harassment in their lifetime compared to 1 in 10 men With the rapid uptake of Internet-enabled devices, such as computers, laptops, mobile phones, and tablets, and online communication services, such as social media networks and social applications, it is perhaps unsurprising that digital technologies might also be used as tools to facilitate sexually based harms. While empirical studies have highlighted perpetrators' use of technology to facilitate domestic violence (see To date, empirical studies of TFSV have almost exclusively focused on children and adolescents, and very little research has examined adult victimization or perpetration. 3 This article provides a literature review of the few existing empirical studies concerning the nature and/or prevalence of various forms of TFSV against adults. The findings of this review are organized according to five different, but interconnected, dimensions, which include (1) online sexual harassment, (2) gender-and sexuality-based harassment, (3) cyberstalking, (4) image-based sexual exploitation, and (5) the use of a carriage service to perpetrate a sexual assault or coerce an unwanted sexual experience. 4 The first section justifies the paper's terminology and inclusion of dimensions. The second section describes the method for the literature review. The third section then presents the key findings from existing empirical studies based on this literature review, and the final section discusses the implications for policy and practice.TFSV: Parameters and TerminologyThere are a number of terms that are used interchangeably to refer to digital abuse, cyberstalking, and cyberbullying in the broader research literature. For instance, empirical studies use terms such as ''electronic aggression'' Other scholars investigating online abuse and harassment tend not to focus on either sexual or gendered harms or violations, although they may include a couple of items relating to sexually based behaviors. Indeed, the line between sexual and nonsexual behaviors is not always clear-cut. One example is intimate partner cyberstalking where a perpetrator uses technology, such as global positional system [GPS] tracking, as a means of control and surveillance over his or her victim. Another example is calling someone a pejorative sexual and gendered name (e.g., a ''slut''), which may not be a form of online sexual abuse when used in good faith (e.g., in a comedy sketch or as a joke among friends). Second, although sexuality and gender are intricately related, not all forms of digital abuse are necessarily ''gendered. '' 5 The widespread mob harassment of a female blogger, for example, might not necessarily constitute a form of gender-based harassment, unless gender is the reason or motivation behind the abuse (otherwise known as ''bias motivation'').While there are murky distinctions that require further investigation, existing legislation on stalking, sexual harassment, and hate speech in offline contexts provide useful guidance on the construction of distinctions. The dimensions discussed in this article are modeled on these existing and not always unproblematic legal definitions. The criteria is that an act is done because of the gender or sexual orientation of the person or some or all people in the group, and/or is perpetrated with the intent to harm, and/or does result in significant harm to the victim; harms which might (or might not) be specifically ''gendered.'' This does not necessarily mean the perpetrator's motives have to be specifically or solely on the grounds of gender or sexuality, but rather the behavior is in part the result of a social context of gender inequality and hierarchization that in turn shapes normative expectations surrounding femininity and masculinity Keeping these important distinctions in mind, a range of online behaviors can be confidently excluded from the categorization of TFSV, such as people defacing a Facebook memorial page of a deceased person or other forms of cyberharassment, cyberstalking, and hate speech that are not explicitly or implicitly linked to sexual violence, or sexual-and gender-based harassment. Accordingly, the use of technology as a surveillance and tracking device in the context of intimate partner violence is excluded in this review since such behaviors are not specifically forms of sexual violence or sexual harassment (although may of course be part of a pattern of abuse). On the other hand, as sexual violence is common in many abusive relationships, we do include some forms of cyberstalking in one of our five dimensions discussed below, when it is unwanted and repeated sexual pursuit that causes fear and apprehension. Likewise, on the same grounds, we include sexually motivated gender-based hate speech.MethodEmpirical research on TFSV is extremely sparse, and as such, the present study does not follow the systematic review approach. Instead the authors present a review of the research literature pertaining either to a collection of sexually based behaviors or to studies that individually examine a particular dimension of TFSV.The analysis of the existing literature sought to explore the gendered prevalence, nature, and impacts of TFSV. All peerreviewed journal articles that were included in this review specifically include one or more of the aforementioned five dimensions relating to adults over the age of 18 years or were on digital abuse more generally but related to adults over the age of 18 years and included at least 1 item relevant to TFSV. The review was based on an extensive electronic literature search using a variety of databases (e.g., Academic Search Premier, CINCH, Google Scholar, PsychINFO). Reference lists in each of the articles were also reviewed. Theoretical and/or legal articles were excluded if they were not based on empirical, quantitative research. 6 Only scholarly articles that were written in English since January 1, 1995, were included in the review.Given the broad range of behaviors included in this study, a variety and combination of key words were used in the review, including cyberstalking/cyberbullying/online stalking/cyber harassment, distribution of sexual assault images/revenge porn/pornography/sexting, electronic/cyber/online sexual harassment/bullying/aggression/coercion, gender and online gaming, gender-based hate speech/vilification, sexploitation, technology/electronic/communication technology/digital/ online/cyber communication technology and sexual violence/ dating violence/sexual abuse, and virtual rape. The original literature search produced in excess of 500 articles, in part due to the broad range of key words used. Of these, a number were selected (based on the criteria above) from the abstract reading for further review or from reference lists of those selected articles. Of these articles reviewed, 11 were included in the final review because they met all of the inclusion criteria: the study included at least one item in their survey that fit with the authors' conceptualization of TFSV, the study was on adults and was quantitative in design, there was sufficient information regarding the methodology and results of the study, and the article was published in a peer-reviewed, English-language journal. A table of existing empirical research on adult forms of TFSV was constructed according to the different dimensions (see Table Although not technically part of the literature review, in the discussion below, where there is a lack of empirical data specifically on digital sexual violence and harassment, the article refers to a number of existing empirical studies that explore more generally online digital abuse and violence. Likewise, some empirical studies on sexually based harms against children and adolescents are also discussed. Finally, a small number of surveys that have been published in nonacademic journals (e.g., surveys conducted by organizations) are referred to in order to supplement what is currently known about TFSV. The studies that do not fit the criteria listed above form part of the narrative discussion presented here but are not included in Table TFSV: Key FindingsTo date, minimal research has been conducted on the manner in which communication technologies are used to facilitate diverse forms of sexual violence and harassment, particularly against adults. The authors, both individually and collectively, have examined some elements of this including challenges for law and policy agencies in responding to these behaviors Researchers often take a broader view of online harassment, inclusive of hate speech and sometimes cyberstalking. Although there are inevitable overlaps between different dimensions, this article draws on the narrower definition of sexual harassment as provided in legal statutes as ''unwanted sexual attention online,'' defined by In comparison to these broader studies of cyber harassment, It is important to note that surveys that do not provide a definition of online sexual harassment to their respondents might yield overall conservative prevalence figures given that victims do not always label their experiences ''sexual harassment''. For example, the Pew Center study simply asked respondents whether they had experienced sexual harassment and did not provide a definition. This is indicated in studies on offline forms of sexual harassment where, according to one study, one in five (18%) people indicated they had not been sexually harassed but then went on to report having experienced behaviors that constituted sexual harassment according to the legal definition (Australian Human Rights Commission, 2012). Those surveys that specifically ask respondents about sexual harassment with a definition or examples provided, do not necessarily capture the range of sexually harassing behaviors seen with the rapid development of smart phone technology or social media use in more recent years.Although it is not possible to report on prevalence of online sexual harassment based on these few existing studies on adult Internet users, these studies are relatively consistent in their findings that sexual harassment disproportionately affects women both in extent and impacts, particularly young women. For instance, in a study of undergraduate U.S. college students (n \u00bc 342), Lindsay, Booth, Messing, and Thaller (in press, p. 9) found that there were gender differences in terms of frequency of harassment, being harassed by a known person, and psychological impacts:Females reported more frequent incidences of harassment than males, with 34% reporting being harassed at least once by someone they knew, 17% having been harassed by someone that they did not know, 21% reporting being harassed by a significant other, and 31% reporting being harassed after they had asked the person to stop. Females also reported feeling fear as a result more often than males. This is also consistent with empirical data on sexual harassment in the workplace. For example, the Australian Human Rights Commission (2012) telephone survey found that 1 in 5 women, compared to 1 in 20 men, reported experiencing sexual harassment in the workplace. The data are also relatively consistent with research on online sexual harassment against children and youth. For example, Although there is some existing research on hostility toward online gamers more generally (see, e.g., In addition, a few studies have examined misogynistic language in social media sites. For example, Cyberstalking. Cyberstalking is commonly defined as an extension of offline forms of stalking using electronic means (see Yet, the empirical literature on cyberstalking is still emerging in comparison to the well-established field of research into the nature, extent, and impacts of stalking behaviors more generally. There is a dearth of literature, for example, focusing more specifically on online intimate partner stalking or monitoring There is a lack of clarity over what constitutes ''cyberstalking'' as opposed to online sexual harassment or even ''cyberbullying'' Online obsessive relational pursuit aligns most usefully with the conceptualization of TFSV presented here. Rather than seeking to encompass a broad and indeed inconsistent definition of cyberstalking, we find it useful to focus on those cyberstalking behaviors that are characterized by sexual and/or relational pursuit. Such behaviors are not necessarily captured by other TFSV dimensions of sexual harassment, or gender/ sexuality-based harassment, and in some cases may further meet the ''course of conduct'' and fear thresholds of criminal stalking.Image-based sexual exploitation. There are two behaviors that come under this fourth dimension. The first is ''sexting coercion,'' which Nearly all of the existing empirical research concerns more broadly ''sexting'' practices among young people. 11 These various sexting studies yield different findings on prevalence, depending on the participant sample, sampling techniques, instruments, as well as the different definitions of sexting used in each study. As such, exact prevalence rates of sexting among young people are difficult to establish Fewer studies have explored sexting behaviors among the adult population. They found that OSV was more common among women, young, and middle-aged adults (e.g., 25-35 years) as well as nonheterosexuals. Furthermore, reporting of victimization increased significantly among this group according to the study, although they found that the dissemination of sexual images without consent was relatively low among respondents (3.7%). By way of contrast, a broader study on ''technologybased coercion'' found a slightly higher percentage (16%) had shared a sexually suggestive message or picture of someone without their consent In relation to broader surveys on technology and intimate relationships, the McAfee (2013) Love, Relationships, and Technology Survey of U.S. adults between the age of 18 and 54 (n \u00bc 1,182) found that 1 in 10 ex-partners had threatened to expose intimate images of their ex-partners online, with 60% having carried out the threat. This survey also found that men (12%) were more likely than women (8%) to have their photos distributed online, and such threats are more likely to be carried out against men (63%) than women (50%). In the second McAfee (2014) survey, 54% of respondents said they had sent or received intimate video, photos, e-mails, or text messages. Similarly, the National Campaign to Prevent Teen and Unplanned In relation to specific revenge porn studies, the only survey to date is the Cyber Civil Rights Initiative's (2014) Effects of Revenge Porn Survey. The survey was hosted on the End Revenge Porn website and respondents self-selected into the study. The survey had a total of 864 respondents, 90% of whom were women. The survey found that 80% of respondents had taken the image themselves and 93% said they experienced significant emotional distress due to their images being distributed without their consent. The results of the survey indicate that victims in this sample predominantly identified as female and that the impacts of these behaviors are gendered because of the sexual double standards that exist surrounding female sexuality. As respondents self-selected into the survey the results are not representative. The survey gives little indication of prevalence, and does not provide sufficient indications of gender differences in either victimization or perpetration. 14  To date, there has been limited empirical research particularly on the phenomenon of revenge pornography. Although some broader studies included items relating to the distribution or sharing of intimate or sexually explicit images of another person without their consent, these results are limited because they did not investigate the impact or severity of these acts. In other words, existing studies have not explored the context surrounding the nonconsensual distribution of intimate images. Moreover, scholarly studies that included items relating either to sexting coercion or to revenge pornography rely on convenience samples of undergraduate students and are not representative of the general population. More research is thus needed to understand the prevalence, nature, and impacts of imagebased sexual exploitation.Technology-facilitated unwanted sexual experiences. Unwanted sexual experiences facilitated through digital technologies can be classified according to three behaviors. The first is sexual coercion or ''sextortion,'' a form of coercion where a person procures ''sexual cooperation by putting some kind of pressure on a victim'' Empirical studies thus far on online predatory sexual behaviors have been almost exclusively focused on children and adolescents, owing to community concerns about children's vulnerability and safety, and perhaps due to the erroneous assumption that only young people are victims of online predatory behavior (see, e.g., Discussion and Avenues for Future ResearchThis literature review sought to examine the empirical, quantitative literature on a range of TFSV behaviors experienced by adults. Research regarding TFSV among adults is still extremely new, and there are significant limitations in both the scope and number of empirical studies conducted to date. First, the instruments used in these studies vary widely, meaning that their findings are not comparable. The lack of standardized, uniform definitions was particularly problematic with a wide variety of behaviors being associated with particular categories, particularly online sexual harassment. Many of the studies were more broadly focusing on either technology and intimate relationships or technology and cyber victimization. As such, most studies only included a small number of items relating to the TFSV behaviors identified in this article. However, these studies did not examine the context underlying these acts. For instance, studies that included items on the nonconsensual distribution of intimate images did not examine the impacts of such behaviors.Second, the quality of findings in the studies included in this review also varied. Some studies did not detail their methodologies rigorously. The generalizability of findings to entire communities or populations was not often discussed, with most studies not examining their participants' characteristics against target populations. Some studies made claims that there were gender differences in behaviors experienced, without giving frequency data regarding how often the behaviors were experienced. Many of the studies too overrepresented either male or female students in their samples.Third, the mean age of all studies' samples was quite young. Most studies used an undergraduate college (university) student sample. This has significant implications for understanding TFSV, and there is as such limited knowledge of how middle to older aged groups experience such behaviors. Similarly, little is known of prevalence rates of TFSV in different racial, ethnic, or sexuality groups. Anecdotal accounts, qualitative research, legal analysis, government inquiries, and media reports suggest that the range of sexually based harms that are identified in this article warrant further investigation. Indeed, part of the impetus for this review is the significant community and government concern regarding what appear to be rapidly increasing instances of sexual violence and harassment both online and via mobile phone or other communications devices. Following several high-profile cases in the United States, United Kingdom, Canada, Australia, New Zealand, and elsewhere, it has become evident that in many jurisdictions there are gaps in legislative responses to newly emerging harms, particularly where the use and distribution of sexual or intimate images is part of a pattern of online abuse or harassment. The authors' own qualitative research with key legal professionals and women's counsellor-advocates has identified cause for concern over the uses of online harassment and blackmail (dubbed ''sexploitation'' by some stakeholders), whereby sexual images in particular have been used to coerce or harass women in domestic and sexual violence situations This phenomenon is troubling, not because these behaviors necessarily constitute ''new'' harms, but rather because the reach, nature, and duration of these harms, as well as the current gaps in legal redress available to victims, makes them both insidious and difficult to respond to. Yet, it is clear that the community is raising concerns about these behaviors, and governments are framing legislative responses, even in the absence of empirical research. Further quantitative research into the prevalence and gender-based nature of TFSV would therefore usefully inform the development of legislative and regulatory responses, as well as community education focused on awareness raising and prevention.Future research needs to focus on identifying the extent and nature of a range of TFSV behaviors, including prevalence and impacts of victimization and perpetration, the gender dynamics of these behaviors, individual actions taken to respond, and the outcomes of those actions. A nuanced approach to examining TFSV is most appropriate, and a mixed methods approach is important to capture an array of behaviors, including those within intimate partner relationships, those that use various specific technologies (such as dating apps and sites), behaviors specific to certain environments (e.g., workplaces), and behaviors documented within the popular media. Future research also needs to investigate the relationship between violence and attitudes to violence, gender, and sexuality, and focus on the intersections between gender, sexuality, race, age, and other factors. Overall, prevalence data regarding adult experiences of TFSV are crucial to further informing both reforms to law and practice, and future research.Conclusion", "conclusions": "To date, the vast majority of empirical studies on different dimensions of online abuse and harassment focus on children and adolescents. In contrast, there are very few quantitative studies on adults and virtually no qualitative studies on the phenomenon of TFSV. Of the studies discussed here, few studies provide reliable data on prevalence, perpetration, and victimization. Most of the studies to date use convenience samples of college students. Overall, there is a clear lack of research into diverse forms of TFSV against adults, with the exception of some forms of online sexual harassment and cyberstalking. However, what these existing studies do indicate is that TFSV against adults is a growing problem with serious and wideranging impacts. Furthermore, although prevalence rates are difficult to establish different participant samples, sampling techniques, instruments, and definitions used, this body of research demonstrates that both women and men may be victims and perpetrators of online sexual violence and harassment; however, women, as well as lesbian, gay, bisexual, trans, intersex persons are more likely to be targeted for specific forms of digital abuse. This is perhaps not unsurprising, given what is already known about conventional forms of sexual harassment, sexual violence, and discrimination. Online forms of sexual violence and harassment likewise stem from the socially constructed beliefs and attitudes about gender and sexuality (including victim blaming and victim shame and stigma) as well as perpetrator motivations for power and control. Digital technologies then serve as a tool to perpetrate more conventional forms of gender-and sexuality-based violence and harassment, with different effects and impacts due to anonymity, the failure of regulation, as well as the sheer speed and vast reach of the Internet. Further empirical research is needed in order to shed light on the nature, scope, prevalence, and impacts of such behaviors.1. Sexual violence is an all-encompassing term that includes ''any sexual act, attempt to obtain a sexual act, unwanted sexual comments or advances, or acts to traffic, or otherwise directed, against a person's sexuality using coercion, by any person regardless of their relationship to the victim, in any setting, including but not limited to home and work'' (World Health Organization [WHO], 2011). This definition captures physical acts, such as rape (generally considered to be a penetrative offense) or sexual assault (a physical attack not necessarily involving penetration) as well as noncontact offenses or behaviors, such as sexual harassment and sexual coercion. The term ''sexual violence'' and its widespread usage among academics and professionals indicates that violence is not simply a physical act involving physical injury. For instance, a rape can result in no direct physical injury to a victim's body but can have long-lasting psychological and physical impacts. Likewise, ''domestic violence'' is usually defined to incorporate finanical, emotional, and physical abuse. Throughout the article, the authors use the term sexual violence to encompass a diverse range of acts involving technology, drawing on the WHO definition. The authors discuss a diversity of acts increasingly identified in the research literature and provide analysis of the benefits and limitations of these definitions in the relevant sections. 2. Please note that the authors intentionally use the term ''harms'' throughout the article because such a term captures acts that may not be deemed a criminal offense or a civil wrong under law. While this is a broad and unspecified term, it has the benefit of capturing impacts on victims where some kind of physical, psychological, social, or financial harm has resulted. 3. The authors define an ''adult'' as a person over the age of 18 years, although they recognize that in some cultures, a child may be classified or defined according to a different chronological age, life experience, or emotional maturity. 4. The dimensions outlined in this article represent a refinement from the authors' previous work, where the dimensions were organized differently (see Henry & Powell, 2014, 2015a. The dimensions are overlapping; for instance, the nonconsensual distribution of intimate images could be both a form of cyberstalking and online sexual harassment. The difficulty in constructing categories in large part stems from the complex entanglement between conventional and online behaviors, as well as the different terminology used in a range of studies examining digital abuse more generally. 5. Gender-based violence has been defined to mean ''violence that is directed against a woman because she is a woman or that affects women disproportionately'' (Art 3 d Council of Europe Convention). However, this definition problematically collapses ''gender'' with ''women.'' In this article, the terms ''sexual violence and harassment'' are preferred, although recognizing that genderbased violence can be usefully thought of in relation to prevalence (e.g., women, men, or trans women/men being disproportionately targeted for specific forms of harassment and violence), nature (e. g., the victim's ''sexed'' body is a source of objectification, currency, ridicule, and insult), and impacts (e.g., victims experience more adverse impacts because of their gender). Gender, however, is not always the most important variable and an intersectional approach is preferable to understanding violence both offline and online. For instance, the sexual abuse of Iraqi prisoners at Abu Ghraib prison, or the targeting of men and young boys for the 1995 Srebrenica massacre during the conflict in the former Yugoslavia, are both examples of gender-based and racial/ethnic-based violence. 6. A small number of scholars have examined online digital abuse in relation to sexual violence and harassment from a conceptual/theoretical and/or legal framework of analysis (see, e.g., , 2015b). Currently, there are very few empirical studies though on TFSV using qualitative methodologies, such as interviews with victim and stakeholders, focus group discussion, content analysis, or case law analysis (see Henry & Powell, 2015b)Bluett-Boyd et al., 2013;Draucker & Marstolf, 2010;. 7. Note that unwanted sex-related e-mails (e.g., advertisements for Viagra) are not discussed in this article because any harms associated with such e-mails are not on a par with other harms discussed in this article. 8. In terms of online sexual harassment perpetration, Henry & Powell, 2015a) in their study on online sexual activity (''the use of the internet for any activity (text, audio, graphics) that involves sexuality''), found that approximately 14% of participants (n \u00bc 7,000) reported that someone had complained to them about their online sexual behavior. This study was not specifically on sexual harassment but rather focused on online sexual compulsivity, which the authors define as attempts to find sexual partners on the Internet as well as cybersex. It is not included in the final table Cooper, Delmonico, Griffin-Shelley, and Mathy (2004, p. 131)Table () because it is not clear whether respondents were referring to behaviors amounting to sexual harassment. 9. Gender or sexuality-based hate speech (or vilification) can be defined as offensive speech or conduct which can incite hatred or contempt on the basis of either sexuality or gender, and is done (out of ill will) because of the gender or sexuality of a person or all people of that group and is likely to offend, insult, humiliate, or intimidate an individual or a group of people. While in some legal jurisdictions, sexuality-based hate speech may be prohibited by law because it incites violence or discrimination against the group, in most common law jurisdictions, gender-based hate speech is not unlawful or criminal under law, unless it constitutes a form of criminal harassment or stalking (see Weston-Scheuber, 2012). 10. ''Virtual rape'' is used to describe a situation when a person's avatar (or digital representation of themselves) is subjected to simulated sexual violence by other avatars, most recently in three-dimensional virtual worlds.111. The term ''sexting'' includes the sending of intimate or sexually explicit text, still images and videos from mobile to mobile or from mobile to Internet sites. ''Nonconsensual sexting'' is used interchangeably with the term ''revenge pornography''. 12.  likewise explored the relationship between sexting and cyber victimization, although in their words did not ''disentangle the effects of sending and receiving sext messages separately.'' 13. In addition to these various quantitative studies on adult forms of sexting, there is complementary qualitative research that has explored the perceptions and experiences of sexting among young people (see Reyns et al. (2013, p. 14)Albury, Crawford, Byron, & Mathews, 2013;Burkett, 2015;Crofts et al., 2015;Ringrose, Gill, Livingstone, & Harvey, 2012;). 14. Although not reliable indications of prevalence or gender differences in victimization and perpetration, figures obtained by police and/or revenge porn helplines internationally provide an indicative picture. Since the establishment of the UK Revenge Porn Helpline, the August 2015 figures revealed that females constituted 75% of all people seeking advice and support. Furthermore, of the 25% of calls to the helpline from men, 40% identified as being ''homosexual'' Walker, Sanci, & Temple-Smith, 2013. In Japan, police records indicated that 110 cases of revenge porn were reported in December 2014 alone following the introduction of the new laws, with the majority of victims being female in their late 20s or older (Gov.uk, 2015).(Wright, 2015)", "SDG": [5]}, "toward_a_political_economic_framework_for_analyzing_digital_development_games_a_case_study_of_three_games_for_africa": {"name": "Toward a Political Economic Framework for Analyzing Digital Development Games: A Case Study of Three Games for Africa", "abstract": " This study analyzes 3 digital games for development in Africa, examining them as individual texts that operate as part of a broader international development discourse, and as political economic products with implications for international development and gaming industries. A conceptual framework is proposed as part of this process. By looking at individual games alongside the organizations and funding structures behind them, I argue that digital games, as a new technological platform, do not in-and-of themselves present a revolutionary approach to development across Africa; rather, they both reinforce and subvert dominant approaches already at play.", "keywords": "Digital Development Games,Games,Gamification,Development,Neoliberalism", "introduction": "", "body": "Game studies scholars argue that a critical textual analysis of games is not only possible, it is necessary Thus, a critical textual analysis of DDGs must be grounded in the broader development industry discourse, beginning with the term \"development\" itself, which has long defied any singular definition. Generally assumed to be an intentional process meant to improve quality of life and create beneficial social change, consensus on what beneficial change looks like, who is in the best position to enact it, how it should be done, and for whom is lacking Alternative perspectives have critiqued the ethnocentrism of the dominant modernization paradigm, calling for more holistic and contextual approaches that take into account the roles of gender, the environment, religion, and indigenous knowledge in creating sustainable development solutions. Whether feminist, environmental, theological, and/or grassroots in nature, these critical approaches work to disrupt the top-down hierarchical structure of a Western-development model and move beyond economic definitions of development. Nonetheless, approaches based on theories of modernization and neoliberal economics remain dominant within the world's largest development institutions, such as the World Bank (WB).Trends and changes in the global development landscape impact how issues and solutions are framed within the discourse DDGs must likewise be understood as material products produced within a specific sociohistorical and economic context Considered \"the media of Empire\" in the 21st century, digital gameplay often works in favor of global hypercapitalism as it \"trains flexible personalities for flexible jobs, shapes subjects for militarized markets, and makes becoming a neoliberal subject fun\" The DDGs analyzed in this article were created to facilitate development across Africa, which, historically, has been a major target of projects spanning the spectrum of media innovations: From radio, to satellite television, to ICT projects, each new innovation has been latched onto as the key to developing the continent Toward a political economic framework for game analysisThe gamification of the development industry must be understood as more than a simple system of applied game mechanics and game elements (i.e., point systems and progress bars) to nongame development activities; rather, it is a process of productive purpose in which games are used to produce specific development-oriented results in nongame contexts The first category, Developing Developers (DD), encompasses games that present an audience located primarily in the Global North with development issues situated in the Global South. While the specific projects vary, the purpose of DD games is to construct for players an understanding of development-what it should look like, how it should be done and who should be in control of it-as well as to call on players as actors in the development process within the game and beyond. The second, Digital Interventions (DI), is made up of games intended for use as on-the-ground development interventions. The purpose of DI games is to \"develop\" individuals living in the Global South, using digital formats as a new avenue for carrying out health, education, and skills-based projects. The final category, Critical Play (CP), is made up of games that address structural inequality as the root cause of underdevelopment and ask players to engage in critique and conceptualize solutions outside of the current system. The purpose of CP games is to challenge local and global power structures through gameplay and educate players about systemic issues that exacerbate development issues.I argue that DDGs must be understood as political economic products that carry an embedded development narrative to a target audience and, in this way, further a particular development agenda. Analyzing games according to their position in the framework proposed here is an important step in this process as it highlights the game's productive purpose. By asking whether a game is attempting to Develop Developers act as a Development Intervention, or engage an audience in Critical Play, an important motive is identified. Thus, this study analyzes one game, and the organization responsible for its production, to illustrate each of the three categories. The analysis presented here is only a first step-more studies applying the framework could help refine it while further illuminating how political economic issues of production influence the types of games created.Digital interventions: The World Bank Institute and Urgent EVOKEThe World Bank (WB), referred to as \"the foremost international development agency\" Designed by award-winning game designer Jane McGonigal, EVOKE was created \"to help empower people all over the world to come up with creative solutions to our most urgent social problems\" (Urgent EVOKE, 2010). Online and free to play, EVOKE offered a new approach for including youth across sub-Saharan Africa (SSA), its target demographic, in the development process by inviting them to voice their own ideas about, and solutions to, specific development issues. Certainly, the potential for EVOKE to generate a new space for participation by stakeholders in the development process was exciting, but in reality the constraints on such participation were significant and the game all but failed in reaching its target demographic. Further, the WBI's narrow definition of game success framed development issues and solutions within a neoliberal framework that greatly constrained the context in which players were able to participate.The 19,386 people from across the world who registered to play EVOKE were tasked with 10 missions, covering topics such as social innovation, food security, the future of money, empowering women, and preserving indigenous knowledge The development ideology embedded in the game acts as its own constraint to participation as it narrowly defines success within the game world and beyond. According to the game's executive producer Robert Hawkins, EVOKE's success was measured by player development of a set of specific \"21st-century skills in relation to social innovation\" A primary goal of modernization has always been to bring specific technological skills to populations in order to promote national economic growth and individual behavior change. Indeed, the need for traditional populations to acquire empathy was one of Daniel EVOKE's powers are based on a framework from the Partnership for 21st Century Skills, a U.S. organization that advocates for educational policy change at \"the local, federal, and state level to shift the conversation for kids and ensure they're ready to lead and take on the challenges of the 21st century\" (Partnership, Partnership for 21st Century Skills, n.d.). The partnership's board, which has included AOL, Apple, Dell, Cable in the Classroom, Cisco, Ford Motor Company, Intel, Microsoft, and The Walt Disney Company, has a vested interest in creating citizens with Web 2.0 skills who value technological innovation, who appreciate and engage with a global economy, and who rely on media and technology systems to do so. Thus, furthering the expansion of these skills in developing regions via an online game serves the interests of both the Partnership for 21st Century Skills and the WBI well. A postgame evaluation recommends that EVOKE be framed as a \"transitional intervention, one that channels university students, recent graduates, and other young, socially minded individuals into development-focused activities and careers\" Although EVOKE theoretically created a new space for stakeholder participation in the development process, such participation was inhibited by material and immaterial constraints. While it is important to consider how and why the WBI missed its target demographic almost entirely, it is even more important to consider how EVOKE, as a DI game, reflects and reinforces the neoliberal development agenda of the WBI. By defining game success according to a skill set that emphasizes entrepreneurship, market participation, and reliance on technology above all else, the game acts as an on-the-ground development intervention that reinforces a problematic development approach in which all elements of social life, including education, are taken up in the market. The purpose of the HTSM is to galvanize audiences around a set of issues Kristof and WuDunn have identified as causes of, or ways out of, the systemic oppression of women and girls, including forced prostitution, sex trafficking, gender-based violence, maternal mortality, economic empowerment, and education. The HTSM's focus on women and economic empowerment as a key to solving a host of development issues is reflective of a mainstream Women in Development (WID) approach in which arguments based on efficiency are used to highlight women's potential as economic producers and the benefits women bring to development when they are included in economic processes. But critics of the WID approach have argued this focus on women and productivity has in fact created greater inequality for women Although it received ample praise from the mainstream, the Half the Sky book was called out by critics for a variety of issues including the oversimplification of complex problems, problematic issues of power and consent that accompany the voyeuristic expose techniques used to tell the stories of young rape victims, and an overall \"white savior\" mentality that provides solutions while failing to explore the transnational contexts and global power structures that exacerbate development issues. The more recent HTSM and its accompanying multimedia products perpetuate many of the problematic approaches and assumptions of the 2009 text. Pertinent to this study is the Half the Sky Movement: The Game (HTSMG), which was developed for Facebook and programmed in English and French for play on desktop and laptop computers. Four months after its 2013 launch, the HTSMG had reached 1 million players, and mainstream media outlets were hailing video games as the new frontier for activism and social change In the HTSMG, players join Radhika, an Indian woman, on a \"journey from oppression to opportunity\" as she completes a series of quests in her hometown and then travels to help other women in Kenya, Vietnam, and Afghanistan, ending her journey in the United States. The game begins with Radhika telling the player she is unable to afford medicine for her sick child; the player then navigates through a series of quests to earn the money necessary to purchase it. Gameplay happens in two ways:In the first, players are able to choose one of two options presented at the end of a brief interactive scene between Radhika and another game character; in the second, players complete basic puzzle games in which similar items must be linked together using the computer's mouse. From selling mangoes in the market, to applying for a microcredit loan, to buying a goat and starting a small business by selling the milk it produces, the \"journey from oppression to opportunity\" that the player joins Radhika on is one firmly rooted in economic self-help and entrepreneurship. Once players have helped Radhika secure a loan to build an addition to her house, they join her as she travels to her next destination: Kenya. When Radhika gets to Kenya she goes directly to a rural clinic where she meets a nurse who informs her of the high rates of human immunodeficiency virus (HIV) in the country. She then meets the nurse's daughter who admits she has tested positive for HIV. Radhika next learns about the high rates of malaria in Kenya and is asked to help the nurses decide if they will sell the clinic's supply of mosquito nets, or give them away for free to local women and children and risk them being sold on the black market. Next, Radhika helps the nurses at the clinic organize a community discussion about family planning and the use of birth control, but she and her supporters are chased away by a group of angry community members before their meeting is able to begin. Radhika eventually moves through the other countries and ultimately ends up at the UN Headquarters in the United States. But just getting from India to Kenya takes a substantial amount of time and real money from the player: The game \"times-out\" after minimal gameplay, forcing players to wait or pay to continue play.The emphasis on women entering the market via microloans and small businesses woven throughout Radhika's story reflects a broader WID-based, neoliberal development agenda that identifies poor women in the Global South as an \"untapped resource\" and frames them as individual entrepreneurs responsible not only for their own development but also for the development of society at large, regardless of the various constraints (e.g., lack of material resources, skills, education, childcare) they may face If the solution to women's empowerment is indeed individual financial support, there is little reason to consider broader political action once this support has been given. DD games present development issues situated in the Global South to a primarily Northern audience while constructing for them a particular understanding of what development should look like, how it should be done, and what their role is in it. In the case of the HTSMG, development is achieved by individual women in the Global South entering the global marketplace via microcredit loans and small businesses; the role of the game player, situated in the Global North, is to facilitate that productivity through material donations. This emphasis on private aid and individual empowerment aligns nicely with a broader neoliberal development agenda that \"reduces social change to entrepreneurship in a market-based system, and civic involvement and voice to clicktivism\" CP: Afroes and Haki 2: Chaguo Ni LakoAfroes, a Southern Africa mobile gaming company, was founded by Anne Githuku-Shongwe to create uniquely African mobile content that is contextually relevant and easily accessible for youth audiences across the continent Unlike the previous games, which were created by large international organizations for broad and often dispersed audiences, Afroes' games are designed to be contextually relevant and easily accessible to a Southern African youth audience. But, to ensure that its content is accessible regardless of the class, economic status, or education level of its audience, it has been necessary for the local gaming company to engage with a global capitalist system and partner with some of the largest communication corporations in the world. The high growth rate of mobile phones in Africa gives games designed for a mobile platform the potential to reach a wide swath of youth audiences in Afroes' target markets of South Africa and Kenya; but the high costs charged by mobile network operators to host games makes them prohibitively expensive, thus limiting accessibility Although Afroes participates in an unabashedly neoliberal project in order to fund its content (see, for instance, Afroes' recent partnership with the Digital Jobs Africa initiative, a $100 million dollar Rockefeller Foundation project meant to facilitate the growth of the tech sector across the continent), the discourse offered up by the organization on how to affect social change and the game content it produces move far beyond a neoliberal framework. Indeed, Afroes' conceptualization of social change is one based on civic participation, institutional change, and a critique of current global processes. Rather than focusing on \"fixing a problem,\" the organization emphasizes \"shaping new mindsets amongst youth\" and \"[empowering] them as citizens to address their countries' challenges\" The CP games created by Afroes are reflective of this discourse and cover a much broader range of issues and solutions than those previously examined in this article. They include issues of gender violence and legal rights; corporate exploitation; democracy, voting, and civic voice; government and corporate corruption; child safety and abuse; environmental rights; and peace through social justice. Although unemployment and local poverty is one of the main issues the organization hopes to address, Afroes does not look to entrepreneurship alone as the solution; rather, the organization is focused on empowering youth as citizens who must address their countries' challenges at a range of social, political, and economic levels. In Haki 1: Shield and Defend, the first game in a series, players are given the opportunity to \"mend the wrongs that tear at the social fabric and in the process defend The Haki series was designed in partnership with the TUVUKE Peace Initiative, a \"countrywide collective platform\" launched after the violence that erupted during 2007-2008 to promote peaceful, free, and fair elections in Kenya The games produced by Afroes operate as CP games in that they are specifically designed as catalysts for Southern African youth to \"reimagine\" and reshape the world around them as they point to structural inequalities as a root cause of development issues. The discourse of the organization and the content of the games themselves take an approach to social change that moves beyond a neoliberal project of market participation as the solution to development issues, instead focusing on civic participation, institutional change, a critique of current global processes, personal and communal rights, and peace and social justice. That being said, calling on Kenyans to fight against issues like illegal logging, or to \"play for peace,\" is a more abstract development approach than presenting specific solutions individuals can take in their daily lives. CP games play an important role in critiquing asymmetrical power structures and institutional inequality that affect development issues and solutions, but CP games that propose specific alternatives alongside such a critique may have more viability in development practice. However, it is Afroes' mission of \"reimagining\" that has, thus far, set it apart. A small, local organization, it has produced contextually relevant content that goes beyond mainstream modernization narratives of narrow economic development and even points to the inequalities exacerbated by such a system. It will be important to pay attention to whether Afroes is able to maintain the critical approach it has taken within game content as the political economic relationships necessary for it to sustain game production continue to develop. And, more critically, it is worth asking whether these relationships force Afroes to be complicit in the continuation of the very transglobal power structures that feed the global inequalities responsible for the issues they are trying to address in the first place.Conclusion", "conclusions": "Regardless of the type of game developed (DD, DI, or CP), each of the organizations analyzed here discusses the potential of its game to \"empower\" players: For the WBI, empowerment is equivalent to the development of 21st-century skills, specifically for young people living in Africa; for the HTSM, women's empowerment is related to financial independence, which game players situated in the Global North can facilitate through in-game giving; and for Afroes, empowerment necessitates civic participation, the securing of individual rights, and the ability to imagine and shape new futures. DDGs, it is clear, are not a revolutionary tool that will necessarily promote a radically new approach to conceptualizing and enacting development; rather they reflect the ideology of the organization behind them. In the case of EVOKE and the HTSMG, that means reinforcing a dominant approach to development that emphasizes capitalist solutions and the broad marketization of all areas of life. Although the first iteration of EVOKE (for which the WBI spent $622,000 USD to develop and market) broadly missed its target audience, a second version is in the works and the many resources available to the WBI will, no doubt, allow the organization to continue to develop and distribute DDGs to enhance 21st-century skills and affect educational policy across the developing world. The Half the Sky Movement Media and Technology Engagement Initiative, a recent alliance among USAID, the Ford Foundation, Show of Force, and Games for Change, and a $1.4 million USAID investment in the HTSM's mobile games makes wider dissemination of their games an almost certainty. Such support will allow the HTSM to further distribute its WID-based, neoliberal agenda that prioritizes microcredit loans, individual entrepreneurship, and economic participation by women in a global marketplace.Games such as these take an uncritical stance in assuming that the processes of globalization and neoliberalism are not only beneficial for poor, marginalized people across the developing world but also are necessary for development to occur. By design, DDGs like EVOKE and the HTSMG promote a global capitalist system as a change agent for eradicating global poverty, while ignoring the role such a system plays in exacerbating inequality. But their contribution is not only discursive. In cases like the HTSMG, developed by the largest Canadian-owned game studio, the production of DDGs is directly tied to the growth of the global digital games sector, a highly concentrated media system in which the Western loci of production, technology, and skills reinforces established North/South knowledge/power divides.All that being said, DDGs still hold the potential to be radical players in the development process. The case of Afroes highlights how local organizations can work to produce relevant content for local audiences and offer up alternatives to established development narratives. Reflective of their mission to help others \"reimagine\" Africa, Afroes' games consider not only solutions to development issues but also their causes as well. Digital games have shown great possibility for use as educational tools and their dynamic nature could, theoretically, be used to incorporate a host of participatory feedback loops and user directed narratives that would disrupt current top-down development constraints. Further, open-world gaming platforms create the potential for audiences to take control of the game space, presenting opportunities for bringing players into the construction of development projects in games and beyond (see, for example, the UN Habitat's Block by Block project). And, of course, the rate of dissemination of mobile phones presents an undeniably broad channel for reaching people, allowing a level of accessibility unheard of with other ICT-based projects. But, as seen in the case of Afroes, trying to keep games accessible to a Southern African audience has meant engaging in a range of partnerships and business strategies that are, or could become, counterintuitive to what they hope to achieve with their games. While smaller organizations and independent developers might have the necessary tools to create DDGs, the cost to distribute them and ensure they reach their target audience is often prohibitive and might impact their sustainability in the industry.As the field continues to grow, the great potential of DDGs will necessitate more studies that ask how games are actually being used, by whom, and to what end. Further use of the framework presented here, including identifying overlapping categories, will help generate important micro and macrolevel analysis on the production and use of DDGs. By situating the analysis of a game's productive purpose and political economic foundations within the framework, broader trends in the DDG field will be highlighted. As with any technological innovation in the development field, potential must be understood within the context of actual use. In the case of DDGs, which operate both as texts that espouse a particular organization's ideology and as political economic products with implications for local and global development and gaming industries, actual use is varied and must be analyzed. It is likely, for instance, that some players of DD and DI games read them critically, while some players of CP games do not understand messages as intended. Hence, studies to tap how players and others understand and use DDGs are encouraged.", "SDG": [5]}, "a_review_on_experimental_design_for_pollutants_removal_in_water_treatment_with_the_aid_of_artificial_intelligence": {"name": "A review on experimental design for pollutants removal in water treatment with the aid of artificial intelligence", "abstract": " Fundamentals, advantages and limitations were discussed for ANNs, GA and PSO. Studies were summarized on modeling of removal processes using ANNs, GA and PSO. Predicting performances for removal processes were compared between different ANNs. Developments of AI tools were described for the optimization of removal processes.", "keywords": "Water,treatment,Environmental,pollutants,Experimental,design,Artificial,intelligence,Artificial,neural,networks,Genetic,algorithm", "introduction": "Clean water is not easily available to all since increasing human activities and industrialization have led to the release of contaminated effluents into freshwater systems (Huang et al., 2009c;. These effluents cause the water contamination due to inorganic and organic pollutants, e.g., nutrients, heavy metals and persistent organic pollutants (POPs) Dubey et al., 2015)(Cheung et al., 2003;Hoh and Hites, 2005;Luo et al., 2013;. Problems with water pollution have received a significantly more attention than ever before from both the public and environmentalists Li et al., 2016b)(Ma et al., 2011;. The pollutants removal in water treatment has been extensively studied with different methods, such as adsorption, solvent extraction, reduction, flocculation, coagulation, chemical or biological oxidation and membrane filtration Liu et al., 2014)(Kumar et al., 2007. The modeling and optimization are important procedures in pollutants removal processes to increase their efficiency without increasing the costs (Kumar et al., , 2008)). The conventional methods for the modeling and optimization of pollutants removal processes require the determination of a dependent variable for every combination of independent variables, just varying only one at a time while keeping all others as constants in batch studies (Shojaeimehr et al., 2013). Such methods obviously require a broad range of experiments to be performed, which would be expensive and time consuming (Singh et al., 2010). Moreover, these methods would not be able to reveal the influence of the interactions between the independent variables (Tak et al., 2015). The complex mechanisms (such as adsorption, reduction and oxidation) for water treatment may also hinder the modeling by using traditional methods (Sahu et al., 2009).(Nandi et al., 2010)Artificial intelligence (AI) has recently gained a tremendous advance in various applications e.g., autonomous driving, big data, pattern recognition, intelligent search, image understanding, automatic programming, robotics and human-computer game, thus it will greatly impact the human society. AI tools mainly encompass artificial neural networks (ANNs), genetic algorithm (GA), support vector machine (SVM), random forest (RF), boosted regression tree (BRT), simulated annealing (SA), Monte Carlo simulation (MCS), particle swarm optimization (PSO), immune algorithm (IA), ant colony algorithm (ACA), imperialist competitive algorithm (ICA) and decision tree (DT). AI tools have also been combined with experimental design (e.g., response surface methodology and uniform design) in order to improve the precision of optimal solution prediction. Both ANN and RSM models take full advantage of the experimental data due to their connatural capability to extract information from operation parameters, which can describe the complex relationships in environmental engineering without understanding the in-depth mechanisms of removal processes (Bagheri et al., 2015;. With the deepening of sustainable development concept, this technique has been used for pollutants removal because of their obvious advantages, such as reduction in reagent consumption and experimental work Fan et al., 2016). However, the applications of AI tools have still been limited in environmental protection as a whole due to their focus in water treatment (Acharya et al., 2006)(Mandal et al., 2015;Mendozacastillo et al., 2015;.Reynelavila et al., 2015)The objective of this review is to critically discuss the fundamentals and advantages of AI tools (e.g., ANNs, GA and PSO) as well as combined approaches (e.g., orthogonal design, response surface methodology and uniform design). This article also summarizes upto-date studies concerning the modeling and optimization of the pollutants removal processes in water treatment by using multilayer perception network, fuzzy neural network, radial basis function network, self-organizing map network. Furthermore, this study demonstrates that hybrid models of ANN coupled with GA or PSO (ANN-GA or ANN-PSO) can be successfully applied in water treatment with satisfactory accuracies. Finally, we briefly describe the limitations of current AI tools and their new developments (e.g., deep learning ANNs, SVM, SA and MCS) for the modeling and optimization of pollutants removal processes.", "body": "Clean water is not easily available to all since increasing human activities and industrialization have led to the release of contaminated effluents into freshwater systems Artificial intelligence (AI) has recently gained a tremendous advance in various applications e.g., autonomous driving, big data, pattern recognition, intelligent search, image understanding, automatic programming, robotics and human-computer game, thus it will greatly impact the human society. AI tools mainly encompass artificial neural networks (ANNs), genetic algorithm (GA), support vector machine (SVM), random forest (RF), boosted regression tree (BRT), simulated annealing (SA), Monte Carlo simulation (MCS), particle swarm optimization (PSO), immune algorithm (IA), ant colony algorithm (ACA), imperialist competitive algorithm (ICA) and decision tree (DT). AI tools have also been combined with experimental design (e.g., response surface methodology and uniform design) in order to improve the precision of optimal solution prediction. Both ANN and RSM models take full advantage of the experimental data due to their connatural capability to extract information from operation parameters, which can describe the complex relationships in environmental engineering without understanding the in-depth mechanisms of removal processes The objective of this review is to critically discuss the fundamentals and advantages of AI tools (e.g., ANNs, GA and PSO) as well as combined approaches (e.g., orthogonal design, response surface methodology and uniform design). This article also summarizes upto-date studies concerning the modeling and optimization of the pollutants removal processes in water treatment by using multilayer perception network, fuzzy neural network, radial basis function network, self-organizing map network. Furthermore, this study demonstrates that hybrid models of ANN coupled with GA or PSO (ANN-GA or ANN-PSO) can be successfully applied in water treatment with satisfactory accuracies. Finally, we briefly describe the limitations of current AI tools and their new developments (e.g., deep learning ANNs, SVM, SA and MCS) for the modeling and optimization of pollutants removal processes.Fundamentals, advantages and limitations of experimental designs and AI toolsArtificial neural networksANNs as one of the major AI approaches were derived from biological neurons Simulating water treatment processes and establishing the models can be applied with different types of ANNs, including multilayer perception (MLP) network, radial basis function (RBF) network, fuzzy neural network (FNN), self-organizing map (SOM) network, recurrent neural network (RNN), chaotic neural network (CNN), convolutional neural network and deep belief network (DBN) The category of artificial neural networksMLP-ANN is composed of an input layer, an output layer and one or more hidden layers. The MLP-ANN and RBF-ANN are the feedforward ANNs in which the neurons of each layer are interconnected from each preceding layer to the following layer, without lateral or feedback connections. The structure of RBF-ANN is similar to that of MLP-ANN, while the activation functions of RBF-ANN are different from those of MLP-ANN (Fig. Unlike the feed-forward neural networks, the RNN can handle one example at a time, retaining a state and memory since other networks training has long been difficult with a number of parameters (Fig. The CNN and ESN models can be used for the prediction of pollutants removal in water treatment. Back-propagation (BP) artificial neural networkIt has been well recognized that a neural network with one hidden layer is capable of approximating any finite nonlinear function with high accuracy, whereas an over-three-hidden-layer system is known to cause an unnecessary computational overload where b 1 is the bias of hidden layer, a i is the ith output from the input layer, w ij is the weight connecting neuron i in the input layer to neuron j in the hidden layer, d is the number of input variables, k is the transfer function, and c j is the jth output in the hidden layer.The output from the neuron in the output layer is the transformation of the weighted sum of output in the hidden layer, which is given as follows:where b 2 is the bias of output layer, w jk is the weight connecting neuron j in the hidden layer to the neuron k in the output layer, m is the number of neuron in output layer, k 0 is the transfer function, and e k is the kth output from the output layer.Training and testing of artificial neural networksThere are various transfer functions used in the ANN model, such as hard linear (hardlim), symmetrical hard linear (hardlim), linear (purelin), saturately linear (satlin), log-sigmoid (logsig) and hyperbolic tangent sigmoid (tansig). The transfer function commonly used is the tansig function and purelin function. The tansig transfer function can be used in input layer and hidden layer, while a purelin transfer function can be used in the output layer. The tansig transfer function can be expressed as:The purelin transfer function is defined as:The primary goal of training is to minimize the error function by searching for a set of connection weights and biases that cause the ANNs to produce the outputs equal to (or close to) targets (Fig. Determining the number of neurons in the hidden layer is an important task when an ANN is designed To ensure that all variables are important in the input data, principal component analysis can be performed as an effective procedure for the determination of input parameters. The input data used for the ANNs are generated from the experimental design, such as OD and RSM (the related description can be found in supplementary material). It is thus ensured that the data are uniformly distributed in the domain with high representativeness. The available data are generally divided into training, validation and testing subsets to develop an ANN model. The training set is used to estimate the weights and biases to fit the input-output relation according to its error. The validation set is used to decide when to stop training in order to avoid over-fitting and/or which network structure is optimal, whereas the test set is used to assess the generalization ability of the trained model. The data of input and output for an efficient ANN are normalized between 0.1 and 0.9 to avoid a numerical overflow due to large or small weights. The normalization equation used is as follows:where y is the normalized value of xi, and xmax and xmin are the maximum and minimum values of xi, respectively. The Garson's equation partitions the hidden and output connection weights, which can be utilized to evaluate the relative importance of each independent variable in the ANN models. This equation is given as follows:where R stands for the percentage of influence for the input neurons, w is the connection weight between the input, hidden and output neurons, B represents the number of neurons in input layer, A is the number of neurons in hidden layer and n is the number of output neurons.The experimental value is compared with the predicted value by calculating mean square error (MSE). If the MSE is higher than a suggested value, it is propagated back from the output layer to the input layer, and the connection weights are modified accordingly until the number of iterations is achieved or the MSE is obtained under the suggested value. To determine the performance of the ANN model, the MSE and the coefficient of determination (R2) can be used. The MSE represents the errors associated with the model and should be calculated with the following equation:where N is the total number of model output, Yp is the predicted output, and Ye is the experimental value. And R2 represents the degree of correlation between the experimental and predicted values, which can be computed with the following equation:where\u203eYe is the average of the experimental values.ANNs have been applied to model the removal processes in environmental engineering, including adsorption Different heuristic algorithmsIt is well known that a plain gradient descent algorithm can become trapped at shallow local optima Genetic algorithmsGA is a numerical search tool operating according to procedures that resemble the principles of natural selection and genetics Particle swarm optimization (PSO)PSO as a well-known heuristic approach is inspired by the behavior of a bird flock. It is an evolutionary algorithm proposed by Kennedy and Eberhart, which can avoid trapping in a local minimum because it is not based on gradient descent algorithm Other heuristic algorithmsACA is superior to the above mentioned algorithms because of its strong robustness, however it needs a long search time, has a slow convergence rate and is easy to fall into a local optimum. ICA, unlike GA, PSO and ACA, was inspired from social behavior simulating the colonial assimilation mechanism and competition mechanism of the empire. The convergence rate and accuracy of ICA is higher than those of PSO and GA. Besides, SA is based on the ideas from statistical mechanics that is a probabilistic optimization technique with the potential of finding a global optimal solution for practical large-scale problems. SA method differs from the traditional descent algorithms in that this search algorithm for a neighborhood solution search allows not only the downhill moves but also the uphill moves.Uncertainty in input parameters is related to the accuracy and representativeness of the input parameters used for predictions. MCS is a statistical sampling technique in obtaining a probabilistic approximation to the solution of a mathematical model. The uncertainty in model predictions due to uncertainties in input parameters is a function of the magnitudes and shapes of the probability density functions (PDFs) of the uncertainties in individual inputs Comparison of ANN-GA and ANN-PSO with RSMThe ANN-GA and ANN-PSO models with the higher R 2 value and the smaller average error offered more accurate predictions than the RSM models for the pollutants removal as demonstrated in Table Although ANN is superior to RSM, these models complement each other in interpreting the experimental removal efficiency. ANN is more reliable in capturing the nonlinear relationship between the removal efficiency and process variables, while RSM notes the statistical importance of the individual process variables and their interactions via ANOVA. However, the main limitation of RSM is that it assumes only quadratic non-linear correlation. Therefore, RSM can be effectively used when the search window is appropriately narrowed down, which makes the search process highly dependent upon the searched space. It will require either extra experiments or good priori knowledge of the system to fix the search window. ANN can easily overcome the above discussed limitation of RSM because it can inherently capture almost any form of non-linearity. Thus, the liberal search space can be chosen by using ANN although the correlation in the search space is more complex than the quadratic.Applications of AI tools in the modeling and optimization of pollutant removal processesA classification tree of AI techniques for pollutants removal in water treatment is presented in Fig. In Table Datasets used for artificial intelligence approaches in water treatmentIn order to evaluate the performance of artificial intelligence approaches, it is required that the related datasets define their level of accuracy. Nutrients removalA number of researchers have studied the BP-ANN model and recommended it for the modeling and optimization of the water treatment For BP-ANN, the individual relations between the input and output variables cannot be developed by engineering judgment so that the model tends to be a black box, while FNN is transparent and hence its if-then rules are ready to understand and interpret. A FNN model and a BP-ANN model were proposed by The RBF networks have also been successfully applied for solving the dynamic system problems, because they can predict the behavior directly from the input and output data. Bagheri et al. The SOM model is based on an unsupervised neural network algorithm, which has been used to analyze, cluster and model various types of large databases Several studies took advantage of the GA coupled with ANN to generate optimum operational variables for the studied process because GA does not easily get trapped in a local minimum. Zhang and Pan (2014) explored the potential of ANN and RSM for modeling phosphate removal from water by the nanocomposite absorbent HFO-201. In combination with GA, the optimized ANN model exhibited an acceptable predictive strength for the experimental runs, indicating its potential for predicting the behavior of complex water treatment processes. Moreover, ANOVA tests and sensitivity analysis were employed to find the relative importance of independent variables. The order for these variables influencing the removal process was: sulfate concentration > adsorbent dosage > pH > temperature.Heavy metals removalIt is apparent that the numerous applications of BP-ANN have been successfully applied in the removal of heavy metals because of their reliable, robust and salient characteristics for establishing the non-linear relationships between variables in water treatment   It can be concluded that though RSM is the widely used approach  POPs removalThe applications of BP-ANN to simulate wastewater treatment processes, especially POPs removal, have recently gained an increasing attention COD removalThe used datasets were obtained from the previous study The used data came from five pilot-scale horizontal subsurface flow constructed wetlands for two years 812 samples A comparative modeling study has been carried out by Other pollutants removalOther neural networks (e.g., RBF-ANN, FNN and SOM) have also been used for other pollutants removal in wastewater treatment. Comparison between the performances of single methods and hybrid methodsIn order to get an overall comprehension, the obtained values of MSE and R 2 for single methods and hybrid methods are compared in Figs. Conclusions and prospects", "conclusions": "AI techniques (e.g., ANNs, GA and PSO) are presently considered as attractive alternative tools in the environmental engineering due to their specific features, such as non-linearity, adaptivity and generalization . ANNs are good AI tools for optimizing the removal processes because of their following advantages: (i) in comparison to other approaches, ANNs can model a multivariable system to extract complex nonlinear relationships between the variables by means of the data training; (ii) ANNs can overcome the limitations of conventional mathematical models by extracting the required information using the training data, which have not required a prior specification of suitable fitting functions. The disadvantages of ANNs are as follows: (i) ANNs have a poor reproducibility because the weight and bias between neurons are given randomly; (ii) ANNs are easy to fall into a local optimal solution, thus they cannot provide an optimal condition for the engineering application in water treatment. Therefore, hybridization of ANNs with other AI tools (such as SA, MCS, PSO, IA, ACA, ICA and DT) have a potential to be used for generating optimal operational variables for the removal processes due to the ability to search for the global optima.(Mohebbi et al., 2008)ANN-GA and ANN-PSO models have recently been developed for modeling and optimization of the pollutant removal processes. The training data for ANN are usually selected using either a random sampling method or an experimental design (e.g., RSM and OD). However, the random sampling method does not ensure that the training data uniformly cover the domain, especially when the number of input variables is large while training dataset is relatively small. The main drawback of RSM is that a relatively large input dataset is required to build a sufficiently accurate response surface. Therefore, uniform design (UD) in combination with ANNs can be used for predicting the efficiency of the pollutants removal processes in laboratories especially when the experiment has many variables and multiple levels or the experimental cost is highly expensive. The reason for this is that the experimental trials for UD are slightly fewer than those for OD and significantly fewer than those for RSM. Finally, if we need to model the removal processes of operational plants with real-time sensors, we can utilize ANNs to treat useful data available directly from the actual operation of the plants rather than from experimental design.Deep learning models (such as convolutional neural network and DBN) can be used to learn large volumes of data because their learning capacities can be controlled by varying their depth and breadth. Thus, convolutional neural network and DBN have a potential use for the modeling and optimization of the pollutants removal processes that have large and complex datasets (i.e. big data).", "SDG": [6]}, "diversity_and_distributions_2019_domisch_spatially_explicit_species_distribution_models_a_missed_opportunity_in": {"name": "Spatially explicit species distribution models: A missed opportunity in conservation planning?", "abstract": " This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.", "keywords": "Bayesian hierarchical modelling,connectivity,gurobi,integer linear programming,spatial autocorrelation,spatial unit", "introduction": "", "body": "This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.| INTRODUC TI ONIn the light of the ongoing decline in global biodiversity In the past, conservation goals mainly reflected habitat or species representation. Only recently the focus has shifted towards considering environmental and ecological processes, which are essential for securing species persistence (e.g., When accounting for spatial connectivity, conservation plans inherently build the protected areas based on the spatial dependencies in the planning units as well. For example, it is vital to account for the spatial structure of the environment around a given planning unit for assessing its importance as part of a protected area The most widely used conservation feature in conservation planning is the geographical distribution of multiple species, as species distributions are often best known compared to other biodiversity facets (such as functional or phylogenetic characteristics of species; Currently, species distribution models (SDMs) are the main tools used to produce such range-wide species distribution data Species distribution models are considered useful in conservation planning when used within a \"structured and transparent decision-making process\" Species distribution models require rigorous testing for methodological issues and statistical shortcomings In contrast, spatially explicit SDMs (\"spatial SDMs\") account for the proximity and mobility (i.e., connectivity) in species populations. Hence, they provide more powerful inference about species distributions and niche relations Species distribution models can be made spatially explicit in numerous ways (see Spatial aspects are included in the SDM given the spatial autocorrelation of species distributions Here, we analyse whether accounting for spatial dependencies in both steps, the spatial prioritization process and in the underlying conservation features (here: species), influences the arrangement of potential protected areas. Using species survey data, we build nonspatial and spatial SDMs and compare the resulting mapped spatial conservation plans across a range of conservation targets, that is, proportions of potential species distribution areas. We hypothesize that (a) spatial SDMs would outperform non-spatial SDMs in terms of model evaluation scores, since spatial SDMs account for the influence of proximity in species populations; (b) this effect cascades through to the spatial conservation plans, and that those derived from spatial SDMs would differ significantly from the ones based on non-spatial SDMs. We test these hypotheses in three case studies covering terrestrial, marine and the freshwater realms. We do so, because protected areas in each realm are typically planned using F I G U R E 1 Spatial arrangement of potential protected areas derived from non-spatial and spatial SDMs, as well as their overlap in the terrestrial (a), marine (b) and freshwater realms (c), each using a specific planning unit shape (grids, hexagons, subcatchments). Spatial plans were defined for a 20% conservation target within a 10% gap to optimality. The inset shows the location of the study areas in Australia, US East Coast and the Danube river basin. See Supporting Information Figures | ME THODS| General workflow of the analysisOur aim is to test whether, and if so, how the addition of spatial in- | Study areas and species dataThe terrestrial case study (Figure Eucalyptus species in Eastern Australia derived from The marine case study (Figure Environmental data on ocean topography, currents, nutrients and light were obtained from the Bio-Oracle (approx. 9 km spatial grain at the Equator, All environmental data were harmonized to 7,123 hexagonal grids of approx. 19 km 2 , and the species data were summarized for each hexagonal grid cell (created using the R-package sP; The freshwater case study (Figure subcatchments with an average size of 108 km 2 . We then extracted climatic Species data, including species detections, non-detections and sampling dates, were aggregated to the planning units. Within each planning unit, environmental data were aggregated using various techniques (e.g., average, standard deviation; see Supporting Information Table From a large set of layers per case study, we selected those that are meaningful from an ecological perspective to describe the distri- for all predictors used). All (continuous) predictors were centred (so all predictors have a mean of 0) and scaled by dividing by their standard deviations. The final number of predictors was 13, 11 and 8 for SDMs in the terrestrial, marine and freshwater realms, respectively.| Defining spatial connectivityRegarding spatial SDMs, the connectivity among planning units for the terrestrial and marine realms was computed from a polygon shapefile using the R-package sPdeP (using the queen's move; In the freshwater realm, it is crucial to account for the longitudinal connectivity among planning units | Detecting spatial autocorrelationWe first assessed the degree of spatial autocorrelation in species occurrences among planning units (where occurrences were aggregated within planning units). Here, we used the Monte Carlo simulation method and calculated Moran's I and the associated p-value for each species (at \u03b1 = 0.05), using the R-package sPdeP (with 999 permutations). Spatial weights were kept identical to the connectivity definition of planning unit above, specifying proportional weights to the connected planning unit (row standardization).| Species distribution modellingWe ran Bayesian occupancy models for predicting the habitat suitability of species in each realm. Our aim was to minimize the methodological shortcomings in SDMs that could be ported to the spatial prioritization.Hence, we refrained from artificial pseudo-absences to draw species non-detections, but used survey data with repeated visits to account for the detection probability of species at given sites. Specifically, we ran zero-inflated binomial SDMs in a Bayesian framework using the hsdm R-package These functions integrate two processes, a Bernoulli suitability and a Binomial observability process, into a hierarchical zero-inflated binomial model. The Bernoulli suitability process uses species point records and environmental predictors as the response and explanatory variables, respectively where z i is a random variable describing the binary habitat suitability at planning unit i , which follows a Bernoulli distribution of parameter \u03b8 i ; that is, the probability that the habitat is suitable in the planning unit i . \u03b8 i is expressed as a linear model combining the matrix of environmental predictors X i and parameters \u03b2 using a logit link function.In the spatial models, Equation 2 is extended by an intrinsic conditional autoregressive model (iCAR) in the suitability process:where \u03c1 i is the spatial random effect of the planning unit i . \u03c1 i accounts for the spatial autocorrelation of the presence probabilities variability in suitability that is not explained by the environmental variables:where \u03bc i is the mean of \u03c1 i in the neighbourhood of i, V \u03c1 is the variance of the spatial random effects, and n i is the number of neighbours for the planning unit i (see also Regarding the observability process, we counted the number of repeat surveys within each planning unit; that is, we aggregated the surveys over time, yielding the information on how often each planning unit was visited. The model then estimates the probability of observation given the species presence in a planning unit, where we assume that, if the species was observed at least once during multiple visits in a given planning unit, the habitat is suitable and the absence of the species during other visits in the planning unit is due to imperfect detection. For instance, a given species was observed once in planning units A and B. Planning unit A was visited once, yielding a detection probability of 1, whereas B was visited 10 times (yielding a detection probability of 1/10 = 0.1).In combination with the suitability process (i.e., environmental conditions) and the spatial autocorrelation (i.e., the spatial neighbourhood information), the observability information yields more robust estimates in unsampled locations, that is, whether a species actually occurs there but was not detected, or if it is actually not present (and hence not detected). The probability of observing the species (\u03b4 \u03c1 ) was specified as:where y i is a vector of the total number of observed presences in planning unit i . z i is the binary habitat suitability in planning unit i from the suitability process (Equation (1)We ran three Markov chain Monte Carlo (MCMC) simulations with 200,000 iterations each, a burn-in phase of 50,000 iterations and a thinning interval of 10. Model convergence was assessed by the multivariate potential scale reduction factor (MPSRF; | Spatial prioritizationWe transformed the predictive posterior mean probability maps from SDMs into a semi-binary scheme using TSS as a threshold To create the boundary files (describing the spatial connectivity) for the terrestrial and marine case studies, we used the QMarxan toolbox in QGIS (QGIS-Development-Team, 2017). For the freshwater realm, we applied an inverse-distance connectivity penalty, where subcatchments located closer to a given focal planning unit would get a higher penalty if not chosen as part of the protected area (opposed to those planning units located more distant, as proposed by We first calibrated the boundary length modifier (BLM) for our analyses. The BLM is dimensionless and balances the spatial aggregation and patchiness of spatial plans. For the BLM calibration, we used the software marxan v.2.43v Again, we ran sensitivity runs within increments of 10 and identified the BLM to 35, 20, 0.15 for the terrestrial, marine and freshwater realms, respectively (Supporting Information Figure We then used the gurobi oPtimizer 7.5 software (Gurobi Optimization, 2017) to find optimal conservation planning solutions based on integer linear programming (ILP) within the R-package Prioritizr For each run, we allowed a 10% gap to optimality in the spatial plans (as trade-off between optimality, and the time the optimizer takes to converge). To rule out the effect of the planning unit itself (cost, i.e., area of the planning unit), we set a constant value of 1 as the cost of each planning unit. Likewise, we set no weights for single species. This means that only the conservation features, that is, the probabilistic information on species habitat suitability and the connectivity penalty (set by the BLM), were decisive to the objective function that was set to minimize the total number of planning units to be part of the selected spatial plan | Statistical analysesFor each realm and model type, we compared model accuracy given the model evaluation scores, the estimated detection probability, the range size estimates of model outputs considering all planning units that had a probability value above the threshold (equal to binary predictions), and the summed probability of habitat suitability across all species within each planning unit as a proxy for species richness | RE SULTS| Spatial autocorrelationAcross the terrestrial, marine and freshwater realms, all but three of the 171 modelled species showed a significantly positive spatial autocorrelation (\u03b1 > 0.05 for the freshwater fish species White-eye bream [Ballerus sapa], sunbleak [Leucaspius delineates] and brown trout [Salmo trutta lacustris], see Supporting Information Table | Model performanceThe SDM performance indicators AUC, TSS, sensitivity and specificity were on average consistently higher for the spatial SDMs than for the counter-parts (Figure | Range size estimatesSpatial SDMs yielded generally more compact and less dispersed habitat suitability estimates (see exemplary maps in Supporting Information Figure 1,057 \u00b1 690 planning units, paired t-test: t = 2.95, df = 32, p = 0.006; freshwater: 2,874 \u00b1 1,491 and 2,532 \u00b1 1,409 planning units, mean \u00b1 SD, paired t-test: t = 2.0493, df = 84, p = 0.044). In the marine realm, no significant difference in range size estimates was observed (non-spatial vs. spatial SDMs: 2,698 \u00b1 1,564 and 2,628 \u00b1 1,500 planning units, paired t-test: t = 0.783, df = 52, p = 0.436).F I G U R E 2 Model evaluation scores representing AUC, TSS, sensitivity, specificity and DIC, as well as the estimated detection probability, summarized across 33 terrestrial (a-f), 53 marine (g-l) and 85 freshwater species (m-r) derived from non-spatial (blue) and spatial SDMs (green). Bars represent median values and boxes the 1st and 3rd quartiles, respectively| Summed probabilities of habitat suitabilityAcross realms, spatial SDMs produced on average a lower summed habitat suitability across planning units (i.e., a proxy for species richness), than predicted by non-spatial SDMs (non-spatial vs. spatial SDMs in the terrestrial realm: 3.00 \u00b1 1.28 vs. 2.37 \u00b1 1.00 planning units, paired t-test: t = 61.845, df = 7,762, p < 0.001; marine:12.27 \u00b1 3.01 vs. 11.79 \u00b1 2.97 planning units, mean \u00b1 SD, paired t-test: t = 28.458, df = 7,122, p < 0.001; freshwater: 21.36 \u00b1 11.91 vs. 16.50 \u00b1 9.48 planning units, paired t-test: t = 46.057, df = 7,375, p < 0.001). See Supporting Information Figure | Spatial similarity of spatial plansAll species targets across SDM types and spatial conservation plans were met in all spatial prioritization runs (data shown in the Pangaea repository). The degree of spatial overlap of potential protected areas derived from spatial and non-spatial SDMs was targetdependent. The overlap was lowest for the smallest conservation target (10%) with the per cent overlap ranging from 1% to 2%, and increased up to a maximum of 30% to 39% for a conservation target of 50% (Figure The number of required planning units increased linearly with increasing conservation targets: the higher the conservation target, the higher the required amount of planning units to meet the given target (Figure The relative difference in the number of planning units needed for a given solution between spatial and non-spatial SDMs was within a margin of 5% (Figure | D ISCUSS I ONIncorporating connectivity has been successfully adopted in systematic conservation planning for building species migration and movement corridors We  We acknowledge that a 10% gap to optimality, as we used it here, can introduce noise in the spatial arrangement of potential protected areas (e.g., Figure ", "conclusions": "", "SDG": [6]}, "exploring_the_high_resolution_mapping_of_gender_disaggregated_development_indicators": {"name": "Exploring the high-resolution mapping of gender-disaggregated development indicators", "abstract": " Improved understanding of geographical variation and inequity in health status, wealth and access to resources within countries is increasingly being recognized as central to meeting development goals. Development and health indicators assessed at national or subnational scale can often conceal important inequities, with the rural poor often least well represented. The ability to target limited resources is fundamental, especially in an international context where funding for health and development comes under pressure. This has recently prompted the exploration of the potential of spatial interpolation methods based on geolocated clusters from national household survey data for the high-resolution mapping of features such as population age structures, vaccination coverage and access to sanitation. It remains unclear, however, how predictable these different factors are across different settings, variables and between demographic groups. Here we test the accuracy of spatial interpolation methods in producing gender-disaggregated high-resolution maps of the rates of literacy, stunting and the use of modern contraceptive methods from a combination of geolocated demographic and health surveys cluster data and geospatial covariates. Bayesian geostatistical and machine learning modelling methods were tested across four low-income countries and varying gridded environmental and socio-economic covariate datasets to build 1\u00c21 km spatial resolution maps with uncertainty estimates. Results show the potential of the approach in producing high-resolution maps of key gender-disaggregated socio-economic indicators, with explained variance through cross-validation being as high as 74-75% for female literacy in Nigeria and Kenya, and in the 50-70% range for many other variables. However, substantial variations by both country and variable were seen, with many variables showing poor mapping accuracies in the range of 2-30% explained variance using both geostatistical and machine learning approaches. The analyses offer a robust basis for the construction of timely maps with levels of detail that support geographically stratified decision-making and the monitoring of progress towards development goals. However, the great variability in results between countries and variables highlights the challenges in applying these interpolation methods universally across multiple countries, and the importance of validation and quantifying uncertainty if this is undertaken.", "keywords": "Life Sciences -Mathematics interface geo-statistics,development indicators,mapping,geographic information system", "introduction": "The UN sustainable development goals (SDGs), an intergovernmental set of 17 aspirational goals and 169 targets to be achieved by 2030 , were launched in 2015. These include ending poverty and malnutrition, improving health and education, and building resilience to natural disasters and climate change. A particular focus across the goals and targets is achievement 'everywhere', ensuring that no one gets left behind and that progress is monitored at subnational levels to avoid national-level statistics masking local heterogeneities. This requires consistent, comparable evaluation and monitoring of key SDG indicators at high levels of subnational detail across the 2015-2030 period of the goals.[1]The increasing focus on subnational assessments for the SDGs, as well as for efficient targeting of resources, and the improvement in accuracy for health and development metrics has prompted an emphasis on subnational data collection and the continued development of mapping approaches. Principal among these approaches is small area estimation [2][3] whereby survey data on the variable of interest mapped at coarse spatial scales are integrated with census data at fine spatial scales to infer fine resolution mapping of key development metrics. This approach has seen widest application in the field of poverty mapping [4][5,, but it is limited due to its reliance on census data. With national population censuses undertaken typically only every 10 years, and sometimes longer in many low-income countries 6], this makes the application of such small area estimation approaches challenging for the ongoing monitoring of SDG indicators.[7]National household surveys are undertaken in low-and middle-income countries more regularly than censuses, typically every 3-5 years, with the Demographic and Health Surveys (DHS) (http://dhsprogram.com), Living Standard Measurement Surveys (LSMS) [8, and Multiple Indicator Cluster Surveys (MICS) (http://mics.unicef.org) being the largest international programmes. These are vital for providing SDG, policy and operational relevant metrics; however, the data are typically only summarized at national or large subnational areas, which can be inappropriate for identifying the significant heterogeneities that need to be captured for the 'leave no one behind' agenda. The increasing use of global positioning systems (GPS) for recording the locations of survey clusters and the increasing availability of these data provide more fine-grained information 9]. However, these cluster-level data are drawn from small sample sizes and only represent samples of small areas. Spatial interpolation approaches that exploit spatial relationships between cluster-located survey data and geospatial covariates have, therefore, been explored recently [10][11,, with applications seen in mapping age structures 12], malaria prevalence [13], vaccination coverage [14] and poverty [15] among others. Moreover, the DHS programme is now routinely providing modelled surfaces with each new country survey produced through spatial interpolation (http://spatialdata.dhspro gram.com/modeled-surfaces/) [16].[17]While spatial interpolation approaches are growing in popularity, due to advantages over small area estimation in their ability to produce high spatial resolution maps in the absence of census data, it remains unclear, however, how predictable different SDG-related variables are across different settings and between demographic groups. Here we test the accuracy of several spatial interpolation methods through quantification of uncertainty and model fit by combining DHS cluster data and geospatial covariates through a set of case studies, producing gender-disaggregated maps of literacy rates, stunting and the use of modern contraceptive methods. Bayesian geostatistical (BGS) and machine learning modelling methods were implemented across four low-income countries and gridded environmental and socio-economic covariate datasets to predict 1 \u00c2 1 km spatial resolution maps with uncertainty estimates, and tested through validation.", "body": "The UN sustainable development goals (SDGs), an intergovernmental set of 17 aspirational goals and 169 targets to be achieved by 2030 The increasing focus on subnational assessments for the SDGs, as well as for efficient targeting of resources, and the improvement in accuracy for health and development metrics has prompted an emphasis on subnational data collection and the continued development of mapping approaches. Principal among these approaches is small area estimation National household surveys are undertaken in low-and middle-income countries more regularly than censuses, typically every 3-5 years, with the Demographic and Health Surveys (DHS) (http://dhsprogram.com), Living Standard Measurement Surveys (LSMS) While spatial interpolation approaches are growing in popularity, due to advantages over small area estimation in their ability to produce high spatial resolution maps in the absence of census data, it remains unclear, however, how predictable different SDG-related variables are across different settings and between demographic groups. Here we test the accuracy of several spatial interpolation methods through quantification of uncertainty and model fit by combining DHS cluster data and geospatial covariates through a set of case studies, producing gender-disaggregated maps of literacy rates, stunting and the use of modern contraceptive methods. Bayesian geostatistical (BGS) and machine learning modelling methods were implemented across four low-income countries and gridded environmental and socio-economic covariate datasets to predict 1 \u00c2 1 km spatial resolution maps with uncertainty estimates, and tested through validation.Material and methodsThe study focused on three countries in sub-Saharan Africa (Nigeria, Kenya and Tanzania) and one country in south Asia (Bangladesh). Development indicators that underlie key SDGs, and for which significant gender differences often exist, were chosen for testing: literacy, stunting and the use of modern contraceptive methods.Empirical estimates of literacy levels at national level among women of childbearing age are low in all four countries In Nigeria, Kenya and Bangladesh a relatively high proportion of children under the age of five (35 -41%) are stunted. Even in Kenya where the rate of illiteracy is relatively low, the numbers of malnourished children remain high (35% of children under five are stunted with a 14% of severely stunted) In Nigeria, only 11% of the women aged 15 -49 reported using a modern method of contraception Geolocated household surveysThe gender-disaggregated indicators investigated in these analyses were collected through the DHS programme, which collects and analyses data on populations through more than 300 surveys in over 90 countries. DHS household surveys adopt a multistage cluster sampling design The boundaries of the EAs are defined by the country's census bureau, as are the urban and rural status of each cluster. The georeferenced datasets can be linked to individual and household records in DHS household surveys through unique cluster identifiers. To protect the confidentiality of respondents, cluster locations are displaced up to 5 km in rural areas and up to 2 km in urban areas at the processing stage. A further 1% of the rural clusters can be displaced up to 10 km Gender-disaggregated maps of stunting, literacy and use of modern contraceptive methods were constructed, with a subset of these indicators analysed in each country (with the exception of Nigeria, where all indicators were modelled). These indicators are clearly defined by the DHS programme and were constructed following their instructions contained in individual country final reports Stunting in childrenIndicators of nutritional status in children from DHS surveys are calculated using growth standards published by WHO A measure of stunting for children under the age of 5 who slept in the household the night before the survey was extracted for the countries of interest using height-for-age Z-scores. The cluster-level proportions of stunted children disaggregated by gender were used in the analyses.LiteracyIn the DHS surveys, literacy status is determined by assessing the respondent's ability to read a sentence during the interview, when surveyors ask respondents to read sentences written in their native language or English. Those who attended at least secondary school or were able to read at least part of a sentence were defined as literate. In Bangladesh, the female literacy rate regards only ever married women. Cluster-level proportions of literate people aged 15 -49 were used, disaggregated by gender.Use of modern contraception methodsWithin the DHS surveys, all women aged between 15 and 49 years old were asked about their use of family planning at the time of the survey. Information about the current use of any modern method of contraception (defined in the DHS as being, e.g. pill, male condom and sterilization) was reported for each women interviewed. In Bangladesh, this information was only collected from ever married women. Cluster-level proportions of women using a modern method of contraception were derived and used in these analyses.Defining a suite of covariates for predicting health and demographic indicators at fine spatial resolutionMany indices of population health and well-being are correlated with variables describing the surrounding environmental, geographical, socio-economic and infrastructure conditions. Spatial interpolation approaches have been developed to exploit these correlative relationships, along with the spatial autocorrelation present Owing to the displacement affecting DHS data, the mean value of each variable in a buffer of 2 km from the cluster location for urban areas and 5 km for rural areas was used, following published recommendations Selection of geospatial covariatesSelecting an optimal set of covariates is fundamental to maximize the predictive accuracy of a model. Including too few informative covariates could result in loss of explanatory power, while the inclusion of too many could cause the resulting high-dimensional multivariate model to overfit the data, especially when an ANN is applied. In statistical modelling, selection of the better performing covariates within the chosen modelling architecture is a common, widely accepted, exercise Modelling architectureThe BGS GeoSemAP is the geospatial application of the semantic array programming (SemAP) paradigm Artificial neural networksAn ANN is a D-TM able to derive from a set of input data a corresponding set of outputs. Neural networks resemble the human brain because of knowledge acquisition through learning, and storage of acquired knowledge within inter-neuron connection strengths. An ANN is implemented through a system of interconnected nodes. Information propagates through nodes, transforming the inputs in intermediate derived signals up to generate the final outputs. The internal nodes are called neurons and define the ANN hidden layers. Each of the processing neurons calculates the weighted sum of all interconnected signals from the previous layer plus a bias term and then produces an output through the activation function. The effective incoming signal s j to node j iswhere W ij is the connection weight, x i is the input to the network and b j is the bias term.The activation function associating individual nodes typically has a sigmoid shape. The sigmoid function most often used for ANNs is the logistic function:in which s j can vary in the range +1 but y is bounded between 0 and 1.The power and main advantage of using ANNs lie in their capacity to model both linear and nonlinear relationships and to learn these relations directly from the data. Because many complex problems are characterized by their intrinsic nonlinear behaviour, traditional linear models are often inadequate.Two different feed-forward neural networks were selected among the main network architectures. In a feed-forward network, the information moves in only one direction, from the input nodes to the output nodes without cycles or loops. The first was created using the R language according to the structure established by the 'AMORE' package (A MORE flexible neural network) The setting of the parameters for running the applied ANNs comes from a repeated random sub-sampling validation using 70% of the DHS data, keeping the remaining 30% for the final validation of the models.Bayesian generalized linear modelsA Bayesian modelling approach is a statistical technique that uses the Bayesian method The integrated nested Laplace approximations (INLA) approach, available as package implemented in GNU R, was applied here To produce continuous maps of the estimated proportion of gender-disaggregated literacy, stunting and use of modern contraception methods, Bayesian hierarchical spatial models, implemented through a SPDE approach, were created using the R INLA package.A summarized form of these models can be represented as where y(s) was the realization of the overall process at the cluster location s i , i \u00bc 1, . . . , n. The mean structure m(s) \u00bc X T (s)b is driven by the covariates. The residual structure is then partitioned into v(s) arising from zero-centred stationary Gaussian process capturing the spatial association at cluster level and the e(s) N(0, s e ) as the uncorrelated error terms. This spatial association is implemented through a solution to the SPDE expressed aswhere (k 2 2 D) a/2 is a differential operator, k is a scaling parameter, D is the Laplacian, a controls the smoothness of realization, t controls the variance and x(s) is the spatial field/domain for s (s 1 , . . . , s n ) locations.Because of the time necessary to properly calibrate the applied models, after an initial test where the results of many different modelling architectures were compared, the decision was made to initially apply only the models implemented in INLA. These models are considerably less time demanding than ANNs but have a similar predictive capacity (table Model validationBoth the ANN models and those implemented in INLA use a crossvalidation (repeated random sub-sampling) to set the model parameters. Owing to the size of the Nigeria datasets, here we split the data into training, validation and test sets, using the validation dataset (20% of the data) for building the final model.The validation process was implemented in two steps. First, the cross-validation approach (with the exception of Nigeria) was applied to the training dataset for selecting the best model for each of the applied modelling architectures (ANN, BGS). The relationship between predicted and observed values (the accuracy of the model) was quantified using the root mean square error (RMSE) and the mean absolute error (MAE). Although some authors suggest inter-comparisons of average model performance should be based on MAE The model with the highest explained variance and lowest RMSE and MAE was selected to be applied for producing the final map at 1 \u00c2 1 km resolution. For calculating the explained variance the pseudo-R 2 reported in equation (2.1) was used:where var(obs) is the variance of the observed data and MSE is the mean square error.A comparison of RMSE and MAE of different models based on different datasets may only capture part of the relevant statistical information. For example, both RMSE and MAE indices cannot directly preserve the information concerning the sign of the modelling errors. In particular, a model with given RMSE and MAE may locally display errors both negative (underestimation) and positive (overestimation) so as for the overall bias to be mitigated by the compensating local under/over-estimations. Another model with the same RMSE and MAE may instead systematically underestimate the modelled quantity. In order for these modelling situations to be better discriminated, we introduced a new parameter for calculating the general bias of the models (equation (2.2)):where s obs and obs are the standard deviation and mean of observed data and pred is the mean of predicted values.ResultsThe following sections document the performance of the applied modelling architectures in each of the investigated countries. Example maps and graphs for selected indicators are presented, with further results provided in the electronic supplementary material information. The results highlight that relatively accurate high-resolution maps of key genderdisaggregated socio-economic indicators can be produced, with explained variance through validation being as high as 74 -75% for female literacy in Nigeria and Kenya, and in the 50 -70% range for many other variables. However, substantial variations between countries and variables were seen, with many variables showing poor mapping accuracies in the range of 2-30% explained variance using both geostatistical and machine learning approaches. Both Bayesian modelling techniques and ANNs were able to extract the information present within the available covariates to predict development indicators in different areas. Midinfrared reflectance (the surface reflectance in the middle infrared part of the electromagnetic spectrum), elevation, accessibility, the distance to settlements and roads and the distance to conflicts (Nigeria) emerged as important covariates for mapping different indicators. For example, the mid-infrared reflectance shows a correlation of 0.58, 0.63 and 0.7 with stunting of females in Nigeria and female literacy in Kenya and Nigeria, respectively. However, not all countries and not all modelling outputs produced such a high correlation between the dependent variable and available covariates. Modelling the proportion of stunted girls under the age of 5 in Kenya, the distance to roads was found to have the highest correlation, but this was only 0.13. NigeriaTable KenyaTable TanzaniaTable BangladeshTable Discussion", "conclusions": "The focus of the SDGs on reaching the furthest behind first creates a need for approaches that can identify who and where these people are to be able to reach them. Moreover, regular updates to such information are required to be able to track progress towards meeting the goals. Traditional data sources, such as census data on their own or integrated with survey data, can provide detailed data on some indicators for specific snapshots in time, but are limited by the irregularity of population censuses. Here we have explored the potential of spatial interpolation methods built on geolocated survey data, which are growing in popularity and application  to meet these needs through a series of case studies.[13 -17]The results show the potential of the approaches, and also reveal challenges in constructing consistently accurate layers across regions and variables.The results highlight clearly that producing highresolution maps of development indicators using spatial interpolation approaches is a challenge in some cases. There are many obstacles, starting from the lack of input data and the difficulty in selecting and setting the most appropriate modelling architecture, to the difficulties in selecting the right modelling proxy. Here we modelled 16 different indicators in four countries. In six of the 16 maps we produced, the value of the variance explained by the model was around 0.6 or higher (tables 3-6), but other models did not perform well, with values of explained variance around 0.1 or lower.In some countries very different results were produced in modelling the same gender-disaggregated indicator. In modelling literacy (figures 1, 3 and 5), the predictive capacity of our models was lower for males than for females (tables 3-6). For Nigeria, the proportion of variance explained by the model for male literacy was satisfactory (0.57), but in Kenya even the best of the models explained only 0.32 of the variance, whereas 0.75 was achieved for female literacy. Comparison of the results of ANN and BGS when used to model the same indicators shows they have similar performance.  The ANN results were better for modelling female literacy in Bangladesh, and male literacy in Tanzania, but in modelling modern use of contraception methods in Tanzania, the Bayesian architecture performed better (tables 5 and 6).If the distribution of the modelled variable is substantially different from Gaussian, the ANN approach generally appears to perform slightly better than BGS.A number of factors underlie the differences seen in modelling performance between countries and variables, principally driven by the amount and spatial scale of variation displayed by each indicator, and the extent to which the indictor was associated with and/or driven by the available geospatial covariates. If limited or no information is present within the covariates, the models unsurprisingly fail to predict the phenomena well, but good performance was obtained where strong correlations existed. Some covariates showed strong relations with the development indicators being modelled. For example, the correlation of travel times with literacy in Kenya was between 0.5 and 0.6 for males and females; however, this was only between 0.2 and 0.27 in Nigeria. By contrast, middle infrared reflectance in Nigeria showed a high correlation with both female stunting and literacy, but showed a correlation of only 0.1 for stunting in girls in Kenya. In some cases, therefore, the covariates likely did not relate to the driving factors behind the spatial heterogeneity seen, and additional geospatial covariates that encompass factors relating to socio-economic differences are required to capture these country-specific sociological dynamics in the models.Precipitation, temperature and vegetation cover [53, have been found to be important correlates of malnutrition.  Temperature, for example, is directly linked to aridity, which in turn has an impact on malnutrition 54], and the enhanced vegetation index (EVI) was found to be a significant common factor in describing rates of stunting [54]. Other examples of biophysical and geographical factors often cited in the literature include evapotranspiration, productivity of agricultural lands, distance to urban areas, topography and access to markets through road networks [54][55,. Unfortunately, the literature on this subject remains sparse because survey data rarely include metrics of these factors 56].[56]Correlations between literacy and urbanization have been shown previously , and use of contraception methods is also known to be strongly associated with levels of education, socio-economic status and access to health facilities [57][58,. In some previous work 59], it has been also shown that there exists a possible correlation between road networks and literacy, which is likely related to ease of access to schools and market. Other covariates have been tested to explore their possible indirect links with the selected health and development indicators. For example, middle infrared reflectance has previously been used to exploit its link with vegetation (vegetation spectral signatures are characterized by low reflectance in middle infrared). Nevertheless, our study shows that the same covariates can have different behaviours in different countries and occasionally also within the same country. For example, literacy rates in Nigeria have a  relatively high correlation with urbanization (0.53 and 0.42 for female and male literacy, respectively), whereas female literacy rate in Kenya, despite good modelling performance, shows urbanization and literacy to be poorly correlated (0.17). Many reasons are likely behind this and further studies are necessary to better understand these differences. Even when model performance was satisfactory here, some sources of errors contributed to the uncertainty of the model. The introduction of cluster location random displacement for protecting the anonymity of the respondent population can introduce further uncertainty to the modelled relationships [60][12,. This potential error was mitigated by extracting mean values through a defined buffer around the survey points 22]. The extent of the impact of the displacement can vary between indicators and different survey datasets but, in general, its impact on modelling performance should be modest [10]. In addition to the displacement issues, in urban areas the covariates used do not capture well the local spatial scales of variation. In general, urban areas were predicted with the same homogeneous values, not capturing any intra-city variation. Upcoming datasets (e.g. the global human settlement layer [12] and global urban footprint [61]) could lead to a future better representation of within-city variation [62]. The work presented should be considered as a preliminary study to test the strength and limits of spatial interpolation approaches. Future work should, therefore, focus on refinements of methods. This may, for example, include updating accessibility layers to include more recent and detailed road networks and settlement layers. Moreover, it could also involve modelling key driving factors of the phenomena under study, such as poverty or access to sanitation, and then using these as covariates themselves. The effect that a country-specific focus, tailored as much as possible to a specific indicator, can have on mapping accuracies rather than using globally consistent covariates should be explored. In addition, many socio-economic factors, not captured by the suite of covariates we used, and often available at aggregate levels such as administrative units, could be obtained and their ability to improve mapping accuracy tested. A challenging, but potentially very fruitful next step, could also come from integrating community based household surveys (e.g. DHS), data from governmental monitoring systems and data from different civic systems (schools, health facilities) and comparing different predictive surfaces.[12]The rising international focus on inequalities in the SDG era requires a detailed and strong evidence base with an explicit quantification of uncertainties. Some of the maps produced in this study have a sufficiently accurate prediction capacity to be summarized to a level of administrative unit that is relevant for policy-making and the allocation of resources. In particular, the maps of female literacy in Nigeria and Kenya, use of modern contraception methods in Nigeria or male and female stunting in Nigeria have reasonable levels of accuracy to be used for planning purposes.The work undertaken here shows the value of combining data from geolocated household surveys with spatial covariates within advanced modelling architectures, and such approaches are growing in popularity and impact  with provision of surfaces now being a regular output accompanying new surveys [13 -16]. However, limitations and warnings about extending such approaches across varying geographies and indicators are clear. The variability in model performance between countries and variables highlights the need for tailored approaches and robust methods with full quantification of model uncertainty to communicate where poor model fits exist. With geolocated household surveys being undertaken regularly, the potential exists for the continuous update and monitoring of SDG-relevant indicators across wide areas, but results here highlight that caution is needed.[17]Authors' contributions. C.B. was responsible for study design, data cleaning, modelling development and implementation, analysis, interpretation, drafting and production of the final manuscript. C.P., R.C., G.H., J.S. and A.S. were responsible for assembling or creating many of the raster covariates and dependent variables used in the analysis and for interpretation of the final manuscript. T.B. and V.A. were responsible for interpretation and production of the final manuscript and modelling support. N.R. was responsible for script support. A.J.T., L.B. and E.W. were responsible for overall scientific management, interpretation and preparation of the final manuscript. All authors read and approved the final manuscript.Competing interests. We declare we have no competing interests. Funding. C.B. was supported through a grant from the UN Foundation as part of the Data2x programme. A.J.T. is supported by funding from NIH/NIAID (U19AI089674), the Bill and Melinda Gates Foundation (OPP1106427, 1032350, OPP1134076, OPP1094793), the Clinton Health Access Initiative, National Institutes of Health, and a Wellcome Trust Sustaining Health grant (no. 106866/Z/15/Z). This work forms part of the outputs of WorldPop (www.worldpop. org) and the Flowminder Foundation (www.flowminder.org). The funders had no role in study design, data collection and analysis, decision to publish, and preparation of the manuscript.", "SDG": [6]}, "optimal_design_of_compact_and_functionally_contiguous_conservation_management_areas": {"name": "European Journal of Operational Research", "abstract": " Compactness and landscape connectivity are essential properties for effective functioning of conservation reserves. In this article we introduce a linear integer programming model to determine optimal configuration of a conservation reserve with such properties. Connectivity can be defined either as structural (physical) connectivity or functional connectivity; the model developed here addresses both properties. We apply the model to identify the optimal conservation management areas for protection of Gopher Tortoise (GT) in a military installation, Ft. Benning, Georgia, which serves as a safe refuge for this 'at risk' species. The recent expansion in the military mission of the installation increases the pressure on scarce GT habitat areas, which requires moving some of the existent populations in those areas to suitably chosen new conservation management areas within the boundaries of the installation. Using the model, we find the most suitable and spatially coherent management areas outside the heavily used training areas.", "keywords": "(D),OR,in,natural,resources,Compactness,Landscape,connectivity,Integer,programming", "introduction": "In many parts of the world conservation reserves are established to protect critical habitat areas from agricultural/urban development and managed to maintain or enhance species survival chances. Due to the scarcity of financial resources, determination of the optimal amount and location of those areas is an important issue. Typically, this is done by dividing the landscape into discrete land units (sites) and selecting an optimal subset of them assuming that each site provides measurable habitat services to the targeted species. This problem is often stated as minimization of the cost of selected sites while meeting the conservation goals (e.g., minimum occurrence of each species in selected sites), or maximization of a conservation objective (e.g. number of species protected) subject to the available resource constraints ). These problems were addressed initially by using heuristic approaches (e.g., (Moilanen, Wilson, and Possingham 2009Pressey, Humphries, Margules, Vane-Wright, & Williams, 1993. Later, they were formulated as linear mixed-integer programs (MIP) in the framework of the set covering problem (SCP) and maximal cov-ering problem (MCP) , 1997)(Camm, Polasky, Solow, & Csuti, 1996;Church & ReVelle, 1974;Church et al. 1996;Cocks & Baird, 1989;Kirkpatrick, 1983;Polasky, Camm, & Garber-Yonts, 2001;Possingham, Ball, & Andelman, 2000;Toregas & ReVelle, 1973;Underhill, 1994;. Although the optimal solutions of these MIP formulations are economically efficient, they usually lack spatial coherence. This may limit the chances of inter-site dispersal and long-term survival of species within the conservation reserve areas. Also, managing a spatially coherent reserve network is more convenient and efficient than managing many sites scattered over a large area. Therefore, additional mechanisms need to be introduced in the SCP and MCP formulations to take spatial properties into account when determining the optimal site selection.Williams & ReVelle, 1997)Spatial criteria in reserve site selection may take a variety of forms (Haight & Snyder, 2009;. Most commonly used criteria include compactness Williams, ReVelle, & Levin, 2005)(Fischer & Church, 2003;Jafari & Hearne, 2013;\u00d6nal and Briers, 2003;T\u00f3th & McDill 2008;, proximity of selected sites Wright, ReVelle, & Cohon, 1983)(Briers 2002;Dissanayake, \u00d6nal, Westervelt, & Balbach, 2012;Miller, Snyder, Skibbe, & Haight, 2009;Nalle, Arthur, Montgomery, & Sessions, 2002;\u00d6nal and Briers, 2002;Rothley, 1999;Snyder, Miller, Skibbe, & Haight, 2007;, habitat fragmentation Williams, 2008)(\u00d6nal & Briers, 2005;, contiguity \u00d6nal & Wang, 2008)(Cerdeira & Pinto, 2005;Cerdeira et al., 2005Cerdeira et al., , 2010;;Cova & Church, 2000;Duque et al., 2011;Jafari & Hearne, 2013;Marianov, ReVelle, & Snyder, 2008;\u00d6nal & Briers, 2006;T\u00f3th et al., 2009;Wang & \u00d6nal, 2011Wang & \u00d6nal, , 2013;;Williams, 2001;, existence of buffers and corridors Carvajal et al., 2013)(Conrad, Gomes, van Hoeve, Sabharwal, & Suter, 2012;Williams, 1998;Williams & ReVelle, 1996, 1998;, and accessibility Williams & Snyder, 2005)(\u00d6nal & Yanprechaset, 2007;. Incorporating these criteria in optimum site selection requires more sophisticated and computationally complex mathematical models than the SCP and MCP formulations. Consideration of multiple attributes together increases this challenge further. This article presents a linear integer programming model to incorporate compactness and connectivity criteria simultaneously.Ruliffson, Haight, Gobster, & Homans, 2003)Connectivity is an important factor for efficient functioning of conservation reserves. A well-connected reserve network 1 allows the species to utilize all the resources available in the reserve and increases the likelihood of species survival and ability to colonize suitable habitat areas. This depends not only on the habitat characteristics of an individual reserve site, but also on the characteristics of the neighboring reserve sites . Connectivity is approached in different ways. Metapopulation connectivity deals with spatially separated but interacting local populations in the reserve network (Van Teeffelen et al., 2006)(Hanski, 1999;Moilanen & Hanski, 1998;. Landscape connectivity, on the other hand, deals with the degree to which the landscape facilitates movement of species within reserves. Landscape connectivity can be achieved either by structural connectivity (or physical contiguity) that allows species to dwell in the reserve without having to get out of the protected area, or functional connectivity which deals with the degree to which a reserve facilitates species' capability to move within the reserve Moilanen & Hanski 2001)(Bunn, Urban, & Keitt, 2000;Taylor, Fahrig, & With, 2006;Taylor, Fahrig, Henein, & Merriam, 1993;Tischendorf & Fahrig, 2000;. A structurally connected reserve may not necessarily be functionally connected if physical characteristics of some sites impede movement within or between the reserved areas (e.g. presence of steep rocky terrains or water bodies, lack of sufficient vegetation or forest cover). Although the importance of functional connectivity has been widely acknowledged, a generally agreed upon operational definition of the concept is not yet available Urban & Keitt 2001)(B\u00e9lisle, 2005;. Incorporating these two connectivity criteria in site selection may lead to dramatically different configurations. For instance, minimization of the reserve size along with the physical contiguity requirement may lead to an elongated, narrow and winding reserve configuration containing the best available but spatially dispersed sites (see, for instance, Kadoya, 2009)Cerdeira, Gaston, & Pinto, 2005;\u00d6nal & Briers, 2006;. This would increase the likelihood of species' exposure to unfavorable conditions within and outside the reserve area and may not work effectively if the individuals tend to roam around or move in random directions. A contiguous reserve configuration may include poor quality sites just to obtain physical connections (bridges) between good habitats. Such a reserve would not be functionally connected if the targeted species do not have the capability to cross those bridging sites. Therefore, in essence the reserve would consist of multiple 'functionally detached' sub-reserves some of which may not be large enough to provide adequate habitat services for a minimum viable population of the target species. On the other hand, a functionally connected reserve may not be structurally connected if the species (e.g. birds, butterflies) can crossover between closest, but not necessarily adjacent areas in the reserve. In many cases a network of multiple connected reserves is a preferred configuration than a single large connected reserve to safeguard against catastrophic events such as fire, diseases, etc. 2 In this article we address these issues and present a linear integer programming model to determine an optimal compact and connected reserve network configuration where connectivity can be enforced in the form of structural connectivity and/or functional connectivity. We apply this approach to the protection of a ground-bound species where compactness, structural connectivity, and functional connectivity must be enforced together.Williams & Snyder, 2005 )", "body": "In many parts of the world conservation reserves are established to protect critical habitat areas from agricultural/urban development and managed to maintain or enhance species survival chances. Due to the scarcity of financial resources, determination of the optimal amount and location of those areas is an important issue. Typically, this is done by dividing the landscape into discrete land units (sites) and selecting an optimal subset of them assuming that each site provides measurable habitat services to the targeted species. This problem is often stated as minimization of the cost of selected sites while meeting the conservation goals (e.g., minimum occurrence of each species in selected sites), or maximization of a conservation objective (e.g. number of species protected) subject to the available resource constraints Spatial criteria in reserve site selection may take a variety of forms Connectivity is an important factor for efficient functioning of conservation reserves. A well-connected reserve network 1 allows the species to utilize all the resources available in the reserve and increases the likelihood of species survival and ability to colonize suitable habitat areas. This depends not only on the habitat characteristics of an individual reserve site, but also on the characteristics of the neighboring reserve sites Problem descriptionMany rare, threatened, and endangered species in the U.S. are found within the boundaries or in the vicinity of military installations The modelTo address the issues described above we first partition the area considered for development of a conservation reserve 5 into disjoint spatial units (e.g., a uniform square grid cover 6 ). Each spatial unit (site) is either selected and becomes part of a reserve in the network or is left out. When selecting sites the spatial locations of indi-vidual sites relative to other selected sites and their contributions to the conservation objectives are both taken into account. For reasons that will be explained later, we represent each reserve by a 'central site' to which other selected sites are assigned. Both the central site and assignment of sites to the center are determined by the model simultaneously.The algebraic notation used in the model is as follows: n \u2265 1 denotes the number of reserves in the network. L is the set of all sites where individual sites are denoted by symbols i, j, k \u2208 L. Site selection and assignment to a reserve is represented by a binary variable X ki , where X ki = 1if site i is selected and belongs to the reserve centered at site k and X ki = 0 otherwise. If X kk = 1, then site k is selected as a central site to form a reserve around it. The symbol d ki denotes the distance between the centroids of sites k and i, and h i denotes the habitat quality of site i. Each reserve is required to provide a minimum amount of habitat quality, denoted by vh, in order to support a viable population of the targeted species. Finally, the total habitat quality provided by all reserves must exceed a specified level denoted by th.Modeling compactnessCompactness is considered as a measure of shape simplicity and equated to near circular or square shapes. Although the concept may seem obvious, there is no universally agreed upon definition of compactness in the spatial analysis literature (see MinimizeThe objective function (1) is the sum of distances from individual sites in each reserve to the center of that reserve, summed over all reserves. Eq. ( (3) states that each site can belong to at most one reserve. Constraint (4) implies that if site k is selected as a central site, i.e., X kk = 1, then up to m sites can be assigned to the reserve formed around site k, where m is an arbitrarily selected large integer. Otherwise, X kk = 0 and no site can be assigned to it, i.e. X ki = 0 for all i. Conversely, if site i is selected and assigned to a central site k, i.e. X ki = 1, then a reserve must be formed around (centered at) site k, i.e. X kk = 1. 8 Constraint (5) requires that each reserve provides the minimum habitat quality required from individual reserves, while constraint (6) ensures that all reserves collectively provide the desired aggregate level of habitat quality.Modeling connectivityIn the landscape ecology literature a distinction has been made between structural connectivity and functional connectivity In general, the model described by ( X ki for all k, j that are not adjacent that connects the two sites, namely the union of the paths connecting those sites to the common center. Therefore, constraint (8) ensures that each reserve is spatially connected. 9  The strategy employed in the contiguity constraint ( The graph-theoretic formulations employing cycle-breaking constraints lead to large and computationally difficult MIP models. The computational disadvantage is exacerbated and can be fatal particularly when multiple reserves are to be configured from a large number of sites. 10 In the next section we test and compare the computational efficiency of the model described by ( In the model ( When functional connectivity is of concern, the degree to which a connecting path facilitates or impedes movement of species would depend not only on the distance but also on the habitat quality of the individual sites in that path. This is not taken into account in constraint (8). Therefore, an optimal solution may include some sites with poor habitat quality just because their selection provides bridges to physically connect high-quality habitat patches. Instead, a longer path formed by sites with moderately good habitat may be a preferred alternative if this offers a more convenient movement across the protected areas. This leads to the concept of functional distance (or habitat-adjusted distance), dij , defined by: dij =where i and j are adjacent sites (have a common edge), l is a threshold habitat level required by the species to dwell in or cross those sites, and m>0 is an arbitrarily selected large number. All other symbols are as defined earlier. The functional distance between any two sites (not necessarily adjacent) is then defined as the length of the shortest path with respect to functional distances between mutually adjacent sites in that path. When both h i and h j are larger than the threshold habitat level l, the denominator term represents the average habitat level of sites i and j. Therefore, the value of dij is small (large) if both sites have good (poor) habitat. If one of the two sites has less than the threshold habitat level, then dij becomes very large (namely equal to m). This would drive out such pairs of sites when identifying the best functional connections (routes), which is consistent with the movement behavior of species that do not generally venture into poor quality areas. 11  The functional distance and shortest path approach described above is similar to the least-cost path method used in spatial analysis where the purpose is to find a path which links a given origin and destination and minimizes the transportation cost between them. If we interpret the inverse of the average habitat quality used in (9) as the 'travel cost' of moving from site i to site j, the model incorporating dij in the objective function determines the least-cost network including multiple origins and destinations and the least-cost paths between them to minimize the total cost associated with the entire network. In the ecological context, dij can be considered as a measure of movement resistance, thus minimizing the objective function (1) expressed in this distance measure determines the optimal habitat areas to facilitate movement of species within those areas to the extent possible. 12 The concept of movement resistance measured by the total travel cost has been discussed extensively in the ecology literature To explain the concept of habitat-adjusted distances and functional connectivity, consider the example given in Fig. 12 Using Euclidean distances in the objective function promotes circular reserve shapes, but this may leave out some functionally well-connected sites. On the other hand, using functional distances promotes the selection of well-connected sites, but this may compromise compactness and lead to skewed/stretched shapes instead of circular configurations. When working with functional distances, constraint (8) can be used in a similar way to using ordinary distances, i.e. if a site is to be selected a neighboring site that has a shorter functional distance to the central site must also be selected. However, unlike the ordinary distances, functional distances may restrict the eligibility of neighbors in site selection. Fig. We note two important characteristics of the optimal solutions obtained from the model with the use of habitat adjusted distances. First, the optimal reserve configuration is always structurally contiguous. This is because constraint (8) enforces the selection of an immediately adjacent neighbor when selecting a reserve site. Second, selection of a particular site does not necessarily require selection of the entire shortest path connecting that site to the associated central site. Although this would happen in most cases, there is no explicit mechanism in the model that enforces this property.To investigate the merits of the approaches described above in terms of compactness and functional contiguity of the resulting reserve configuration, we generated several synthetic data sets and solved the model. Fig. Computational efficiencyIn general, discrete optimization models are difficult to solve, even in the linear MIP case, when a large number of constraints and discrete variables is involved. Therefore, the usefulness of the MIP formulation presented above may be an issue in large-scale reserve selection models. In this section, we test the computational efficiency of our formulation against two alternative contiguity and compactness formulations presented by b The presolved model sizes for multiple reserve cases are slightly different, but differences are negligible. c k=number of reserves configured. The presolved model sizes are slightly different, but differences are negligible.d The model size is reported for the single reserve case; the size is invariant when multiple reserves are considered. The presolved model sizes are slightly different, but the differences are negligible. a Only two runs could be completed within the allowed processing time limit, relative gaps were > 38 percent. b Only one run could find the optimal solution, two other runs were terminated due to the processing time limit, relative gaps were > 83 percent.c At most eight runs could be completed within the allowed time limit, the solution times are averages of the completed runs.d Out of memory while solving the model or terminated due to the processing time limit without finding a solution.e Only two runs could be completed within the allowed processing time limit, relative gaps were > 36 percent. f Thirty-six or more runs were completed within the allowed processing time limit, the solution times are averages of the completed runs.each having a different species distribution across the sites and obtained with the same specifications of parameters n, vh and th. The processing time limit for each run was specified as one hour and the solver was terminated after completing 50 runs or after running for two hours (whichever occurs first). We use the problems that are solved successfully by each model and report the average solution times of the completed runs as indicators of the models' computational performance. The test runs were carried out using GUROBI 5.0 on an Intel Pentium computer with a CPU of 2.80 gigahertz and 8 gigabyte RAM.The model statistics are displayed in Table Table An empirical applicationWe present an empirical application of the model described by ( The current and future military training areas were obtained as raster files from Ft. Benning. The habitat areas suitable for GT were obtained as raster files from the national biological information infrastructure The management of the GT populations within the installation can be conducted using a single large reserve or multiple smaller reserves. The reasons for considering multiple reserves are three-fold.First, dividing the total GT population into smaller populations, each to be located in a different part of the installation, may safeguard each of them against potential diseases that may occur in other protected areas. Second, the habitat density in the southeast and northeast of the installation (see Fig. Results and discussionWe first found the optimum spatially unrestricted selection of GT habitat sites, namely the minimum number of sites that collectively provide 20,000 units of habitat suitability. 14 This solution is displayed in Fig. Imposing constraint (8) establishes spatial connectivity, as shown in Fig. Both structural and functional contiguity requirements led to larger CMAs, which increased the total distances from the reserve centers (thus, decreased compactness). Instead of the total distance (plain or habitat-adjusted) a more representative measure of compactness can be the average distance obtained by dividing the total distance from the reserve center(s) by the number of sites included in the reserve(s). Fig. Concluding remarks", "conclusions": "This article presented a linear integer programming formulation to incorporate reserve compactness and landscape connectivity as spatial criteria in reserve site selection. Compactness is achieved by minimizing the sum of pairwise distances between all sites assigned to a reserve and a central site of that reserve, both determined by the model simultaneously. The model includes an explicit constraint to achieve spatial contiguity, namely if a site is to be selected an adjacent site closer to the central site must also be selected. Landscape connectivity is defined in two different ways: structural contiguity and functional connectivity. In the first case, we use ordinary distances between selected sites and the central sites they are associated with, while in the second case we use habitat adjusted distances to reflect the difficulty of species' movement within the protected areas. We presented a case study involving the protection of a keystone species at risk. The results show that the optimal reserves become less compact and include more sites with lower quality as the targeted habitat quality is increased.The model and the empirical example presented here focus on one species only. With appropriate modifications, the model can be extended to the case of multiple species. This requires additional index sets, more variables and more constraints (as in . For the sake of space and readability, we did not present the details of the multi-species extension here.Dissanayake, \u00d6nal, & Westervelt, 2011)The present analysis focuses on spatial properties of the reserved areas only, ignoring the properties of the remaining landscape. In some cases, islands or thin deep bays of non-reserve areas may be placed within the reserved areas (Fig. ). Such areas may not be suitable for alternative uses, thus they have to be managed as part of the reserve. In the particular case study presented here this was not an issue because the land is already owned by the military, but in general this means additional cost that must be accounted for. Finally, spatial layout of the non-reserve areas can often be equally important as that of the reserved areas (e.g., they may have to be contiguous). Incorporating spatial considerations for both reserved and non-reserved areas can be done by using a multiple land use model, as in 6d or by including additional variables and constraints in the model to achieve the desired properties. We note, however, that this may adversely affect the size and computational efficiency of the model.Dissanayake et al. (2011)", "SDG": [6]}, "public_participation_modelling_using_bayesian_networks_in_management_of_groundwater_contamination": {"name": "Public participation modelling using Bayesian networks in management of groundwater contamination", "abstract": " Negotiation and active involvement with participation of water managers, experts, stakeholders and representatives of the general public requires decision support tools (Environmental Decision Support Systems; EDSS) that build on transparency and flexibility in order to reach sound action plans and management instruments. One possible EDSS for active involvement of stakeholders is application of Bayesian networks (Bns). The paper gives an example of a case study (The Danish case) where farmers and hydrologists disputed the degree to which pesticide application affected the quality of deep groundwater. Instead of selecting one opinion or another, the decision was made to include both in the Bns. By adopting this approach, it was possible to view the results from either point of view, accepting the reality of the situation, not becoming mired in an insoluble conflict, and in this way laying the foundation for future compromises. The paper explores Bns as a tool for acting on and dealing with management of groundwater protection. Bns allow stakeholders' divergent values, interests and beliefs to be surfaced and negotiated in participatory processes for areas where conventional physically based groundwater models are insufficient due to lack of data, physical understanding, flexibility or lack of integration capability. In this way, the agency will be able to address the institutional arrangement influencing groundwater protection in all its complexity.", "keywords": "Public participatory modelling,Pesticides,Groundwater management,Bayesian networks,Active involvement,Negotiation,Planning,Stakeholders,MERIT,EDSS", "introduction": "The objective of the present paper is to describe the way Bayesian networks (Bns) can be applied as a tool for public participatory modelling in the management of groundwater contamination.The topic will be discussed and exemplified by making reference to a real world case study in Denmark, focusing on pesticide contamination and protection strategies for a well field outside Copenhagen in Denmark, where drinking water is abstracted from a deeper groundwater aquifer for the Danish capital area.In the following we define public participatory modelling (PP modelling) as a modelling process that concerns reasoning and decision making about whole systems using computerbased modelling and analysis technology, and with active involvement of stakeholders. The guidelines for planning in relation to the  encourage active involvement of stakeholders but do not give details for use of EDSS.WFD (EC, 2003)Development of EDSS and PP modelling involves disciplines such as computer science, decision theory, statistics, psychology, information and knowledge engineering, and organisational science (Eom and Farris, 1996;Eom, 1999;.Myrsiak et al., 2005)Traditional groundwater models for flow and transport are constructed according to model protocols in dialogue between model experts and the water manager with reviews when passing different milestones (Refsgaard and Henriksen, 2004;Refsgaard et al., 2005;. Such types of models are excellent for water resource assessments and predictive simulations, but in most cases they do not link directly to the wider social, cultural and economic aspects of water management.Scholten et al., 2004)A basic requirement for PP models with high level stakeholder engagement in the decision process, is that manager, expert and stakeholders should try to use a shared language for the dialogue and communication in order to reach informed decisions. Even though the goal is not necessarily consensus, dialogue can only make progress with an enlivened process where stakeholders exchange viewpoints. They may not agree on the outcome, problems, results etc., but the process may reveal common ground (as well as uncommon). What we share is not necessarily as interesting as what we do not share . Differences in ideas, attitudes and experiences may result in new types of knowledge and new solutions to common problems. This raises the question, whether Bns can support and enable such dialogue and negotiation processes.(Campell, 2000)The paper is structured in the following way. After the present introduction, Section 2 describes the theoretical basis.Section 3 describes EU WFD terminology and how Bns as EDSS for PP modelling can be applied. Section 4 describes institutional framework. Finally, Section 5 describes a test case where Bayesian networks were constructed with active involvement of stakeholders for the purpose of groundwater management and protection.", "body": "The objective of the present paper is to describe the way Bayesian networks (Bns) can be applied as a tool for public participatory modelling in the management of groundwater contamination.The topic will be discussed and exemplified by making reference to a real world case study in Denmark, focusing on pesticide contamination and protection strategies for a well field outside Copenhagen in Denmark, where drinking water is abstracted from a deeper groundwater aquifer for the Danish capital area.In the following we define public participatory modelling (PP modelling) as a modelling process that concerns reasoning and decision making about whole systems using computerbased modelling and analysis technology, and with active involvement of stakeholders. The guidelines for planning in relation to the Development of EDSS and PP modelling involves disciplines such as computer science, decision theory, statistics, psychology, information and knowledge engineering, and organisational science Traditional groundwater models for flow and transport are constructed according to model protocols in dialogue between model experts and the water manager with reviews when passing different milestones A basic requirement for PP models with high level stakeholder engagement in the decision process, is that manager, expert and stakeholders should try to use a shared language for the dialogue and communication in order to reach informed decisions. Even though the goal is not necessarily consensus, dialogue can only make progress with an enlivened process where stakeholders exchange viewpoints. They may not agree on the outcome, problems, results etc., but the process may reveal common ground (as well as uncommon). What we share is not necessarily as interesting as what we do not share The paper is structured in the following way. After the present introduction, Section 2 describes the theoretical basis.Section 3 describes EU WFD terminology and how Bns as EDSS for PP modelling can be applied. Section 4 describes institutional framework. Finally, Section 5 describes a test case where Bayesian networks were constructed with active involvement of stakeholders for the purpose of groundwater management and protection.Theoretical basisEU Guidelines on public participationAs illustrated in Fig. Information provision (about management timetables, issues and to the participants. It is considered the foundation for all further participation activities). Consultation (encouraging written and oral responses). Active involvement (involving people in ''developing and implementing plans'' that could form the final plan decided upon). There is also a meta-level of participation termed ''awareness raising and developing a learning approach'', something that will support all the other levels of participation and management, which should be remembered. Participation in river basin management concerns three separate groups (Table Bayesian networks as EDSS for active involvement and dialogueA Bayesian network (Bn), also called a Bayesian belief network, is a type of decision support system based on probability theory. This rule, devised by Thomas Bayes, an eighteenthcentury English clergyman, shows mathematically how existing beliefs can be modified with the input of new evidence. Bns organise the body of knowledge in any given area by mapping out cause-and-effect relationships among key variables and encoding them with numbers that represent the extent to which one variable is likely to affect another Bns have gained a reputation of being powerful techniques for modelling complex problems involving uncertain knowledge and impacts of causes. Ideally, Bns are techniques to assist decision-making and are especially helpful when there is uncertainty in the data and the factors are highly interlinked. Constructing the qualitative part of a Bayesian network (nodes and links), although elaborate, is relatively straightforward, and experts are comfortable doing so.This part of the net is relatively easily communicated to stakeholders In the early stages of the Bn construction phase, it may be most efficient to have broader groups of stakeholders and the general public to provide input to Bn development. In the later, more quantitative stages, it is often better to consult stakeholders at individual meetings focusing on ''domains of interest'' Many commercial software tools are available for implementing Bn based models, among which Hugin is one (www.hugin.com). Hugin was selected for the case study in Denmark as graphical user interface and decision engine providing the following main features:1. The Graphical User Interface:Construction, maintenance and usage of knowledge bases using Bayesian networks and Influence diagrams technology Supports development of object-oriented Bayesian networks Interface to automated learning of Bayesian networks from databases Wizard for generation of probability tables 2. The Hugin Decision Engine:Calculation of revised beliefs, optimal strategy, conflict analysis, and other features for exploiting graphical probabilistic models Automated learning/construction of Bayesian networks 3. New concepts for relation between decision making and modelling processPlanning and modelling in WFDWFD planning is an ongoing process that can be described as a cyclic process of four main steps: Identification, design, implementation and evaluation. For each of these steps different types of modelling may be carried out to support decision making (see Fig. Identification consists of establishment of status and overall goals, building commitment to reform process and gaps analysis. Design includes the preparation of strategy and action plan Competent authorities are the authorities given final responsibility for deciding on and implementing the management plan.Institutional frameworkAccording to North An institutional arrangement of major importance in the context of groundwater management relates to contracts and property rights, since economic activity takes place in the form of contracting. The institutional arrangement can facilitate these actionsdfor instance, by providing clear rules on contract layout and by helping to enforce such rules The institutional framework defines the management instruments to be used (e.g. groundwater use rights, pricing, information, water user participation, farming contracts etc.). This framework in turn has an impact on the management results (e.g. groundwater allocation, use and future groundwater quality). It should be noted that there is a feedback between institutional framework and management instruments and again between management instruments and management results The institutional framework does not only consist of formal institutional arrangements (like groundwater laws and decrees, etc.), but also strongly depends on informal institutional arrangements in the water sector (cultural and social norms, beliefs and values). Furthermore, other institutional arrangements indirectly may affect groundwater (e.g. energy, health and agricultural policy etc.).In the following we will use the term ''T-organisation'' (temporary organisation) for the collective of an agency and the temporarily involved stakeholders in the groundwater protection activities.Using the definition of an organisation by ''An organisation is (a) A plurality of parts, (b) Maintaining themselves through their interrelatedness, and (c) Achieving specific objectives. While accomplishing (a) and (b), an organisation (d) Adapt to the external environment, thereby (e) Maintaining their interrelated state of the parts'', the T-organisation is characterised by a highly dynamic network of collective action, an arena for exchange of meaning and influence, social learning and awareness.In the T-organisation (see Fig. A T-organisation, or any working group within it, is exposed to both intra-group and inter-group characteristics In order to deal properly with the primary task and to achieve the goals of the participatory process different requirements are necessary: (a) Structural subsystem which interacts with home stakeholder organisations (e.g. clear rules, planning, authority and decision making); (b) Technological subsystem selected according to PP modelling objectives (e.g. use of appropriate tools, acknowledgement of guidelines, procedures, work methods and knowledge bases); and (c) The philosophy of leadership and style balanced to consider both formal (e.g. involvement plan and task descriptions) and informal factors (roles and emotions), and appropriate according to authority relation and delegation of working group members.Several ingredients are necessary for individuals (managers, experts and stakeholders) to come together and generate new meanings in dialogue. Primarily, each person must come as a responsible individual, aware of ideas and actions that he or she wants to contribute to the construction of new meaning. Secondly, an appreciation of difference among different people and points of view, which must be seen as a resource, not a threat. We cannot learn or progress from shared meaning; we only learn by encountering new ideas and acting them out in intersubjective acts. Thirdly, each person must honour an obligation to create meaning for the other's ideas or actions Creating new meaning in dialogues require clear rules of the game. It is necessary to prepare a stakeholder involvement plan describing how to involve stakeholders and general public which is balanced with respect to problem framing and the type of decision support systems used for the planning and/or implementation. Developing common understanding, defining goals, objectives and principles and the character of public participation and clarifying team roles and responsibilities are parts of stakeholder involvement plans. Furthermore, a list of all stakeholders to include with evaluation of groups' interest and responsibilities are valuable for the subsequent forming of working groups and planning of public meetings, selection of facilitator and clarifying plans for informing the general public. Finally, mission statements for all groups, time schedule for meetings and milestones, including allocation of resources for implementation of the stakeholder involvement plan, has to be decided and written down.An enabling environment (the government's regulatory role and capacity building) is required if groundwater protection in relation to pesticide applications is to be efficient. In addition, a combination of top-down and bottom-up approaches has to be applied. The new management tool package should not only focus on market-based instruments such as pesticide taxes and/or water pricing but should also include PP tools and methods for involving the groundwater users in decisionmaking processes. Groundwater management is a field, which has to be developed in order to cope with present and future demands (WDF Guidelines and the new Groundwater Daughter Directive). Formerly, groundwater was simply an ''invisible resource''. Today it has become a highly visible resource in many areas due to problems of overabstraction and pollution affecting stakeholders' access to groundwater in different ways Bayesian networks may be a good tool for dealing with the informal institutional arrangements in the water sector, e.g. the cultural and social norms, and the different perceptions among stakeholders, may they be of political, historical or religious origin. The question of how to actively involve stakeholders, e.g. farmers, residents and other interested parties, more effectively in the decisions necessary, to assure an improved protection of groundwater in local areas is important issues. The need is to illuminate the significance of various instruments (e.g. voluntary agricultural agreements, taxes on pesticides, pesticide-free buffer zones, etc.) and their influence on water cycle, groundwater quality, value of natural resources, macroeconomics, and commercial and social aspects.The question is whether establishment of a T-organisation with active involvement of stakeholders and application of Bns can provide a broader local acceptance of innovative decisions, as well as improve the dialogue between water companies, local stakeholders and the authorities. This is what we set out to investigate in the Danish case, Copenhagen Energy being the competent user responsible for groundwater management and experts and farmers' organisations as the main stakeholders involved.5. Case study: Groundwater protection for Havelse well field with Bn construction with full stakeholder involvementIntroduction to case studyBasic hydrological data and land coverage maps were collected from several sources, and geochemical data from groundwater was extracted from databases at the Geological Survey of Denmark and Greenland (GEUS). Data were also obtained from reports made available by Frederiksborg County and Copenhagen Energy The Havelse waterworks well field catchment area covers an area of 26 km 2 with a well field zone of about 3.7 km 2 with 21 boreholes. The land use is mostly agricultural, with small villages. The Havelse waterworks catchment area covers the lower part of the Havelse Creek catchment area. The geology at the depth of one meter below the surface is in the southern part of the area covered with a clayey till; the northern part is meltwater sand. Stretched SE-NW across the area is a large sand and gravel esker. Along the streams are postglacial freshwater deposits, and deposits from the Stone Age period near downstream part of the Havelse river.The primary groundwater reservoir in the area is limestone and chalk deposits from the Danian period. The limestone may be in hydraulic contact with overlaying meltwater sand. The surface of the limestone is located at about 20 m below sea level. The reservoir is generally well protected by the overlying clayey till, which is from 15e30 m thick in large regions of the catchment area. Running through the area along NeS direction is an area where the thickness of the clay is less than 5 m, and it may be totally lacking in some spots. This area may represent a 500-m-wide and buried valley filled with meltwater sand. The clay thickness is also reduced along the esker, to a layer five to ten metres thick in a 250-m-wide zone In the area there is a threat of pesticide contamination of the groundwater. South-east, in an area with thin clay layers above the primary reservoir, there have been a few findings of pesticides but in general the data coverage is limited. The primary reservoir is meltwater sand overlaying limestone and covered by 4e16 m of clayey deposits. The pesticide found is BAMd2,6 dichlorbenzamidedwhich is a metabolite of the herbicides dichlobenil or chlorthiamide. BAM is the greatest threat to groundwater quality at the moment in Denmark and the use of these herbicides is now prohibited.Similar vulnerable or very vulnerable areas are also found inside the catchment area and in the present well field zone. The vulnerability is here an expression of the thickness of the clay layer above the primary reservoir and whether the aquifers are unconfined or confined. A recent pesticide survey in five drilled wells, four dug wells and Havelse Creek show pesticides in two dug wells and one drilled well, but also in Havelse Creek, with all findings above the MAC value of pesticides used today.Groundwater is the ''backbone'' for drinking water supply, industrial supply and supply for aquatic environment in the greater Copenhagen area. In Denmark, 99% of the water supply is groundwater. Furthermore, most Danes agree that clean groundwater and drinking water has the highest priority of all environmental issues. Chemical treatment of groundwater is rarely accepted before it is supplied to the consumers.From a preventive groundwater protection point of view, the goal should be no pesticides in the groundwater above the maximum limit value. Perhaps the vulnerability of the area with respect to the deeper aquifers is less than for other Danish aquifers, but we do not know in detail which parameters or factors we should base such an assessment on. In fact, we know very little about the vulnerability of the deep groundwater in the Havelse area at the depths from which groundwater is abstracted. We do know that pesticides have a very low degradation rate (nearly none) once they have reached the anaerobic parts of the aquifer, which in this area is located a few meters below the surface.Bn construction processIn the Danish case study the water company responsible for decision making and actions towards well field protection is Copenhagen Energy representing the Municipality of Copenhagen.The construction process of Bayesian networks with stakeholder involvement followed a seven step procedure (see Fig. In Step 1: Define context (see Fig.  States are implemented in Bns for all variables. Step 6: Construct CPTsdconstruction of conditional probability tables (CPTs)dincludes a review of the networks at stakeholder meetings. Structural learning is encouraged as a method of consulting stakeholders in an interactive mode. Input from models and experts for CPTs is also part of this step. Bns should also be carefully checked for internal consistency at this stage. Finally, in Step 7: Collect feedback from stakeholders, stakeholders and general public opinions on the final network (perceptions, motivation etc.) are collected, and a conclusion based on the final Bn is drawn.Like the overall planning cycle PP modelling (Steps 1e7) is a cycle that may be circulated several times in a specific case study. In our case study at least three full rounds were done.The aim of the case study was to test the benefits of Bns as an EDSS with full stakeholder involvement. The scope was to identify instruments against pesticide threats, which could be implemented as part of groundwater protection based on voluntary agreements with farmers. In the present section we want to analyse to which extent Bns can be used as a decision support tool for water resource management, in what ways Bns are helpful for focusing on the primary task in temporary organisations (T-organisations)? We also want to evaluate what was experienced about efficient involvement of stakeholder groups in the decision-making process and the construction of Bns. What was learned from the process of involving stakeholders and citizens in T-organisations?Copenhagen Energy (CE), is the largest water supply company in Denmark. It supplies roughly one million inhabitants in the greater Copenhagen area with drinking water each day. CE operates Havelse well field together with 55 other large well fields located in northern and eastern Zealand (Fig. The final Bn and the outcome of the exerciseThe final results of the Bn construction process are shown in Fig. The results of the Bns analysis are described in more details in The general idea with the Bn for farming contracts was to analyse the effects of compensation payments to farmers for not using pesticides on agricultural fields. The higher the compensation level, the more farmers will join such a voluntary contract. However, farmers signing a contract will also try to optimise land use by growing crops more suitable for farming without pesticides, and this means that contracts will also affect crop rotation.Farming contract restrictions and crop rotation affect the farmers' bottom line, so to speak, and this, together with the compensation payment, has an impact on farm economics as a whole. The relationship between size of compensation and farmers' acceptance rate in this part of the Bn was provided by an expert The final Bn documents that compensation payment must be in the highest state of the variable ''compensation'', the rather costly compensations of DKK 4400 per ha/year, if at minimum a 95% probability for the state ''true'' of the safe supply should be achieved. This could be a relevant goal since clean groundwater is very important and also of limited amount in the capital area.For a compensation of DKK 500 per ha/year, only very few farmers (4%) would join voluntary farming agreements prescribing no pesticide application. For DKK 1000 per ha/year, a slightly larger fraction of 11% would join. At DKK 2500 per ha, nearly 50% would join, but their willingness to sign voluntary farming contracts is much less than evaluated by the expert For water quality the final Bn showed that the probability of polluting deep groundwater drops to below 5% with a compensation level of DKK 2500 per ha/year, given that only farming contracts are implemented (no removal of point sources). If both measures are taken the total effect (5% level) can be achieved for a compensation payment of DKK 1000 per ha/year (cost of removing point sources not included).Shallow groundwater has very high probabilities of pesticide content above the Maximum Allowable Content (MAC) (42% in current situation) and 33% (with farming contracts at DKK 1500 per ha/year). Similar results were the outcome for surface water. This is documented by findings of pesticides in shallow groundwater and in the Havelse River.The variable ''perception of vulnerability'' was included in order to communicate disagreement and a special uncertainty regarding the controlling factor ''vulnerability of the subsurface with respect to pesticide leaching'' Stakeholder involvement processThe case study had a project leading committee with four members: two from GEUS (project leader and secretary) and two from CE (project responsible for CE input and a process specialist in stakeholder engagement). The group first met when the case study was initiated in June 2001. In the following this group will be named the ''leadership group''. The group represented different perspectives about leadership, both from experiences with the field of project leadership but also experiences with stakeholder involvement in partnership building between CE and local communities.There was considerable scientific knowledge about groundwater problems and comprehensive groundwater modelling. By the start of the project we did not know anything about Bns or how to apply Bns in a proper way, and which capacity such models had. Before the primary task of testing Bns and involving stakeholders in the construction, the leadership group therefore spent some time with ''hands-on experiences'' with the software Hugin Researcher \u00d3 .The leadership group was a core group of the T-organisation, which only consisted of these four members in the period from June 2001 to November 2002. However, during this period the leadership group formed the core of the subsequent T-organisation by selecting a case study among different optional, balancing vision with strategy, looking for the whole picture, and drawing a road map for the process.The starting point was to identify stakeholders, additional domain experts and to list categories of water users, potential groundwater pollution sources, and authorities in the area: local waterworks, water consumers, farmers, industry, anglers, county and municipalities. Draft evaluations of formal institutional arrangements (groundwater laws, decrees etc.) and other institutional arrangements indirectly affecting groundwater (e.g. health and agricultural policies) were discussed in the leadership group.A broad range of various management instruments was evaluated (current groundwater user partnerships, innovative instruments for groundwater protection) and current management results (groundwater allocation and use, quality of shallow and deep groundwater etc.) were initially identified. However, at this point the informal institutional arrangements (e.g. cultural and social norms and historical perceptions) in the water sector in the case study was rather unknown, and a local water user partnership had not been established dealing with groundwater protection in the area. The leadership group in this initial phase developed a first Bn for the purpose of informing stakeholders and the general public about different management instruments and allow prediction of most likely management results (e.g. groundwater quantity and quality and exploitable resources). The preliminary network was made up of three main branches: Management actions against urban pesticide sources (removal of urban point sources, use of pesticides on public areas and use of pesticides in private gardens). Management actions against rural pesticide sources (instruments directed towards rural point sources, farm forestry, low use farming and set aside areas). Management actions related to groundwater use allocation (well field renovation).The purpose of this initial Bn was to allow an open dialogue at the subsequent kick-off meetings with stakeholders providing a broad perspective and debate about various water problems in the area.Another important decision taken by the leadership group was to identify and contract with a facilitator for the subsequent stakeholder meetings, which were planned for November 2002. A facilitator from the local joint municipality Agenda-21-Centre was contracted to run the meetings of the citizens' group (representatives of the general public).Two external subcontractors were contracted to deliver input for the Bn construction, e.g. farm economics 2003), value of biodiversity, land use, etc. The PP process was initiated by inviting all stakeholders and professional stakeholder organisations with a potential interest in groundwater protection in the specific area to a oneday workshop in October 2002. One result of this workshop was the formation of a professional, stakeholder-working group with 10 institutions in addition to the members of the leadership group and the facilitator. At the kick-off meeting with the professional stakeholder the initial Bn was shown but not discussed in details.Since the case study was linked to a specific local area, the Havelse well field capture zone area, and because involvement of landowners and farmers is vital for groundwater protection, the leadership group decided to involve the local citizens in a citizens' group, a parallel working group to the group of professionals. A public meeting was arranged in November 2002 in the local community house. Invitations were distributed to around 1100 local households, and the meeting was announced in the local newspaper. Around 100 persons, and the local TV, showed up for this meeting. At the end of the meeting a local citizens' group of 9 persons was formed. The stakeholders were asked to present issues and problems they found important in relation to groundwater protection.The initial Bn was not presented at this meeting. Instead, a small multi-choice test had been created that tested the knowledge of the locals. A sheet containing 15 questions and answers was to be filled in and returned before the correct answers were shown using an overhead projector and various charts. After a short collaboration in breakout groups, the various groups displayed their different suggestions for groundwater protection and land use at Havelse well field catchment area.The representatives of stakeholders were organised in two different groups, the ''professional'' stakeholder group, and ''local citizens'' stakeholder group. The idea behind the two groups was the perception that the professional stakeholders were already deeply involved in groundwater management and protection, whereas local citizens might have another starting point for their involvement in groundwater management and protection.Three workshops in the professional group were held during 2003. At the first workshop the main topic was to get stakeholder opinions on roles and responsibilities (in order to map the formal institutional arrangements) and on consequences of different management instruments in active groundwater protection. This had the dual purpose of creating a common understanding (not consensus) within the group of the different responsibilities and viewpoints among the stakeholders (a first input to informal institutional arrangements). The following two workshops were dedicated to Bn development and included inputs from external experts, especially from the expert in agricultural economics (including an initial picture of other institutional arrangements e.g. agricultural policy indirectly affecting groundwater).The institutional framework defines the management instruments to be used and management instruments impact management results. Based on management results there is a feed back into the system for reinforcement, adjustment and refinement of institutional framework. Bns are an excellent tool for this dialogue process, because it is possible gradually to develop a network consisting of the most important variables (and states) and to establish links and conditional probability tables, which quantify relationships between management instruments and management results. And because the Bns are developed with active involvement of stakeholders, it is also possible to incorporate the possible effects of informal institutional arrangements (e.g. farmers that build up a social norm that voluntary farming contracts against pesticides should not be contracted).The three workshops were followed by individual meetings with Frederiksborg County Council, Sjaellandske Familielandbrug (Zealand Farmers' Union) to collect more data for the Bns, and to discuss the Bns in greater detail.The citizens' group met five times in the first half of 2003. The idea was to give the group the opportunity to develop its own identity without being influenced by professional stakeholders. A facilitator was attached to the citizens' group to help organise meetings and to produce a newsletter with information about progress in the project to the local community. Two newsletters were published in the first half-year of 2003 and a third newsletter July 2004 introducing the members of the citizens' group to the general public and bringing papers related to groundwater protection, water supply and water quality. The newsletters were distributed to 1000 households in the local area. GEUS and CE were invited to participate in two of the five meetings to answer specific questions from the citizens and to introduce and discuss the development of the Bns.At the final joint meeting in March 2004, the stakeholder groups were asked to comment on the involvement process on the basis of four questions: (1) Is there a need for further initiatives for the protection of groundwater? (2) How have you experienced project progress (Bns, citizens' meeting, workshops, citizen groups, newsletter, individual meetings, etc.)? (3) How should stakeholders be involved in future in, for example, active groundwater protection and the establishment of wetlands? and (4) Other comments to the process?Positioning of stakeholders towards deep groundwater qualityBoth the professional stakeholder group and the citizens' group clearly expressed their opinions and concerns about water resource management and groundwater situation. This initial stage of stakeholder involvement was quite successful, among other things because it resembles the Danish administrative system built on information dissemination and public hearings, and the fact that groundwater abstraction, flooding problems and a planned restoration of a wetland area had been hot topics in the area during the past couple of years. The Havelse well field was established in 1955e1956 and is on CE's investment plan from 2002 to 2006. Abstraction has stopped in 2001 due to water quality problems as well as inundation problems. The well fields siphon system will be altered to a system based on individual centrifugal pumps in each new borehole. The well field will most likely be moved to a new location due to the County's plans to establish new wetlands in the area. Re-establishment of wetlands involve removal of drains and inundation of large areas in the downstream part of the river Havelse in order to reduce the total discharge of nitrates from the catchment to the Fiord of Roskilde.The initial involvement of stakeholders during October and November 2002 were followed by half-day workshops with presentations and discussions in the first half-year of 2003. In the process it became clear that it is important to use facilitators to run workshops because they are considered neutral actors and are able to mediate between the parties involved while the project leadership group members are considered as stakeholders as well, especially when dealing with politically sensitive issues. The issue of what affects the quality of deep groundwater is such a sensitive issue, because it is very closely interlinked with drinking water quality (99% of the drinking water supply is based on groundwater in Denmark). Similarly, it is the experience from the project that active involvement of stakeholders in the construction of Bns and decision-making processes necessitates the existence of clearly defined rules for participation.Workshops and meetings with representatives from several interest groups tended to become a political and tactical arena rather than a forum of open discussion and were therefore not very useful for more detailed discussions. This raised the question to which extent the professional group was dominated by individual motives, interest group relations or coalition groups' relationships that developed during meetings.If we compare our results with an ordinary SWOT (Strengths, Weaknesses, Opportunities and Threats) analysis for public participation, then we did not experience particular strengths such as ''Brings our technical knowledge from the public and others'' or ''Allows the public to understand the system better''. We did, however, identify issues not thought of beforehand, the barriers towards voluntary farming contracts, which we had not expected. We had solely based our work on expert assessments (e.g. the report from the Royal Veterinary and Agricultural College).An important statement about stakeholder engagement based on our Danish case study experience is as follows: The use of Bns goes beyond information and consultation and requires the full involvement of stakeholders during the implementation phase of a given action plan.Bns are excellent for structural learning, strategic considerations, integration and breakdown of barriers between different domains. However, they can be difficult to understand for non-experts, they are not useful for implementation of specific protection zones.In the comments from the citizens' group, the group believes that Bns are necessary tools in making complicated decisions. They also say that it is essential that specialists prepare the Bns, but that it is also an absolute necessity that the citizens take part as well.The citizens' group felt that it was a shame that the outcome of the activities concerning stakeholder involvement were not larger. The group's first impression was that the specialists did not want to involve themselves with the citizens.At the final joint workshop in March 2004 only two groups presented their comments: the citizens' group and the farmers' organisations. Both groups positioned themselves against the leadership group. The farmers' organisation contested the need for further groundwater protection on the large scale. The organisation did, however, agree on establishment of larger well field zones and active groundwater protection in vulnerable zones. In other words a more ''pinpointed'' groundwater protection towards specific areas.The citizens' group had decided to continue with a group of six or seven people under the title ''clean water''. This group had been reduced to the members who had expressed interest in groundwater protection. The group felt they were very much in opposition to the official representatives (CE and GEUS), the county and other authorities to form ''common front'' actively opposed by the interests of the citizens' group. Old stories were given new life, the group felt that everything had been arranged in advance between the official representatives and the other authorities participating in the project, and that the citizen's group would never stand a chance anyway, no matter what.The citizens' group never made it to a real performing phase. As a member of the group said it: ''Yes, it has been nice, but also highly frustrating because the meetings lacked structure and trailed off into a lot of local and personal problems''. It was apparent, seeing the attempts of the meeting attendees to form a group, that all attempts to lead the group to a greater degree were futile. There was a strong group process that had its own inner logic, and in contrast to the professional group, where members were representatives of organisations, the members of the citizens' group needed to achieve a kind of consensus (at least that was what they thought). First at the very last meeting in the citizens' group, the group seemed to acknowledge the leader they had selected (an organic farmer). For some reason it was difficult for the citizens' group to take follow their selected leader.Finally, two very important messages for the subsequent WFD design phase came out of the joint meeting in March 2004. The farmers' organisation (the strong leader of interest group against farming contracts) agreed that voluntary farming contracts is not the way forward, instead delineation of vulnerable groundwater areas should be the way forward and the waterworks should buy those vulnerable areas for groundwater protection. They also told us that use of Bns was not sufficient for the subsequent design phase and that more detailed modelling should be performed, based on comprehensive modelling. In spite of a lot of frustration, there was a clear outcome of the engagement of stakeholders.Based on the voiced and written feedback from stakeholders, the Bns were adjusted with an additional variable, the ''perception of vulnerability'', in order to incorporate the splitting into the final networks, by an additional ''uncertainty variable''.Conclusions", "conclusions": "To be effective, groundwater protection strategies must win broad-based support from stakeholders to be effectively implemented. But they must also not fall into the trap of endless consultation at the expense of action. So mechanisms for negotiation and managing conflict are an important ingredient.Bayesian networks comprise a tool that enables and supports decision makers to make rational and informed choices between alternative actions, which are based on agreed policies, environmental impacts and social and economic consequences. Bns are excellent tools for quantification of the whole picture, e.g. economics, hydrology, hydraulics, environmental and sociological impacts of various instruments and offers a transparent, inclusive, coherent and equitable methodology for dealing with groundwater management that incorporates both formal and informal institutional arrangements (including social norms) of different stakeholder groups.Bayesian networks allow stakeholders divergent values, interests and beliefs to be surfaced and negotiated in participatory processes for areas where conventional physically based groundwater models are insufficient due to lack of data, knowledge, or mutual trust between parties. Social and ecological issues can be incorporated and coupled with hydrology. Bns are excellent for strategic considerations and dialogue. Expert knowledge and data can be combined. Algorithms for structural learning based on data are available for analysis of complex systems, or in cases where data sets are incomplete. Finally, uncertainty can be dealt with in a very practical and transparent way.The Danish case study has documented that active involvement of stakeholders in construction and validation of Bns are not only possible, but are imperative for a proper construction of variables, selection of states, identification of links and evaluation of numbers for the conditional probability tables. The real strength and opportunity of Bns are to use this tool in a participatory process in an interactive dialogue and negotiation process.Hereby, public participation and Bns construction did make use of local and citizens' knowledge not known by the agency and the experts. It did encourage diverse perspectives and helped identify important issues not originally thought of. It did enable a better and broader evaluation of the water issues. It did not only integrate the various natural and social science issues (hydrology, economy, ecology and informal institutional arrangements), but also provided the opportunity for an open debate.Public participation can be weakened by a lack of resources (time, money, staff), a lack of rules of participation, a lack of in-depth involvement of authorities, a lack of hands-on use of Bns for the stakeholders, and a lack of professional supervision of the process. All these threats were at some time present in the Danish case. In contrast to the professional stakeholders, the citizens' group did not participate as much as they wanted to in the Bn construction but, in the end, they participated in the evaluation of the outcome and provided valuable input to data collection, unfolded uncertainty issues not incorporated, etc. Even though there was a lack of rules for the participation, much was learned from that case study experience. A better training of stakeholders in the use of the tool would have been an advantage, because it requires a new way of thinking about the system to use Bns and probabilistic reasoning. It could also have been helpful with a bit more professional supervision of the stakeholder involvement process, establishment of working groups, facilitation of meetings, etc. These concerns call for the importance of carefully preparing a stakeholder involvement plan that addresses all these issues: rules of the game, mission statements for working groups, time schedule for important milestones, allocation of sufficient resources, etc.There are some pitfalls that should be avoided when using Bns. Bns are difficult to understand for non-experts without any training. Bns can be malconstructed and should be reviewed by an experienced Bn expert for consistency. Bns require expert input and should not be constructed for domains without such experts on board. Validation with participation of experts, stakeholders and citizens are imperative in order to increase the credibility of Bns. Skills in communication, organisation of participatory processes and practical psychological insight into group relations and human behaviour are vital.Experts and non-expert stakeholders do not necessarily agree, as happened in the Danish case study where the farmers and hydrologists disputed the degree to which pesticide application affected the quality of deep groundwater. However, instead of selecting one opinion or another, the decision was made to include both in the networks through the addition of an extra variable with two states to represent the two different opinions. By adopting this course it was possible to view the results from either point of view, accepting the reality of the situation, not becoming mired in an insoluble conflict, and laying the foundations for future compromises.", "SDG": [6]}, "social_equity_shapes_zone_selection_balancing_aquatic_biodiversity_conservation_and_ecosystem_services_delivery_in_the_transboundary_danube_river_ba": {"name": "Social equity shapes zone-selection: Balancing aquatic biodiversity conservation and ecosystem services delivery in the transboundary Danube River Basin", "abstract": " produced by the Artificial Intelligence for Ecosystem Services (ARIES) platform (with ESS supply defined as carbon storage and flood regulation, and demand specified as recreation and water use). These are then used for (iii) a joint spatial prioritisation of biodiversity and ESS employing Marxan with Zones, laying out the spatial representation of multiple management zones. Given the transboundary setting of the Danube River Basin, we also run comparative analyses including the country-level purchasing power parity (PPP)adjusted gross domestic product (GDP) and each country's percent cover of the total basin area as potential cost factors, illustrating a scheme for balancing the share of establishing specific zones among countries. We demonstrate how emphasizing various biodiversity or ESS targets in an EBM model-coupling framework can be used to cost-effectively test various spatially explicit management options across a multi-national case study. We further discuss possible limitations, future developments, and requirements for effectively managing a balance between biodiversity and ESS supply and demand in freshwater ecosystems.", "keywords": "Ecosystem-based,management,Species,distribution,model,Ecosystem,services,ARIES,Marxan,with,Zones,Model,coupling", "introduction": "In light of the strong anthropogenic pressures faced by freshwater ecosystems, such as habitat degradation, pollution, flow regulation and water extraction, freshwater biodiversity is currently facing a crisis with observed population declines for many species (Ricciardi and Rasmussen, 1999;Loh et al., 2005;. Given both the growing awareness of the importance of ecosystem services (ESS) provided by freshwater ecosystems Dudgeon et al., 2006) and their ineffective protection (Aylward et al., 2005), there is increasing recognition that new management schemes are required for safeguarding freshwater ecosystems and the ESS they deliver to people. For instance, the European Union has committed to designing a network of green and blue infrastructure (Dudgeon et al., 2006), highlighting the policy application when assessing the balance of biodiversity and ESS.(Maes et al., 2012)One such management scheme is Ecosystem-Based Management (EBM), with its central principle to concurrently consider biodiversity and human society as integral parts of the ecosystem (Long et al., 2015;. The key goal of EBM is to protect and restore ecosystem resilience, while maintaining biodiversity and the provision of ESS. EBM principles were developed in the 1970s Langhans et al., in press), but moving from their conceptual to practical implementations has been a major challenge (Caldwell, 1970)(Slocombe, 1993;. While the concept of EBM has a long tradition in terrestrial Leslie and McLeod, 2007) and marine realms (Slocombe, 1993), EBM in freshwater systems has been targeted somewhat more recently (Botsford et al., 1997). The complexity of developing EBM measures for freshwater ecosystems can be exemplified by looking, for instance, at Europe's second longest river, the Danube. The Danube comprises a highly stressed and highly vulnerable system given its high level of socio-economic usage (Falkenmark, 2003). The high use of ESS, especially in a transboundary setting with 19 involved countries whose demands partially conflict (Hein et al., 2018)(Sommerwerk et al., 2010;, can carry potentially high costs for biodiversity (e.g., navigation and locks have likely detrimental effects on fish migration). However, because ESS are partly based on biodiversity itself (e.g., food webs and food security, or recreation potential and aesthetic value), the value to society of protecting biodiversity should be evident, particularly as science identifies and demonstrates these linkages Hein et al., 2018).(Mace et al., 2012)This raises two questions: (1) how a balance between biodiversity and ESS could be achieved , and more importantly, (2) could such a balance be tested (or simulated) in any given area of interest? A first general and useful approximation can be achieved, for instance, using linkage frameworks (similar to e.g., the Driver-Pressure-State-Impact-Response, DPSIR framework; EEA, 1999). As shown by (Mart\u00ednez-Fern\u00e1ndez et al., 2014) in the marine realm, management measures can be achieved by assessing ecosystem complexity and evaluating impact chains across multiple marine sectors and activities. Such an approach can help provide an overall picture of possible management options, for example, regarding sustainable fisheries. Linkage frameworks, however, lack the spatial aspect that is crucial in riverine systems given their longitudinal connectivity. This is especially true for transboundary river basins such as the Danube, where the locations of specific protection or management actions are important from a multi-national management perspective.Knights et al. (2013)Recently, ) reviewed the requirements for a successful EBM planning process, and highlighted several needed components: knowledge of biodiversity, ESS, and deficits in reaching their targets, plus external scenarios, management strategies and spatial planning, with the outcome being a spatial optimisation plan that informs EBM implementation. The authors laid out the theory behind a workflow consisting of three elements: the spatial representation of (i) biodiversity, (ii) ESS, and (iii) the concurrent spatial prioritisation of biodiversity and ESS supply and demand. Langhans et al. (in press propose three versions of this workflow, which they term \"ultralight\", \"light\" and \"full\" versions. The ultralight version considers the spatial representation of the three elements at the present time step. Beside evaluating the status quo, it allows the user to test how biodiversity and ESS could be balanced by management zones sensu Langhans et al. (in press), given specific management targets (Fig. Abell et al. (2007)). The light workflow adds complexity to the ultralight one by considering different management strategies that are evaluated and ranked according to relevant criteria, such as proximity to management targets, deficits in achieving biodiversity and ESS targets, and evaluating effectiveness, efficiency, and social equity of management scenarios. Here, effectiveness refers to which degree management impacts the ecological outcomes, whereas efficiency refers to the benefit-to-cost ratio. Social equity, in turn, refers to the fair treatment of individuals or groups 1. Finally, the full workflow adds data-driven scenarios (e.g., potential alterations in biodiversity and ESS under climate or land use change impacts) to the light version, allowing for detailed estimates of how and where spatial biodiversity and ESS management zones could be planned. As opposed to a binary \"protect vs. non-protect\" scheme when laying out reserve network plans (Langhans et al., in press), zones with varying intensities of conservation and anthropogenic use decrease the area required for strict conservation, while still meeting species protection targets (e.g., by up to 62% as shown by (Margules and Pressey, 2000) in the Iberian Peninsula). The three versions differ in their complexity, and hence also in their data requirements. While the ultralight workflow can be achieved with data that can be readily developed (spatial biodiversity and ESS estimates), the light and full versions require additional data and information on external scenarios, as well as stakeholder involvement Hermoso et al. (2016).(Langhans et al., in press)Our aim in this paper is to provide a spatially explicit and generic model-coupling framework that can be adapted to virtually any region, following the theoretical workflow proposed by ) and shown in Fig. Langhans et al. (in press. For demonstration purpose, we illustrate the ultralight workflow, and discuss data requirements and future developments required for light or full applications.1In a first step, we assess the spatial representation of both biodiversity and ESS supply/demand. In a subsequent step, we perform a spatial prioritisation, balancing biodiversity and ESS into different management zones according to user-defined management targets. We illustrate the EBM model-coupling framework for the Danube River Basin. We use the potential distribution of 85 fish species as modelled by spatially explicit Bayesian hierarchical species distribution models as a surrogate for biodiversity. ESS are represented by four layers depicting terrestrial carbon storage and flood regulation supply, and recreation and water use demand, modelled using the ARtificial Intelligence for Ecosystem Services (ARIES) platform (Villa et al., 2014;. We then use the systematic conservation planning software Marxan with Zones Mart\u00ednez-L\u00f3pez et al., 2018)(Ball et al., 2009; to spatially allocate the distribution of biodiversity and ESS into three freshwater conservation management zones defined by Watts et al., 2009): a conservation zone with a strict emphasis on biodiversity protection, a management zone allowing for greater use of ESS, and a critical (buffer) zone allowing for an intermediate use of ESS that simultaneously provides connectivity between conservation zones. We also include a production zone, defined as a zone where increased water use is allowed, while not compromising biodiversity conservation Abell et al. (2007). We further simulate possible multi-national constraints in establishing potential zones for EBM by employing purchasing power parity (PPP)-adjusted gross domestic product (GDP) per capita (i.e., GDP adjusted for differences in relative spending power between different countries that result from different prices for goods) and the relative share of each country's area of the Danube River Basin within the spatial prioritisation.(Hermoso et al., 2018)This proof-of-concept, as demonstrated for the transboundary Danube, aims to highlight how multiple model-coupling elements link to each other. We also discuss further possible extensions and limitations of the approach towards the larger goal of more integrated freshwater management that balances biodiversity and ESS.", "body": "In light of the strong anthropogenic pressures faced by freshwater ecosystems, such as habitat degradation, pollution, flow regulation and water extraction, freshwater biodiversity is currently facing a crisis with observed population declines for many species One such management scheme is Ecosystem-Based Management (EBM), with its central principle to concurrently consider biodiversity and human society as integral parts of the ecosystem This raises two questions: (1) how a balance between biodiversity and ESS could be achieved Recently, Our aim in this paper is to provide a spatially explicit and generic model-coupling framework that can be adapted to virtually any region, following the theoretical workflow proposed by In a first step, we assess the spatial representation of both biodiversity and ESS supply/demand. In a subsequent step, we perform a spatial prioritisation, balancing biodiversity and ESS into different management zones according to user-defined management targets. We illustrate the EBM model-coupling framework for the Danube River Basin. We use the potential distribution of 85 fish species as modelled by spatially explicit Bayesian hierarchical species distribution models as a surrogate for biodiversity. ESS are represented by four layers depicting terrestrial carbon storage and flood regulation supply, and recreation and water use demand, modelled using the ARtificial Intelligence for Ecosystem Services (ARIES) platform This proof-of-concept, as demonstrated for the transboundary Danube, aims to highlight how multiple model-coupling elements link to each other. We also discuss further possible extensions and limitations of the approach towards the larger goal of more integrated freshwater management that balances biodiversity and ESS.Material and methodsStudy areaWe used the Danube River Basin, draining an area of 807,827 km 2 Biodiversity modelsFish survey data consisted of detection/non-detection information for 85 fish species (Supporting Information Table Within each planning unit, we extracted climatic The observability process uses the number of sampling visits within each planning unit to estimate the probability of observing a given species, given the species' presence in a planning unit. Here, we assume that if the species was observed at least once during multiple visits in a planning unit, the habitat is deemed suitable and the absence of the species during other visits in this planning unit is due to imperfect detection.We ran three Markov chain Monte Carlo (MCMC) simulations with 200,000 iterations each, a burn-in phase of 50,000 iterations and a thinning interval of 10. Model convergence was assessed by the multivariate potential scale reduction factor (MPSRF; Ecosystem service modelsWe used the ARtificial Intelligence for Ecosystem Services (ARIES) platform The flood regulation model (Fig. The recreation model (Fig. The water-use model is based on the Corine Land-Cover dataset (EEA, 2012), and uses the expert-derived lookup table from A small fraction of sub-catchments in Moldavia and Ukraine had missing ESS data (a total of 314 out of 7376 sub-catchments). To avoid the potential omission of these sub-catchments, we used the corresponding ESS information from the surrounding subcatchments to fill the gaps. We first aggregated each ESS layer from a 1 km to a coarser 10 km spatial resolution, using the average of 10 \u00d7 10 grids (r.resamp.stats function in GRASS GIS; Spatial prioritisationWe used the conservation planning software \"Marxan with Zones\" Marxan with Zones, an extension of Marxan, additionally aims to minimise the overall costs of the zoning plan, while ensuring that the predefined feature targets are met For the species predictions, we transformed the predictive posterior mean probability maps from SDMs into a semi-binary scheme using TSS as a threshold We specified the 85 fish species and four ESS types as features for which we could set targets in the spatial prioritisation, since our aim was to balance the spatial representation of both biodiversity and ESS supply and demand For each of the four ESS, we set a FPF of 1, and a target of summing up to 30% across the four zones (Table Marxan with Zones used the identical spatial representation and longitudinal connectivity between planning units as in the species distribution models (see Supporting Information for details). The Boundary Length Modifier (BLM), a dimensionless parameter that defines the level of aggregation of planning units Cost factors for the spatial prioritisationWe specified six cost factors to be applied for a planning unit to be considered in a spatial plan (Fig. Human influence (Fig. The GDP and relative national area in the Danube River Basin (Fig. Table 1Targets for all features (biodiversity defined as 85 fish species, and four ecosystem services) per management zone used in the spatial prioritisation analysis. zones given these additional costs. We first extracted the average PPP-adjusted GDP per capita (in 2011 U.S. dollar values) at a 5 degree resolution for the year 2015 Management zone targets [%]The GDP and relative area per country metrics can have a strong influence on spatial plans, as country borders might stand out as zone borders as well. To avoid undermining the influence of longitudinal connectivity, we balanced the weights for GDP and relative area with those for BLM. This yielded a stronger relative influence of longitudinal connectivity on the spatial allocation of zones, while still accounting for macro-scale patterns of GDP and relative area within the Danube River Basin. All costs were scaled from 1 to 100 to enable their direct comparison and to facilitate calibration of cost parameters.We acknowledge that we met these simplified assumptions for illustrating the model-coupling approach using a blank-slate example with a high level of flexibility to simulate different settings in the spatial prioritisation. For simplicity, we refrained from locking in any current, already established protected areas in the spatial plan. The inclusion of established protected areas decreases this flexibility. Additionally, current protected areas have not necessarily been optimised considering the 85 fish species we use as targets. They have generally rarely considered freshwater biodiversity, but have been established regarding multiple (terrestrial) species groups, biodiversity indices such as species richness, or land-cover types as surrogates, in combination with financial constraints Spatial prioritisation using Marxan with ZonesWe ran two spatial prioritisation analyses with the above settings (see Table ResultsBiodiversity and ESS modelsModel validation of SDMs yielded high evaluation scores across all 85 species (Supporting Information Table Regarding the ESS models, greater areas of carbon storage are evident in forested areas of the Carpathian Mountains, the Dinaric Alps, and the Alps that form the western headwaters of the Danube River (Fig. Spatial prioritisationThe best solutions among the 100 repetitions for both \"basic\" and \"GDP-area\" runs showed that all feature targets were achieved for biodiversity and ESS (Fig. Predefining costs and weights for GDP and percent area within the Danube River Basin (Supporting Information Table DiscussionEcosystem-based management (EBM) is considered to derive effective and sustainable management options for freshwater biodiversity, while concurrently considering the needs of biodiversity and human society The basic and GDP-area analyses led to clear differences in the spatial configuration of management zones. Interestingly, a reasonable target of 1/3 of each feature could be reached even with (simulated) national constraining costs, i.e., where high-GDP and high-percent area countries are assumed to devote more resources towards conservation than their counterparts.In the GDP-area analysis, we gave an a priori priority to focal conservation zones and critical management zones to be allocated within high-GDP countries, and countries that cover a larger fraction of the river basin (expressed using the costs). This evoked a change in zone allocation, e.g., in the Upper Danube, where production-and catchment management-dominated zones in the basic analysis were replaced by focal conservation, critical and catchment management zones in the GDP-area analysis (Fig. In the GDP-area analysis, a fraction of the Danube River Basin was left outside the designated management zones. This indicates that the feature targets were met given a different spatial configuration where this fraction was not required. In other words, the GDP-area costs outweighed the cost regarding the area of the planning units, recreation, water use and the human influence index, and indicates how the spatial configuration of management zones can be modified to reach the preferred spatial distribution of zones.The practical implementation of the \"ultralight\" workflow laid out by Considering our two analyses as a demonstration of the ultralight workflow, we acknowledge the hypothetical nature of our study that illustrates the challenges of the approach. Varying targets and costs regarding effectiveness and efficiency are required to yield the optimal spatial representation of management zones in the Danube, along with the spatial configuration of the current protected areas. Moreover, given projected climate and land use impacts on the environment, future potential changes in biodiversity and ESS should be considered in future work The \"light\" and \"full\" frameworks involve additional aspects for effectively pursuing EBM that are not covered in our example To inform decision making geared towards successful application of EBM in a way that builds consensus among all countries and stakeholders, while accounting for social equity While the species distribution models and ecosystem service models use \"hard\" data for creating their outputs, the spatial prioritisation (i) relies on modelled outputs from SDM and ESS models, and (ii) can be considered subjective regarding its parameters (as also in the spatial planning literature). Spatial planning requires the accommodation of multiple assumptions to yield a solution closest to optimum. For instance, while the targets are met, it is crucial that the spatial allocation of the different zones meets expectations and is sensible from both ecological and conservation perspectives (e.g., a focal conservation zone should be located upstream of a catchments management zone, as described by Conclusions", "conclusions": "This quantitative model-coupling framework for EBM , here demonstrated for the Danube River Basin, shows how biodiversity and ESS estimates can be jointly simulated in any area of interest, given the requisite data and models. Based on this generic model-coupling workflow, EBM provides an iterative process that can be incorporated, including stakeholder involvement and ultimately scenario development. Within a flexible framework, such simulations can be vital to communicate biodiversity and ESS targets regarding effectiveness, efficiency, and social equity, especially in transboundary regions such as the Danube River Basin. In a wider context, by following a consistent theoretical framework, the model-coupling approach is applicable to virtually any region given the basic requirements of species point occurrences and freely available, global environmental predictors for the SDMs and ESS models that can be calculated in ARIES (Langhans et al., in press)(Villa et al., 2014;.Mart\u00ednez-L\u00f3pez et al., 2018)", "SDG": [6]}, "solving_conservation_planning_problems_with_integer_linear_programming": {"name": "Solving conservation planning problems with integer linear programming", "abstract": " Deciding where to implement conservation actions in order to meet conservation targets efficiently is an important component of systematic conservation planning. Mathematical optimisation is a quantitative and transparent framework for solving these problems. Despite several advantages of exact methods such as integer linear programming (ILP), most conservation planning problems to date have been solved using heuristic approaches such as simulated annealing (SA). We explain how to implement common conservation planning problems (e.g. Marxan and Marxan With Zones) in an ILP framework and how these formulations can be extended to account for spatial dependencies among planning units, such as those arising from environmental flows (e.g. rivers). Using simulated datasets, we demonstrate that ILP outperforms SA with respect to both solution quality (how close it is to optimality) and processing time over a range of problem sizes. For modestly sized quadratic problems (100,000 spatial units and 10 species), for example, a processing time of approximately 14 h was required for SA to achieve a solution within 19% of optimality, while ILP achieved solutions within 0.5% of optimality within 30 s. For the largest quadratic problems we evaluated processing time exceeding one day was required for SA to achieve a solution within 49% of optimality, while ILP achieved solutions within 0.5% of optimality in approximately one hour. Heuristics are conceptually simple and can be applied to large and non-linear objective functions but unlike ILP, produce solutions of unknown quality. We also discuss how ILP approaches also facilitate quantification of trade-off curves and sensitivity analysis. When solving linear or quadratic conservation planning problems we recommend using ILP over heuristic approaches whenever possible.", "keywords": "Reserve,selection,Optimisation,Heuristics,Simulated,annealing,Prioritisation", "introduction": "Systematic conservation planning (SCP) describes the process of identifying and preserving areas of conservation value (Gaston et al., 2002;). It's goal is to ensure the long-term persistence of a wide range of biodiversity using an explicit, objective, transparent, repeatable and efficient methodology Moilanen et al., 2009. The stages involved in this process typically include quantifying conservation value spatially, setting explicit goals, identifying actions for achieving those goals, identifying combinations of actions that efficiently meet goals in the context of operational limitations (e.g. budgets), and implementing these actions (Pressey et al., 1993)(Margules and Pressey, 2000;Margules et al., 2002;. It is a flexible framework that has been applied to several types of conservation problem including, for example, protected area design Kukkala and Moilanen, 2013)(Klein et al., 2013;, the allocation of resources to deter illegal activity Beger et al., 2015), evaluating the performance of protected areas in the context of climate change (Plumptre et al., 2014)(Game et al., 2011;, terrestrial and marine zoning with multiple zone types Loyola et al., 2013)(Mazor et al., 2014;, and vegetation management Runting et al., 2015). Here, we focus on the problem of deciding where to perform actions in order to efficiently achieve conservation goals.(Levin et al., 2013)In SCP conservation value is quantified within a set of discrete spatial units (\"planning units\") that can either be arbitrary (e.g. a regular grid) or based on existing boundaries (e.g. administrative, ecological, watershed or land ownership boundaries). The value of a planning unit can be estimated in several ways depending on the problem and how much is known about the \"features\" of conservation concern, such as individual species, habitats or ecosystems. In the design of protected areas, for example, a common approach is to estimate the occupancy (presence or absence) of a population or species within each planning unit (e.g. Giakoumi et al., 2013;, though examples of other approaches include using species abundance estimates Plumptre et al., 2014), the area of a species habitat within a planning unit (particularly relevant to irregularly sized planning units), or combining measures of both occupancy and habitat condition (Williams et al., 2014).(Klein et al., 2013)The simplest problem formulation pertains to the binary decision of whether to include a planning unit in the selected set or not. Alternative formulations allow for multiple actions that can be implemented within a planning unit and the problem is to identify which actions to adopt and where. In either case, the value of the planning units under each action and with respect to each feature must also be quantified, along with some measure of the cost of implementing an action. Explicit targets must then be set to achieve conservation goals. In the case of protected area design, for example, the target could be a minimum habitat area falling within protected areas, potentially with different targets for each species of conservation concern. Although persistence of conservation features is a key principal of SCP , targets are generally not explicitly evaluated to determine probabilities of persistence or persistence times. Instead, targets are often set subjectively, through expert opinion (Margules and Pressey, 2000), community consensus (Levin et al., 2013), or are informed by legislation or policy (Game et al., 2011)(Giakoumi et al., 2013;.Runting et al., 2015)Usually there are insufficient resources to manage or protect all planning units, hence the need for an approach for selecting a subset of planning units for conservation purposes. This can be naturally expressed as an optimisation problem in which the goal is to achieve all targets for the least cost. Maximising the efficiency of management is important because conservation resources are scarce and achieving a high return on investment means that other resources can be allocated to conservation problems elsewhere. Inefficient management plans may also be too large and expensive to implement, and less likely to succeed in the face of competing interests .(Possingham et al., 2006, p520)Generally, these are not trivial problems for which optimal solutions can be found using complete enumeration or heuristics. The value of a planning unit is conditional upon the set of other selected planning units (the issue of \"complementarity\", , so planning units cannot be independently ranked. For anything other than the smallest problems (perhaps a few tens of planning units) the number of permutations of planning units is too large to be enumerated and other strategies are required to identify solutions. Many conservation planning problems involve thousands of planning units and sometimes hundreds of conservation features (species, habitats, ecosystem services, etc.).Margules and Pressey, 2000)There are two main approaches to solving optimisation problems of this type. First, integer linear programming (ILP), which minimises or maximises an objective function (a mathematical equation describing the relationship between actions and outcomes) subject to a set of constraints and conditional on the decision variables (the variables corresponding to the selection of actions to implement) being integers. Second, solutions can be found using heuristic methods such as simulated annealing (SA; , which iteratively, stochastically explore the state-space of the decision variables. There are numerous other heuristics (e.g. ranking procedures, genetic algorithms, and mixtures of these approaches) that could also be used. Here, we focus on SA because it is the most widely used heuristic in the conservation planning literature in the form of the conservation planning software Marxan Kirkpatrick et al., 1983)(Ball et al., 2009; and, unlike deterministic heuristics such as ranking, it is possible that SA could find an optimal solution to any problem.Watts et al., 2009)The discussion about the relative merits and disadvantages of linear programming versus heuristics in conservation planning spans more than two decades (Cocks and Baird, 1989;Underhill, 1994;Church et al., 1996;Pressey et al., 1997;Rodrigues and Gaston, 2002;. The key issues in this debate include the quality of the solution (efficiency), the size or complexity of the problem that can be addressed, and the computing time required to find a solution. The main concern with heuristics is that there is no guarantee of the quality of the solutions as it is possible for these approaches to find local rather than global minima solutions, and there is no measure of how far from optimality the solution is \u00d6nal, 2003)(Underhill, 1994;. In contrast, ILP is guaranteed of finding an optimal solution or a solution guaranteed to be within a specified shortfall (\"gap\") of the optimum.\u00d6nal, 2003)The main concern with ILP, on the other hand, is that it cannot be used to solve highly non-linear or complex objective functions and it is sometimes impractical for solving large problems. It is often straightforward to linearise quadratic objective functions using a combination of additional state variables and constraints (e.g. Billionnet, 2011, thereby facilitating optimisation using ILP. But it is often impractical to linearise objective functions that include more complex components, such as ecological dynamics. Perhaps the greatest advantage of SA is that it can be used to find feasible solutions to these more complex, non-linear objective functions (e.g. Billionnet, , 2013)). Further, tests carried out 20 years ago showed the limitations of ILP even on non-linear problems Westphal et al., 2007).(Pressey et al., 1997)Here, we describe how common conservation planning problems can be linearised so that efficient solutions can be found expediently using ILP. We describe the benefits that ILP methods provide with regard to quantifying trade-offs, flexibility in problem formulation and sensitivity analysis. We demonstrate that ILP methods consistently outperform SA with respect to both processing time and solution quality across a wide range of problem sizes. Our work is an improvement over commonly used heuristic approaches as it provides a performance guarantee and finds higher quality solutions considerably faster. Given the manifest benefits of an ILP framework for solving conservation planning problems we recommend using ILP when possible and heuristics only when necessary because of recent advances in algorithms and computing power.", "body": "Systematic conservation planning (SCP) describes the process of identifying and preserving areas of conservation value In SCP conservation value is quantified within a set of discrete spatial units (\"planning units\") that can either be arbitrary (e.g. a regular grid) or based on existing boundaries (e.g. administrative, ecological, watershed or land ownership boundaries). The value of a planning unit can be estimated in several ways depending on the problem and how much is known about the \"features\" of conservation concern, such as individual species, habitats or ecosystems. In the design of protected areas, for example, a common approach is to estimate the occupancy (presence or absence) of a population or species within each planning unit (e.g. The simplest problem formulation pertains to the binary decision of whether to include a planning unit in the selected set or not. Alternative formulations allow for multiple actions that can be implemented within a planning unit and the problem is to identify which actions to adopt and where. In either case, the value of the planning units under each action and with respect to each feature must also be quantified, along with some measure of the cost of implementing an action. Explicit targets must then be set to achieve conservation goals. In the case of protected area design, for example, the target could be a minimum habitat area falling within protected areas, potentially with different targets for each species of conservation concern. Although persistence of conservation features is a key principal of SCP Usually there are insufficient resources to manage or protect all planning units, hence the need for an approach for selecting a subset of planning units for conservation purposes. This can be naturally expressed as an optimisation problem in which the goal is to achieve all targets for the least cost. Maximising the efficiency of management is important because conservation resources are scarce and achieving a high return on investment means that other resources can be allocated to conservation problems elsewhere. Inefficient management plans may also be too large and expensive to implement, and less likely to succeed in the face of competing interests Generally, these are not trivial problems for which optimal solutions can be found using complete enumeration or heuristics. The value of a planning unit is conditional upon the set of other selected planning units (the issue of \"complementarity\", There are two main approaches to solving optimisation problems of this type. First, integer linear programming (ILP), which minimises or maximises an objective function (a mathematical equation describing the relationship between actions and outcomes) subject to a set of constraints and conditional on the decision variables (the variables corresponding to the selection of actions to implement) being integers. Second, solutions can be found using heuristic methods such as simulated annealing (SA; The discussion about the relative merits and disadvantages of linear programming versus heuristics in conservation planning spans more than two decades The main concern with ILP, on the other hand, is that it cannot be used to solve highly non-linear or complex objective functions and it is sometimes impractical for solving large problems. It is often straightforward to linearise quadratic objective functions using a combination of additional state variables and constraints (e.g. Here, we describe how common conservation planning problems can be linearised so that efficient solutions can be found expediently using ILP. We describe the benefits that ILP methods provide with regard to quantifying trade-offs, flexibility in problem formulation and sensitivity analysis. We demonstrate that ILP methods consistently outperform SA with respect to both processing time and solution quality across a wide range of problem sizes. Our work is an improvement over commonly used heuristic approaches as it provides a performance guarantee and finds higher quality solutions considerably faster. Given the manifest benefits of an ILP framework for solving conservation planning problems we recommend using ILP when possible and heuristics only when necessary because of recent advances in algorithms and computing power.TheoryILP formulations of conservation planning problemsAlthough there are numerous variations in the way that conservation planning optimisation problems have been posed (reviewed in where x i is a binary decision variable determining whether planning unit i is selected (1) or not (0), and c i represents the cost of planning unit i or, if the objective is simply to select the smallest number of planning units, then c i = 1 for every i. The parameter r ik is the contribution of planning unit i to feature k and T k is the minimum target to be achieved for feature k among all planning units. Because the objective function is linear with respect to the decision variables the RSP is straightforward to solve as an ILP problem.The RSP can be extended to accommodate further objectives relating to the spatial arrangement of planning units in order to facilitate solutions in which the selected planning units are more aggregated or connected. Such extensions involve the addition of quadratic expressions to the objective function (e.g. the interaction of two decision variables: x i x j ), which are problematic for ILP because the objective function is then non-linear with respect to the decision variables. The key to solving such problems using ILP is to linearise these quadratic terms, which is straightforward in the case of binary decision variables. Specifically, the quadratic term x i x j (x \u2208 {0, 1}) can be linearised in an ILP framework by replacing it with a new decision variable z ij and implementing the following additional constraints The first two of these constraints ensure that z ij cannot be 1 unless both x i and x j are also 1, and the third constraint ensures that z ij is exactly 1 if both x i and x j are also 1. The process of linearisation thus involves the addition of both decision variables and constraints. In practice, only a subset of these constraints needs to be implemented explicitly depending on whether the objective function is minimised or maximised and the sign of the quadratic term because the process of minimisation (or maximisation) inherently ensures some of the constraints are achieved. For example, in the case of minimisation of a negative term the decision variable must be forced to be less than a specified value but would not need to be forced to be greater than a specified value as this is achieved by the minimisation, hence only the first two constraints would be required. The simplicity of this linearisation technique belies its profound implications for solving conservation planning problems in an ILP framework Linearisation of the Marxan objective functionMarxan Specifically, the problem that Marxan solves is:where c i is the cost of selecting site i, N is the number of planning units, K is the number of features (e.g. species) and x i is the binary decision variable that determines whether a site is selected (x i = 1) or not (x i = 0). The objective function includes a cost penalty for selecting non-adjacent planning units based on a property quantifying the spatial relationship between two units (v ij ), such as the length of the shared boundary between them, and a scaling parameter b that is adjusted to control the strength of the penalty thereby influencing the aggregation of planning units in the solution Marxan does not strictly enforce constraints. Instead, it includes the constraints in the objective function using a \"shortfall penalty\" function, an additional penalty that is incurred whenever a target is not met The first term in the objective function is linear with respect to the decision variables x. The second term, however, is non-linear (quadratic) with respect to the decision variable (this is clearer if we rewrite the expressioncan be removed by adding b N j v ij to c i (as a pre-processing step), and x i x j can be linearised as described above. Specifically, for a negative quadratic term in a minimisation problem the first two constraints in Eq. ( The linearisation of each quadratic term involves the addition of one decision variable (z ij ) and two constraints. In the worst case scenario this could result in a total of N + (N \u2212 1) 2 decision variables and 2(N \u2212 1) 2 constraints in addition to the structural constraints defining the targets. However, the linearisation terms can be omitted whenever v ij = 0, which often applies to all nonneighbouring (or otherwise disconnected) planning units. In fact, in many applications the matrix v is sparse resulting in the addition of few constraints relative to the worst-case. Nevertheless, the dimension of the problem can still increase rapidly with N, which is why ILP software may be difficult to apply to very large quadratic problems (millions of planning units).The Marxan objective function allows for asymmetric penalties for non-neighbouring (or disconnected) planning units, i.e. if x i is selected the penalty for not selecting x j can be different than the penalty for not selecting x i if x j is selected. In the context of ILP the Marxan objective function can be expressed more efficiently as:where E defines the set of neighbouring planning units. Here, x i x j is 1 when x i = x j = 1 and 0 otherwise. If the matrix v is symmetric the set of neighbours E is defined according to the condition i < j, thereby ensuring the above expression is evaluated once for each pair of neighbours. If the matrix v is asymmetric then the set E is defined for each combination of i and j.A detailed case study of how this approach can be used to linearise the Marxan With Zones objective function ILP formulations of constraints for enforcing spatial dependencies among planning unitsSpatial dependencies among planning units often arise as a result of underlying ecological, social or geophysical processes that affect the features of management concern. The implication of these effects is that it may not be permissible to select one planning unit without also selecting a neighbouring planning unit, or a set of other planning units (e.g. river or coastal planning problems). Conversely, it could also be necessary to prevent neighbouring planning units from being selected (e.g. to prevent incompatible actions from occurring in adjacent units).In an ILP framework constraints can be used to enforce spatial dependencies in the selection of planning units, and provide exact control over these dependencies (i.e. dependencies specific to each planning unit). They can also be used to enforce both uni-and bidirectional dependencies.Directional, conditional dependency between planning unitsTo ensure that planning unit b is selected only if unit a is also selected, the following constraint is implemented:This constraint is directional because it does not prevent unit a from being selected if b is unselected. Importantly, this constraint can be repeated among many planning units to enforce more complex spatial dependencies. Consider a case where planning units are arranged in sequence along a linear feature, such as a river, and the flow direction of the river determines the spatial dependencies. Implementing the above constraint for each neighbouring pair of planning units will ensure that all planning units upstream of any given unit must also be selected if that unit is selected:A more complex problem involving directional, conditional dependencies occurs when a planning unit can have multiple upstream neighbours, such as an inland planning unit bordering multiple coastal planning units or a planning unit have m units above it in a watershed. The following constraint will ensure that planning unit a is selected only if at least one upstream neighbouring unit is selected:where {1 . . . m} defines the set of all the upstream neighbours to planning unit a. In contrast, the following constraint will ensure that planning unit a is selected only if all upstream neighbouring units are selected:conditional dependency among neighbouring planning unitsOne way of preventing isolated selected planning units is to make the selection of each unit conditional on the selection of a certain number of its neighbours. The following constraint ensures that a planning unit can be selected only if at least n neighbouring units are also selected:where {1 . . . m} defines the set of all the neighbours of planning unit a. Conversely, the following constraint ensures that a planning unit can be selected only if at most n neighbouring units are also selected:It may be desirable to prevent selected planning units from being clumped when the actions is a service that is intended to be widely distributed. The following constraint ensures that a planning unit is selected only if none of its' m neighbouring units are selected:Approaches to facilitating aggregation, compactness and connectivityPlanning unit selections resulting from simple objective functions often result in solutions that are highly fragmented and widely dispersed, yet spatial aggregation of planning units may be desirable for both ecological and management reasons. The ecological justification for aggregation often relates to the 'single large or several small' (SLOSS) debate We distinguish between aggregation and compactness. The former refers to the frequency of selection of adjacent planning units and increasing aggregation corresponds to a decrease in the number of spatially disjoint planning units. Compactness refers to the dispersion of selected planning units (how spread out the planning units are in space) and increasing compactness corresponds to reducing the total area within which selected planning units occur. The cost penalty for selecting non-adjacent planning units in the Marxan objective function determines the degree of aggregation among planning units but has a limited effect on compactness. Conversely, minimising the maximum distance between any two selected planning units There are several ways that aggregation and compactness can be facilitated (reviewed in One issue with these approaches is that they are non-specific in the sense that they continue to cause aggregation among clusters of planning units that may already exceed a minimum size threshold The term connectivity is sometimes used in a general sense to refer to any approach that increases the frequency of selection of adjacent planning units and could, therefore, apply to both aggregation and compactness. But connectivity can also refer to the specific problem of identifying a single, contiguous, fully-connected set of planning units that meet conservation objectives Unlike compactness and connectivity, which typically involve quadratic terms between all pairs of planning units (e.g. see Balancing trade-offs among multiple objectivesObjective functions can contain multiple objectives (also referred to as \"criteria\") that may not share the same units. The simplest approach to combining multiple criteria with different units in a single objective function is the \"scalarisation technique\", in which additional parameters control the relative weighting among the criteria. The weights can be adjusted by the decision maker to balance the contribution of the criteria. For example, in the Marxan objective function the cost objective might be measured as an area while the boundary penalty term has arbitrary units. The relative contribution of these two criteria is controlled by the parameter b (Eq. ( In general the different criteria are at least partially conflicting, implying that not every criterion can be optimised simultaneously. These trade-offs result in a Pareto frontier describing the set of every best compromise solution in the sense that every point of this set is optimal according to a specified set of preferences (relative weights) among the criteria. The role of the decisionmaker is to determine the relative importance of the criteria. A strong approach to informing this subjective decision is to evaluate the objective function across a range of weights and to plot the trade-off. We note that this approach applies regardless of whether ILP or SA is used to solve the problem.MethodsWe illustrate the relative performance of ILP and SA with respect to solution quality and computational time across a range of problem sizes (1E3, 1E4, 1E5 and 1E6 planning units, 10 species targets), for one linear and one quadratic problem (Eqs. ( The ILP was parameterised to stop processing when a gap \u22640.5% was achieved (i.e. when the current best solution was within 0.005 times the guaranteed lower boundary of the optimal solution). Marxan was run with three levels of replicates (the number of times the SA algorithm is independently repeated, with the best solution selected among all replicates) and total number of iterations among all temperature steps: 10 replicates and 1E6 iterations (the default), 100 replicates and 1E7 iterations, and 1000 replicates and 1E8 replicates. The quality of the SA solution (the 'gap') can be quantified when the same problem is solved using ILP. All processing occurred sequentially on a single desktop computer (4-core Intel i7 3.4 GHz processor) with 16 GB RAM that was not used for any other purpose during the trial to ensure comparable processing times.We illustrate balancing trade-offs among two objectives and the value of evaluating a range of objective weights using an ILP implementation of the Marxan objective function. The simulated data included 1E5 planning units with value data for 10 species and land costs (Fig. We evaluate the sensitivity of these solutions to uncertainty in the species data by decreasing all the species values by 10% and repeating the analysis based on these degraded values. Clearly, if the value of each planning unit to each species decreases then many more planning units are required to meet the targets and the total solution cost will be considerably higher than the expected (mean) approach described above.The SA and ILP problems were solved using Marxan (version 2.4.3) and Gurobi (version 5.6.2) respectively (see Appendix C for details). Marxan uses simulated annealing to stochastically explore solution space. Gurobi is proprietary software that uses several algorithms, including simplex and branch and bound algorithms, to solve linear programming problems and is guaranteed of finding optimal solutions given enough time.ResultsILP found higher quality solutions in less processing time compared to SA over the full range of problem sizes and for both linear and quadratic models (Fig. used to tune the SA algorithm) were increased, with associated marked increases in processing time. The three implementations of the SA algorithm we evaluated never matched the solution quality that was achieved by ILP. The increase in processing time for the smallest ILP solutions may result from additional automatic pre-processing that occurs within Gurobi that is omitted for larger problems.These results should also be interpreted in the context of what constitutes an important gap. SA consistently found \"good\" solutions to optimisation problems in approximately 12-24 h of processing time. The primary significance of the differences in processing times between these methods is the opportunities that fast solutions provide for quantifying trade-off curves and facilitating multi-objective optimisation in near real-time, thereby altering the way in which optimisation is used in the decision making process.In the second analysis there is a clear trade-off between increased aggregation and the solution cost (Fig. DiscussionWith advances in algorithms and processing power integer linear programming has become a flexible and efficient framework for identifying optimal or near-optimal solutions to conservation planning problems. Three benefits of ILP over simulated annealing are faster computational speeds, better solution qualities, and guaranteed quantification of solution quality. These advantages further facilitate the efficient and precise description of trade-offs among objectives, analysis of sensitivity of solutions to parameter uncertainty, and the exploration of multi-objective problems interactively in near real time. The primary disadvantage of heuristic methods is that the solution quality is unknown and the quality of the solution is sensitive to the way that heuristic algorithms are tuned (e.g. the number of iterations at each temperature step and the total number of replicates in SA).Comparisons in processing time between ILP and SA are often disingenuous as they fail to account for differences in the quality of solutions found. For both approaches processing times increase as the dimension of the problem increases (e.g. as the number of planning units and actions increases) and, for a given problem size, there is a trade-off between the processing time and the quality of the solution. ILP is usually characterised by a rapid increase in the quality of the solution in the early stages (often the first few seconds) followed by a period where the rate of further improvement is much slower. If allowed to run to completion ILP will find an optimal solution, though this can take considerable time. SA can be tuned in a variety of ways in an attempt to balance processing time with the quality of solution that is likely to be found. However, the only way of definitively quantifying the quality of the solution in an absolute sense is by comparing it to an ILP solution, so it is often not clear how to tune the SA algorithm. Indeed, one of the most pertinent criticisms of SA is that there is no practical guidance on how to parameterise the algorithm for different problem sizes to ensure a consistent quality of solution.ILP processing times may, however, increase substantially for larger or more constrained problems and a key advantage of heuristic methods is that they can find solutions to complex, non-linear problems that would be difficult or impossible to implement in an ILP framework. Adding complexity to objective functions to make them more relevant to real-world problems could have a profound Fig. One reason that conservation research may fail to trigger changes to management is that it is too narrow in scope, considering only a few dimensions of a problem. In contrast, real-world management must balance numerous competing objectives and interests. The optimal solution to a problem that considers only a few conservation objectives may provide little insight into solutions to problems that simultaneously consider multiple objectives, including social, economic and planning objectives. It is incumbent on conservation scientists to work closely with a broad range of stakeholders to bridge the research-implementation gap The assumptions that conservation planning problems are based on can also be obstacles to implementation. Although they are often not stated explicitly, common assumptions inherent in reserve selection problems are that all planning units are available for selection, that the costs associated with the selection of each planning unit are not dynamic, and that the benefits associated with the selection of a planning unit are guaranteed, are not dynamic, and are independent of what happens in other planning units. In reality, planning units may only become available for purchase or management asynchronously, the costs of planning units change, and the value of planning units to conservation is both uncertain and often subject to long lags (e.g. as a result of forest restoration). Considerable progress has been made in explicitly incorporating these sorts of complexities into an ILP framework Targets are often formulated as constraints because it is straightforward to solve a single objective function subject to constraints. The issues with implementing targets as constraints, however, are that: (i) it may result in a problem that is insoluble; (ii) the targets are often subjectively defined and treating them as strict thresholds belies the uncertainty associated with these values; and (iii) these strict constraints constrain the solution space, potentially precluding more efficient solutions that only just miss one or more targets. Future applications could adopt multi-objective optimisation approaches (e.g. the interactive method; There may be considerable uncertainty in estimates of the costs of actions and values of planning units. The risk in ignoring such uncertainties is that the optimal solution identified may ultimately violate some constraints, hence becoming an infeasible solution, or may be far from optimal An extension of worst-case optimisation involves specifying a different level of risk of violation for each constraint Conclusions", "conclusions": "This paper provides guidance on the conceptual and practical aspects of implementing a variety of conservation planning problems in an ILP framework. The three key benefits of ILP over simulated annealing are faster computational speeds, better solution qualities, and guaranteed quantification of solution quality. Despite these advantages and widespread application in operations research, the adoption of integer linear programming methods in conservation has been slow because early trials proved unsatisfactory. When solving linear and quadratic conservation planning problems we recommend the use of exact methods (e.g. integer linear programming) when possible, and heuristics only when necessary.", "SDG": [6, 14]}, "state_of_the_art_review_of_ant_colony_optimization_applications_in_water_resource_management": {"name": "State of the Art Review of Ant Colony Optimization Applications in Water Resource Management", "abstract": " Among the emerged metaheuristic optimization techniques, ant colony optimization (ACO) has received considerable attentions in water resources and environmental planning and management during last decade. Different versions of ACO have proved to be flexible and powerful in solving number of spatially and temporally complex water resources problems in discrete and continuous domains with single and/or multiple objectives. Reviewing large number of peer reviewed journal papers and few valuable conference papers, we intend to touch the characteristics of ant algorithms and critically review their state-of-the-art applications in water resources and environmental management problems, both in discrete and continuous domains. The paper seeks to promote Opportunities, advantages and disadvantages of the algorithm as applied to different areas of water resources problems both in research and practice. It also intends to identify and present the major and seminal contributions of ant algorithms and their findings in organized areas of reservoir operation and surface water management, water distribution systems, urban drainage and sewer systems, groundwater managements, environmental and watershed management. Current trends and challenges in ACO algorithms are discussed and called for increased attempts to carry out convergence analysis as an active area of interest.", "keywords": "Ant colonyoptimization,Review,Water resources,Watermanagement,Algorithm,ACO", "introduction": "As the spatial and temporal complexity of the water resources management and environmental problems increases, application of metaheuristic algorithms extends dramatically. Since the introduction of ACO , Different versions and refinements to the original ant algorithm are proposed and applied to solve various problems in water resources and environmental management. (Dorigo et al. 1996) presented a review paper on ACO for water resources systems analysis. Although his attempt is acknowledged, it does not fully address the advances made in continuous and multiple objective ACO, recent wide range applications of different versions of ACO, and the enhancements in their convergences and constraint handlings.Ostfeld (2011)Reviewing large number of peer reviewed journal papers and selected conference papers with less touched subjects, we intend to describe the theory of ant algorithms and critically review their state-of-the-art applications in water resources and environmental management, both in discrete and continuous domains. The paper seeks to promote its understanding, advantages, and disadvantages when applied to different areas of water resources problems both in research and practice.", "body": "As the spatial and temporal complexity of the water resources management and environmental problems increases, application of metaheuristic algorithms extends dramatically. Since the introduction of ACO Reviewing large number of peer reviewed journal papers and selected conference papers with less touched subjects, we intend to describe the theory of ant algorithms and critically review their state-of-the-art applications in water resources and environmental management, both in discrete and continuous domains. The paper seeks to promote its understanding, advantages, and disadvantages when applied to different areas of water resources problems both in research and practice.Ant Colony Optimization; an OverviewACO is a discrete combinatorial optimization algorithm based on the collective behavior of ants in their search for food. It is argued that a colony of ants is able to find the shortest route from their nest to a food source via an indirect form of communication that involves deposition of a chemical substance, called pheromone, on the paths as they travel. Over time, shorter and more desirable paths are reinforced with greater amounts of pheromone thus becoming the dominant path for the colony.The double bridge experiments clearly shows that ant colonies have a built-in optimization capability; whereby, an ant can find the shortest path between two points in their environment by the use of probabilistic rules based on available local information ACO is a metaheuristic algorithm in which a colony of artificial ants cooperates in finding good solutions to various static and dynamic optimization problems both in discrete and continuous domains. It allocates the computational resources to a set of relatively simple agents (i.e., artificial ants) that communicate indirectly by pheromone trail. It is a probabilistic multi-agent algorithm which uses a probability distribution to make the transition between iterations. Although, the ant algorithms are especially effective for solving discrete combinatorial optimization problems; their successful application in continuous search space has been widely reported.In ACO algorithms, the optimization search procedure is conducted by the number of artificial ants moving on a graph in the search space. The artificial ants are stochastic constructive agents that build solutions by moving on the construction graph. In fact, their constructive aspects distinguish ACO algorithms from other optimization algorithms. Each ant builds its own solution, or a component of it, starting from a predefined initial state. Parallel to making its own solution, each ant collects information on both the problem characteristics and its own performance. This information will be used to modify the representation of the problem, as seen by the other ants. Each ant builds a solution by moving through a finite sequence of neighbor states. Moves are selected by applying a stochastic local search policy directed by (1) ant private information and (2) publicly available pheromone trail and predefined problem-specific local information. The pheromone trail encodes a long-term memory about the entire ant search process, and is updated by the ants themselves. Differently, the heuristic value, often called heuristic information, represents a priori information about the problem instance or run-time information provided by a source different from the ants.ACO is structured into three main phases: (1) solution construction, (2) pheromone updating, and, (3) daemon action. In solution construction phase, artificial ants move through adjacent states of a problem according to a transition rule, iteratively building solutions. In the pheromone update phase the pheromone trail updates by both pheromone trail reinforcement and pheromone trail evaporation. Daemon actions is an optional step in the algorithm which involves applying additional updates from a global perspective, such as pheromone promotion, pheromone re-initiation, etc. A stochastic functional composition of the locally available pheromone and heuristic values is used to direct their search towards the most interesting regions of the search space. The stochastic component of the move choice in the decision space and the pheromone evaporation mechanism may avoid from an immature convergence on which all the ants may rapidly drift towards the same part of the search space. The balance between the exploration of new points in the state space and the exploitation of accumulated knowledge may be controlled by the level of stochasticity in the policy. Once a solution is built and the pheromone is updated, the ant will be deleted from the system (i.e., new ants are generated for the next iteration).Although, very many versions of ACO algorithms have been emerged, their main characteristics may comply with one or combination of ant system (AS) Continuous Ant Colony AlgorithmsAnt colony optimization was originally developed for discrete optimization problems. Its application in continuous domain started with discretization of the search space. Although, some advances in the discretization methodology, such as discrete-refining (DR) approach, to improve the performance of the original ACOs in continuous domains are reported The most recent approach to continuous problems proposed by As the number of available options at each construction step increases, the discrete probability distribution (in discrete ACO) approaches to a probability density function. In this case, instead of choosing a solution component from an allowable set, an ant samples the solution components from a continuous probability density function (pdf).To define a bi-modal pdf, they proposed a Gaussian kernel pdf, G i , as weighted sum of several one-dimensional Gaussian functions g l i (x):Here, k is the number of single pdfs, used to form the Gaussian kernel pdf at the ith construction step. As presented in Eq. 13, is parameterized with vectors of \u03c9 ! , \u03bc ! , and \u03c3 ! . In any kernel function, \u03c9 ! is the vector of weights, whereas \u03bc ! and \u03c3 ! are the vectors of means and standard deviations, respectively. The size of all these vectors is equal to the number of Gaussian functions constituting the Gaussian kernel (k).For more information the interested readers are referred to Multiple Objectives ACOA large selection of multi-objective ACO (MOACO) algorithms are listed and discussed by The existing MOACO algorithms which seek the Pareto fronts either use single pheromone matrix or benefit from multiple pheromone matrix. More detail information and taxonomy of the existing MOACO algorithms are available at Application of ACO Algorithms in Water Resource Management (WRM)A graph is required for a successful application of the ACO algorithms to combination optimization problems. Consider G=(D, L, C), in which D={di} is the set of decision points at which some decisions are to be made, L={lij} is defined as the set of the options j=1,2, \u2026, NC, at each decision point i=1, 2, \u2026, NT and C={cij} is the set costs associated with L={lij}. An acceptable solution based on the graph is called an answer and the path associated with minimum cost is called the optimum solution.This review paper intends to present an exhaustive survey of the applications of ant based algorithms in water resources and environmental engineering problems. Application and algorithmic innovations are explored within the major areas of (1) reservoir operation and surface water management, (2) water distribution systems, (3) drainage and wastewater engineering, (4) groundwater systems including remediation, monitoring, and management, (5) Environmental and Watershed Management Problems, and (6) other applications.Reservoir Operation and Surface Water ManagementLike all search based algorithms, the significant advantage of ACO algorithms in reservoir operation is its capability to be directly linked with any full scale and detailed system simulation models. Different versions of ACO have been applied to derive rule curve parameters Although less cited, the first application of the basic ACO algorithm in reservoir operation for hydroelectric generation scheduling was addressed by In order to minimize the possibility of losing the global optimum domain in continuous problems, Not very many applications of ant algorithms have been reported that efficiently benefits from the concept of heuristic function in the formulation. In fact, direct exploitation of the heuristic information has often been overlooked due to formulation difficulties. Only few articles have explicitly or implicitly employed the heuristic information in definition of decision points and search space Introducing the Non-dominated Archiving Ant Colony Optimization (NA-ACO) as a multiple objective version of ACO algorithm, The first application of the early formulation of continuous ant algorithm in reservoir operation may be attributed to the work of Application of hybrid ACO to reservoir operation is in the early stages of development. Water Distribution SystemWater distribution networks (WDNs) are known as non-linear, constrained and multi-modal problems The very first use of ACO in WDN optimization may be attributed to Introducing a new transition rule, In a series of papers, Zecchin and his colleagues applied different versions of ACO algorithms andtested their performances in design of water distribution systems with various topologies Constraint handling in all ant based algorithms is a major and challenging issue. Penalized objective function seems to be the most attractive and generalized approach for tackling this problem. Determination of the most desirable penalty coefficient in modeling WDNs has remained as an ongoing argument Realizing the importance of operational cost, relatively large number of researchers has focused on pump scheduling as a means of reducing energy costs by taking advantage of offpeak electricity and reservoir storage in a water distribution system. Parallel to application of GA and other search-based algorithms, few researchers focused on application of various structures of ACO for optimum operation of pumping stations in WDNs The search space of the pump scheduling problem grows exponentially with the number of control elements such as pumps leading to increased number of function evaluations required to adequately explore the search space. More important, in an operational scheduling problem, calculation of the objective function value and performance indices requires integration of an extended period hydraulic simulation model with an optimizer. Hybrid methods are sometimes practiced to reduce the computational time.Highlighting the computational burdens and benefits of using variable-speed pump (VSP) in water distribution systems, Hashemi et al. ( Successful applications of different versions of ACO algorithms with varying structures for layout and size optimization of tree-like and looped pipe networks has been reported This section is closed by citing successful applications of ACO algorithms to joint layout and size optimization of pipe networks Urban Drainage and Sewer SystemAlthough, use of the evolutionary computation in optimum sewer design is fairly old, application of ACO in this area is of recent origin. When coupled with full scale and appropriate hydraulic simulator in a simulation-optimization framework, it highly reduces the need for simplification of system representation and holistically considers inter-network effects such as surcharge and backwater.An adaptive refinement procedure was proposed by Simplifying the original structure of the ACOR introduced by The sewer network design optimization problem may be viewed as two sub-problems addressed as optimal layout determination and optimal sizing of its components. Groundwater ManagementsGroundwater management problems are often very complex because they are highly nonlinear in nature with discrete and/or discontinuous decision domain and computationally expensive numerical simulation model. Groundwater optimization problems may include groundwater aquifer management, monitoring network design, aquifer parameters estimation, pollution source identification and groundwater remediation. Although, seems to be quite suitable for all classes of groundwater management, ACO algorithms have not been extensively used in groundwater management problems. Authors believe that the large number of required function evaluations and significant computational requirement of the numerical simulation model may impose sever constraints both on practical and research works. Although these limitations stay valid for all variety of search-based algorithms, overwhelming application of GA in groundwater management problems have been reported. Nonetheless, application of ACO algorithms in groundwater management has been receiving growing attention. One of the earliest applications of ACO in water resources uses an inverse modeling procedure to estimate the unsaturated soil hydraulic parameters When compared to the results of complete enumeration, the solutions of ACO based long term groundwater monitoring (ACO-LTM) were globally optimal for the cases with 21 to 27 remaining wells. They argued that the results from the proposed ACO-LTM algorithm provide a proof-of-concept for the application of the general ACO analogy to address the optimum locations of groundwater LTM sampling stations.Hybrid ACO with simulated annealing was used to provide an adaptive algorithm for estimating the transmissivity and storage coefficient for a two-dimensional, unsteady groundwater flow model. The inverse problem of parameter identification was formulated as an optimization model based on information from the observed and calculated water heads. It was claimed that the ill-posedness of the inverse problem was overcome by employing computational intelligence Environmental and Watershed ManagementThe most recent and interesting application of ACO in watershed management was reported by Emami Dealing with instream quality management issues, Realizing the ever increasing attention on sensor location and consequence management on intentionally polluted water distribution network, Other ApplicationsOther than those listed and cited in the above sections, some diverse applications of different versions of ACO algorithms has also been reported While both the performance of ACO algorithms and our theoretical understanding of their working have significantly increased, there are several areas in which only preliminary steps have been taken and much more research are to be done. Extension of ACO algorithms to more complex optimization problems such as (1) dynamic problems, (2) stochastic problems, and (3) multiple objective problems is known to be the first area. Other active research directions in ACO include the effective parallelization of ACO algorithms and understanding and characterization of the behavior of ACO algorithms and their convergences while solving a novel problem Despite the rapid development and application of ACO algorithms, their mathematical analysis remains partly unsolved. This difficulty is largely due to the fact that the interaction of various components in all metaheuristic algorithms, including ACO, is highly nonlinear, complex, and stochastic. Although, various studies have attempted to carry out convergence analysis, it remains an active and challenging topic. On the other hand, however, we have not proved or cannot mathematically prove their convergence but, we still can compare the performance of various algorithms. This has indeed formed a majority of current research in developing and applying ACO algorithms in the research community of optimization Despite its popularity, mathematical analysis of the algorithm lacks behind. Its popularity is partly attributed to its potential in solving nonlinear, nonconvex, multimodal, in both discrete and continuous domains for which deterministic search techniques incur difficulty or fail completely.", "conclusions": "", "SDG": [6]}, "the_what_the_why_and_the_how_of_artificial_explanations_in_automated_decision_making": {"name": "The What, the Why, and the How of Artificial Explanations in Automated Decision-Making", "abstract": " The increasing incorporation of Artificial Intelligence in the form of automated systems into decision-making procedures highlights not only the importance of decision theory for automated systems but also the need for these decision procedures to be explainable to the people involved in them. Traditional realist accounts of explanation, wherein explanation is a relation that holds (or does not hold) eternally between an explanans and an explanandum, are not adequate to account for the notion of explanation required for artificial decision procedures. We offer an alternative account of explanation as used in the context of automated decision-making that makes explanation an epistemic phenomenon, and one that is dependent on context. This account of explanation better accounts for the way that we talk about, and use, explanations and derived concepts, such as 'explanatory power', and also allows us to differentiate between reasons or causes on the one hand, which do not need to have an epistemic aspect, and explanations on the other, which do have such an aspect. Against this theoretical backdrop we then review existing approaches to explanation in Artificial Intelligence and Machine Learning, and suggest desiderata which truly explainable decision systems should fulfill. 1. Many people are good at constructing post-hoc rationalizations of their decisions (especially, but clearly not exclusively, in the case of intuitive, spontaneous and/or \"unconscious\" decisions), but generally we do not count these rationalizations as adequate explanations.", "keywords": "", "introduction": "", "body": "and the proof to Zorn's Lemma can be thought to explain Zorn's Lemma, and conversely Zorn's Lemma and the proof to the Axiom of Choice can be thought of as an explanation of the Axiom of Choice. Which one is taken to explain the other will depend, in part, on which one the person in question was introduced to first. This highlights one important aspect of explanations that we will discuss more in the next section: Explanations are context sensitive. Lastly, as with any transitive relation, it would not be difficult to construct a sorites wherein X 0 explains X 1 , X 1 explains X 2 , and X i explains X i+1 for every i up to some n, but by the time one gets to X n , X 0 is not an explanation of X n . This again highlights the importance of context sensitivity.Fourth, this account of explanation does not provide any room to distinguish explanations from reasons. There are many reasons that can be given why a certain thing is the case; which of these turns out to be an explanation will depend on context, we argue below.What we have seen from the preceding is that Nozick's account of explanation entirely overlooks what we can call the epistemic dimension of explanation. Now, this is not to deny Nozick's implicit point that some explanations are known to us and some are not-in fact, quite the opposite. For it is precisely the fact that there are explanations that we do not have that we wish to have that we ask the question \"Why?\" But this question is never asked in isolation, and that is the epistemic dimension we are interested in.The epistemic dimension of explanationSuppose someone asks you \"Why is that car red?\" There are a number of possible answers you might give:1. Because it reflects light in wavelengths between approximately 625-740 nm.2. Because someone painted it red.3. Because no one painted it blue.4. Because its owner's favorite color is red. 5. Because there were no non-red cars at the dealer when the owner bought their car.All of these can plausibly count as reasons for why the car is red. But which of these reasons will count as an explanation of why the car is red will-as exemplified by the examples above-depend on the circumstances in which the asker is asking the question. 6 6. For the \"Why is that car red?\" example above, these could, for instance, be:1. In the context of a physics class in school, watching cars drive by on the street.2. In the context of a police search for a blue car of a certain type with a given number plate, after finding the targeted car which turns out to be red.3. In the context of a repainting effort in an autoshop, processing an order to convert red cars into blue ones.4. In the context of a car dealership, selling a red version of a usually blue production model.5.In the context of a conversation about the new car of a person who usually prefers other colors than red.These circumstances include the asking agent's belief state and knowledge set, as well as her reason for asking that question, as opposed to another question, and what she hopes to do with the answer once she's obtained it.These factors are what make up what we call the 'epistemic dimension' of explanation. We are not the first to highlight the importance of this dimension; Kim First, note that when we talk about the \"epistemic dimension of explanation\", we do not mean the sort of thing that Campbell is talking about here: Kim's own understanding of the epistemological dimension of explanation does not actually concern understanding per se, or how it is that an explanation generates or contributes to understanding; instead it is a question about what kinds of facts constitute explanatory knowledge The question of how an explanation generates understanding is a question of epistemology, not a question of explanation. Similarly, the notion of \"explanatory knowledge\" is narrower than the notion of explanation we are interested in; knowledge implies truth but, as we argue below, practical explanations need not be truthful in order to count as explanatory.We argue that there are three things necessary for a particular reason to count as a practical explanation in a given context: (1) The reason must be relevant to the purpose of the question; (2) the reason must provide the hearer with the power to act in a more informed way; (3) the reason need not be true, though it does need to be at least an approximation of the truth. We treat each of these characteristics in turn.Ad (1): Irrelevant reasons cannot be explanatory. If you are asking me for an explanation, then you have a particular epistemic need to be filled or epistemic longing to be satisfied. This need circumscribes the possible acceptable answers. Any answer which does not (attempt to) satisfy this need will not be relevant and cannot be an explanation. Further, not only does the epistemic context of the explanation determine (in part) what reasons are actually explanation, but the type of the explanation matters too, whether it is a formal explanation, a mechanistic one, a teleological one, etc. As Vasilyeva et al. point out, \"Research increasingly supports the idea that (many) representations and judgments are sensitive to contextual factors, including an individual's goals and the task at hand. . . This raises the possibility that judgments concerning the quality of explanations are similarly flexible\" Ad (2): The reason the type of explanation matters for determining relevance is because the type of explanation affects what you can do with the explanation afterwards. This brings us to the second characteristic of explanations, and the question of what makes a reason relevant in a given context? That is to say, what makes some particular reason satisfy an epistemic need, but not another? The answer is rooted in the notion of \"explanatory power\"-when we say, e.g., that one scientific theory has \"more explanatory power\" than another, we are saying that it gives us (or, at the very least, the scientists involved in applying the theory) the power to do more things. We can explain other phenomena, we can make new predictions, we can understand more than we understood before. Thus, the power of an explanation is rooted in its capacity to allow us to act in a way that we could not have acted otherwise. Irrelevant reasons do not give us such a power. If you tell someone who asks you \"Why is that car red?\" that it is because it reflects light of a particular wavelength, but the person who asked for the explanation has no concept of wavelengths or reflection, or indeed of light as an abstract concept, this answer will not be explanatory because it does not allow her to act in a more informed way; the answer is, quite simply, irrelevant because it is uninformative, and it is uninformative because it does not fit within the epistemic context of the asker. This not to say that such an answer cannot be explanatory; it can, in another context. From this, it is clear that whether something is an explanation varies according to context.Ad (3): This is perhaps the biggest way in which our account of explanation differs from realist accounts. On a realist account, the relation of explanation either holds or doesn't hold at all times and only holds (presumably) when there is in fact a genuine connection between the two events. If the car owner's favorite color is green, then saying that the car is red because red is his favorite color is not an explanation because it is false.However, truth is an enormously high bar to put on explanation, and in fact explanations that are later determined not to be true can still have enormous explanatory power (in the sense of explanatory power that we defined above). In our pursuit of scientific progress, \"we do not necessarily replace wrong theories with right ones, but rather look for greater explanatory power\" We do, however, want to encourage truth-seeking in our quest for explanations, and to prioritize as good explanations those which have a better fit with the set of knowledge claims relevant for the context (and are thus in at least some sense \"closer to the truth\"). Consider folk-explanations for thunder, whether it be Zeus throwing his thunderbolt, Thor banging his hammer on an anvil, or Leigong hitting his drum with his mallet. In the right context, each of these can be relevant to satisfying the hearer's epistemic longing; in such contexts they also provide the hearer with the power to act in a more informed way (for example, one can then consider whether to sacrifice a virgin to appease the god and make the thunder stop); however, as an approximation of the truth each of these explanations all falls quite short. As a result, it is legitimate for us to say that they are not very good explanations. Thus our account of explanation avoids one potential criticism of non-realist accounts, namely that anything whatever can count as an explanation, given the right context. Even if that is the case, we are still able to distinguish good explanations from bad ones.The question of how much truth is required is a question of great importance in its own right, and one that we will set aside for the remainder of this paper.We've noted one consequence of this account of explanation above, namely that one and the same reason can be an explanation in one context and not in another, because of the individual nature of individual epistemology. A further consequence is that one need not give up Nozick's account of explanation as a metaphysical relation between things entirely, of course. As Campbell notes, The pluralist's emphasis on the epistemology of explanation does not render her position irrealist because the correctness of the explanation of one event in terms of another is in part a function of the metaphysical relations between them Note, though, that the idea of \"the correctness\" of an explanation potentially smuggles in some problematic notions-not stemming from the \"correctness\" but from \"the\". Even the pluralist's approach to explanations requires that there be the correct explanation, and that this unique explanation's correctness is rooted in certain metaphysical facts (see quote from Campbell earlier in the previous section). If, however, we scrap the notion of there being the correctness of an explanation, and allow there to be many different ways of grounding what makes an explanation a good explanation in a given epistemic context, then we can allow that the existence of some metaphysical relation between events can be sufficient for possessing an explanation, but it is not necessary (and it is not even always sufficient). Thus, we allow for the possibility that we can have multiple possible explanations for a single event or phenomenon, not all of which will be actual explanations in a given context.In this, our explanatory pluralism differs from Kim's, on which \"it is possible to have more than one explanation for a given event provided that one has an account of the way the explanations are related\" A final important consequence of this account is that no one can determine whether something is an explanation for someone else, because of the private nature of individual epistemology. This raises interesting issues in the implementation of mechanisms of explanation into decision procedures in artificial systems, for it means that there is no single answer that the decision procedure can give that can be guaranteed to be explanatory for all people.Our conclusion is that explanation is \"an epistemological activity\" and explanations are \"an epistemological accomplishment\" Explanation in AIWe now shift our focus to how our conception of practical explanation plays out in the context of AI-both in terms of how it compares to previous accounts of explanation and how well it can play the role needed in AI. The purpose of this section is primarily historical, outlining what has been said previously as well as the current discussions, before we move on to more normative matters in the next section.Such a historical discussion is not straightforward: Not only is there no unified or uniform concept of 'explanation' that is used in AI contexts, quite often the term is neither defined nor explained. It is outside the scope of this paper to give a complete history; instead, we focus on two important contexts in which explanations play an important role: explanations in what is called \"Good Old-Fashioned Artificial Intelligence\" or GOFAI ( \u00a74.1) and explanations in machine learning ( \u00a74.2), and then specific challenges concerning explanations that arise in the context of automated decision-making ( \u00a74.3).Explanation in GOFAIDiscussions concerning (the need for) explanations of the reasoning and behavior of AI systems are not a new phenomenon, but already started during the time GOFAI In a more recent effort originating from the cognitive systems lines of research, Forbus emphasized the importance of the human comprehensibility of the behavior and the output of AI systems in the context of his software social organisms Explanation in Machine LearningMachine Learning (ML) methods have seen impressive successes over the last few years. ML-based systems have consequently been introduced into more and more complex application domains, with a significant share of efforts targeting decision support and automated decision-making systems. In the wake of these developments, questions of how to interpret or explain the applied methods and systems have become important. Taking stock of the current variety of ways \"explanations\" and related notions are treated within ML as a field, Lipton points out that \"the term interpretability holds no agreed upon meaning, and yet machine learning conferences frequently publish papers which wield the term in a quasimathematical way\" \u2022 Opaque systems where the mechanisms mapping inputs to outputs are invisible to the user. This basically converts the system into an oracle making predictions over an input, without indicating how and why predictions are made.\u2022 Interpretable systems where a user can not only see, but also study and (given potentially required expertise, resources, or tools) understand how inputs are mathematically mapped to outputs.\u2022 Comprehensible systems which emit symbols along with their outputs, allowing the user to relate properties of the input to the output. 7When comparing the notions of interpretable and comprehensible systems, it is important to note that while interpretable systems are pushing towards becoming \"white boxes\" (in contrast with the \"black box\" nature of opaque systems), a comprehensible system can well remain a \"black box\" concerning its inner workings, but is required to provide the user with symbolic output suitable to serve as basis for subsequent reasoning and action (possibly resulting in a \"communicating black box\". Against the backdrop of these three types of AI/ML systems, Doran et al. require that any definition or characterization of 'explanation' must involve the presence of \"a line of reasoning that explains the decisionmaking process of a model using human-understandable features of the input data\" corresponding information to the system output (and, in doing so, likely enhances the general degree of explanation) compared to most \"standard\" ML approaches, there is no built-in check or guarantee that users fully comprehend the learned hypotheses.Explanation-Based Learning (EBL) (e.g., In the context of artificial neural networks and related statistical approaches, regression models Finally, a somewhat popular explanation strategy is to create a more comprehensible representation of the learned model, which in most cases necessitates a trade-off between fidelity and comprehensibility (Van de 8. An M -of-N rule is a classification rule of the form \"IF (M of the following N antecedents are true) THEN . . . ELSE . . .\" Explanation and automated decision-makingHaving looked at AI/ML in terms of technical approaches to explainable or interpretable methods and systems, in this section we focus on the specific conceptual role and challenges regarding explainability arising from the use of AI/ML methods in systems for decisionsupport and, even more importantly, automated decision-making. As noted in the introduction, these systems are used across a wide range of applications. What is common to the vast majority of such systems is their partial or complete reliance on statistical-and, as such, necessarily data-driven-approaches to solving the respective task. This central role of large amounts of data as key input element, processed using complex statistical methods without the explicit generation of interpretable knowledge along the way, gives rise to a certain form of opacity from the user's perspective. They are opaque \"in the sense that if one is a recipient of the output of the algorithm (the classification decision), rarely does one have any concrete sense of how or why a particular classification has been arrived at from inputs. Additionally, the inputs themselves may be entirely unknown or known only partially\" On the one hand, intentional secrecy can be hard to solve; and in some cases, a solution might not even be desired. On the other hand, opacity due to technical illiteracy and opacity due to processing differences between AI/ML algorithms and human reasoningwhile very different in the nature and quality of underlying ailment-can both be addressed by equipping decision systems with explanation capacities. Wachter et al. point out that in a systems context, these explanations can operate on one of two levels, either operating on the level of the decision mechanism itself (i.e., targeting the system functionality in terms of the logic, significance, envisaged consequences and general functionality of an automated decision-making system), or on an instance level (i.e., targeting individual decisions in terms of the particular rationale, reasons, and individual circumstances of a specific automated decision) count, potentially addressing aspects of the system functionality as well as the rationale of the specific decision.Putting these conceptual considerations into the context of actual AI systems as currently (and likely in the short to midterm) deployed for automated decision-making, in the vast majority of cases a divergence between a system's level of explainability and its performance levels has to be noted. At the moment, Deep Learning (DL) approaches In the context of current initiatives on the side of the regulatory authorities on different levels (such as, e.g., the EU General Data Protection Regulation 2016/679) and societal discussions regarding a desire for transparency and corresponding accountability of automated decision systems, work on better interpretable or explainable methods and systems in AI and ML is ongoing. It is against this backdrop, combined with our considerations concerning the nature and virtues of practical explanations, that we want to have a look at what are desirable properties of explanations provided by AI systems in the following section. The resulting list of desiderata can then help to guide the further development of methods and systems towards the goal of providing actual explanations to the subjects' of automated decision-making.10. An important factor in the success of DL approaches to ML is the capacity of the artificial neural networks to learn their own internal representations for relevant input features, making use of the numerous degrees of freedom offered by the high number of network layers. Still, at the moment there is no method assuring that the content of these internal representations refers to anything humans would recognize as meaningful in their conceptualization of the world. Claimed correspondences between the representation layers of deep networks and human conceptualizations are either accidental or misleading.Desiderata for explanations in AIIn the early days of ML, Michie \u2022 Weak machine learning: The system's predictive performance improves with increasing amounts of data.\u2022 Strong machine learning: In addition to meeting the weak criterion, the system provides its learned hypotheses in symbolic form.\u2022 Ultra-strong machine learning: In addition to meeting the strong criterion, the presentation of the hypotheses has to be communicatively effective in that the user is made to understand the hypotheses and their consequences, subsequently improving the joint performance to a level beyond that of a user studying only the training data.Whilst most modern ML systems meet the first, weak, criterion, it is the third (i.e., ultrastrong) demand that resonates most strongly with our discussion of explanation in \u00a72 and \u00a73.From a pragmatic point of view, it seems necessary for an explanation to be communicatively effective. In an applied scenario, an explanation only counts as an explanation if it also fulfills an explanatory function.A similar intuition, though augmented by a second communicative direction back from the user to the system-converting the previously one-dimensional communicative situation into a real two-way interaction also in terms of information transfer, and not only in joint behavior-underlies the account given by Stumpf: First, the system's explanations of why it has made a prediction must be usable and useful to the user. Second, the user's explanation of what was wrong (or right) about the system's reasoning must be usable and useful to the system. Both directions of communication must be viable for production/processing by both the system and the user The demand for effective communication in the direction from the user to the system in our account goes beyond the requirements a system would have to meet to be considered explainable. Still, one of the underlying theoretical requirements for such an exchange to be possible in the first place is worth to be noted: It must be possible for the user to process the explanation provided by the system in such a way as to be able to point out where the system's reasoning in producing the prediction and the subsequent explanation was right or wrong. This goes beyond the mere requirement of the explanation being usable and useful, but poses a stronger demand in terms of content, structure, and presentation of the explanation.Several of these aspects also resonate, for instance, in Bohanec and Bratko's observation that in the context of explainable AI, simple, though possibly not perfectly accurate definitions of concepts (demanding for the definition to correspond to the concept in a sufficient rather than a perfect manner) may well be more useful than completely accurate, but complex and very detailed ones Summarizing the current literature, we find at least two main desiderata regarding explanations in the context of AI and automated decision-making. Each of them constitutes a necessary criterion regarding the status of a system's output as explanation, though none of them is sufficient by itself. First, the explanations a system provides for its reasoning and behavior have to be communicatively effective relative to the system's user, both in content as well as in presentation (\"communicative effectiveness\"). Users must be capable to understand both the presented explanation, as well as its ramifications, in such a way as to be empowered to subsequently adapt their interactions with the system in a beneficial way. Second, the explanations a system provides must be sufficiently accurate (as opposed to perfectly accurate) relative to the explanans and to the context of the system and its user (\"accuracy sufficiency\"). It might be worth trading off some accuracy for improved comprehensibility of the resulting explanation for the user, supporting the communicative efficiency of the explanation. Based on our previous analysis of what makes a practical explanation, we add another two necessary criteria to the list. Third, the explanations a system provides must be sufficiently truthful (as opposed to perfectly truthful) (\"truth sufficiency\"). On the one hand, a trade-off similar to the accuracy vs. comprehensibility consideration might be required or even desirable, while on the other hand considerations akin to the doctor example from the previous paragraph also might warrant to opt for a not perfectly truthful explanation. Fourth, the explanations a system provides must quit the respective user's subjective epistemic longing (\"epistemic satisfaction\"). It is not fully sufficient to meet a user's epistemic needs (which are mostly addressed by the conjunction between the initial two desiderata), but the user also must indeed be under the impression that her search for an explanation has been completed successfully. Taking all four desiderata together, the combination between communicative effectiveness, accuracy sufficiency, truth sufficiency, and epistemic satisfaction of the users for us provides a sufficient characterization of what is needed for an AI system's output to constitute an effective explanation to its users.Conclusion", "conclusions": "Any discussion of the implementation of decision theory into artificial systems or AI research cannot overlook the importance of the role that explanations play in automated decisionmaking. Due to the \"imperfect\" nature of human beings when held to the normative standards set by classical models of decision-making, the latter are inadequate for providing decisions which can be explained in real-life contexts. A second factor that contributes to this difficulty is that practical explanation is-contra what many more metaphysicallyoriented philosophers have argued-best understood not as an abstract relationship that always holds or never holds between two events or facts, but rather has an epistemic dimension that means what counts as an explanation varies by context. Many things constitute this epistemic dimension, including the knowledge of the person requesting the explanation, the notion of 'epistemic longing', and the need for explanations to provide the receiver with the power to act in a way that she would not have otherwise been able to act. Keeping the importance of this epistemic dimension in mind, and looking at previous approaches to constructing explainable AI systems, it turns out that current methods are not sufficient yet. Many efforts have been and are being undertaken to increase the explainability of automated decision systems, with different techniques focusing on different aspects of what constitutes a practical explanation. Still, what is hitherto lacking are clear criteria for explainable AI systems which are conceived in a way so that they can serve as guiding beacons for the corresponding developments in AI theory and engineering. with this article we aim to contribute to closing this gap by putting four candidate desiderata up for discussion: communicative efficiency of the system relative to its users, a sufficient degree of accuracy and a sufficient degree of truthfulness of the provided explanations, and the need to quit a user's epistemic longing.", "SDG": [6]}, "putting_the_\u201csmarts\u201d_into_the_smart_grid_a_grand_challenge_for_artificial_intelligence": {"name": "Putting the \"Smarts\" into the Smart Grid: A Grand Challenge for Artificial Intelligence", "abstract": "", "keywords": "", "introduction": "The phenomenal growth in material wealth experienced in developed countries throughout the twentieth century has largely been driven by the availability of cheap energy derived from fossil fuels (originally coal, then oil, and most recently natural gas). However, the continued availability of this cheap energy cannot be taken for granted given the growing concern that increasing demand for these fuels (and particularly, demand for oil) will outstrip our ability to produce them (so called 'peak oil') . Many mature oil and gas fields around the world have already peaked and their annual production is now steadily declining. Predictions of when world oil production will peak vary between 0-20 years into the future, but even the most conservative estimates provide little scope for complacency given the significant price increases that peak oil is likely to precipitate [9]. Furthermore, many of the oil and gas reserves that do remain are in environmentally or politically sensitive regions of the world where threats to supply create increased price volatility (as evidenced by the 2010 Deepwater Horizon disaster and 2011 civil unrest in the Middle East). Finally, the growing consensus on the long term impact of carbon emissions from burning fossil fuels suggests that even if peak oil is avoided, and energy security assured, a future based on fossil fuel use will expose regions of the world to damaging climate change that will make the lives of many of the world's poorest people even harder [1]. Against this background, many governments around the world have begun taking action to transition to a low carbon economy. For example, the United Kingdom has legislated to reduce CO2 emissions by 80% by 2050 (compared to 1990 levels) [15]. Achieving this aim requires that the direct use of fossil fuels that we are familiar with today is almost entirely eliminated. Thus, the use of electric vehicles and high speed electric trains will have to become widespread in order to reduce our reliance on oil for transportation. 1 Likewise, our homes and offices will have to be heated by efficient ground and air source heat pumps powered by electricity rather than existing natural gas and oil fired boilers [8]. As a result (and given the general growth of the world economy), electricity demand across the world is predicted to increase by 76%, or 4800 gigawatts (GW), by 2030 (compared to 2007 levels) [22]. Crucially, much of the electricity needed to meet this demand will have to be generated from renewable wind, so- 1 Electric motors are inherently more efficient than internal combustion engines, and are 'future proof' in that their carbon emissions reduce as the electricity used to supply them become cleaner. lar, and tidal sources rather than the coal and natural gas power plants that we use today.[20]It is this increased demand for electricity, and the requirements for its generation, that present perhaps the greatest challenge. In most countries, the electricity grid has changed very little since it was first installed, and all existing grids are predicated on the central idea that electricity is produced by a relatively small number of large fossil fuel burning power stations and is delivered to a much larger number of customers, often some distance from these generators, on-demand. The grid itself relies on ageing infrastructure (e.g., -year old transmission lines and transformers, and 20-year old power stations), is plagued by poor information flow (e.g., most domestic electricity meters are read at intervals of several months), and has significant inefficiencies arising from losses within the transmission (on a national level) and distribution (on a local level) networks 40.[12]The vision of an electricity grid that makes extensive use of renewable generation challenges this current situation. Renewable generation is both intermittent and distributed, with the output of such generators being determined by local environmental conditions (such as wind speeds and cloud cover in the case of wind turbines and photo-voltaic (PV) solar panels, respectively) that can vary significantly over minutes and hours. Thus, it will no longer be possible for supply to continuously follow the vagaries of consumer demand, but rather, the demand-side will have to be managed to ensure that demand for electricity is matched against the available supply. Electric vehicles will play a part in this, since not only do they represent a significant extra load that must be satisfied, but more positively, they also provide a distributed form of energy storage 2 which may allow the grid to smooth out this variable supply. Furthermore, meeting the increased demand for renewable generation may require hundreds of thousands, or even millions of such generators, distributed across both the transmission and distribution networks. These generators may need to act together, effectively working as virtual power plants, or may be located on every building across the grid, resulting in a distributed network of prosumers 3 who both produce and consume electricity depending on their local requirements. Thus, unlike existing grids where electricity generally flows one-way from generators to consumers, this will result in flows of electricity that vary in magnitude and direction continuously. To guarantee the security of the network (i.e., the maintenance of stable voltages and frequencies, and the reliability of supply) and to avoid the cascading failures that plague today's grid, 4 new control procedures must be devised. Indeed, the number and variability of generators will require that the grid is able to act autonomously, under human supervision but not necessarily under human control, to diagnose potential problems and self-heal.Thus, there is a growing consensus that existing grids cannot simply be extended to address these challenges, but rather, a fundamental re-engineering of the grid is required; one that envisages the creation of a 'smart grid', described by the US Department of Energy  as:[12]A fully automated power delivery network that monitors and controls every customer and node, ensuring a two-way flow of electricity and information between the power plant and the appliance, and all points in between. Its distributed intelligence, coupled with broadband communications and automated control systems, enables real-time market transactions and seamless interfaces among people, buildings, industrial plants, generation facilities, and the electric network.What is perhaps most striking about this vision is that not only does it present many challenges in terms of power systems engineering, telecommunications, and cyber-security, but at its core are concepts, such as distributed intelligence, automation, and information exchange, that have long been the focus of research within the computer science and the artificial intelligence (AI) communities. In particular, in this paper we argue that the smart grid provides significant new challenges for research in AI since smart grid technologies will require algorithms and mechanisms that can solve problems involving a large number of highly heterogeneous actors (e.g., consumers with different demand profiles or generators with different volatilities), each with their own aims and objectives, having to operate within significant levels of uncertainty (i.e., where the network conditions and the outcome of actions taken by individual entities on the grid will be more unpredictable or uncontrollable) and dynamism (i.e., where demand and supply at different points in the network will be in a significant a state of flux). Hence, in the following sections, we illustrate how such issues arise within the key components of the smart grid -demand-side management, electric vehicles, virtual power plants, the emergence of prosumers, and self-healing networks -and by showing which components and which interactions need to be smart, we provide a research agenda for this community for making the smart grid a reality.", "body": "The phenomenal growth in material wealth experienced in developed countries throughout the twentieth century has largely been driven by the availability of cheap energy derived from fossil fuels (originally coal, then oil, and most recently natural gas). However, the continued availability of this cheap energy cannot be taken for granted given the growing concern that increasing demand for these fuels (and particularly, demand for oil) will outstrip our ability to produce them (so called 'peak oil') It is this increased demand for electricity, and the requirements for its generation, that present perhaps the greatest challenge. In most countries, the electricity grid has changed very little since it was first installed, and all existing grids are predicated on the central idea that electricity is produced by a relatively small number of large fossil fuel burning power stations and is delivered to a much larger number of customers, often some distance from these generators, on-demand. The grid itself relies on ageing infrastructure (e.g., The vision of an electricity grid that makes extensive use of renewable generation challenges this current situation. Renewable generation is both intermittent and distributed, with the output of such generators being determined by local environmental conditions (such as wind speeds and cloud cover in the case of wind turbines and photo-voltaic (PV) solar panels, respectively) that can vary significantly over minutes and hours. Thus, it will no longer be possible for supply to continuously follow the vagaries of consumer demand, but rather, the demand-side will have to be managed to ensure that demand for electricity is matched against the available supply. Electric vehicles will play a part in this, since not only do they represent a significant extra load that must be satisfied, but more positively, they also provide a distributed form of energy storage 2 which may allow the grid to smooth out this variable supply. Furthermore, meeting the increased demand for renewable generation may require hundreds of thousands, or even millions of such generators, distributed across both the transmission and distribution networks. These generators may need to act together, effectively working as virtual power plants, or may be located on every building across the grid, resulting in a distributed network of prosumers 3 who both produce and consume electricity depending on their local requirements. Thus, unlike existing grids where electricity generally flows one-way from generators to consumers, this will result in flows of electricity that vary in magnitude and direction continuously. To guarantee the security of the network (i.e., the maintenance of stable voltages and frequencies, and the reliability of supply) and to avoid the cascading failures that plague today's grid, 4 new control procedures must be devised. Indeed, the number and variability of generators will require that the grid is able to act autonomously, under human supervision but not necessarily under human control, to diagnose potential problems and self-heal.Thus, there is a growing consensus that existing grids cannot simply be extended to address these challenges, but rather, a fundamental re-engineering of the grid is required; one that envisages the creation of a 'smart grid', described by the US Department of Energy A fully automated power delivery network that monitors and controls every customer and node, ensuring a two-way flow of electricity and information between the power plant and the appliance, and all points in between. Its distributed intelligence, coupled with broadband communications and automated control systems, enables real-time market transactions and seamless interfaces among people, buildings, industrial plants, generation facilities, and the electric network.What is perhaps most striking about this vision is that not only does it present many challenges in terms of power systems engineering, telecommunications, and cyber-security, but at its core are concepts, such as distributed intelligence, automation, and information exchange, that have long been the focus of research within the computer science and the artificial intelligence (AI) communities. In particular, in this paper we argue that the smart grid provides significant new challenges for research in AI since smart grid technologies will require algorithms and mechanisms that can solve problems involving a large number of highly heterogeneous actors (e.g., consumers with different demand profiles or generators with different volatilities), each with their own aims and objectives, having to operate within significant levels of uncertainty (i.e., where the network conditions and the outcome of actions taken by individual entities on the grid will be more unpredictable or uncontrollable) and dynamism (i.e., where demand and supply at different points in the network will be in a significant a state of flux). Hence, in the following sections, we illustrate how such issues arise within the key components of the smart grid -demand-side management, electric vehicles, virtual power plants, the emergence of prosumers, and self-healing networks -and by showing which components and which interactions need to be smart, we provide a research agenda for this community for making the smart grid a reality.DEMAND-SIDE MANAGEMENTA key requirement for a safe and efficient electricity grid is that supply and demand are always in perfect balance. Now, in the day to day running of the today's electricity grid, this is achieved by varying the supply-side in real-time to match demand (increasing and decreasing the output of generators such that voltage and frequency are maintained across the grid). Hence, the idea that electricity should be available at all times at the flick of a switch has permeated most, if not all, of our daily activities in the modern world.However, as far back as the 1980s, Schweppe and colleagues highlighted numerous reasons why demand for electricity should be made more adaptive to supply conditions The need for demand-side management is even more apparent within a grid that makes extensive use of intermittent renewable generation. In this case, there is a high likelihood that there will be periods when there is insufficient generation capacity to meet demand. It is thus imperative that demand can be reduced at these times. Conversely, there may also be times when renewable energy is plentiful, and demand should increase to make the best use of this energy.To date, approaches to reduce demand have been limited to either directly controlling the devices used by the consumers (e.g., automatically switching off high load devices such as air conditioners at peak times), or to providing customers with tariffs that deter peak time use of electricity. The advent of the smart grid with two way information flows, and smart meters making real-time measurements of consumption, would allow demand-side management to be deployed at scale across the entire grid, providing every home and every commercial and industrial consumer with the ability to automatically reduce load in response to signals from the grid.However, doing so may be ineffective, or at worst, detrimental, since such initiatives tend to reduce the natural diversity of consumers' peak demands and shift all of these peaks to specific periods Thus, it appears that demand-side management technologies that simply rely on reacting to control or price signals will not be enough. Rather, what is necessary are more sophisticated approaches that are truely adaptive to the state of the grid, that are able to learn the correct response given any particular situation, and that can look ahead and predict both supply and demand trends in the near future, in order to prepare for future reductions in available supply, or to make the most effective use of supply when it is available.The design of such intelligent systems is challenged by the complexity of the domains in which they are deployed. For example, within a home, demand reduction may involve shifting the time of use of a number of electrical appliances, each with their own individual constraints (e.g., lighting cannot be shifted, a washing machine can be shifted by a day or two, while a dishwasher may be shiftable by a few hours Similarly, commercial and industrial consumers will be constrained by existing contracts and commercial considerations (e.g., a factory may have to deliver products within certain deadlines, while a data centre has to be available to its customers twenty four hours a day), and must balance demand reduction against these additional factors. Large industrial consumers of electricity with significant heating, cooling, or pumping loads may have considerable flexibility regarding when they actually consume electricity as long as some overarching constraints are satisfied. 5 However, to do so in a responsive way, requires that the usage optimisation algorithm that is deployed is able to model and predict both the prices within the grid, and also the industrial processes themselves (similar to the home heating setting above where a thermal model of the home must be learnt). Furthermore, in both settings, it will be essential that the householders and business owners are able to understand the consequences of the automated actions that are taken, and are happy to delegate control to an intelligent device or software agent. In this respect, it will be important to define the adjustable autonomy of such systems; to what extent should the agent automatically decide to shift devices to run at certain times, and when should it ask for confirmation from the user Against this background, recent work has begun to research the use of autonomous agents, representing individual consumers, that interact through markets Thus, in summary, we believe the key AI challenges in demandside management are:\u2022 Designing automation technologies for heterogeneous devices that learn to adapt their energy consumption against real-time price signals when faced with uncertainty in predictions of future demand and supply, the individual users' preferences, and the constraints of the overarching system (domestic, commercial, or industrial) within which it is deployed.\u2022 Developing the means by which the automated decisions of these systems can be effectively communicated to, and controlled by, their human owners, whilst allowing a varying range of autonomous behaviours.tremely high spot prices, several bauxite smelters realised that there was greater profit to be had in reselling electricity that they had bought in long-term forward contracts, than in using it themselves to produce aluminium \u2022 Developing simulation and prediction tools to allow the system-wide consequences of deploying pricing mechanisms and energy management agents to be assessed by grid operators and suppliers.ELECTRIC VEHICLESWith the advent of commercially viable electric vehicles (EV), such as the Nissan Leaf and the Chevy Volt, the coming years are likely to see the large-scale adoption of electric vehicles that will shift the energy requirements of transport from fossil fuels to renewable electricity from the smart grid In more detail, electric vehicles place a considerable additional load on the grid due to the high charging rates that are necessary to ensure both a reasonable vehicle range of around 100 miles, and the ability to rapidly charge the battery. While a typical house may use between 20 to 50 kWh of energy per day, an EV battery may be charged with 32 kWh of energy in just a few hours Given these continuously changing demands imposed on the local distribution network by the movement and charging of vehicles within it, and the variable supply of renewable energy, it will be necessary to devise sophisticated approaches to schedule the charging of electric vehicles. This scheduling should make the most effective use of what renewable energy is available, while also ensuring that the vehicles' batteries are fully charged when required by their owners. Furthermore, this must be done in the context of uncertainty regarding both the future availability of renewable energy, and future vehicle use. Building upon this, it will be important to design decentralised control mechanisms that can guide the charging of EVs to various points in the network, given its dynamic conditions and constraints. In particular, these mechanisms will have to take into account that consumers need to be incentivised (e.g., in terms of charging prices or speeds at specific points) to adapt their behaviour as they may only care about their individual travel needs. The challenge is to ensure such incentives are properly designed to induce charging profiles that stabilise the grid (i.e., ensure flows are secure and transformers are not overloaded) while satisfying the needs and preferences of the highly heterogeneous population of EVs each with their individual battery capacity, charging speeds, and usage pattern.More positively, EVs will also be a key resource in the demandside management systems discussed previously. In such systems, the ability to defer demand to times when renewable energy is more plentiful is essential, and currently, this is only possible with subset of electrical loads that are not required to have immediate effect (e.g., washing machines or dishwashers). However, the ability to store energy within large batteries allows any electrical load to be shifted, and we are likely to first see energy from electric vehicle batteries support the shifting of loads within their owners' home (vehicle-to-home or V2H), and then to providing energy back to the grid itself (V2G) Addressing these challenges requires intelligent systems that can fully automate the charging and discharging of these vehicles, whilst taking account of the current and future availability of the renewable generation, and being aware of the local constraints of the distribution network. Recent work has begun to address these challenges with online mechanism design being used to elicit users' travel requirements (i.e., the amount of charge required and the time at which the EV is needed) and schedule the charging of their vehicles Thus, against this background, we identify the key AI challenges in the deployment of EVs in the smart grid as follows:\u2022 Predicting an individual user's EV charging needs based on data about her daily activities and travel needs.\u2022 Predicting aggregate EV charging demands at different points in the network given the continuous movement of EVs, the available charge in their batteries, and the social activities their users engage in.\u2022 Designing decentralised control mechanisms that coordinate the movement of EVs (each with different battery capacities and charging speeds) to different charge points by providing incentives to consumers to do so. The aim being to maintain secure flows on the grid and ensure that transformers do not trip due to excess demand.\u2022 Designing algorithms to optimise the charging cycles of EVs to satisfy the predicted needs of the user (to shift loads or to travel) while maximising the profits generated from participating in V2G sessions.VIRTUAL POWER PLANTSAs larger numbers of actors (e.g., EVs, homes, or renewable energy providers) in the smart grid communicate and coordinate with each other to control demand at different points in the network (e.g., using demand-side management to ensure that demand is able to follow the supply of renewable energy, and EV discharging to the grid to cope with excess demand), it will be important to harness synergies that exist between them to improve the efficiency of the grid (e.g., EV discharging to satisfy demand at times when demandside management techniques cannot shift enough usage to later times). To this end, the concept of a virtual power plant (VPP) heterogeneous services they provide within the VPP in an agile fashion so as to meet the requirements of the contracts they make with their customers. In particular, individual actors need to estimate the impact of their individual production (or demand reduction) on the aggregate performance of the VPP, and communicate and optimise the joint actions taken to meet the VPPs' objectives (i.e., satisfy demand). These technical arrangements may need to be specified on a daily, and even on an hourly basis to maximise the profits of the individual actors. This is because, if some actors can only produce energy at specific times of the day (e.g., PVs generate energy during the day and tidal energy may be available at night), they will want to choose those partners they can complement better at those times (e.g., a PV farm and a tidal generator may generate energy out of phase with each other and hence be highly complementary, while wind energy providers whose turbines are located in the same region will generate energy at the same time and hence be less complementary). In turn, if new actors become better partners due to changes in the environment (e.g., more wind blows at night resulting in higher predicted wind energy production than tidal or more EVs converge to a specific region due to a social event, resulting in more storage being available), then some of them might decide to leave their current VPP and form a new one (e.g., PV owners may be better off storing their excess energy during the day in the EVs to be able to supply at night rather than collaborate with a tidal energy provider). Given the scale and dynamism of this optimisation problem, it will be important to design decentralised coordination algorithms and strategies that allow individual VPP participants to come to the most efficient arrangements within a reasonable time. Moreover, they will need to ensure such arrangements do not overload the local distribution networks, in which they are connected. Given this, and the restrictions imposed by the network operator due to possible network congestion, the VPP may further have to re-optimise individual members' operations. Typically, such optimisations would have to be done while being confronted with uncertainty about the individual members' generation and consumption capacity.The negotiation of technical arrangements needs to take into account that each potential member of a VPP is typically motivated to maximise its own profit, even though, as a group they compete against other actors (individuals, VPPs or large power stations) in the system to maximise the group's profits. Hence, it is in each actor's interest to take actions that will cost it the least while maximising its share of the profits obtained by the VPP operations as a whole. This leaves some room for any individual resource to manipulate what it reveals as its predicted capability (i.e., production, demand-response, or storage ability) as opposed to what it actually delivers on the day. For example, given their uncertainty about their production, some resources may prefer to understate their predicted production profile in case they get penalised by the group for under producing. Alternatively, some resources may prefer to overstate their predicted production in the case that penalties for under producing are not significant, and doing so increases their share of the profits. Such strategic considerations highlight the need to capture the provenance of decision made by the VPP, such that it is possible to track and verify the individual actions, reports, and resulting rewards of each VPP member. The amount of provenance information this will generate will require efficient frameworks and mechanisms to represent, store, audit, and share it. Building upon provenance information it may then be possible to model the trustworthiness of individual VPP members through trust and reputation mechanisms similar to those used in online marketplaces such as eBay or Amazon for example Assuming trust and reputation mechanisms can render VPPs reliable, it is important to ensure that the negotiations that individual energy providers engage in, converge in such a way that the most efficient VPPs (i.e., generating the maximum social welfare) are most effectively formed (i.e., in minimum time and with minimum communication costs) in the system All these issues will require the definition of computationally efficient search algorithms to allocate the payoffs to individual members of VPPs (as defined by game-theoretic solution concepts), while taking into account uncertainty in defining the relative contributions of each member to the aggregate performance (i.e., mainly the profits generated) of the VPP. Moreover, given that different coalitions may be formed over time, an energy provider will choose its membership of coalitions in such a way as to maximise its revenues in the long run. This makes the search for efficient payoff allocations exponentially harder since it extends the search space to include future possible coalitions (and their expected returns) as well as present ones. Initial work in applying multi-agent systems approaches to the VPP formation process include To advance the state of the art in this domain, the following key AI challenges still need to be addressed:\u2022 Designing agent-based models of different VPP actors and processes in order to capture the complexity of the technical arrangements needed to form and manage VPPs.\u2022 Distributed combinatorial optimisation of the technical arrangements of demand-side management, V2G sessions, and micro-generation, to maximise rewards.\u2022 Designing online mechanisms to form statistically correct trust measures for energy providers and automatically capture, track, and reason about the provenance of information revealed by energy providers to form VPPs.\u2022 Designing search algorithms and negotiation mechanisms for individual actors to agree on which VPP to form at different points in time and how to share the profits, using computationally efficient game-theoretic solution concepts, of a VPP given uncertainty in their performance, trust in their revealed capabilities, and changing weather and demand patterns.ENERGY PROSUMERSOur discussion, so far, has highlighted the significant heterogeneity of the large numbers of renewable energy resources in the smart grid and the complexity of the interactions between them and consumers. When taken altogether, this will neccesitate significant changes in the way energy is bought and sold. In particular, this is set against the current operation of the grid where, in many countries (e.g., the US, UK, and in many parts of the EU), the electricity market is deregulated, such that large generators (located far from the point of use) trade directly with retailers who then sell the electricity on to consumers through fixed contracts and tariffs In contrast, in the smart grid, market operations will have to adjust to a much larger number of heterogeneous entities, distributed throughout the network (closer to the point of use of electricity), trading much smaller amounts of energy. Indeed, the widespread adoption of renewable generation at the level of individual homes and businesses will lead to the creation of markets composed of many millions of prosumers who both produce and consume energy To do so, however, means that prosumers will need to be endowed with effective trading strategies that can cope with uncertainty in the market. To minimise this uncertainty, they will need to be informed by predictions of their own demand (that may vary according to their needs and social activities) and generation capacity (e.g., using weather forecasts or their EV usage needs), as well as the future price of electricity on the market. Given that these trading decisions may need to be taken in real-time, these predictions will also need to be generated in real-time, and furthermore, to ensure users understand the life-style or operational implications of, and agree to, autonomously chosen trading decisions, human-computer interaction mechanisms will have to be designed to ensure that large numbers of users trust and participate in these markets.Essentially, as more prosumers populate the market, electricity will become a commodity with similar properties to those traded on stock markets. Given this, prosumers will be able to speculate in markets, buying and selling not simply to consume or supply electricity, but also to profit. However, while speculation may help make the market more efficient, it may also adversely impact on the operation of the grid, if the traded flows do not actually satisfy the physical constraints of the distribution network. Potential solutions point to the application of regulatory measures to reduce speculation and more importantly, to congestion pricing mechanisms In summary, the AI challenges involved in endowing prosumers with the intelligence to trade in electricity markets whilst ensuring safe network flows include:\u2022 Developing computationally efficient learning algorithms that can accurately predict both the prosumers' consumption and generation profiles (instead of only the usage profile for a consumer) as well as the price of electricity in real-time in order to inform profitable trading decisions.\u2022 Developing autonomous trading agents that can use such predictions to maximise their profit in the electricity market, and efficient algorithms to marry congestion management with market operation in distribution networks while guaranteeing good equilibrium conditions in the system.\u2022 Developing human-agent interaction mechanisms, to allow prosumers to guide their agents trading decisions, that take into account the prosumers' daily constraints and preferences to consume or produce energy.SELF-HEALING NETWORKSSo far, we have discussed a number of ways in which the electricity flows are likely to become both more unpredictable and bidirectional in the smart grid. This will result in a greater need for decentralised control strategies given the sheer numbers of active entities embedded in the system. While this renders fault-correction mechanisms in the network even more complex, the intelligence on which these active entities rely to make their consumption or generation decisions, could also be used to naturally distribute (and hence make more robust) the decision making needed to apply self-healing strategies on the network when faults occur. Generally speaking, faults may arise either because lines become overloaded or because of old infrastructure becoming more prone to failure. To prevent such faults and remedy them, network operators already rely on a number of intelligent systems at the transmission network level. Traditionally, this is achieved with the help of automatic voltage regulators and using supervisory control and data acquisition systems Extending these techniques to the management of the distribution network where large numbers of prosumers will operate, will require a much larger number of phasor measure units to be deployed, both because the distribution network contains many more nodes, but also because the heterogeneity of the prosumers within it means that network conditions are likely to vary more rapidly, necessitating accurate and timely monitoring and control. Fully instrumenting such networks is likely to be too expensive, and thus, there is a clear need for the development of state estimation systems that do not need to have every node in the network monitored. More importantly, we will need systems that can, using information gleaned from across the grid, learn correlations between state parameters at different nodes to provide accurate and robust estimates of the system state. The vast amount of data generated from multiple actors and sensors, and the micro-second level measurements being made, will present formidable computational challenges in trying to estimate or predict the future state of the system. 8 Phasor measurement units measure both magnitudes and phase angles of voltages and currents within the network, and are used to assess the state of a power system in realtime.Now, if accurate information about the network can be obtained, active network management techniques, supported by distributed intelligence in the network, could help recover from faults faster than previously possible. For example, if voltages tend to drift in some parts of the network, automatic actions on transformers may be taken to re-establish the correct voltage levels, or assistance may be requested from EVs that are currently plugged into the network To build such self-healing mechanisms, however, will require that all these actors can communicate their action space (e.g., limits on voltage regulation, generation capacity, demand reduction ability) and agree on joint actions to implement islanding strategies. Given the uncertainty that permeates the actions of some of these entities (e.g., weather patterns that affect generation or social activities that affect the movement of EVs), it will be important to predict the impact of such uncertainty on the joint actions chosen to avoid electing those that may result in cascading failures in the worst case. Moreover, given the individual preferences of all actors involved (e.g., to consume electricity for specific activities or to sell electricity to maximise profits) these joint actions may need to be negotiated rapidly among them to ensure they end up in an agreement all parties commit to \u2022 Designing computationally efficient state estimation algorithms that can predict voltage and phase information at different nodes in the (partially observable) distribution network, in real-time, given the prosumers' current and predicted energy demand and supply.\u2022 Enabling distributed coordination of automatic voltage regulators and energy providers and consumers for voltage control and balancing demand and supply during recovery from faults.\u2022 Automating distributed active network management strategies given the uncertainty (either because they cannot be accurately measured or there is incomplete information about certain nodes) about demand and supply at different points in the network.CONCLUSIONS", "conclusions": "There is a significant drive within the developed world to reduce our reliance on fossil fuels and move to a low-carbon economy in order to guarantee energy security and mitigate the impact of energy use on the environment. This transition requires a fundamental re-think and re-engineering of the electricity grid. The ensuing smart grid must be able to make efficient use of intermittent renewable energy sources and supply the additional electricity required by electric vehicles. Doing so, will require extensive use of demand-side management and virtual power plants to balance supply and demand. It will also see large numbers of prosumers, buying and selling electricity in real-time, whilst automated network control algorithms maintain the safe operation of the grid, and allow it to self-heal when something does go wrong.The automation, information exchange, and distributed intelligence needed to deliver such technologies creates many new challenges for the AI communities investigating machine learning, search, distributed control, and optimisation. In this paper, we have enumerated what we believe to the main challenges that, if met, will allow the full potential of the smart grid to be realised. Our claims build upon an extensive survey of the state of the art that goes beyond the papers cited and includes a large number of references (spanning technical papers, books, and policy documents relating to the deployment of specific smart grid technologies and evaluations of these) provided in the online appendix. In particular, we have highlighted the key issues in learning and predicting demand or supply at various points in the network given the variety of demand control mechanisms (e.g., demand-side management and EV charging) and energy sources, each with different degrees of uncertainty in their production capability (e.g., VPPs or renewable energy sources). Moreover, we showed that the automated decentralised coordination between such entities (to balance demand and supply while ensuring flows on the network are always secure) will need to factor in both the individual properties of all actors (e.g., EVs with different batteries, different types of renewable energy sources, users with their own understandings of trading decisions and their agents' decisions) involved and the incentives given to them to behave in certain ways (e.g., consumers shifting demand due to real-time pricing, or VPPs sharing profits equitably).Building upon this, we also discussed some initial attempts at solving them within the various sub-areas of the smart grid. Cutting across these various challenges are the issues of human-computer interaction, heterogeneity, dynamism, and uncertainty that are an intrinsic part of decision making and acting in the smart grid. By dealing effectively with these factors, we believe it will be possible for future generations to rely on their energy systems to deliver electricity efficiently, safely, and reliably.Finally, we note that many of the issues present within the smart grid also arise within other domains such as water distribution, transportation, and telecommunication networks where large numbers of heterogeneous entities act and interact in a similar fashion to those within the grid. Hence, there is potential to transfer technologies across these domains and also address broader issues that affect the sustainability of such systems in a unified manner, such as cyber-security and the ethics of delegating human decision making to intelligent systems.", "SDG": [7]}, "artificial_intelligence_and_economic_growth": {"name": "NBER WORKING PAPER SERIES ARTIFICIAL INTELLIGENCE AND ECONOMIC GROWTH", "abstract": " and participants at the NBER Conference on Artificial Intelligence for helpful discussion and comments. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research. NBER working papers are circulated for discussion and comment purposes. They have not been peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications.", "keywords": "", "introduction": "This paper considers the implications of artificial intelligence for economic growth.Artificial intelligence (A.I.) can be defined as \"the capability of a machine to imitate intelligent human behavior\" or \"an agent's ability to achieve goals in a wide range of environments.\" 1 These definitions immediately evoke fundamental economic issues.For example, what happens if A.I. allows an ever-increasing number of tasks previously performed by human labor to become automated? A.I. may be deployed in the ordinary production of goods and services, potentially impacting economic growth and income shares. But A.I. may also change the process by which we create new ideas and technologies, helping to solve complex problems and scaling creative effort. In extreme versions, some observers have argued that A.I. can become rapidly self-improving, leading to \"singularities\" that feature unbounded machine intelligence and/or unbounded economic growth in finite time , (Good (1965), Vinge (1993)). Kurzweil (2005) provides a detailed overview and discussion of the prospects for a singularity from the standpoint of economics.Nordhaus (2015)In this paper, we speculate on how A.I. may affect the growth process. Our primary goal is to help shape an agenda for future research. To do so, we focus on the following questions:\u2022 If A.I. increases automation in the production of goods and services, how will it impact economic growth?\u2022 Can we reconcile the advent of A.I. with the observed constancy in growth rates and capital share over most of the 20 th century? Should we expect such constancy to persist in the 21 st century?\u2022 Do these answers change when A.I. and automation apply to the production of new ideas?\u2022 Can A.I. drive massive increases in growth rates, or even a singularity, as some observers predict? Under what conditions, and are these conditions plausible?\u2022 How are the links between A.I. and economic growth modulated by firm-level considerations, including market structure and innovation incentives? How does A.I. affect the internal organization of firms, and with what implications?In thinking about these questions, we develop two main themes. First, we model A.I. as the latest form in a process of automation that has been ongoing for at least 200 years. From the spinning jenny to the steam engine to electricity to computer chips, the automation of aspects of production has been a key feature of economic growth since the Industrial Revolution. This perspective is taken explicitly in two key papers that we build upon,  and Zeira (1998). We view A.I. as a new form of automation that may allow tasks that were previously thought to be out of reach from automation to succumb. These tasks may be non-routine (to use the language of Acemoglu and Restrepo (2016)), like self-driving cars, or they may involve high levels of skill, such as legal services, radiology, and some forms of scientific lab-based research. An advantage of this approach is that it allows us to use historical experience on economic growth and automation to discipline our modeling of A.I.Autor, Levy and Murnane (2003)A second theme that emerges in our paper is that the growth consequences of automation and A.I. may be constrained by Baumol's \"cost disease.\"  observed that sectors with rapid productivity growth, such as agriculture and even manufacturing today, often see their share of GDP decline while those sectors with relatively slow productivity growth -perhaps including many services -experience increases.Baumol (1967)As a consequence, economic growth may be constrained not by what we do well but rather by what is essential and yet hard to improve. We suggest that combining this feature of growth with automation can yield a rich description of the growth process, including consequences for future growth and income distribution. When applied to a model in which A.I. automates the production of goods and services, Baumol's insight generates sufficient conditions under which one can get overall balanced growth with a constant capital share that stays well below 100%, even with nearly complete automation. When applied to a model in which A.I. automates the production of ideas, these same considerations can prevent explosive growth. 2   The paper proceeds as follows. Section 2 begins by studying the role of A.I. in automating the production of goods and services. In Section 3, we extend A.I. and au-tomation to the production of new ideas. Section 4 then discusses the possibility that A.I. could lead to superintelligence or even a singularity. In Section 5, we look at A.I. and firms, with particular attention to market structure, organization, reallocation, and wage inequality. In Section 6, we examine sectoral evidence on the evolution of capital shares in tandem with automation. Finally, Section 7 concludes.", "body": "This paper considers the implications of artificial intelligence for economic growth.Artificial intelligence (A.I.) can be defined as \"the capability of a machine to imitate intelligent human behavior\" or \"an agent's ability to achieve goals in a wide range of environments.\" 1 These definitions immediately evoke fundamental economic issues.For example, what happens if A.I. allows an ever-increasing number of tasks previously performed by human labor to become automated? A.I. may be deployed in the ordinary production of goods and services, potentially impacting economic growth and income shares. But A.I. may also change the process by which we create new ideas and technologies, helping to solve complex problems and scaling creative effort. In extreme versions, some observers have argued that A.I. can become rapidly self-improving, leading to \"singularities\" that feature unbounded machine intelligence and/or unbounded economic growth in finite time In this paper, we speculate on how A.I. may affect the growth process. Our primary goal is to help shape an agenda for future research. To do so, we focus on the following questions:\u2022 If A.I. increases automation in the production of goods and services, how will it impact economic growth?\u2022 Can we reconcile the advent of A.I. with the observed constancy in growth rates and capital share over most of the 20 th century? Should we expect such constancy to persist in the 21 st century?\u2022 Do these answers change when A.I. and automation apply to the production of new ideas?\u2022 Can A.I. drive massive increases in growth rates, or even a singularity, as some observers predict? Under what conditions, and are these conditions plausible?\u2022 How are the links between A.I. and economic growth modulated by firm-level considerations, including market structure and innovation incentives? How does A.I. affect the internal organization of firms, and with what implications?In thinking about these questions, we develop two main themes. First, we model A.I. as the latest form in a process of automation that has been ongoing for at least 200 years. From the spinning jenny to the steam engine to electricity to computer chips, the automation of aspects of production has been a key feature of economic growth since the Industrial Revolution. This perspective is taken explicitly in two key papers that we build upon, A second theme that emerges in our paper is that the growth consequences of automation and A.I. may be constrained by Baumol's \"cost disease.\" As a consequence, economic growth may be constrained not by what we do well but rather by what is essential and yet hard to improve. We suggest that combining this feature of growth with automation can yield a rich description of the growth process, including consequences for future growth and income distribution. When applied to a model in which A.I. automates the production of goods and services, Baumol's insight generates sufficient conditions under which one can get overall balanced growth with a constant capital share that stays well below 100%, even with nearly complete automation. When applied to a model in which A.I. automates the production of ideas, these same considerations can prevent explosive growth. 2   The paper proceeds as follows. Section 2 begins by studying the role of A.I. in automating the production of goods and services. In Section 3, we extend A.I. and au-tomation to the production of new ideas. Section 4 then discusses the possibility that A.I. could lead to superintelligence or even a singularity. In Section 5, we look at A.I. and firms, with particular attention to market structure, organization, reallocation, and wage inequality. In Section 6, we examine sectoral evidence on the evolution of capital shares in tandem with automation. Finally, Section 7 concludes.A.I. and Automation of ProductionOne way of looking at the last 150 years of economic progress is that it is driven by automation. The industrial revolution used steam and then electricity to automate many production processes. Relays, transistors, and semiconductors continued this trend. Perhaps artificial intelligence is the next phase of this process rather than a discrete break. It may be a natural progression from autopilots, computer-controlled automobile engines, and MRI machines to self-driving cars and A.I. radiology reports.While up until recently, automation has mainly affected routine or low-skilled tasks, it appears that A.I. may increasingly automate non-routine, cognitive tasks performed by high-skilled workers. 3 An advantage of this perspective is that it allows us to use historical experience to inform us about the possible future effects of A.I.The Zeira (1998) Model of Automation and GrowthA clear and elegant model of automation is provided by While Zeira thought of the X i 's as intermediate goods, we follow If the aggregate capital K and labor L are assigned to these tasks optimally, the production function can be expressed (up to an unimportant constant) aswhere it is now understood that the exponent \u03b1 reflects the overall share and importance of tasks that have been automated. For the moment, we treat \u03b1 as a constant and consider comparative statics that increase the share of tasks that get automated.Next, embed this setup into a standard neoclassical growth model with a constant investment rate; in fact, for the remainder of the paper this is how we will close the capital/investment side of all our models. The share of factor payments going to capital is given by \u03b1 and the long-run growth rate of y \u2261 Y /L iswhere g is the growth rate of A. An increase in automation will therefore increase the capital share \u03b1 and, because of the multiplier effect associated with capital accumulation, increase the long-run growth rate.Zeira emphasizes that automation has been going on at least since the industrial revolution, and his elegant model helps us to understand that. However, its strong predictions that growth rates and capital shares should be rising with automation go against the famous Several other important contributions to this rapidly expanding literature should also be noted. Automation and Baumol's Cost DiseaseThe share of agriculture in GDP or employment is falling toward zero. The same is true for manufacturing in many countries of the world. Maybe automation increases the capital share in these sectors and also interacts with nonhomotheticities in production or consumption to drive the GDP shares toward zero. The aggregate capital share is then a balance of a rising capital share in agriculture/manufacturing/automated goods with a declining GDP share of these goods in the economy.Looking toward the future, 3D-printing techniques and nanotechnology that allow production to start at the molecular or even atomic level could someday automate all manufacturing. Could A.I. do the same thing in many service sectors? What would economic growth look like in such a world?This section expands on the ModelGDP is a CES combination of goods with an elasticity of substitution less than one:where \u03c1 < 0 (5)where A t = A 0 e gt captures standard technological change, which we take to be exogenous for now. Having the elasticity of substitution less than one means that tasks are gross complements. Intuitively, this is a \"weak link\" production function, where GDP is in some sense limited by the output of the weakest links. Here, these will be the tasks performed by labor, and this structure is the source of the Baumol effect.As in Zeira, another part of technical change is the automation of production. Goods that have not yet been automated can be produced one-for-one by labor. When a good has been automated, one unit of capital can be used instead:This division is stark to keep the model simple. An alternative would be to say that goods are produced with a Cobb-Douglas combination capital and labor, and when a good is automated, it is produced with a higher exponent on capital. 5   The remainder of the model is neoclassical:We assume a fixed endowment of labor for simplicity.Let \u03b2 t be the fraction of goods that that have been automated as of date t. Here and throughout the paper, we assume that capital and labor are allocated symmetrically across tasks. Therefore, K t /\u03b2 t units of capital are used in each automated task and L/(1\u2212\u03b2 t ) units of labor are used on each non-automated task. The production function can then be written asCollecting the automation terms simplifies this toThis setup therefore reduces to a particular version of the neoclassical growth model, and the allocation of resources can be decentralized in a standard competitive equilibrium. In this equilibrium, the share of automated goods in GDP equals the share of capital in factor payments:Similarly, the share of non-automated goods in GDP equals the labor share of factor payments:Therefore the ratio of automated to nonautomated output -or the ratio of the capital share to the labor share -equalsWe specified from the beginning that we are interested in the case in which the elasticity of substitution between goods is less than one, so that \u03c1 < 0. From equation ( Following Balanced Growth (Asymptotically)To understand some of these possibilities, notice that the production function in equation ( With \u03c1 < 0, notice that \u2191 \u03b2 t \u21d2 \u2193 B t and \u2191 C t . That is, automation is equivalent to a combination of labor-augmenting technical change and capital-depleting technical change. This is surprising. One might have thought of automation as somehow capital augmenting. Instead, it is very different: it is labor augmenting and simultaneously dilutes the stock of capital. Notice that these conclusions would be reversed if the elasticity of substitution were greater than one; they importantly rely on \u03c1 < 0.The intuition for this surprising result can be seen by noting that automation has two basic effects; these can be seen most easily by looking back at equation ( Notice that for labor, the opposite forces are at work: automation concentrates a given quantity of labor onto a smaller number of tasks and hence is labor augmenting whenThis opens up one possibility that we will explore next: what happens if the evolution of \u03b2 t is such that C t grows at a constant exponential rate? This can occur if 1 \u2212 \u03b2 t falls at a constant exponential rate toward zero, meaning that \u03b2 t \u2192 1 in the limit and the economy gets ever closer to full automation (but never quite reaches that point).The logic of the neoclassical growth model suggests that this could produce a balanced growth path with constant factor shares, at least in the limit. (This requires A t to be 7 In order for automation to increase output, we require a technical condition:For \u03c1 < 0, this requires K/\u03b2 > L/1 \u2212 \u03b2. That is, the amount of capital that we allocate to each task must exceed the amount of labor we allocate to each task. Automation raises output by allowing us to use our plentiful capital on more of the tasks performed by relatively scarce labor.constant.)In particular, we want to consider an exogenous time path for the fraction of tasks that are automated, \u03b2 t , such that \u03b2 t \u2192 1 but in a way that C t grows at a constant exponential rate. This turns out to be straightfoward. LetBecause the exponent is negative (\u03c1 < 0), if \u03b3 falls at a constant exponential rate, C t will grow at a constant exponential rate. This occurs if \u03b2t = \u03b8(1 \u2212 \u03b2 t ), implying that g \u03b3 = \u2212\u03b8.Intuitively, a constant fraction, \u03b8, of the tasks that have not yet been automated become automated each period.Figure Along such a path, however, sectors like agriculture and manufacturing exhibit a structural transformation. For example, let sectors on the interval [0, 1/3] denote agriculture and the automated portion of manufacturing as of some year, such as 1990.These sectors experience a declining share of GDP over time, as their prices fall rapidly.The automated share of the economy will be constant only because new goods are becoming automated.The analysis so far requires A t to be constant, so that the only form of technical change is automation. This seems too extreme: surely technical progress is not only about substituting machines for labor, but also about creating better machines. This can be incorporated in the following way. Suppose A t is capital-augmenting rather than grows at a constant exponential rate (2% per year in this example), leading to an asymptotic balanced growth path. The share of tasks that are automated approaches 100% in the limit. Interestingly, the capital share of factor payments (and the share of automated goods in GDP) remains bounded, in this case at a value around 1/3. With a constant investment rate of s, the limiting value of the capital share isHick's-neutral, so that the production function in (16) becomesIn this case, one could get a BGP if A t rises at precisely the rate that B t declines, so that technological change is essentially purely labor-augmenting on net: better computers would decrease the capital share at precisely the rate that automation raises it, leading to balanced growth. At first, this seems like a knife-edge result that would be unlikely in practice. However, the logic of this example is somewhat related to the model in Constant Factor SharesAnother interesting case worth considering is under what conditions can this model produce factor shares that are constant over time? Taking logs and derivatives of (15), the capital share will be constant if and only ifwhere g kt is the growth rate of k \u2261 K/L. This is very much a knife-edge condition. It requires the growth rate of \u03b2 t to slow over time at just the right rate as more and more goods get automated.Figure The perhaps surprising result in this example is that the constant factor shares occur while the growth rate of GDP rises at an increasing rate. From the earlier simulation in Figure The key to the explanation is to note that with some algebra, we can show that the constant factor share case requiresFirst, consider the case with g A = 0. We know that a true balanced growth path requires g Y = g K . This can occur in only two ways if g A = 0:The first case is the one that we explored in the previous example back in Figure Now we can see the reconciliation between Figures 1 and 2. In the absence ofg A > 0, the growth rate of the economy would fall to zero. Introducing g A > 0 with constant factor shares does increases the growth rate. To see why growth has to accelerate, equation ( Regime SwitchingA final simulation shown in Figure We assume that automation alternates between two regimes. The first is like Figure By playing with parameter values, including the growth rate of A t and \u03b2 t , it is possible to get a wide range of outcomes. For example, the fact that the capital share in the future is lower than in period 100 instead of higher can be reversed. Note: This simulation combines aspects of the two previous simulations to produce results closer in spirit to our observed data. We assume that automation alternates between two regimes. In the first, a constant fraction of the remaining tasks are automated each year. In the second, \u03b2t is constant and no new automation occurs. In both regimes, At grows at a constant rate of 0.4% per year. Regimes last for 30 years. Period 100 is highlighted with a black circle. At this point in time, the capital share is relatively high and growth is relatively low.Summing UpAutomation -an increase in \u03b2 t -can be viewed as a\"twist\" of the capital-and laboraugmenting terms in a neoclassical production function. From Uzawa's famous theorem, since we do not in general have purely labor-augmenting technical change, this setting will not lead to balanced growth. In this particular application (e.g. with \u03c1 < 0), either the capital share or the growth rate of GDP will tend to increase over time, and sometimes both. We showed one special case in which all tasks are ultimately automated that produced balanced growth in the limit with a constant capital share less than 100%. A shortcoming of this case is that it requires automation to be the only form of technological change. If, instead, the nature of automation itself improves over time -consider the plow, then the tractor, then the combine-harvester, then GPS tracking -then the model is best thought of as featuring both automation and something like improvements in A t . In this case, one would generally expect growth not to be balanced. However, a combination of periods of automation followed by periods of respite, like that shown in Figure A.I. in the Idea Production FunctionIn To keep things simple, suppose the production function for goods and services just uses labor and ideas:But suppose that various tasks are used to make new ideas according towhere \u03c1 < 0 (20)Assuming some fraction \u03b2 t of tasks have been automated -using a similar setup to that in Section 2 -the idea production function can be expressed aswhere S t is the research labor used to make ideas, and B t and C t are defined as before,Several observations then follow from this setup. First, consider the case in which \u03b2 t is constant at some value but then increases to a higher value (recall that this leads to a one-time decrease in B t and increase in C t ). The idea production function can then be written aswhere the \"\u223c\" notation means \"is asymptotically proportional to.\" The second line follows if K t /S t is growing over time (i.e. if there is economic growth) and if the elasticity of substitution in F (\u2022) is less than one, which we've assumed. In that case, the CES function is bounded by its scarcest argument, in this case researchers. Automation then essentially produces a level effect but leaves the long-run growth rate of the economy unchanged if \u03c6 < 1. Alternatively, if \u03c6 = 1 -the classic endogenous growth casethen automation raises long-run growth.Next, consider this same case of a one-time increase in \u03b2, but suppose the elasticity of substitution in F (\u2022) equals one, so that F (\u2022) is Cobb-Douglas. In this case, as in the Zeira model, it is easy to show that a one-time increase in automation will raise the long-run growth rate. Essentially, an accumulable factor in production (capital) becomes permanently more important, and this leads to a multiplier effect that raises growth.Third, suppose now that the elasticity of substitution is greater than one. In this case, the argument given before reverses, and now the CES function asymptotically looks like the plentiful factor, in this case K t . The model will then deliver explosive growth under fairly general conditions, with incomes becoming infinite in finite time. 9But this is true even without any automation. Essentially, in this case researchers are not a necessary input and so standard capital accumulation is enough to generate explosive growth. This is one reason why the case of \u03c1 < 0 -i.e. an elasticity of substitution less than one -is the natural case to consider. We focus on this case for the remainder of this section.Continuous AutomationWe can now consider the special case in which automation is such that the newlyautomated tasks constitute a constant fraction, \u03b8, of the tasks that have not yet been automated. Recall that this was the case that delivered a balanced growth path back in Section 2.2.2. In this case, B t \u2192 1 and \u010at Ct \u2192 g C = \u2212 1\u2212\u03c1 \u03c1 \u2022 \u03b8 > 0 asymptotically. The same logic that gave us equation ( where the second line holds as long as BK/CS \u2192 \u221e, which holds for a large class of parameter values. 10This reduces to the Jones (1995) kind of setup, except that now \"effective\" research grows faster than the population because of A.I. Dividing both sides of the last expression by A t gives \u0226tIn order for the left-hand side to be constant, we require that the numerator and de-9 A closely-related case is examined explicitly in the discussion surrounding equation ( nominator on the right side grow at the same rate, which then impliesIn SingularitiesTo this point, we've considered the effects of gradual automation in the goods and idea production functions and shown how that can potentially raise the growth rate of the economy. However, many observers have suggested that A.I. opens the door to something more extreme -a \"technological singularity\" where growth rates will explode.John Von Neumann is often cited as first suggesting a coming singularity in technology Ray Kurzweil inThe Singularity is Near also argues for a coming intelligence explosion through non-biological intelligence In this section, we consider singularity scenarios in light of the production functions for both goods and ideas. Whereas standard growth theory is concerned with matching the Kaldor facts, including constant growth rates, here we consider circumstances in 11 Substituting in for other solutions, the long-run growth rate of the economy is gy =, where n is the rate of population growth. which growth rates may increase rapidly over time. To do so, and to speak in an organized way to the various ideas that borrow the phrase \"technological singularity\", we can characterize two types of growth regimes that depart from steady-state growth. In particular, we can imagine:\u2022 a \"Type I\" growth explosion, where growth rates increase without bound but remain finite at any point in time.\u2022 a \"Type II\" growth explosion, where infinite output is achieved in finite time.Both concepts appear in the singularity community. While it is common for writers to predict the singularity date (often just a few decades away), writers differ on whether the proposed date records the transition to the new growth regime of Type I or an actual singularity occurring of Type II. 12   To proceed, we now consider examples of how the advent of A.I. could drive growth explosions. The basic finding is that complete automation of tasks by an A.I. can naturally lead to the growth explosion scenarios above. However, interestingly, one can even produce a singularity without relying on complete automation, and one can do it without relying on an intelligence explosion per se. Further below, we will consider several possible objections to these examples.Examples of Technological SingularitiesWe provide four examples. The first two examples take our previous models to the extreme and consider what happens if everything can be automated -that is, if people can be replaced by A.I. in all tasks. The third example demonstrates a singularity through increased automation but without relying on complete automation. The final example looks directly at \"superintelligence\" as a route to a singularity.Example 1: Automation of Goods ProductionThe Type I case can emerge with full automation in the production for goods. This is the well-known case of an AK model with ongoing technological progress. In particular, take the model of Section 2, but assume that all tasks are automated as of some date t 0 . The production function is thereafter Y t = A t K t and growth rates themselves grow exponentially with A t . Ongoing productivity growth -for example through the discovery of new ideas -would then produce ever-accelerating growth rates over time.Specifically, with a standard capital accumulation specification ( Kt = sY t \u2212 \u03b4K t ) and technological progress proceeding at rate g, the growth rate of output becomeswhich grows exponentially with A t .Example 2: Automation of Ideas ProductionAn even stronger version of this acceleration occurs if the automation applies to the idea production function instead of (or in addition to) the goods production function.In fact, one can show that there is a mathematical singularity: a Type II event where incomes essentially become infinite in a finite amount of time.To see this, consider the model of Section 3. Once all tasks can be automated -i.e. once an A.I. replaces all people in the idea production function -the production of new ideas is given byWith \u03c6 > 0, this differential equation is \"more than linear.\" As we discuss next, growth rates will explode so fast that incomes become infinite in finite time.The basic intuition for this result comes from noting that this model is essentially a two-dimensional version of the differential equation \u0226t = A 1+\u03c6 t (e.g. replacing the K with an A in equation ( And it is easy to see from this solution that A(t) exceeds any finite value before date. This is a singularity.For the two dimensional system with capital in equation ( Writing these in growth rates givesFirst, we show that \u0226t At > Kt Kt . To see why, suppose they were equal. Then equation (30) implies that Kt Kt is constant, but equation ( So it must be that \u0226tAt > Kt Kt . 13 Notice that from the capital accumulation equation, this means that the growth rate of capital is rising over time, and then the idea growth rate equation means that the growth rate of ideas is rising over time as well. Both growth rates are rising. The only question is whether they rise sufficiently fast to deliver a singularity.To see why the answer is yes, set \u03b4 = 0 and sL = 1 to simplify the algebra. Now multiply the two growth rate equations together to getWe've shown that \u0226t At > Kt Kt , so combining this with equation (31) yieldsimplying that \u0226tThat is, the growth rate of A grows at least as fast as A \u03c6/2 t . But we know from the analysis of the simple differential equation given earlier -see equation ( Because A grows faster than that, it also exhibits a singularity.Because ideas are nonrival, the overall economy is characterized by increasing returns, a la creasing returns applies to \"accumulable factors,\" which then leads to a Type II growth explosion -i.e., a mathematical singularity.Example 3: Singularities without Complete AutomationThe above examples consider complete automation of goods production (Example 1)and ideas production (Example 2). With the CES case and an elasticity of substitution less than one, we require that all tasks are automated. If only a fraction of the tasks are automated, then the scarce factor (labor) will dominate, and growth rates do not explode. We show in this section that with Cobb-Douglas production, a Type II singularity can occur as long as a sufficient fraction of the tasks are automated. In this sense, the singularity might not even require full automation.Suppose the production function for goods is Y t = A \u03c3 t K \u03b1 t L 1\u2212\u03b1 (a constant population simplifies the analysis, but exogenous population growth would not change things). The capital accumulation equation and the idea production function are then specified aswhere 0 < \u03b1 < 1 and 0 < \u03b2 < 1 and where we also take S (research effort) to be constant. Following the Zeira (1998) model discussed earlier, we interpret \u03b1 as the fraction of goods tasks that have been automated and \u03b2 as the fraction of tasks in idea production that have been automated.The standard endogenous growth result requires \"constant returns to accumulable factors.\" To see what this means, it is helpful to define a key parameter:In this setup, the endogenous growth case corresponds to \u03b3 = 1. Not surprisingly, then, the singularity case occurs if \u03b3 > 1. Importantly, notice that this can occur with both \u03b1 and \u03b2 less than one -i.e. when tasks are not fully automated. For example, in the case in which \u03b1 = \u03b2 = \u03c6 = 1/2, then \u03b3 = 2 \u2022 \u03c3, so explosive growth and a singularity will occur if \u03c3 > 1/2. We show that \u03b3 > 1 delivers a Type II singularity in the remainder of this section. The argument builds on the argument given in the previous subsection.In growth rates, the laws of motion for capital and ideas areIt is easy to show that these growth rates cannot be constant if \u03b3 > 1. 14   If the growth rates are rising over time to infinity, then eventually either g At > g Kt , or the reverse, or the two growth rates are the same. Consider the first case, i.e. g At > g Kt ;the other cases follow the same logic. Once again, to simplify the algebra, set \u03b4 = 0, S = 1, and sL 1\u2212\u03b1 = 1. Multiplying the growth rates together in this case givesSince g A > g K , we then haveWith \u03b3 > 1, the growth rate grows at least as fast as A t raised to a positive power. But even if it grew just this fast we would have a singularity, by the same arguments given before. The case with g Kt > g At can be handled in the same way, using K's instead of A's. QED.14 If the growth rate of K is constant, then \u03c3gA = (1 \u2212 \u03b1)gK , so K is proportional to A \u03c3/(1\u2212\u03b1) . Making this substitution in (35) and using \u03b3 > 1 then implies that the growth rate of A would explode, and this requires the growth rate of K to explode.Example 4: Singularities via SuperintelligenceThe examples of growth explosions above are based in automation. These examples can also be read as creating \"superintelligence\" as an artifact of automation, in the sense that advances of A t across all tasks include, implicitly, advances across cognitive tasks, and hence a resulting singularity can be conceived of as commensurate with an intelligence explosion. It is interesting that automation itself can provoke the emergence of superintelligence. However, in the telling of many futurists, the story runs differently, where an intelligence explosion occurs first and then, through the insights of this superintelligence, a technological singularity may be reached. Typically the A.I. is seen as \"self-improving\"through a recursive process. This idea can be modeled using similar ideas to those presented above. To do so in a simple manner, divide tasks into two types: physical and cognitive. Define a common level of intelligence across the cognitive tasks by a productivity term A cognitive , and further define a common productivity at physical tasks, A physical . Now imagine we have a unit of A.I. working to improve itself, where progress followsWe have studied this differential equation above, but now we apply it to cognition alone. If \u03c9 > 0, then the process of self-improvement explodes, resulting in an unbounded intelligence in finite time.The next question is how this superintelligence would affect the rest of the economy.Namely, would such superintelligence also produce an output singularity? One route to a singularity could run through the goods production function: to the extent that physical tasks are not essential (i.e. \u03c1 \u2265 0 ), then the intelligence explosion will drive a singularity in output. However, it seems noncontroversial to assert that physical tasks are essential to producing output, in which case the singularity will have potentially modest effects directly on the goods production channel.The second route lies in the idea production function. Here the question is how the superintelligence would advance the productivity at physical tasks, A physical . For example, if we writewhere \u03b3 > 0, then it is clear that A physical will also explode with the intelligence explosion. That is, we imagine that the superintelligent A.I.can figure out ways to vastly increase the rate of innovation at physical tasks. In the above specification, the output singularity would then follow directly upon the advent of the superintelligence. Of course, the idea production functions ( Objections to singularitiesThe above examples show ways in which automation may lead to rapid accelerations of growth, including ever increasing growth rates or even a singularity. Here we can consider several possible objections to these scenarios, which can broadly be characterized as \"bottlenecks\" that A.I. cannot resolve.Automation LimitsOne kind of bottleneck, which has been discussed above, emerges when some essential input(s) to production are not automated. Whether A.I. can ultimately perform all essential cognitive tasks, or more generally achieve human intelligence, is widely debated. If not, then growth rates may still be larger with more automation and capital intensity (Sections 2 and 3) but the \"labor free\" singularities featured above (Section 4.1) become out of reach.Search LimitsA second kind of bottleneck may occur even with complete automation. This type of bottleneck occurs when the creative search process itself prevents especially rapid producitivy gains. To see this, consider again the idea production function. In the second example above, we allow for complete automation and show that a true mathematical singularity can ensue. But note also that this result depends on the parameter \u03c6. In the differential equation \u0226t = A 1+\u03c6 t we will have explosive growth only if \u03c6 > 0. If \u03c6 \u2264 0, then the growth rate declines as A t advances. Many models of growth and associated evidence suggest that, on average, innovation may be becoming harder, which is consistent with low values of \u03c6 on average. 15 Fishing out or burden of knowledge processes can point toward \u03c6 < 0.Interestingly, the burden of knowledge mechanism (Jones ( Baumol Tasks and Natural LawsA third kind of bottleneck may occur even with complete automation and even with a superintelligence. This type of bottleneck occurs when an essential input does not see much productivity growth. That is, we have another form of Baumol's cost disease.To see this, generalize slightly the task-based production function (5) of Section 2 aswhere we have introduced task-specific productivity terms, a it .In contrast to our prior examples, where we considered a common technology term, A t , that affected all of aggregate production, here we imagine that productivity at some tasks may be different than others and may proceed at different rates. For example, machine computation speeds have increased by a factor of about 10 11 since World War II. 16   By contrast, power plants have seen modest efficiency gains and face limited prospects given constraints like Carnot's theorem. This distinction is important, because with \u03c1 < 0, output and growth end up being determined not by what we are good at, but by what is essential but hard to improve.In particular, let's imagine that some superintelligence somehow does emerge, but that it can only drive productivity to (effectively) infinity in a share \u03b8 of tasks, which we index from i \u2208 [0, \u03b8].Output thereafter will beClearly, if these remaining technologies a it cannot be radically improved, we no longer have a mathematical singularity (Type II growth explosion) and may not even have much future growth. We might still end up with an AK model, if all the remaining tasks can be automated at low cost, and this can produce at least accelerating growth if the a it can be somewhat improved but, again, in the end we are still held back by the productivity growth in the essential things that we are worst at improving. In fact, Moore's Law, which stands in part behind the rise of artificial intelligence, may be a cautionary tale along these lines. Computation, in the sense of arithmetic operations per second, has improved at mind-boggling rates and is now mind-bogglingly fast. Yet economic growth has not accelerated, and may even be in decline.Through the lens of essential tasks, the ultimate constraint on growth will then be the capacity for progress at the really hard problems. These constraints may in turn be determined less by the limits of cognition (i.e., traditionally human intelligence limits, which an A.I. superintelligence may overcome) and more by the limits of natural laws, such as the second law of thermodynamics, which constrain critical processes. 17 Some additional thoughtsWe conclude this section with additional thoughts on how A.I. and its potential singularity effects might affect growth and convergence.A first idea is that new A. Finally, with imitation and learning being performed mainly by super-machines in developed economies, then research labor would become (almost) entirely devoted to product innovation, increasing product variety or inventing new products (new prod-uct lines) to replace existing products. Then, more than ever, the decreasing returns to digging deeper into an existing line of product would be offset by the increased potential for discovering new product lines. Overall, ideas might end up being easier to find, if only because of the singularity effect of A.I. on recombinant idea-based growth.A.I., Firms, and Economic GrowthTo this point, we have linked artificial intelligence to economic growth emphasizing features of the production functions of goods and ideas. However, the advance of artificial intelligence and its macroeconomic effects will depend on the potentially rich behavior of firms. We have introduced one such view already in the prior section, where considerations of creative destruction provide an incentive-oriented mechanism that may be an important obstacle to singularities. In this section, we consider firm's incentives and behavior more generally to further outline the A.I. research agenda. We examine potentially first-order issues that emerge when introducing market structure, sectoral differences, and organizational considerations within firms.Market StructureExisting work on competition and innovation-led growth points to the existence of two counteracting effects: on the one hand, more intense product market competition (or imitation threat) induces neck-and-neck firms at the technological frontier to innovate in order to escape competition; on the other hand, more intense competition tends to discourage firms behind the current technology frontier to innovate and thereby catchup with frontier firms. Which of these two effects dominates, in turn depends upon the degree of competition in the economy, and/or upon how advanced the economy is:while the escape competition effect tends to dominate at low initial levels of competition and in more advanced economies, the discouragement effect may dominate for higher levels of competition or in less advanced economies. 18  Another channel whereby A.I. and the digital revolution may affect innovation and growth through affecting the degree of product market competition is in relation to the development of platforms or networks. A main objective of platform owners is to maximize the number of participants to the platform on both sides of the corresponding two-sided markets. For example Google enjoys a monopoly position as a search platform, Facebook enjoys a similar position as a social network with more than 1.7 billion users worldwide each month, and so does Booking.com for hotel reservations (more than 75% of hotel clients resort to this network). And the same goes for Uber in the area of individual transportation, Airbnb for apartment renting, and so on. The development of networks may in turn affect competition in at least two ways. First, data access may act as an entry barrier for creating new competing networks, although it did not prevent Facebook from developing a new network after Google. More importantly, networks can take advantage of their monopoly positions to impose large fees on market participants (and they do), which may discourage innovation by these participants, whether they are firms or self-employed individuals.At the end, whether escape competition or discouragement effects will dominate, will depend upon the type of sector (frontier/neck-and-neck or older/lagging), the ex-tent to which A.I. facilitates reverse engineering and imitation, and upon competition and/or regulatory policies aimed at protecting intellectual property rights while lowering entry barriers. Recent empirical work (e.g. see points at patent protection and competition policy being complementary in inducing innovation and productivity growth. It would be interesting to explore how A.I. affects this complementarity between the two policies.Sectoral ReallocationA recent paper by It then immediately follows that the diffusion of IT -and A.I. for our purposeshould lead to an expansion of sectors which rely more on external knowledge (in which the knowledge diffusion effect dominates) at the expense of the more traditional (and more self-contained) sectors where firms do not rely as much on external knowledge.Thus, in addition to its direct effects on firms' innovation and production capabilities, the introduction of IT and A.I. involve a knowledge diffusion effect which is augmented by a sectoral reallocation effect at the benefit of high-tech sectors which rely more on knowledge externalities from other fields and sectors. The positive knowledge diffusion effect is partly counteracted by the negative business-stealing effect (Baslandze shows that the latter effect has been large in the US and that without it the IT revolution would have induced yet a much higher acceleration in productivity growth for the whole US economy).Based on her analysis, We believe that the same points can be made with respect to A.I. instead of IT, and one could try and reproduce Baslandze's calibration exercise to assess the relative importance of the direct and indirect effects of A.I., to decompose the indirect effect of A.I. into its positive knowledge diffusion effect and its potentially negative competition effect, and to assess the extent to which A.I. affects overall productivity growth through its effects on sectoral reallocation. Let us revisit these various points in more details. A.I., skills, and wage premia: On A.I. and the increased gap between skilled and unskilled wage, the prediction brings us back to OrganizationA first, not surprising, finding is that more R&D intensive firms pay higher wages on average and employ a higher fraction of high-occupation workers than less R&D intensive firms (see Figure Similarly, we should expect more A.I.-intensive firms to: (i) employ a higher fraction of (more highly paid) high-skill workers; (ii) outsource an increasing fraction of lowoccupation tasks; (iii) give a higher premium to those low-occupation workers they keep within the firm (unless we take the extreme view that all the functions to be performed by low-occupation workers could be performed by robots).To rationalize the above findings and these latter predictions, let us follow Aghion,  Bergeaud, Blundell and Griffith ( In particular an important difference with the common wisdom, is that here innovativeness (or A.I. intensity) impacts on the organizational form of the firm and in particular on complementarity or substitutability between workers with different skill levels within the firm, whereas the common wisdom view takes this complementarity or substitutability as given. Think of a low-occupation employee (for example an assistant) who shows outstanding ability, initiative and trustworthiness. That employee performs a set of tasks for which it might be difficult or too costly to hire a high-skill worker; furthermore, and perhaps more importantly, the low-occupation employee is expected to stay longer in the firm than higher-skill employees, which in turn encourages the firm to invest more in trust-building and firm-specific human capital and knowledge. Overall, such low-occupation employees can make a big difference to the firm's performance.This alternative view of A.I. and firms, is consistent with the work of theorists of the firm such as Luis Garicano. Thus in Presumably, the more innovative or more A.I. intensive-the firm is, the harder it is to solve the more difficult questions, and therefore the more valuable the time of upstream high-occupation employees becomes; this in turn makes it all the more important to employ downstream -low-occupation -employees with higher ability to make sure that less problems will be passed on to the upstream -high-occupationemployees within the firm, so that these high-occupation employees will have more free time to concentrate on solving the most difficult tasks. Another interpretation of the higher complementarity between low-occupation and high-occupation employees in more innovative (or more A.I.-intensive) firms, is that the potential loss from unreliable low-occupation employees is bigger in such firms: hence the need to select out those low-occupation employees which are not reliable. This higher complementarity between low-occupation workers and other production factors in more innovative (or more A.I. intensive) firms in turn increases the bargaining power of low-occupation workers within the firm (it increases their Shapley Value if we follow Note that so far R&D investment has been used as the measure of the firm's innovativeness or frontierness. We would like to test the same predictions but using explicit measures of A.I. intensity as the RHS variable in the regressions (investment in robots, reliance on digital platforms,..). A.I. and firm's organizational form: Recent empirical studies (e.g. see A potentially helpful framework to think about firms' organizational forms, is How should the introduction of A.I. affect this trade-off between loss of control and initiative? To the extent that A.I. makes it easier for the principal to monitor the agent, more delegation of authority will be required in order to still elicit initiative from the agent. The incentive to delegate more authority to downstream agents, will also be enhanced by the fact that with A.I., suboptimal decision-making by downstream agents can be more easily corrected and reversed: in other words, A.I. should reduce the loss of control involved in delegating authority downstream. A third reason for why A.I. may encourage decentralization in decision-making, has to do with coordination costs: namely, it may be costly for the principal to delegate decision making to downstream units if this prevents these units from coordinating within the firm (see More delegation of authority in turn can be achieved through various means: in particular by eliminating intermediate layers in the firm's hierarchy, or by turning downstream units into profit centers or fully independent firms, or through horizontal integration which will commit the principal to spending time on other activities. Overall, one can imagine that the development of A.I. in more frontier sectors should lead to larger and more horizontally integrated firms, to flatter firms with more profit centers, which outsource an increasing number of tasks to independent self-employed agents.The increased reliance on self-employed independent agents will in turn be facilitated The interplay between A.I. and self-employment also involves potentially interesting dynamic aspects. Thus it might be worth looking at whether self-employment helps individuals accumulate human capital (or at least protects them against the risk of human capital depreciation following the loss of a formal job), and the more so in sectors with higher A.I. penetration. Also interesting would be to look at how the interplay between self-employment and A.I. is itself affected by government policies and institutions, and here we have primarily in mind education policy and social or income insurance for the self-employed. How do these policies affect the future performance of currently self-employed individuals, and are they at all complemented by the introduction of A.I.? In particular, do currently self-employed individuals move back to working for larger firms, and how does the probability of moving back to a regular employment vary with A.I., government policy, and the interplay between the two?Presumably, a more performing basic education system and a more comprehensive social insurance system should both encourage self-employed individuals to better take advantage of A.I. opportunities and support to accumulate skills and reputation and thereby improve their future career prospects. On the other hand, some may argue that A.I. will have a discouraging effect on self-employed individuals, if it lowers their prospects of ever reintegrating a regular firm in the future, as more A.I. intensive firms reduce their demand for low-occupation workers.Evidence on Capital Shares and Automation to DateModels that conceptualize A.I. as a force of increasing automation suggest that an upswing in automation may be seen in the factor payments going to capital -the capital share. In recent years, the rise in the capital share in the U.S. and around the world has been a central topic of research. For example, see While the facts are broadly consistent with automation (or an increase in automation), it is also clear that capital and labor shares involve many other economic forces as well. For example, Keeping that important caveat in mind, Figure The capital share in transportation equipment (including motor vehicles, but also aircraft and shipbuilding) shows a large increase in the United States, France, Germany, and Spain in recent decades. Interestingly, Italy and the U.K. exhibit declines in this capital share since 1995. The absolute level differences in the capital share for transportation equipment in 2014 are also interesting, ranging from a high of more than 50 percent in the U.S. to a low of around 20 percent in recent years in the U.K. Clearly it would be valuable to better understand these large differences in levels and trends.Automation is likely only a part of the story. Two main facts stand out from the figure. First, as noted earlier, the motor vehicles industry is by far the largest adopter of industrial robots. For example, more than 56 percent of new industrial robots purchased in 2014 were installed in the motor vehicles industry; the next highest share was under 12 percent in computers and electronic products.Second, there is little correlation between automation as measured by robots and the change in the capital share between 2004 and 2014. The overall level of industrial robot penetration is relatively small, and as we discussed earlier, other forces including changes in market power, unionization, and composition effects are moving capital shares around in a way that makes it hard for a simple data plot to disentangle. Conclusion", "conclusions": "In this paper we discussed potential implications of A.I. for the growth process. We began by introducing A.I. in the production function of goods and services and tried to reconcile evolving automation with the observed stability in the capital share and per capita GDP growth over the last century. Our model, which introduces Baumol's \"cost disease\" insight into Zeira's model of automation, generates a rich set of possible outcomes. We thus derived sufficient conditions under which one can get overall balanced growth with a constant capital share that stays well below 100%, even with nearly complete automation. Essentially, Baumol's cost disease leads to a decline in the share of GDP associated with manufacturing or agriculture (once they are automated), but this is balanced by the increasing fraction of the economy that is automated over time.The labor share remains substantial because of Baumol's insight: growth is determined not by what we are good at but rather by what is essential and yet hard to improve. We also saw how this model can generate a prolonged period with high capital share and relatively low aggregate economic growth while automation keeps pushing ahead.Next, we speculated on the effects of introducing A.I. in the production technology for new ideas. A.I. can potentially increase growth, either temporarily or permanently, depending on precisely how it is introduced. It is possible that ongoing automation can obviate the role of population growth in generating exponential growth as A.I. increasingly replaces people in generating ideas. Notably, in this paper, we've taken automation to be exogenous and the incentives for introducing A.I. in various places clearly can have first order effects. Exploring the details of endogenous automation and A.I. in this setup is a crucial direction for further research.,We then discussed the (theoretical) possibility that A.I. could generate some form of a singularity, perhaps even leading the economy to achieve infinite income in finite time. If the elasticity of substitution in combining tasks is less than one, this seems to require that all tasks be automated. But with Cobb-Douglas production, a singularity could occur even with less than full automation because the nonrivalry of knowledge gives rise to increasing returns. Nevertheless, here too the Baumol theme remains relevant: even if many tasks are automated, growth may remain limited due to areas that remain essential yet are hard to improve. Thus in the Appendix we show that if some steps in the innovation process require human R&D, then super A.I. may end up slowing or even ending growth by exacerbating business-stealing which in turn discourages human investments in innovation. Such possibilities, as well as other implications of \"super -A.I.\" (for example for cross-country convergence and property right protection), remain promising directions for future research.The paper next considered how firms may influence, and be influenced by, the advance of artificial intelligence, with further implications for understanding macroeconomic outcomes. We considered diverse issues of market structure, sectoral reallocations, and firms' organizational structure. Among the insights here we see that A.I. may in part discourage future innovation by speeding up imitation; similarly, rapid creative destruction, by limiting the returns to an innovation, may impose its own limit on the growth process. From an organizational perspective, we also conjectured that while A.I. should be skill-biased for the economy as a whole, more A.I.-intensive firms are likely to: (i) outsource a higher fraction of low-occupation tasks to other firms, and; (ii) pay a higher premium to the low-occupation workers they keep inside the firm.Finally, we examined sectoral-level evidence regarding the evolution of capital shares in tandem with automation. Consistent with increases in the aggregate capital share, the capital share also appears to be rising in many sectors (especially outside services), which is broadly consistent with an automation story. At the same time, evidence linking these patterns to specific measures of automation at the sectoral level appears weak, and overall there are many economic forces at work in the capital share trends. The average growth rate is equal to the size of each step, ln \u03b3, times the average number of innovations per unit of time, \u03bbn : i.e., g = \u03bbn ln \u03b3.", "SDG": [8]}, "artificial_intelligence_and_the_modern_productivity_paradox_a_clash_of_expectations_and_statistics": {"name": "NBER WORKING PAPER SERIES ARTIFICIAL INTELLIGENCE AND THE MODERN PRODUCTIVITY PARADOX: A CLASH OF EXPECTATIONS AND STATISTICS", "abstract": " We live in an age of paradox. Systems using artificial intelligence match or surpass human level performance in more and more domains, leveraging rapid advances in other technologies and driving soaring stock prices. Yet measured productivity growth has declined by half over the past decade, and real income has stagnated since the late 1990s for a majority of Americans. We describe four potential explanations for this clash of expectations and statistics: false hopes, mismeasurement, redistribution, and implementation lags. While a case can be made for each, we argue that lags have likely been the biggest contributor to the paradox. The most impressive capabilities of AI, particularly those based on machine learning, have not yet diffused widely. More importantly, like other general purpose technologies, their full effects won't be realized until waves of complementary innovations are developed and implemented. The required adjustment costs, organizational changes, and new skills can be modeled as a kind of intangible capital. A portion of the value of this intangible capital is already reflected in the market value of firms. However, going forward, national statistics could fail to measure the full benefits of the new technologies and some may even have the wrong sign.", "keywords": "", "introduction": "", "body": "Another potential explanation for the paradox is mismeasurement of output and productivity. In this case, it is the pessimistic reading of the empirical past, not the optimism about the future, that is mistaken. Indeed, this explanation implies that the productivity benefits of the new wave of technologies are already being enjoyed but have yet to be accurately measured. Under this explanation, the slowdown of the past decade illusory. This \"mismeasurement hypothesis\" has been put forth in several works (e.g., There is a prima facie case for the mismeasurement hypothesis. Many new technologies, like smartphones, online social networks, and downloadable media involve little monetary cost, yet consumers spend large amounts of time with these technologies. 8 Thus, the technologies might deliver substantial utility even if they account for a small share of GDP due to their low relative price. However, a set of recent studies provide good reason to think that mismeasurement is not the entire, or even a substantial, explanation for the slowdown. Concentrated Distribution and Rent DissipationA third possibility is that the gains of the new technologies are already attainable, but that through a combination of concentrated distribution of those gains and dissipative efforts to attain or preserve them (assuming the technologies are at least partially rivalrous), their effect on average productivity growth is modest overall, and is virtually nil for the median worker. For instance, two of the most profitable uses of AI to date have been for targeting and pricing online ads, and for automated trading of financial instruments, both applications with many zero-sum aspects.One version of this story asserts that the benefits of the new technologies are being enjoyed by a relatively small fraction of the economy, but the technologies' narrowly scoped and rivalrous nature creates wasteful \"gold rush\"-type activities. Both those seeking to be one of the few beneficiaries, as well as those who have attained some gains and seek to block access to others, engage in these dissipative efforts, destroying many of the benefits of the new technologies. 9Recent research offers some indirect support for elements of this story. Productivity differences between frontier firms and average firms in the same industry have been increasing in recent years Differences in profit margins between the top and bottom performers in most industries have also grown Although this evidence is important, it is not dispositive. The aggregate effects of industry concentration are still under debate, and the mere fact that a technology's gains aren't evenly distributed is no guarantee that resources will be dissipated in trying to capture them-especially that there would be enough waste to erase noticeable aggregate benefits.Implementation and Restructuring LagsEach of the first three possibilities, especially the first two, relies on explaining away the discordance between high hopes and disappointing statistical realities. One of the two elements is presumed to be somehow \"wrong.\" In the misplaced optimism scenario, the expectations for technology by technologists and investors are off base. In the mismeasurement explanation, the tools we use to gauge empirical reality aren't up to the task of accurately doing so. And in the concentrated distribution stories, the private gains for the few may be very real, but they don't translate into broader gains for the many.But there is a fourth explanation that allows both halves of the seeming paradox to be correct. It asserts that there really is good reason to be optimistic about the future productivity growth potential of new technologies, while at the same time recognizing that recent productivity growth has been low. The core of this story is that it takes a considerable time-often more than is commonly appreciated-to be able to sufficiently harness new technologies. Ironically, this is especially true for those major new technologies that ultimately have an important effect on aggregate statistics and welfare.That is, those with such broad potential application that they qualify as general purpose technologies (GPTs). Indeed, the more profound and far-reaching the potential restructuring, the longer the time lag between the initial invention of the technology and its full impact on the economy and society. This explanation implies there will be a period in which the technologies are developed enough that investors, commentators, researchers, and policy makers can imagine their potentially transformative effects even though they have had no discernable effect on recent productivity growth. It isn't until a sufficient stock of the new technology is built and the necessary invention of complementary processes and assets occurs that the promise of the technology actually blossoms in aggregate economic data. Investors are forward-looking and economic statistics are backward looking. In times of technological stability or steady change (constant velocity), the disjoint measurements will seem to track each other. But in periods of rapid change, the two measurements can become uncorrelated.There are two main sources of the delay between recognition of a new technology's potential and its measureable effects. One is that it takes time to build the stock of the new technology to a size sufficient enough to have an aggregate effect. The other is that complementary investments are necessary to obtain the full benefit of the new technology, and it takes time to discover and develop these complements are and to implement them.While the fundamental importance of the core invention and its potential for society might be clearly recognizable at the outset, the myriad necessary co-inventions, obstacles and adjustments needed along the way await discovery over time, and the required path may be lengthy and arduous. Never mistake a clear view for a short distance.This explanation resolves the paradox by acknowledging that its two seemingly contradictory parts are not actually in conflict. Rather, both parts are in some sense natural manifestations of the same underlying phenomenon of building and implementing a new technology.While each of the first three explanations for the paradox might have a role in describing its source, the explanations also face serious questions in their ability to describe key parts of the data. We find the fourth-the implementation and restructuring lags story-to be the most compelling in light of the evidence we discuss below. Thus it is the focus of our explorations in the remainder of this essay.The Argument in Favor of the Implementation and Restructuring Lags ExplanationImplicit or explicit in the pessimistic view of the future is that the recent slowdown in productivity growth portends slower productivity growth in the future. We begin by establishing one of the most basic elements of the story: that slow productivity growth today does not rule out faster productivity growth in the future. In fact, the evidence is clear that it is barely predictive at all.Total factor productivity growth is the component of overall output growth that cannot be explained by accounting for changes in observable labor and capital inputs. It has been called a \"measure of our ignorance\" As it turns out, while there is some correlation in productivity growth rates over short intervals, the correlation between adjacent ten-year periods is not statistically significant. We present below the results from a regression of different measures of average productivity growth on the previous period's average productivity growth for 10year intervals as well as scatterplots of productivity for each 10-year against the productivity in the subsequent period. The regressions in Table In all cases, the R 2 of these regressions is low, and the previous decade's productivity growth does not have statistically discernable predictive power over the next decade's growth. For labor productivity, the R 2 is 0.009. Although the intercept in the regression is significantly different from zero (productivity growth is positive, on average), the coefficient on the previous period's growth is not statistically significant. The point estimate is economically small, too. Taking the estimate at face value, one percent higher annual labor productivity growth in the prior decade (around an unconditional mean of about two percent per year) corresponds to less than 0.1 percent faster growth in the following decade. In the TFP growth regression, the R 2 is 0.023, and again the coefficient on the previous period's growth is insignificant. Similar patterns hold in the utilizationadjusted TFP regression (R 2 of 0.03). The lack of explanatory power of past productivity growth is also apparent in the scatterplots.   The old adage that \"past performance is not predictive of future results\" applies well to trying to predict productivity growth in the years to come, especially in periods of a decade or longer. Historical stagnation does not justify forward-looking pessimism.A Technology-Driven Case for Productivity OptimismSimply extrapolating recent productivity growth rates forward is not a good way to estimate the next decade's productivity growth. Does that imply we have no hope at all of predicting productivity growth? We don't think so.Instead of relying only on past productivity statistics, we can consider the technological and innovation environment we expect to see in the near future. In particular, we need to study and understand the specific technologies that actually exist and make an assessment of their potential.One does not have to dig too deeply into the pool of existing technologies or assume incredibly large benefits from any one of them to make a case that existing but still nascent technologies can potentially combine to create noticeable accelerations in aggregate productivity growth. We begin by looking at a few specific examples. We will then make the case that AI is a GPT, with broader implications.First, let's consider the productivity potential of autonomous vehicles. According to the US Bureau of Labor Statistics, in 2016 there were 3.5 million people working in private industry as \"motor vehicle operators\" of one sort or another (this includes truck drivers, taxi drivers, bus drivers, and other similar occupations). Suppose autonomous vehicles were to reduce, over some period, the number of drivers necessary to do the current workload to 1.5 million. We do not think this is a far-fetched scenario given the potential of the technology. Total nonfarm private employment in mid-2016 was 122 million. Therefore, autonomous vehicles would reduce the number of workers necessary to achieve the same output to 120 million. This would result in aggregate labor productivity (calculated using the standard BLS nonfarm private series) increasing by 1.7 percent (= 122/120). Supposing this transition occurred over 10 years, this single technology would provide a direct boost of 0.17 percent to annual productivity growth over that decade. This gain is significant, and it doesn't include many potential productivity gains from complementary changes that could accompany the diffusion of autonomous vehicles. For instance, self-driving cars are a natural complement to transportation-as-a-service rather than individual car ownership. The typical car is currently parked 95% of the time, making it readily available for its owner or primary user A second example is call centers. As of 2015, there were about 2.2 million people working in more than 6,800 call centers in the United States, and hundreds of thousands more work as home-based call center agents or in smaller sites. 11 Improved voicerecognition systems coupled with intelligence question-answering tools like IBM's Watson might plausibly be able to handle 60-70% or more of the calls, especially since, in accordance with the Pareto principle, a large fraction of call volume is due to variants on a small number of basic queries. If AI reduced the number of workers by 60%, it would increase US labor productivity by 1%, perhaps again spread over 10 years. Again, this would likely spur complementary innovations, from shopping recommendation and travel services to legal advice, consulting, and real-time personal coaching. Relatedly, citing advances in AI-assisted customer service, Beyond labor savings, advances in AI have the potential to boost total factor productivity. In particular, energy efficiency and materials usage could be improved in many large-scale industrial plants. For instance, a team from Google DeepMind recently trained an ensemble of neural networks to optimize power consumption in a data center.By carefully tracking the data already collected from thousands of sensors tracking temperatures, electricity usage, and pump speeds, the system learned how to make adjustments in the operating parameters. As a result, the AI was able to reduce the amount of energy used for cooling by 40% compared to the levels achieved by human experts. The algorithm was a general-purpose framework designed to account complex dynamics, so it is easy to see how such a system could be applied to other data centers at Google, or indeed around the world. Overall, data center electricity costs in the U.S. are about $6 billion per year, including about $2 billion just for cooling. 12   What's more, similar applications of machine learning could be implemented in a variety of commercial and industrial activities. For instance, manufacturing accounts for about $2.2 trillion of value added each year. Manufacturing companies like GE are already using AI to forecast product demand, future customer maintenance needs, and analyze performance data coming from sensors on their capital equipment. Recent work on training deep neural network models to perceive objects and achieve sensorimotor control at the same time have yielded robots that can perform a variety of hand-eye coordination tasks (e.g., unscrewing bottle caps and hanging coat hangers) Artificial Intelligence is a General Purpose TechnologyAs important as specific applications of AI may be, we argue that the more important economic effects of AI, machine learning, and associated new technologies stem from the fact that they embody the characteristics of general purpose technologies (GPTs). 13 Videos of these efforts available here: https://sites.google.com/site/imitationfromobservation/ 14 One factor that might temper the aggregate impact of AI-driven productivity gains is if product demand for the sectors with the largest productivity AI gains is sufficiently inelastic. In this case, these sectors' shares of total expenditure will shrink, shifting activity toward slower-growing sectors and muting aggregate productivity growth a la The steam engine, electricity, the internal combustion engine, and computers are each examples of important general purpose technologies. Each of them increased productivity not only directly but also by spurring important complementary innovations.For instance, the steam engine not only helped to pump water from coal mines, its most important initial application, but also spurred the invention of more effective factory machinery and new forms of transportation like steamships and railroads. In turn, these co-inventions helped give rise to innovations in supply chains and mass marketing, to new organizations with hundreds of thousands of employees, and even to seemingly unrelated innovations like standard time, which was needed to manage railroad schedules. AI, and in particular machine learning, certainly has the potential to be pervasive, to be improved upon over time, and to spawn complementary innovations, making it a candidate for an important GPT.As noted by Machine learning systems are also designed to improve over time. Indeed, what sets them apart from earlier technologies is that they are designed to improve themselves over time. Instead of requiring an inventor or developer to codify, or code, each step of a process to be automated, a machine learning algorithm can discover on its own a function that connects a set of inputs X to a set of outputs Y as long as its given a sufficiently large set of labeled examples mapping some of the inputs to outputs The improvements reflect not only the discovery of new algorithms and techniques, particularly for deep neural networks, but also their complementarities with vastly more powerful computer hardware and the availability of much larger digital datasets that can be used to train the systems Consider machine vision-the ability to see and recognize objects, to label them in photos, and to interpret video streams. As error rates in identifying pedestrians improve from one per 30 frames to about one per 30 million frames, self-driving cars become increasingly feasible Improved machine vision also makes practical a variety of factory automation tasks and medical diagnoses. Gill Pratt has made an analogy to the development of vision in animals 500 million years ago, which helped ignite the Cambrian explosion and a burst of new species on earth This in turn increases the rate of improvement. For instance, self-driving cars that encounter an unusual situation can upload that information with a shared platform where enough examples can be aggregated to infer a pattern. Only one self-driving vehicle needs to experience an anomaly for many vehicles to learn from it. Waymo, a subsidiary of Google, has cars driving 25,000 \"real\" autonomous and about 19 million simulated miles each week. 16 All of the Waymo cars learn from the joint experience of the others. Similarly, a robot struggling with a task can benefit from sharing data and learnings with other robots that use a compatible knowledge-representation framework. 17When one thinks of AI as a GPT, the implications for output and welfare gains are much larger than in our earlier analysis. For example, self-driving cars could substantially transform many non-transport industries. Retail could shift much further toward home delivery on demand, creating consumer welfare gains and further freeing up valuable highdensity land now used for parking. Traffic and safety could be optimized, and insurance risks could fall. With over 30,000 deaths due to automobile crashes in the U.S. each year, and nearly a million worldwide, there is an opportunity to save many lives. 18Why Future Technological Progress Is Consistent with Low Current ProductivityGrowthHaving made a case for technological optimism, we now turn to explaining why it is not inconsistent with-and in fact may even be naturally related to-low current productivity growth.Like other GPTs, AI has the potential to be an important driver of productivity.However, as We discussed above that a GPT can at one moment both be present and yet not affect current productivity growth if there is a need to build a sufficiently large stock of the new capital, or if complementary types of capital, both tangible and intangible, need to be identified, produced, and put in place to fully harness the GPT's productivity benefits.The time necessary to build a sufficient capital stock can be extensive. For example, it wasn't until the late 1980s, more than 25 years after the invention of the integrated circuit, that the computer capital stock reached its long-run plateau at about 5 percent (at historical cost) of total nonresidential equipment capital. It was at only half that level 10 years prior. Thus, when Solow pointed out his now eponymous paradox, the computers were finally just then getting to the point where they really could be seen everywhere. Consider the measured lag between large investments in IT and productivity benefits within firms. They interpret this as evidence of substantial IT-related intangible assets and show that firms that combine IT investments with a specific set of organizational practices are not just more productive: they also have disproportionately higher market values than firms that invest in only one or the other. This pattern in the data is consistent with a long stream of research on the importance of organizational and even cultural change when making IT investments and technology investments more generally (e.g., There is no assurance that the adjustments will be successful. Indeed, there is evidence that the modal transformation of GPT-level magnitude fails. Transforming industries and sectors requires still more adjustment and reconfiguration. Retail offers a vivid example. Despite being one of the biggest innovations to come out of the 1990s dot-com boom, the largest change in retail in the two decades that followed was not e-commerce but instead the expansion of warehouse stores and supercenters The case of self-driving cars discussed earlier provides a more prospective example of how productivity might lag technology. Consider what happens to the current pools of vehicle production and vehicle operation workers when autonomous vehicles are introduced. Employment on production side will initially increase to handle R&D, AI development, and new vehicle engineering. Furthermore, learning curve issues could well imply lower productivity in manufacturing a these vehicles during the early years Viewing Today's Paradox through the Lens of Previous General Purpose TechnologiesWe have indicated in the discussion above that we see parallels between the current paradox and those that have happened in the past. It is closely related to the Solow Paradox era circa 1990, certainly, but it is also tied closely to the experience during the diffusion of portable power (combining the contemporaneous growth and transformative effects of electrification and the internal combustion engine).Comparing the productivity growth patterns of the two eras is instructive. Figure Labor productivity during the portable power era shared remarkably similar patterns with the current series. In both eras, there was an initial period of roughly a quarter century of relatively slow productivity growth. Then both eras saw decade-long accelerations in productivity growth, spanning 1915 to 1924 in the portable power era and 1995 to 2004 more recently.The late-1990s acceleration was the (at least partial) resolution of the Solow Paradox. We imagine that the late 1910s acceleration could have similarly answered some economist's query in 1910 as to why one sees electric motors and internal combustion engines everywhere but in the productivity statistics.  Of course this past breakout growth is no guarantee that productivity must speed up again today. However, it does raise two relevant points. First, it is another example of a period of sluggish productivity growth followed by an acceleration. Second, it demonstrates that productivity growth driven by a core GPT can arrive in multiple waves.Expected Productivity Effects of an AI-Driven AccelerationTo understand the likely productivity effects of AI, it is useful to think of AI as a type of capital, specifically a type of intangible capital. It can be accumulated through investment; it is a durable factor of production; and its value can depreciate. Treating AI as Portable Power IT a type of capital clarifies how its development and installation as a productive factor will affect productivity.As with any capital deepening, increasing AI will raise labor productivity. This would be true regardless of how well AI capital is measured (which we might expect it won't be for several reasons discussed below) though there may be lags.AI's effects on total factor productivity (TFP) are more complex and the impact will depend on its measurement. If AI (and its output elasticity) were to be measured perfectly and included in the both the input bundle in the denominator of TFP and the output bundle in the numerator, then measured TFP will accurately reflect true TFP. In this case, AI could be treated just like any other measurable capital input. Its effect on output could be properly accounted for and \"removed\" by the TFP input measure, leading to no change in TFP. This isn't to say that there wouldn't be productive benefits from diffusion of AI; it is just that it could be valued like other types of capital input.There are reasons why economists and national statistical agencies might face measurement problems when dealing with AI. Some are instances of more general capital measurement issues, but others are likely to be idiosyncratic to AI. We discuss this next.Measuring AI CapitalRegardless of the effects of AI and AI-related technologies on actual output and productivity, it is clear from the productivity outlook above that the ways AI's effects will be measured are dependent on how well countries' statistics programs measure AI capital.The primary difficulty in AI capital measurement is, as mentioned above, that many of its outputs will be intangible. This issue is exacerbated by the AI the extensive use of as an input in making other capital, including new types of software, as well as human and organizational capital, rather than final consumption goods. Much of this other capital, including human capital, will, like AI itself, be mostly intangible To be more specific, effective use of AI requires developing datasets, building firmspecific human capital, and implementing new business processes. These all require substantial capital outlays and maintenance. The tangible counterparts to these intangible expenditures, including purchases of computing resources, servers, and real estate, are easily measured in the standard neo-classical growth accounting model The intuition for this effect is that in any given period t, the output of (unmeasured) AI capital stock in period t+1 is a function the input (unmeasured) existing AI capital stock in period t. When AI stock is growing rapidly, the unmeasured outputs (AI capital stock created) will be greater than the unmeasured inputs (AI capital stock used).Furthermore, suppose the relevant costs in terms of labor and other resources needed to create intangible assets are measured, but the resulting increases in intangible assets are not measured as contributions to output. In this case, not only will total GDP be undercounted but so will productivity, which uses GDP as its numerator. Thus periods of rapid intangible capital accumulation may be associated with lower measured productivity growth, even if true productivity is increasing.With missing capital goods production, measured productivity will only reflect the fact that more capital and labor inputs are used up in producing measured output. The 30 inputs used to produce unmeasured capital goods will instead resemble lost potential output. For example, a recent report from the Brookings Institution estimates that investments in autonomous vehicles have topped $80 billion from 2014 to 2017 with little consumer adoption of the technology so far. 21 This is roughly 0.44% of 2016 GDP (spread over 3 years). If all of the capital formation in autonomous vehicles was generated by equally costly labor inputs, this would lower estimated labor productivity by 0.1% per year over the last 3 years since autonomous vehicles have not yet led to any significant increase in measured final output. Similarly, according to the AI Index, enrollment in AI and ML courses at leading universities has roughly tripled over the past 10 years, and the number of venture-back AI-related start-ups has more than quadrupled. To the extent they create intangible assets beyond the costs of production, GDP will be underestimated.Eventually the mismeasured intangible capital goods investments are expected yield a return (i.e. output) by their investors. If and when measurable output is produced by these hidden assets, another mismeasurement effect leading to overestimation of productivity will kick in. When the output share and stock of mismeasured or omitted capital grows, the measured output increases produced by that capital will be incorrectly attributed to total factor productivity improvements. As the growth rate of investment in unmeasured capital goods decreases, the capital service flow from unmeasured goods effect on TFP can exceed the underestimation error from unmeasured capital goods.Combining these two effect produces a \"J-Curve\" wherein early production of intangible capital leads to underestimation of productivity growth, but later returns from the stock of unmeasured capital creates measured output growth that might be incorrectly attributed to TFP.Formally:Output Y and unmeasured capital goods with price z (zI2) are produced with production function f. The inputs of f(\u2022) are the total factor productivity A, ordinary capital K1, unmeasured capital K2, and labor L. Equation (2) describes the total differential of 21 https://www.brookings.edu/research/gauging-investment-in-self-driving-cars/ output as a function of the inputs to the production function. If the rental price of ordinary capital is r1, the rental price of unmeasured capital is r2, and the wage rate is w, we have:andwhere \ud835\udc46\ud835\udc46 \u0302 is the familiar Solow Residual as measured and \ud835\udc46\ud835\udc46 * is the correct Solow Residual accounting for mismeasured capital investments and stock.The mismeasurement is then:The right side of the equation describes a hidden capital effect and a hidden investment effect. When the growth rate of new investment in unmeasured capital multiplied by its share of output is larger (smaller) than the growth rate of the stock of unmeasured capital multiplied by its share of output, the estimated Solow Residual will underestimate (overestimate) the rate of productivity growth. Initially, new types of capital will have a high marginal product. Firms will accumulate that capital until its marginal rate of return is equal to the rate of return of other capital. As capital accumulates, the growth rate of net investment in the unmeasured capital will turn negative, causing a greater overestimate TFP. In steady state, neither net investment's share of output nor the net stock of unmeasured capital grows and the productivity mismeasurement is zero. Figure For instance, a substantial part of the value of AI output may be firm-specific. Imagine a program that figures out individual consumers' product preferences or price elasticities and matches products and pricing to predictions. This has different value to different companies depending on their customer bases and product selection, and knowledge may not be transferrable across firms. The value also depends on companies' abilities to implement price discrimination. Such limits could come from characteristics of company's market, like resale opportunities, which are not always under firms' control, or from the existence in the firm of complementary implementation assets and/or abilities. Likewise, each firm will likely have a different skill mix that it seeks in its employees, unique needs in its production process, and a particular set of supply constraints. In such cases, firmspecific data sets and applications of those data will differentiate the machine learning capabilities of one firm from another Conclusion", "conclusions": "There are plenty of both optimists and pessimists about technology and growth. The optimists tend to be technologists and venture capitalists, and many are clustered in technology hubs. The pessimists tend to be economists, sociologists, statisticians, and government officials. Many of them are clustered in major state and national capitals. There is much less interaction between the two groups than within them, and it often seems as though they are talking past each other. In this paper, we argue that in an important a sense, they are.When we talk with the optimists, we are convinced that the recent breakthroughs in AI and machine learning are real and significant. We also would argue that they form the 34 core of a new, economically important potential GPT. When we speak with the pessimists, we are convinced that productivity growth has slowed down recently and what gains there have been are unevenly distributed, leaving many people with stagnating incomes, declining metrics of health and well-being, and good cause for concern. People are uncertain about the future, and many of the industrial titans that once dominated the employment and market value leaderboard have fallen on harder times.These two stories are not contradictory. In fact, any many ways, they are consistent and symptomatic of an economy in transition. Our analysis suggests that while the recent past has been difficult, it is not destiny. Although it is always dangerous to make predictions, and we are humble about our ability to foretell the future, our reading of the evidence does provide some cause for optimism. The breakthroughs of AI technologies already demonstrated are not yet affecting much of the economy, but they portend bigger effects as they diffuse. More importantly, they enable complementary innovations that could multiply their impact. Both the AI investments and the complementary changes are costly, hard to measure, and take time to implement, and this can, at least initially, depress productivity as it is currently measured. Entrepreneurs, managers and end-users will find powerful new applications for machines that can now learn how to recognize objects, understand human language, speak, make accurate predictions, solve problems, and interact with the world with increasing dexterity and mobility.Further advances in the core technologies of machine learning would likely yield substantial benefits. However, our perspective suggests that an underrated area of research involves the complements to the new AI technologies, not only in areas of human capital and skills, but also new processes and business models. The intangible assets associated with the last wave of computerization were about ten times as large as the direct investments in computer hardware itself. We think it is plausible that AI-associated intangibles could be of a comparable or greater magnitude. Given the big changes in coordination and production possibilities made possible by AI, the ways that we organized work and education in the past are unlikely to remain optimal in the future.Relatedly, we need to update our economic measurement toolkits. As AI and its complements more rapidly add to our (intangible) capital stock, traditional metrics like GDP and productivity can become more difficult to measure and interpret. Successful companies don't need large investments in factories or even computer hardware, but they do have intangible assets that are costly to replicate. The large market values associated with companies developing and/or implementing AI suggest that investors believe there is real value in those companies. In the case that claims on the assets of the firm are publicly traded and markets are efficient, the financial market will properly value the firm the present value of its risk-adjusted discounted cash flows. This can provide an estimate of the value of both the tangible and intangible assets owned by the firm. What's more, the effects on living standards may be even larger than the benefits that investors hope to capture. It is also possible, even likely, that many people will not share in those benefits. Economists are well positioned to contribute to a research agenda of documenting and understanding the often-intangible changes associated with AI and its broader economic implications.Realizing the benefits of AI is far from automatic. It will require effort and entrepreneurship to develop the needed complements, and adaptability at the individual, organizational, and societal levels to undertake the associated restructuring. Theory predicts that the winners will be those with the lowest adjustment costs and that put as many of the right complements in place as possible. This is partly a matter of good fortune, but with the right roadmap, it is also something for which they, and all of us, can prepare.", "SDG": [8]}, "artificial_intelligence_automation_and_work": {"name": "NBER WORKING PAPER SERIES ARTIFICIAL INTELLIGENCE, AUTOMATION AND WORK", "abstract": " We summarize a framework for the study of the implications of automation and AI on the demand for labor, wages, and employment. Our task-based framework emphasizes the displacement effect that automation creates as machines and AI replace labor in tasks that it used to perform. This displacement effect tends to reduce the demand for labor and wages. But it is counteracted by a productivity effect, resulting from the cost savings generated by automation, which increase the demand for labor in non-automated tasks. The productivity effect is complemented by additional capital accumulation and the deepening of automation (improvements of existing machinery), both of which further increase the demand for labor. These countervailing effects are incomplete. Even when they are strong, automation in-creases output per worker more than wages and reduce the share of labor in national income. The more powerful countervailing force against automation is the creation of new labor-intensive tasks, which reinstates labor in new activities and tends to increase the labor share to counterbalance the impact of automation. Our framework also highlights the constraints and imperfections that slow down the adjustment of the economy and the labor market to automation and weaken the resulting productivity gains from this transformation: a mismatch between the skill requirements of new technologies, and the possibility that automation is being introduced at an excessive rate, possibly at the expense of other productivity-enhancing technologies.", "keywords": "", "introduction": "The last two decades have witnessed major advances in artificial intelligence (AI) and robotics. Future progress is expected to be even more spectacular and many commentators predict that these technologies will transform work around the world (Brynjolfsson and McAfee, 2012;Ford, 2016;Boston Consulting Group, 2015;. Recent surveys find high levels of anxiety about automation and other technological trends, underscoring the widespread concerns about their effects McKinsey, 2017).(Pew Research Center, 2017)These expectations and concerns notwithstanding, we are far from a satisfactory understanding of how automation in general, and AI and robotics in particular, impact the labor market and productivity. Even worse, much of the debate in both the popular press and academic circles centers around a false dichotomy. On the one side are the alarmist arguments that the oncoming advances in AI and robotics will spell the end of work by humans, while many economists on the other side claim that because technological breakthroughs in the past have eventually increased the demand for labor and wages, there is no reason to be concerned that this time will be any different.In this essay, we build on , as well as Zeira (1998) and Acemoglu and Restrepo (2016) to develop a framework for thinking about automation and its impact on tasks, productivity, and work.Acemoglu and Autor (2011)At the heart of our framework is the idea that automation and thus AI and robotics replace workers in tasks that they previously performed, and via this channel, create a powerful displacement effect. In contrast to prevailing presumptions in much of macroeconomics and labor economics, which maintain that productivity-enhancing technologies always increase overall labor demand, the displacement effect can reduce the demand for labor, wages and employment. Moreover, the displacement effect implies that increases in output per worker arising from automation will not result in a proportional expansion of the demand for labor. The displacement effect causes a decoupling of wages and output per worker, and a decline in the share of labor in national income.We then highlight several countervailing forces, which push against the displacement effect and may imply that automation, AI, and robotics could increase labor demand. First, the substitution of cheaper machines for human labor creates a productivity effect: as the cost of producing automated tasks declines, the economy will expand and increase the demand for labor in non-automated tasks. The productivity effect could manifest itself as an increase in the demand for labor in the same sectors undergoing automation or as an increase in the demand for labor in non-automating sectors. Second, capital accumulation triggered by increased automation (which raises the demand for capital) will also raise the demand for labor. Third, automation does not just operate at the extensive margin-replacing tasks previously performed by labor-but at the intensive margin as well, increasing the productivity of machines in tasks that have already been automated. This phenomenon, which we refer to as deepening of automation, tends to create a productivity effect but no displacement, and thus increases labor demand.Though these countervailing effects are important, they are generally insufficient to engender a \"balanced growth path,\"meaning that even if these effects were powerful, ongoing automation would still reduce the share of labor in national income (and possibly employment which tends to be linked to the labor share). We argue that there is a more powerful countervailing force that increases the demand for labor as well as the share of labor in national income: the creation of new tasks, functions and activities in which labor has a comparative advantage relative to machines. The creation of new tasks generates a reinstatement effect directly counterbalancing the displacement effect.Indeed, throughout history, we have not just witnessed pervasive automation, but a continuous process of new tasks creating new employment opportunities for labor. As tasks in textiles, metals, agriculture and other industries were being automated in the 19th and 20th centuries, a new range of tasks in factory work, engineering, repair, backoffice, management and finance generated demand for displaced workers. The creation of new tasks is not an autonomous process advancing at a predetermined rate, but one whose speed and nature are shaped by the decisions of firms, workers and other actors in society, and which might be fueled by new automation technologies. First, this is because automation, by displacing workers, may create a greater pool of labor that could be employed in new tasks. Second, the currently most discussed automation technology, AI itself, can serve as a platform to create new tasks in many service industries.Our framework also highlights that even with these countervailing forces, the adjustment of an economy to the rapid rollout of automation technologies could be slow and painful. There are some obvious reasons for this related to the general slow adjustment of the labor market to shocks, for example, because of the costly process of workers being reallocated to new sectors and tasks. Such reallocation will involve both a slow process of searching for the right matches between workers and jobs, and also the need for retraining, at least for some of the workers.A more critical, and in this context more novel, factor is a potential mismatch between technology and skills-between the requirements of new technologies and tasks and the skills of the workforce. We show that such a mismatch slows down the adjustment of labor demand, contributes to inequality, and also reduces the productivity gains from both automation and the introduction of new tasks (because it makes the complementary skills necessary for the operation of new tasks and technologies more scarce).Yet another major factor to be taken into account is the possibility of excessive automation. We highlight that a variety of factors (ranging from a bias in favor of capital in the tax code to labor market imperfections create a wedge between the wage and the opportunity cost of labor) and will push towards socially excessive automation, which not only generates a direct inefficiency but also acts as a drag on productivity growth. Excessive automation could potentially explain why, despite the enthusiastic adoption of new robotics and AI technologies, productivity growth has been disappointing over the last several decades.Our framework underscores as well that the singular focus of the research and the corporate community on automation, at the expense of other types of technologies including the creation of new tasks, could be another factor leading to a productivity slowdown, because it forgoes potentially valuable productivity growth opportunities in other domains.In the next section, we provide an overview of our approach without presenting a formal analysis. Section 3 introduces our formal framework, though to increase readability, our presentation is still fairly non-technical (and formal details and derivations are relegated to the Appendix). Section 4 contains our main results, highlighting both the displacement effect and the countervailing forces in our framework. Section 5 discusses the mismatch between skills and technologies, potential causes for slow productivity growth and excessive automation, and other constraints on labor market adjustment to automation technologies. Section 6 concludes, and the Appendix contains derivations and proofs omitted from the text.", "body": "The last two decades have witnessed major advances in artificial intelligence (AI) and robotics. Future progress is expected to be even more spectacular and many commentators predict that these technologies will transform work around the world These expectations and concerns notwithstanding, we are far from a satisfactory understanding of how automation in general, and AI and robotics in particular, impact the labor market and productivity. Even worse, much of the debate in both the popular press and academic circles centers around a false dichotomy. On the one side are the alarmist arguments that the oncoming advances in AI and robotics will spell the end of work by humans, while many economists on the other side claim that because technological breakthroughs in the past have eventually increased the demand for labor and wages, there is no reason to be concerned that this time will be any different.In this essay, we build on At the heart of our framework is the idea that automation and thus AI and robotics replace workers in tasks that they previously performed, and via this channel, create a powerful displacement effect. In contrast to prevailing presumptions in much of macroeconomics and labor economics, which maintain that productivity-enhancing technologies always increase overall labor demand, the displacement effect can reduce the demand for labor, wages and employment. Moreover, the displacement effect implies that increases in output per worker arising from automation will not result in a proportional expansion of the demand for labor. The displacement effect causes a decoupling of wages and output per worker, and a decline in the share of labor in national income.We then highlight several countervailing forces, which push against the displacement effect and may imply that automation, AI, and robotics could increase labor demand. First, the substitution of cheaper machines for human labor creates a productivity effect: as the cost of producing automated tasks declines, the economy will expand and increase the demand for labor in non-automated tasks. The productivity effect could manifest itself as an increase in the demand for labor in the same sectors undergoing automation or as an increase in the demand for labor in non-automating sectors. Second, capital accumulation triggered by increased automation (which raises the demand for capital) will also raise the demand for labor. Third, automation does not just operate at the extensive margin-replacing tasks previously performed by labor-but at the intensive margin as well, increasing the productivity of machines in tasks that have already been automated. This phenomenon, which we refer to as deepening of automation, tends to create a productivity effect but no displacement, and thus increases labor demand.Though these countervailing effects are important, they are generally insufficient to engender a \"balanced growth path,\"meaning that even if these effects were powerful, ongoing automation would still reduce the share of labor in national income (and possibly employment which tends to be linked to the labor share). We argue that there is a more powerful countervailing force that increases the demand for labor as well as the share of labor in national income: the creation of new tasks, functions and activities in which labor has a comparative advantage relative to machines. The creation of new tasks generates a reinstatement effect directly counterbalancing the displacement effect.Indeed, throughout history, we have not just witnessed pervasive automation, but a continuous process of new tasks creating new employment opportunities for labor. As tasks in textiles, metals, agriculture and other industries were being automated in the 19th and 20th centuries, a new range of tasks in factory work, engineering, repair, backoffice, management and finance generated demand for displaced workers. The creation of new tasks is not an autonomous process advancing at a predetermined rate, but one whose speed and nature are shaped by the decisions of firms, workers and other actors in society, and which might be fueled by new automation technologies. First, this is because automation, by displacing workers, may create a greater pool of labor that could be employed in new tasks. Second, the currently most discussed automation technology, AI itself, can serve as a platform to create new tasks in many service industries.Our framework also highlights that even with these countervailing forces, the adjustment of an economy to the rapid rollout of automation technologies could be slow and painful. There are some obvious reasons for this related to the general slow adjustment of the labor market to shocks, for example, because of the costly process of workers being reallocated to new sectors and tasks. Such reallocation will involve both a slow process of searching for the right matches between workers and jobs, and also the need for retraining, at least for some of the workers.A more critical, and in this context more novel, factor is a potential mismatch between technology and skills-between the requirements of new technologies and tasks and the skills of the workforce. We show that such a mismatch slows down the adjustment of labor demand, contributes to inequality, and also reduces the productivity gains from both automation and the introduction of new tasks (because it makes the complementary skills necessary for the operation of new tasks and technologies more scarce).Yet another major factor to be taken into account is the possibility of excessive automation. We highlight that a variety of factors (ranging from a bias in favor of capital in the tax code to labor market imperfections create a wedge between the wage and the opportunity cost of labor) and will push towards socially excessive automation, which not only generates a direct inefficiency but also acts as a drag on productivity growth. Excessive automation could potentially explain why, despite the enthusiastic adoption of new robotics and AI technologies, productivity growth has been disappointing over the last several decades.Our framework underscores as well that the singular focus of the research and the corporate community on automation, at the expense of other types of technologies including the creation of new tasks, could be another factor leading to a productivity slowdown, because it forgoes potentially valuable productivity growth opportunities in other domains.In the next section, we provide an overview of our approach without presenting a formal analysis. Section 3 introduces our formal framework, though to increase readability, our presentation is still fairly non-technical (and formal details and derivations are relegated to the Appendix). Section 4 contains our main results, highlighting both the displacement effect and the countervailing forces in our framework. Section 5 discusses the mismatch between skills and technologies, potential causes for slow productivity growth and excessive automation, and other constraints on labor market adjustment to automation technologies. Section 6 concludes, and the Appendix contains derivations and proofs omitted from the text.Automation, Work, and Wages: An OverviewAt the heart of our framework is the observation that robotics and current practice in AI are continuing what other automation technologies have done in the past: using machines and computers to substitute for human labor in a widening range of tasks and industrial processes.Production in most industries requires the simultaneous completion of a range of tasks. For example, textile production requires production of fiber, production of yarn from fiber (e.g., by spinning), production of the relevant fabric from the yarn (e.g., by weaving or knitting), pre-treatment (e.g., cleaning of the fabric, scouring, mercerizing and bleaching), dyeing and printing, finishing, as well as various auxiliary tasks including design, planning, marketing, transport, and retail. 1 Each one of these tasks can be performed by a combination of human labor and machines. At the dawn of the British Industrial Revolution, most of these tasks were heavily labor-intensive (some of them were merely performed). Many of the early innovations of that era were aimed at automating spinning and weaving by substituting mechanized processes for the labor of skilled artisans The mechanization of US agriculture offers another example of machines replacing workers in tasks they previously performed Yet another example of automation comes from the development of the factory system in manufacturing and its subsequent evolution. Beginning in the second half of the 18th century, the factory system introduced the use of machine tools, such as lathes and milling machines, replacing the more labor-intensive production techniques relying on skilled artisans Examples of automation are not confined to industry and agriculture. Computer software has already automated a number of tasks performed by white-collar workers in retail, wholesale, and business services. Software and AI-powered technologies can now retrieve information, coordinate logistics, handle inventories, prepare taxes, provide financial services, translate complex documents, write business reports, prepare legal briefs, and diagnose diseases. They are set to become much better at these tasks during the next workers during the Captain Swing riots to destroy threshing machines. Though these workers often appear in history books as misguided, there was nothing misguided about their economic fears. They were quite right that they were going to be displaced. Of course, had they been successful, they might have prevented the Industrial Revolution from gaining momentum with potentially disastrous consequences for technological development and our subsequent prosperity.several years (e.g., As these examples illustrate, automation involves the substitution of machines for labor and leads to the displacement of workers from the tasks that are being automated. This displacement effect is not present-or present only incidentally-in most approaches to production functions and labor demand used in macroeconomics and labor economics. The canonical approach posits that production in the aggregate (or in a sector for that matter) can be represented by a function of the form F (AL, BK), where L denotes labor and K is capital. Technology is assumed to take a \"factor-augmenting\"form, meaning that it multiplies these two factors of production as the parameters A and B do in the production function we wrote down.It might appear natural to model automation as an increase in B, that is, as capitalaugmenting technological change. However, this type of technological change does not cause any displacement and always increases labor demand and wages (see Labor-augmenting technological change, corresponding to an increase in A, does create a type of displacement if the elasticity of substitution between capital and labor is small. But in general, this type of technological change also expands labor demand, especially if capital adjusts over the long run (see Tasks, Technologies and DisplacementWe propose, instead, a task-based approach, where the central unit of production is a task as in the textile example discussed above. 3 Some tasks have to be produced by labor, while other tasks can be produced either by labor or by capital. Also, labor and capital have comparative advantages in different tasks, meaning that the relative productivity of labor varies across tasks.Our framework conceptualizes automation (or automation at the extensive margin) as an expansion in the set of tasks that can be produced with capital. If capital is sufficiently cheap or sufficiently productive at the margin, then automation will lead to the substitution of capital for labor in these tasks. This substitution results in a displacement of workers from the tasks that are being automated, creating the aforementioned displacement effect.The displacement effect could cause a decline in the demand for labor and the equilibrium wage rate. The possibility that technological improvements that increase productivity can actually reduce the wage of all workers is an important point to emphasize because it is often downplayed or ignored.With an elastic labor supply (or quasi-labor supply reflecting some labor market imperfections), a reduction in the demand for labor also leads to lower employment. In contrast to the standard approach based on factor-augmenting technological changes, a task-based approach immediately opens the way to productivity-enhancing technological developments that simultaneously reduce wages and employment.Countervailing EffectsThe presence of the displacement effect does not mean that automation will always reduce labor demand. In fact, throughout history, there are several periods where automation was accompanied by an expansion of labor demand and even higher wages. There is a number of reasons why automation will also create a positive impact on labor demand.1. The productivity effect: By reducing the cost of producing a subset of tasks, automation raises the demand for labor in non-automated tasks The productivity effect could manifest itself in two complementary ways. First, labor demand might expand in the same sectors that are undergoing automation. 4  A telling example of this process comes from the effects of the introduction of automated teller machines (ATMs) on the employment of bank tellers. Bessen (2016) documents that concurrent with the rapid spread of ATMs, a clear example of automating technology which enabled these new machines to perform tasks that were previously performed more expensively by labor-there was an expansion in the employment of bank tellers. Bessen suggests that this is because ATMs reduced the costs of banking and encouraged banks to open more branches, raising the demand for bank tellers who then specialized in a range of tasks that ATMs did not automate.Another interesting example of this process is provided by the dynamics of labor demand in spinning and weaving during the British Industrial Revolution as recounted by The productivity effect also leads to higher real incomes and thus to greater demand for all products, including those not experiencing (much) automation. The greater demand for labor from other industries might then counteract the negative displacement effect of automation. The clearest historical example of this comes from the adjustment of the US and many European economies to the mechanization of agriculture. By reducing food prices, mechanization enriched consumers who then demanded more non-agricultural goods This discussion also implies that, in contrast to the popular emphasis on the negative labor market consequences of \"brilliant\" and highly productive new technologies set to replace labor (e.g., 2. Capital accumulation: As our framework in the next section clarifies, automation corresponds to an increase in the capital intensity of production. The high demand for capital triggers further accumulation of capital (e.g., by increasing the rental rate of capital). Capital accumulation then raises the demand for labor. This may have been an important channel of adjustment of the British economy during the Industrial Revolution and of the American economy in the first half of the 20th century in the face of mechanization of agriculture, for in both cases there was rapid capital accumulation As we discuss in the next section, under some (albeit restrictive) assumptions often adopted in neoclassical models of economic growth, capital accumulation can be sufficiently powerful that automation will always increase wages in the long run (see Deepening of automation:The displacement effect is created by automation at the extensive margin-meaning the expansion of the set of tasks that can be produced by capital. But what happens if technological improvements increase the productivity of capital in tasks that have already been automated? This will clearly not create additional displacement, because labor was already replaced by capital in those tasks. But it will generate the same productivity effects we have already pointed out. These productivity effects then raise labor demand. We refer to this facet of advances in automation technology as the deepening of automation (or as automation at the intensive margin, because it is intensifying the productive use of machines).A clear illustration of the role of deepening automation comes from the introduction of new vintages of machinery replacing older vintages used in already automated tasks. For instance, in U.S. agriculture, the replacement of horse-powered reapers and harvesters by diesel tractors increased productivity, presumably with limited additional substitution of workers in agricultural tasks. 6 In line with our account of the potential role of deepening automation, agricultural productivity and wages increased rapidly starting in the 1930s, a period that coincided with the replacement of horses by tractors Another example comes from the vast improvements in the efficiency of numericallycontrolled machines used for metal cutting and processing (such as mills and lathes) as the early vintages controlled by punched cards were replaced by computerized models during the 1970s. The new computerized machines were used in the same tasks as the previous vintages, and so the additional displacement effects were probably minor. At the same time, the transition to CNC (computer numerical control) machines increased the productivity of machinists, operators, and other workers in the industry The three countervailing forces we have listed here are central for understanding why the implications of automation are much richer than the direct displacement effects might at first suggest, and why automation need not be an unadulterated negative force against the labor market fortunes of workers. Nevertheless, there is one aspect of the displacement effect that is unlikely to be undone by any of these four countervailing forces: as we show in the next section, automation necessarily makes the production process more capital intensive and tends to increase productivity more than the wage, as a consequence reducing the share of labor in national income. Intuitively, this is because it entails the substitution of capital for tasks previously performed by labor, thus squeezing labor into a narrower set of tasks.If, as we have suggested, automation has been ongoing for centuries, with or without powerful countervailing forces of the form listed here, we should have seen a fairly \"nonbalanced\" growth process with the share of labor in national income declining steadily since the beginning of the Industrial Revolution. That clearly hasn't been the case (see, e.g., New TasksAs already discussed in the Introduction, periods of intensive automation have often coincided with the emergence of new jobs, activities, industries and tasks. In 19th-century Britain, for example, there was a rapid expansion of various new industries and jobs ranging from engineers, machinists, repairmen, conductors, back-office workers and managers involved with the introduction and operation of new technologies (e.g., Our task-based framework highlights that the creation of new, labor-intensive tasks (tasks in which labor has a comparative advantage relative to capital) may be the most powerful force balancing the growth process in the face of rapid automation. Without the demand for workers from new factory jobs, engineering, supervisory tasks, accounting and managerial occupations in the second half of the 19th and much of the 20th centuries, it would have been impossible to employ millions of workers exiting the agricultural sector and traditional labor-intensive tasks.In the same way that automation has a displacement effect, we can think of the creation of new tasks as engendering a reinstatement effect. In this way, the creation of new tasks has the opposite effect of automation. It always generates additional labor demand, which notably increases the share of labor in national income. Consequently, one powerful way in which technological progress could be associated with a balanced growth path is via the balancing of the impacts of automation by the creation of new tasks.The creation of new tasks need not be an exogenous, autonomous process, entirely unrelated to automation, AI, and robotics. This is at least for two reasons.1. As emphasized in 2. Some automation technology platforms, especially AI, may facilitate the creation of new tasks. A recent report by Accenture identified entirely new categories of jobs that are emerging in firms using AI as part of their production process The applications of AI to education, health care, and design may also result in employment opportunities for new workers. Take education. Existing evidence suggests that many students, not least those with certain learning disabilities, will benefit from individualized education programs and personalized instruction Revisiting the False DichotomyThe conceptual framework outlined above, which will be further elaborated in the next section, clarifies why the current debate is centered on a false dichotomy between disastrous and totally benign effects of automation.Our task-based framework underscores that automation will always create a displacement effect. Unless neutralized by the countervailing forces, this displacement effect could reduce labor demand, wages, and also in general, employment. At the very least, this displacement effect implies that a falling share of the productivity gains will accrue to labor. The possibility that the displacement effect keeps the demand for labor from expanding at par with output pushes against the benign accounts emphasizing that technology always increases the demand for labor and benefits workers.Our framework does not support the alarmist perspectives stressing the disastrous effects of automation for labor either. Rather, it highlights several countervailing forces which soften the impact of automation on labor. More importantly, as we have argued in the previous subsection, the creation of new labor-intensive tasks has been a critical part of the adjustment process in the face of rapid automation. The creation of new tasks is not just mana from heaven. There are good reasons why market incentives will endogenously lead to the creation of new tasks that gain strength when automation itself becomes more intensive. Also, some of the most defining automation technologies of our age, such as AI, may create a platform for the creation of new sets of tasks and jobs.At the root of some of the alarmism is the belief that AI will have very different consequences for labor than previous waves of technological change. Our framework highlights that the past is also replete with automation technologies displacing workers, but this need not have disastrous effects for labor. Nor is it technologically likely that AI will replace labor in all or almost all of the tasks it is currently specializing in. This limited remit of AI can be best understood by contrasting the current nature and ambitions of AI with those of its first coming under the auspices of \"cybernetics\". The intellectual luminaries of cybernetics, such as Norbert Wiener, envisaged the production of Human-Level Artificial Intelligence-computer systems capable of thinking in a way that could not be distinguished from human intelligence-replicating all human thought processes and faculties Current practice in the field of AI, especially in its most popular and promising forms based on deep learning and various other \"big data\" methods applied to unstructured data, eschews these initial ambitions and aims at developing applied Artificial Intelligence-commercial systems specializing in clearly delineated tasks related to prediction, decision making, logistics, and pattern recognition Flies in the OintmentOur framework so far has emphasized two key ideas. First, automation does create a potential negative impact on labor through the displacement effect and also by reducing the share of labor in national income. But second, it can be counterbalanced by the creation of new tasks (as well as the productivity effect, capital accumulation and the deepening of automation, which tend to increase the demand for labor, even though they do not generally restore the share of labor in national income to its pre-automation levels).The picture we have painted does underplay some of the challenges of adjustment, however. The economic adjustment following rapid automation can be more painful than the process we have outlined for a number of reasons.Most straightforwardly, automation changes the nature of existing jobs, and the reallocation of workers from existing jobs and tasks to new ones is a complex and often slow process. It takes time for workers to find new jobs and tasks in which they can be productive, and periods during which workers are laid off from their existing jobs can create a depressed local or national labor market, further increasing the costs of adjustment. These effects are visible in recent studies that have focused on the adjustment of local US labor markets to negative demand shocks, such as The historical record also underscores the painful nature of the adjustment. The rapid introduction of new technologies during the British Industrial Revolution ultimately led to rising labor demand and wages, but this was only after a protracted period of stagnant wages, expanding poverty, and harsh living conditions. During an eighty year period extending from the beginning of the Industrial Revolution to the middle of the 19th century, wages stagnated and the labor share fell, even as technological advances and productivity growth were ongoing in the British economy, a phenomenon which There should thus be no presumption that adjustment to the changed labor market brought about by rapid automation will be a seamless, costless and rapid process.Mismatch between Skills and TechnologiesIt is perhaps telling that wages started growing in the 19th-century British economy only after mass schooling and other investments in human capital expanded the skills of the workforce. Similarly, the adjustment to the large supply of labor freed from agriculture in the early 20th-century America may have been greatly aided by the \"high school movement\" which increased the human capital of the new generation of American workers At stake here is not only the speed of adjustment, but potential gains from new technologies. If certain skills are complementary to new technologies, their absence will imply that the productivity of these new technologies will be lower than otherwise. Thus the mismatch between skills and technologies not only slows down the adjustment of employment and wages, but holds back potential productivity gains. This is particularly true for the creation of new tasks. The fact that while there is heightened concerns about job losses from automation, many employers are unable to find workers with the right skills for their jobs underscores the importance of these considerations (Deloitte and the Manufacturing Institute, 2011).Missing Productivity and Excessive AutomationThe issues raised in the previous subsection are important not least because a deep puzzle in any discussion of the impact of new technologies is missing productivity growth-the fact that while so many sophisticated technologies are being adopted, productivity growth has been slow. As pointed out by Gordon (2016), US productivity growth since 1974 (with the exception of the period from 1995-2004) compares dismally to its post-war performance. While the annual rate of labor productivity growth of the US economy averaged 2.7 percent between 1947 and 1973, it only averaged 1.5 percent between 1974 and 1994. Average productivity growth rebounded to 2.8 percent between 1995 and 2004, and then fell again to only 1.3 percent between 2005 and 2015 One line of attack argues that there is plenty of productivity growth, but it is being mismeasured. But, as pointed out by Our conceptual framework suggests some possible explanations. They center around the possibility of \"excessive automation,\" meaning faster automation than socially desirable There are two broad reasons for excessive automation, both of which we believe to be important. The first is related to the biases in the US tax code, which subsidizes capital relative to labor. This subsidy takes the form of several different provisions, including additional taxes and costs employers have to pay for labor, subsidies in the form of tax credits and accelerated depreciation for capital outlays, and additional tax credit for interest rate deductions in case of debt-financed investments Even absent such a fiscal bias, there are natural reasons for excessive automation. Labor market imperfections and frictions also tend to imply that the equilibrium wage is above the social opportunity cost of labor. Thus a social planner would use a lower shadow wage in deciding whether to automate a task than the market, creating another force towards excessive automation. The implications of this type of excessive automation would again include slower productivity growth than otherwise.Finally, it is possible that automation has continued at its historical pace, or may have even accelerated recently, but the dismal productivity growth performance we are witnessing is driven by a slowdown in the creation of new tasks or investment in other productivity-enhancing technologies (see There are natural reasons why too much emphasis on automation may come at the cost of investments in other technologies, including the creation of new tasks. For instance, in a setting where technologies are developed endogenously using a common set of resources (e.g., scientists), there is a natural trade-off between faster automation and investments in other types of technologies 3 A Model of Automation, Tasks, and theDemand for LaborIn the previous section, we provided an intuitive discussion of how automation in general, and robotics and AI in particular, is expected to impact productivity and the demand for labor. In this section, we outline a formal framework which underlines these conclusions. Our presentation will be somewhat informal and without any derivations, which are all collected in the Appendix.A Task-Based FrameworkWe start with a simplified version of the task-based framework introduced in where Y denotes aggregate output and y(x) is the output of task x. The fact that tasks run between N \u22121 and N enables us to consider changes in the range of tasks, for example because of the introduction of new tasks, without altering the total measure of tasks in the economy. Each task can be produced by human labor, \u2113(x), or by machines, m(x), depending on whether it has been (technologically) automated or not. In particular, tasks x \u2208 [0, I] are technologically automated, so can be produced by either labor or machines, while the rest are not technologically automated, so must be produced with labor:Here, \u03b3 L (x) is the productivity of labor in task x and is assumed to be increasing, while \u03b3 M (x) is the productivity of machines in automated tasks. We assume that \u03b3 L (x) \u03b3 M (x) is increasing in x, and thus labor has a comparative advantage in higher-indexed tasks. 7The threshold I denotes the frontier of automation possibilities: it describes the range of tasks that can be automated using current available technologies in AI, industrial robots, various computer-assisted technologies, and other forms of \"smart machines\".We also simplify the discussion by assuming that both the supply of labor, L, and the supply of machines, K, are fixed and inelastic. The fact that the supply of labor is inelastic implies that changes in labor demand impact the share of labor in national income and the wage, but not the level of employment. We outline below how this framework can be easily generalized to accommodate changes in employment and unemployment.Types of Technological ChangeOur framework incorporates four different types of technological advances. All advances increase productivity but as we will see with a very different impact on the demand for labor and wages.1. Labor-augmenting technological advances: Standard approaches in macroeconomics and labor economics typically focus on labor-augmenting technological advances. Such technological changes correspond to increases (or perhaps an equi-proportionate increase) in the function \u03b3 L (x). Our analysis will show that they are in fact quite special, and the implications of automation and AI are generally very different from those of labor-augmenting advances.7 Our theoretical framework builds on Zeira (1998) who develops a model where firms produce intermediates using labor-intensive or capital-intensive technologies. Zeira focuses on how wages affect the adoption of capital-intensive production methods and how this margin amplifies productivity differences across countries and over time. In contrast, we focus on the implications of automation-modeled here as an increase in the set of tasks that can be produced by machines, represented by I-for the demand for labor, wages, and employment, and we also study the implications of the introduction of new tasks. In Automation (at the extensive margin):We consider automation to be an expansion of the set of tasks that are technologically automated as represented by the parameter I.3. Deepening of automation (or automation at the intensive margin): Another dimension of advances in AI and robotics technology will tend to increase the productivity of machines in tasks that are already automated, for example, by replacing existing machines with newer, more productive vintages. In terms of our model, this corresponds to an increase in the \u03b3 M (x) function for tasks x < I. We will see that this type of deepening of automation has very different implications for labor demand than automation (at the extensive margin).Creation of new tasks:As emphasized in EquilibriumThroughout, we denote the equilibrium wage rate by W and the equilibrium cost of machines (or the rental rate) by R. An equilibrium requires firms to choose the costminimizing way of producing each task, and labor and capital markets to clear.To simplify the discussion, we impose the following assumptionThe second inequality implies that all tasks in [N \u2212 1, I] will be produced by machines.The first inequality implies that the introduction of new tasks-an increase in N-will increase aggregate output. This assumption is imposed on the wage to rental rate ratio, which is an endogenous object; the Appendix provides a condition on the stock of capital and labor which is equivalent to this assumption (see Assumption A2). We also show in the Appendix that aggregate output (GDP) in the equilibrium takes the formwhere B = expAggregate output is given by a Cobb-Douglas aggregate of the capital stock and employment. This resulting aggregate production function in (3) is itself derived from the allocation of the two factors of production to tasks. More importantly, the exponents of capital and labor in this production function depend on the extent of automation, I, and the creation of new tasks, as captured by N.Central to our focus is not only the impact of new technologies on productivity but also on the demand for labor. The Appendix shows that the demand for labor can be expressed asThis equation is intuitive in view of the Cobb-Douglas production function in (3), since it shows that the wage (the marginal product of labor) is equal to the average product of labor-which we will also refer to as \"productivity\"-times the exponent of labor in the aggregate production function. Equation ( 4 Technology and Labor DemandThe Displacement EffectOur first result shows that automation (at the extensive margin) indeed creates a displacement effect, reducing labor demand as emphasized in Section 2, but also that it is counteracted by a productivity effect, pushing towards greater labor demand. Specifically, from equation ( Without the productivity effect, automation would always reduce labor demand, because it is directly replacing labor in tasks that were previously performed by workers. Indeed, if the productivity effect is limited, automation will reduce labor demand and wages.Counteracting the Displacement Effect I: The Productivity EffectsThe productivity effect, on the other hand, captures the important idea that technologies that increase productivity will tend to raise labor demand and wages. As highlighted in the previous section, there are two complementary manifestations of the productivity effect. The first works by increasing the demand for labor in non-automated tasks in the industries where automation is ongoing. The second works by raising the demand for labor in other industries that are not automating as much. The productivity effect shown in equation ( One important implication of the decomposition in equation ( To elaborate further on this point and to understand the productivity implications of automation technologies better, let us also express the productivity effect in terms of the physical productivities of labor and machines and factor prices as followsThe fact that this expression is positive, and that new automation technologies will be adopted, follows from Assumption A1. Using this expression, the overall impact on labor demand can be alternatively written asThis expression clarifies that the displacement effect of automation will dominate the productivity effect and thus reduce labor demand (and wages) when \u03b3 M (I) R \u2248 \u03b3 L (I) W , which is exactly the case where new technologies are so-so-only marginally better than labor at newly-automated tasks. In contrast, when \u03b3 M (I) R >> \u03b3 L (I) W , automation will increase productivity sufficiently to raise the demand for labor and wages.Turning next to the implications of automation for the labor share, equation ( Counteracting the Displacement Effect II: Capital AccumulationWe have so far emphasized the displacement effect created by new automation technologies. We have also seen that the productivity effect-resulting from the fact that automation technologies reduced costs-counteracts the displacement effects to some degree. In this and the next subsection, we discuss two additional countervailing forces.The first discussed in this section is capital accumulation. The analysis so far assumed that the economy has a fixed supply of capital that could be devoted to new machines (automation technologies). As a result, a further increase in automation (at the extensive margin) increases the demand for capital and thus the equilibrium rental rate, R. This may be understood as the short-run effect of automation.Instead, we may envisage the \"medium-run\" effect as the impact of these technologies after the supply of machines used in newly automated tasks expands as well. Because machines and labor are q\u2212complements, an increase in the capital stock, with the level of employment held constant at L, increases the real wage and reduces the rental rate. Equation ( In the limit, if capital accumulation fixes the rental rate at a constant level (which will be the case, for example, when we have a representative household with exponential discounting and time-separable preferences), the productivity effect will always dominate the displacement effect. 9Crucially, however, equation ( Counteracting the Displacement Effect III: Deepening of AutomationAnother potentially powerful force counteracting the displacement effect from automation at the extensive margin comes from the deepening of automation (or automation at the intensive margin), for example, because of improvements in the performance of already existing automation technologies or the replacement of such technologies with newer, more productive vintages. This increase in the productivity of machines in tasks that are already automated corresponds in our model to an increase in the function \u03b3 M (x) in tasks below I.To explore the implications of this type of change in the simplest possible way, let us suppose that \u03b3 M (x) = \u03b3 M in all automated tasks, and consider an increase in the productivity of machines by d ln \u03b3 M > 0, with no change in the extensive margin of automation, I. The implications of this change in the productivity of machines on equilibrium wages and productivity can be obtained as:Hence, deepening of automation will tend to increase labor demand and wages, further counteracting the displacement effect. Note, however, that as with capital accumulation, in our model this has no impact on the share of labor in national income, as can be seen from the fact that wages and productivity increase by exactly the same amount.New Tasks and the Comparative Advantage of LaborMuch more powerful than the countervailing effects of capital accumulation and the deepening of automation is the creation of new tasks in which labor has a comparative advantage. These tasks include both new, more complex versions of existing tasks and the creation of new activities, which are made possible by advances in technology. In terms of our framework, they correspond to increases in N.An increase in N-the creation of new tasks-raises productivity bywhich is positive from Assumption A1. More importantly for our focus here, the creation of new tasks also increases labor demand and equilibrium wages by creating a reinstatement effect counteracting the displacement effect. In particular,In contrast to capital accumulation and the deepening of automation, which increase the demand for labor but do not affect the labor share, equation ( The centrality of new tasks can be understood when viewed from a complementary historical angle. Automation is not a recent phenomenon. As we already discussed in Section 2, the history of technology of the last two centuries is full of examples of automation, ranging from weaving and spinning machines to the mechanization of agriculture as discussed in the previous section. Even with capital accumulation and the deepening of automation, if there were no other counteracting force, we would see the share of labor in national income declining steadily. Our conceptual framework highlights a major force preventing such a decline-the creation of new tasks in which labor has a comparative advantage.This can be seen by putting together equations ( and also from ( Consequently, for the labor share to remain stable and for wages to increase in tandem with productivity, we need I-capturing the extensive margin of automation-to grow by the same amount as the range of new tasks, N. When that happens, equilibrium wages grow proportionately with productivity, and the labor share, s L , remains constant, as can be seen from the fact that the first line of ( which is strictly positive because of Assumption A1.A False Dichotomy: RecapWith our conceptual framework exposited in a more systematic manner, we can now briefly revisit the false dichotomy highlighted in the Introduction. Our analysis, in particular equation ( The productivity effect is supplemented by the capital accumulation that automation sets in motion and the deepening of automation, which increases the productivity of machines in tasks that have already been automated. But even with these countervailing effects, equation ( Our framework suggests that the biggest shortcoming of the alarmist and the optimist views is their failure to recognize that the future of labor depends on the balance between automation and the creation of new tasks. Automation will often lead to a healthy growth of labor demand and wages if it is accompanied with a commensurate increase in the set of tasks in which labor has a comparative advantage-a feature that alarmists seem to ignore. Even though there are good economic reasons for why the economy will create new tasks, this is neither a foregone conclusion nor something we can always count on without the requisite investments and adjustments-as the optimists seem to assume. AI and robotics could be permanently altering this balance, causing automation to pace ahead of the creation of new tasks with negative consequences for labor, at the very least in regards to the share of labor in national income.GeneralizationsMany of the features adopted in the previous subsection are expositional simplifications. In particular, the aggregate production function (1) can be taken to be any constant elasticity of substitution aggregate. One implication of this would be that aggregate output in equation ( A final feature that is worth commenting on is the fact that in the aggregate production function, (1), the limits of integration are N \u2212 1 and N, ensuring that the total measure of tasks is one. This is useful for several reasons. First, when the introduction of new tasks expands the total measure of tasks, it becomes more challenging to obtain a balanced growth path (see , where \u03b1 \u2265 0 represents these productivity gains from task variety and ensures that the qualitative results explicit here continue to apply.Employment and UnemploymentAn additional generalization concerns the endogenous adjustment of employment in the face of new automation technologies. We have so far taken labor to be supplied inelastically for simplicity. There are two ways in which the level of employment responds to the arrival of new technologies. The first is via a standard labor supply margin. The second possibility is through labor market frictions, for example as in For now, however, the more important implications of such extensions is to link the level of employment (or unemployment) to labor demand. Automation, when it reduces labor demand, will also reduce the level of employment (or increase the level of unemployment). Moreover, because the supply of labor depends on the labor share, in our framework automation results in a reduction in employment (or an increase in unemployment). As such, our analysis so far also sheds light on (and clarifies the conditions for) the claims that new automation technologies will reduce employment. It also highlights, however, that the fact that automation has been ongoing does not condemn the economy to a declining path of employment. If automation is met by equivalent changes in the creation of new tasks, the share of labor in national income can remain stable and ensure a stable level of employment (or unemployment) in the economy.Constraints and InefficienciesEven in the presence of the countervailing forces limiting the displacement effect from automation, there are potential inefficiencies and constraints limiting the smooth adjustment of the labor market and hindering the productivity gains from new technologies.Here we focus on how the mismatch between skills and technologies not only increases inequality, but also hinders the productivity gains from automation and new tasks. We then explore the possibility that, concurrent with rapid automation, we are experiencing a slowdown in the creation of new tasks, which could result in slow productivity growth. Finally, we examine how a range of factors lead to excessive automation, which not only creates inefficiency but also hinders productivity.Mismatch of Technologies and SkillsThe emphasis on the creation of new tasks counter-balancing the potential negative effects of automation on the labor share and the demand for labor ignores an important caveat and constraint-the potential mismatch between the requirements of new technologies (tasks) and the skills of the workforce. To the extent that new tasks require skilled employees or even new skills to be acquired, the adjustment may be much slower-or even severely hampered-than our analysis so far may suggest.To illustrate these ideas in the simplest possible fashion, we follow In this simple extension of the framework presented so far, the threshold S can be considered as an inverse measure of the mismatch between new technologies and skills. A greater value of S implies that there are plenty of additional tasks for low-skill workers, while a low value of S implies the presence of only a few tasks left that low-skill workers can perform.Assuming that in equilibrium W H > W L , 12 which implies that low-skill workers will perform all tasks in the range (I, S), equilibrium wages satisfyThus, the impact of automation on inequality-defined here as the wage premium between high and low-skill workers-is given byThis equation shows that automation increases inequality. This is not surprising, since the tasks that are automated are precisely those performed by low-skill workers. But in addition, it also demonstrates that the impact of automation on inequality becomes Worse when there is a severe skill mismatch-the threshold S is close to I. In this case, displaced workers will be squeezed into a very small range of tasks, and hence, each of these tasks will receive a large number of workers and will experience a substantial drop in its price, which translates into a sharp decline in the wage of low-skill workers. In contrast, when S is large, displaced workers can spread across a larger set of tasks without depressing their wage as much.A severe mismatch in the skills required in automated tasks and other jobs also affects the productivity gains from automation. In particular, we haveThis equation shows that the productivity gains from automation depend positively on W L R: it is precisely when displaced workers have a high opportunity cost that automation raises productivity. Using the fact that R = Y K (I \u2212 N + 1), we obtainA worse mismatch (a lower S) reduces the opportunity cost of displaced workers further, and via this channel, it makes automation less profitable. This is because a severe mismatch impedes reallocation, reducing the productivity gains of freeing workers from automated tasks. Equally important are the implications of a skill mismatch for the productivity gains from new tasks. Namely,which depends negatively on W H R: it is precisely when high-skill workers have a relatively high wage that the gains from new tasks will be limited. With similar arguments to before, we also havewhich implies that in the presence of a worse mismatch (a lower S), the productivity gains from new tasks will be limited. This is because new tasks require high-skill workers who are scarce and expensive when S is low.An important implication of this analysis is that to limit increasing inequality and to best deploy new tasks and harness the benefits of automation, society may need to simultaneously increase the supply of skills. A balanced growth process requires not only automation and the creation of new tasks to go hand-in-hand, but also the supply of high-skill workers to grow in tandem with these technological trends.Automation at the Expense of New TasksAs discussed in Section 2, a puzzling aspect of recent macroeconomic developments has been the lack of robust productivity growth despite the bewildering array of new technologies. While some have argued that this is because of mismeasurement of true productivity, our conceptual framework provides three novel (and at least to us, more compelling) reasons for slow productivity growth. The first was discussed in the previous subsection.The second one, discussed in this subsection, is that concurrent with the rapid introduction of new automation technologies, we may be experiencing a slow down in the creation of new tasks and investments in other technologies that benefit labor. This explanation comes in two flavors. First, we may be running out of good ideas to create new jobs, sectors, and products capable of expanding the demand for labor (e.g., Both explanations hinge on the redirection of research activity from the creation of new tasks to automation-in the first case exogenously and in the second for endogenous reasons. Recall from our analysis so far that the productivity gains from new tasks in our baseline framework are given bywhile productivity gains from automation areIf the former expression is greater than the latter, then the redirection of research effort from the creation of new tasks towards automation, or a lower research efficiency in creating new tasks, will lead to a slowdown of productivity growth, even if advances in automation are accelerating and being adopted enthusiastically. This conclusion is strengthened if additional effort devoted to automation at the expense of the creation of new tasks is runs into diminishing returns.Excessive AutomationIn this subsection, we highlight the third reason for why there may be missing productivity growth: socially excessive automation (see To illustrate why our framework can generate excessive automation, we modify the assumption that the supply of capital, K, is given, and instead suppose that machines used in automation are produced-as intermediate goods-using the final good at a fixed cost R. Moreover, suppose that because of subsidies to capital, accelerated depreciation allowances, tax credit for debt-financed investment or simply because of the tax cost of employing workers, capital receives a marginal subsidy of \u03c4 > 0.Given this subsidy, the rental rate for machines is R(1 \u2212 \u03c4 ), and Assumption A1 now becomesLet us now compute GDP as value added, subtracting the cost of producing machines. This gives us GDP = Y \u2212 RK.Suppose next that there is an increase in automation. Then we haveThe first term is positive and captures the productivity increase generated by automation. However, when \u03c4 > 0-so that the real cost of using capital is distorted-we have an additional negative effect originating from excessive automation. 13 At the root of this negative effect is the fact that subsidies induce firms to substitute capital for labor even when this is not socially cost-saving (though it is privately beneficial because of the subsidy). This conclusion is further strengthened when there are also labor market frictions as pointed out in Section 2. To illustrate this point in the simplest possible fashion, let us assume that there is a threshold J \u2208 (I, N) such that, when performing the tasks in [I, J], workers earn rents \u03c9 > 0 proportional to their wage in other tasks. In particular, workers are paid a wage W to produce tasks in [J, N], and a wage W (1 + \u03c9 ) to produce tasks in (I, J). 14 Let L A denote the total amount of labor allocated to the tasks in (I, J), and note that these are the workers that will be displaced by automation, i.e., by a small increase in I. Given this additional distortion, Assumption A1 now becomesThe demand for labor in tasks where workers earn rents is nowThe demand for labor in tasks where workers do not earn rents is13 We show in the Appendix that K = Y R (I \u2212 N + 1), which implies that K increases in I. 14 The assumption that there are rents only in a subset of tasks is adopted for simplicity. The same results apply: (i) when there are two sectors and one of the sectors has higher rents/wages for workers and enables automation; (ii) there is an endogenous margin between employment and nonemployment and labor market imperfections (such as search, bargaining or efficiency wages) create a wedge between wages and outside options. In both cases the automation decisions of firms fail to internalize the gap between the market wage and the opportunity cost of labor, leading to excessive automation (see Dividing these two expressions, we obtain the equilibrium condition for L A ,which implies that the total number of workers earning rents declines with automation. Moreover, the Appendix shows that (gross) output is now given byand GDP is still given by Y \u2212 RK. Equation ( Equation ( The new term W \u03c9 dL A dI captures the first-order losses from a decline in employment in tasks (I, J). These losses arise because by automating jobs where workers earn rents, firms are effectively displacing workers to other tasks in which they have a lower marginal product and earn a strictly lower wage, which increases the extent of misallocation.The point highlighted here is much more general. Without labor market frictions, automation increases GDP (and net output), so at the very least it is possible to redistribute the gains that it creates to make workers-of different skill levels-better off. Labor market frictions change this picture dramatically. In the presence of such frictions, firms' automation decisions do not internalize the fact that the marginal product of labor is above its opportunity cost, or equivalently, do not recognize that there are first-order losses that workers will suffer as a result of automation. Consequently, equilibrium automation could reduce GDP and welfare, and there may not be a way to make (all) workers better off even with tools for costless redistribution. Under these circumstances, a utilitarian planner would choose a lower level of automation than the equilibrium. 15Concluding Remarks", "conclusions": "Despite the growing concerns and intensifying debate about the implications of automation for the future of work, the economics profession and popular discussions lack a satisfactory conceptual framework. To us this lack of appropriate conceptual approach is also the key reason why much of the debate is characterized by a false dichotomy-between the view that automation will spell the end of work for humans and the argument that technologies will always tend to increase the demand for labor as they have done in the past.In this paper, we summarized a conceptual framework that can help understand the implications of automation and bridge the opposite sides of this false dichotomy. At the center of our framework is a task-based approach, where automation is conceptualized as replacing labor in tasks that it used to perform. This type of replacement causes a direct displacement effect, reducing labor demand. If this displacement effect is not counterbalanced by other economic forces, it will reduce labor demand, wages and employment. But our framework also emphasizes that there are several countervailing forces. These include the fact that automation will reduce the costs of production and thus create a productivity effect, the induced capital accumulation, and the deepening of automation-technological advances that increase the productivity of machines in tasks that have already been automated.Our framework also emphasizes that these first-order countervailing forces are generally insufficient to totally balance out the implications of automation. In particular, even if these forces are strong, the displacement effect of automation tends to cause a decline in the share of labor in national income. But we know from the history of technology and industrial development that despite several waves of rapid automation, the growth process has been more or less balanced, with no secular downward trend in the share of labor in national income. We argue this is because another powerful force is balancing the implications of automation: the creation of new tasks in which labor has a comparative advantage, which fosters a countervailing reinstatement effect for labor. These tasks increase the demand for labor and tend to raise the labor share. When they go hand-inhand with automation, the growth process is balanced, and it need not imply a dismal scenario for labor.Nevertheless, the adjustment process is likely to be slower and more painful than this account of balance between automation and new tasks at first suggests. This is because the reallocation of labor from its existing jobs and tasks to new ones is a slow process, in part owing to time-consuming search and other labor market imperfections. But even more ominously, new tasks require new skills, and especially when the education sector does not keep up with the demand for new skills, a mismatch between skills and technologies is bound to complicate the adjustment process. We have also argued why such a mismatch will hinder the productivity gains from new technologies.Our framework further suggests that there are additional reasons for the productivity slowdown. At the center of these is a tendency for excessive automation because of the tax treatment of capital investments and labor market imperfections. Excessive automation directly reduces productivity, but may have even more powerful indirect effects, because it redirects technological improvements away from productivity-enhancing activities that lead to the creation of new tasks and deepening automation to excessive efforts at the extensive margin of automation, a picture that receives informal support from the current singular focus on AI and deep learning.We would like to conclude by pointing out a number of additional issues that may be important in understanding the full impact of AI and other automation technologies on future prospects of labor. We believe that these issues can be studied using simple extensions of the framework presented here.First, we have emphasized the role of the productivity effect in partially counterbalancing the displacement effect created by automation. However, this countervailing effect works by increasing the demand for products. As we have also seen, automation tends to increase inequality. If, as a consequence of this distributional impact, the rise in real incomes resulting from automation ends up in the hands of an narrow segment of the population with much lower marginal propensity to consume than those losing incomes and their jobs, these countervailing forces would be weakened and might operate much more slowly. This imbalance in the distribution of the gains from automation might slow down the creation of new tasks as well.Second, our analysis highlighted the negative consequences of a shortage of skills for realizing the productivity gains from automation and for inequality. In practice, the problem may be workers acquiring the wrong types of skills rather than a general lack of skills. For example, if AI and other new automation technologies necessitate a mix of numeracy, communication, and problem-solving skills different than those emphasized in current curricula, this would have implications similar to those of a shortage of skills, but it cannot be overcome by just increasing educational spending with current educational practices remaining intact. One important consideration in this respect is that there is little concrete information about what types of skills new technologies will complement, underscoring the importance of further empirical work in this area.Third, government policies and labor market institutions may impact not just the speed of automation (and thus whether there is excessive automation), but what types of automation technologies will receive more investments. To the extent that some uses of AI may complement labor more or generate opportunities for more rapid creation of new tasks, an understanding of the impact of various policies, including support for academic and applied research, and social factors on the path of development of AI is critical.Last but not least, the development and adoption of productivity-enhancing AI technologies cannot be taken for granted. If we do not find a way of creating shared prosperity from the productivity gains generated by AI, there is a danger that the political reaction to these new technologies may slow down or even completely stop their adoption and development. This underscores the importance of studying the distributional implications of AI, the political economy reactions to it, and the design of new and improved institutions for creating more broadly shared gains from these new technologies.Similarly, aggregating the demand for labor and setting it equal to its inelastic supply, L, we obtain the market-clearing condition for labor as L = Y W (N \u2212 I).\u2022 Rearranging these two equations, the equilibrium rental rate and wage can be obtained aswhich are the expressions used in the text.We next turn to deriving the expression for aggregate output.\u2022 Because we normalized the price of the final good to 1 as numeraire, we have ", "SDG": [8, 9]}, "fair_transparent_and_accountable_algorithmic_decision_making_processes": {"name": "Fair, transparent and accountable algorithmic decision-making processes The premise, the proposed solutions, and the open challenges", "abstract": " The combination of increased availability of large amounts of finegrained human behavioral data and advances in machine learning is presiding over a growing reliance on algorithms to address complex societal problems. Algorithmic decision-making processes might lead to more objective and thus potentially fairer decisions than those made by humans who may be influenced by greed, prejudice, fatigue, or hunger. However, algorithmic decision-making has been criticized for its potential to enhance discrimination, information and power asymmetry, and opacity. In this paper we provide an overview of available technical solutions to enhance fairness, accountability and transparency in algorithmic decision-making. We also highlight the criticality and urgency to engage multi-disciplinary teams of researchers, practitioners, policy makers and citizens to co-develop, deploy and evaluate in the real-world algorithmic decision-making processes designed to maximize fairness and transparency. In doing so, we describe the Open Algortihms (OPAL) project as a step towards", "keywords": "algorithmic decision-making,algorithmic transparency,fairness,accountability,social good", "introduction": "Today's vast and unprecedented availability of large-scale human behavioral data is profoundly changing the world we live in. Massive streams of data are available to train algorithms which, combined with increased analytical and technical capabilities, are enabling researchers, companies, governments and other public sector actors to resort to data-driven machine learning-based algorithms to tackle complex problems [26,. Many decisions with significant individual and societal implications previously made by humans alone -often by experts-are now made or assisted by algorithms, including hiring 67], lending [12], policing [34], criminal sentencing [66], and stock trading [5]. Datadriven algorithmic decision making may enhance overall government efficiency and public service delivery, by optimizing bureacucratic processes, providing real-time feedback and predicting outcomes [33]. In a recent book with the evocative and provocative title \"Technocracy in America\", international relations expert Parag Khanna argued that a data-driven direct technocracy is a superior alternative to today's (alleged) representative democracy, because it may dynamically capture the specific needs of the people while avoiding the distortions of elected representatives and corrupt middlemen [62]. Human decision making has often shown significant limitations and extreme bias in public policy, resulting in inefficient and/or unjust processes and outcomes [35][2,23,57,. The turn towards data-driven algorithms can be seen as a reflection of a demand for greater objectivity, evidence-based decision-making, and a better understanding of our individual and collective behaviors and needs.65]At the same time, scholars and activists have pointed to a range of social, ethical and legal issues associated with algorithmic decision-making, including bias and discrimination [4,, and lack of transparency and accountability 63][15,47,70,. For example, Barocas and Selbst 69] showed that the use of algorithmic decision making processes could result in disproportionate adverse outcomes for disadvantaged groups, in ways suggestive of discrimination. Algorithmic decisions can reproduce and magnify patterns of discrimination, due to decision makers' prejudices [4], or reflect the biases present in the society [46]. A recent study by ProPublica of the COMPAS Recidivism Algorithm (an algorithm used to inform criminal sentencing decisions by predicting recidivism) found that the algorithm was significantly more likely to label black defendants than white defendants, despite similar overall rates of prediction accuracy between the two groups [46]. Along this line, a nominee for the National Book Award, Cathy O'Neil's book, \"Weapons of Math Destruction\", details several case studies on harms and risks to public accountability asso-ciated with big data-driven algorithmic decision-making, particularly in the areas of criminal justice and education [3].[45]In 2014, the White House released a report titled \"Big Data: Seizing opportunities, preserving values\"  highlighting the discriminatory potential of Big Data, including how it could undermine longstanding civil rights protections governing the use of personal information for credit, education, health, safety, employment, etc. For example, data-driven algorithmic decisions about applicants for jobs, schools or credit may be affected by hidden biases that tend to flag individuals from particular demographic groups as unfavorable for such opportunities. Such outcomes can be self-reinforcing, since systematically reducing individuals' access to credit, employment and education will worsen their situation, and play against them in future applications. For this reason, a subsequent White House report called for \"equal opportunity by design\" as a guiding principle in those domains [50]. Furthermore, the White House Office of Science and Technology Policy, in partnership with Microsoft Research and others, has co-hosted several public symposiums on the impacts and challenges of algorithms and Artificial Intelligence, specifically relating to social inequality, labor, healthcare and ethics. 1  At the heart of the matter is the fact that technology outpaces policy in most cases; here, governance mechanisms of algorithms have not kept pace with technological development. Several researchers have recently argued that current control frameworks are not adequate for situations in which a potentially unfair or incorrect decision is made by a computer [43].[4]Fortunately, there is increasing awareness of the detrimental effects of discriminatory biases and opacity of some data-driven algorithmic decisionmaking systems, and of the need to reduce or eliminate them. A number of research and advocacy initiatives are worth noting, including the Data Transparency Lab 2 , a \"community of technologists, researchers, policymakers and industry representatives working to advance online personal data transparency through research and design\", and the DARPA Explainable Artificial Intelligence (XAI) project 3 . A tutorial on the subject was held at the 2016 ACM Knowledge and Data Discovery conference . Researchers from New York University's Information Law Institute -such as Helen Nissenbaum and Solon Barocas-and Microsoft Research -such as Kate Crawford and Tarleton Gillespie-have held several workshops and conferences these past few years on the ethical and legal challenges related to algorithmic governance and decisionmaking. 4 . Lepri et al. [27] recently discussed the need for social good decisionmaking algorithms (i.e. algorithms strongly influencing decision-making and resource optimization of public goods, such as public health, safety, access to finance and fair employment) to provide transparency and accountability, to only use personal information -created, owned and controlled by individuals-with explicit consent, to ensure that privacy is preserved when data is analyzed in aggregated and anonymized form, and to be tested and evaluated in context by means of living lab approaches involving citizens.[38]In this paper, we focus on two of the main risks (namely, discrimination and lack of transparency) posed by data-driven predictive models leading to decisions that impact the daily lives of millions of people. There are additional challenges that we do not discuss in this paper. For example, issues relating to data ownership, privacy, informed consent and limited understanding (literacy) about algorithms' abilities and resulting risks among the general public are not discussed here. Instead, focusing on discrimination and lack of transparency, we provide the readers with a review of recent attempts at making algorithmic decision-making more fair and accountable, highlighting the merits and the limitations of these approaches. Finally, we turn to the description of a recent project, called Open Algorithms (OPAL), whose goal is to enable the design, implementation and monitoring of development policies and programs, accountability of government actions, and citizen engagement while leveraging the availability of large scale human behavioral data in a privacy-preserving and predictable manner.", "body": "Today's vast and unprecedented availability of large-scale human behavioral data is profoundly changing the world we live in. Massive streams of data are available to train algorithms which, combined with increased analytical and technical capabilities, are enabling researchers, companies, governments and other public sector actors to resort to data-driven machine learning-based algorithms to tackle complex problems At the same time, scholars and activists have pointed to a range of social, ethical and legal issues associated with algorithmic decision-making, including bias and discrimination In 2014, the White House released a report titled \"Big Data: Seizing opportunities, preserving values\" Fortunately, there is increasing awareness of the detrimental effects of discriminatory biases and opacity of some data-driven algorithmic decisionmaking systems, and of the need to reduce or eliminate them. A number of research and advocacy initiatives are worth noting, including the Data Transparency Lab 2 , a \"community of technologists, researchers, policymakers and industry representatives working to advance online personal data transparency through research and design\", and the DARPA Explainable Artificial Intelligence (XAI) project 3 . A tutorial on the subject was held at the 2016 ACM Knowledge and Data Discovery conference In this paper, we focus on two of the main risks (namely, discrimination and lack of transparency) posed by data-driven predictive models leading to decisions that impact the daily lives of millions of people. There are additional challenges that we do not discuss in this paper. For example, issues relating to data ownership, privacy, informed consent and limited understanding (literacy) about algorithms' abilities and resulting risks among the general public are not discussed here. Instead, focusing on discrimination and lack of transparency, we provide the readers with a review of recent attempts at making algorithmic decision-making more fair and accountable, highlighting the merits and the limitations of these approaches. Finally, we turn to the description of a recent project, called Open Algorithms (OPAL), whose goal is to enable the design, implementation and monitoring of development policies and programs, accountability of government actions, and citizen engagement while leveraging the availability of large scale human behavioral data in a privacy-preserving and predictable manner.Discriminatory effects of algorithmic decision-makingFrom a legal perspective, Tobler Algorithmic discrimination may arise from different sources. First, input data into algorithmic decisions may be poorly weighted, leading to disparate impact. For example, as a form of indirect discrimination, overemphasis of zip code within predictive policing algorithms can lead to the association of low-income African-American neighborhoods with areas of crime and as a result, the application of specific targeting based on group membership The use of algorithmic data-driven decision processes may also result in individuals being denied opportunities based not on their own action but on the actions of others with whom they share some characteristics. For example, some credit card companies have lowered a customer's credit limit, not based on the customer's payment history, but rather based on analysis of other customers with a poor repayment history that had shopped at the same establishments where the customer had shopped These are both old and new risks. There is ample evidence of detrimental impacts of current non-algorithmic approaches to access to finance, employment, and housing. Backgrounds checks for example are widely used in those procedures, with people's knowledge and consent. But hundreds of thousands of people have been treated unfairly as a result of mistakes (for instance, misidentification) in the procedures used by external companies to perform background checks 5 . On the one hand, the occurence of such trivial procedural mistakes may be bound to decrease once performed through data-driven methodologies. But the effort required to identify the causes of unfair and discriminative outcomes can be expected to be exponentially larger, as exponentially more complex will be the black-box models employed to assist in the decision-making process. But it also means that should such methodologies not be transparent in their inner workings, the effects are likely to stay though with different roots.This scenario thus highlights particularly well the need for machine learning models featuring transparency (understood as openness and communication of both the data being analyzed and the mechanisms underlying the models), accountability (understood as the assumption of accepting the responsibility for actions and decisions) and fairness (understood as the lack of discrimination or bias in the decisions).Techniques to prevent algorithmic discrimination and maximize fairnessA simple way to try to maximize fairness -understood as the lack of bias and discrimination-in machine learning is precluding the use of sensitive attributes In the last few years, several researchers have proposed different technical definitions of fairness in machine learning, most of which formalize some notion of group fairness However, as pointed out by Dwork et al. To overcome these limitations, Dwork et al. Following Dwork et al. In a recent work, Hardt et al. Another interesting result is the one discussed by Kleinberg et al. The results obtained by Kleinberg et al. For this reason, we feel the urgency to extablish a call for action putting together researchers from different fields -including law, ethics, political philosophy and machine learning-to devise, evaluate and validate in the real-world alternative fairness metrics for different tasks. In addition to this empirical research, we believe it will be necessary to propose a modeling frameworksupported by empirical evidence-that would assist practitioners and policy makers in making decisions aided by algorithms that are maximally fair.Information asymmetry and lack of transparencyThe mandate for accountable algorithms in government and corporations' decision-making tools is fundamental in both validating their utility toward the public interest as well as redressing potential harms generated by these algorithms.Transparency, which refers to the understandability of a specific model, can be a mechanism that facilitates accountability. More specifically, transparency can be considered at the level of the entire model, at the level of individual components (e.g. parameters), and at the level of a particular training algorithm. In the strictest sense, a model is transparent if a person can contemplate the entire model at once. Thus, models should be characterized by low computational complexity. A second and less strict notion of transparency might be that each part of the model (e.g. each input, parameter, and computation) admits an intuitive explanation However, the ability to access and analyze behavioral data about customers and citizens on an unprecedented scale gives corporations and governments powerful means to reach and influence segments of the population through targeted marketing campaigns and social control strategies. In particular, we are witnessing an information asymmetry situation where a powerful few have access and use resources and tools that the majority do not have access to, thus leading to an -or exacerbating the existing-asymmetry of power between the state and big companies on one side and the people on the other side Burrell Techniques to improve transparency and accountabilityAs previously described, algorithmic decision-making might lack transparency. A simple solution to this limitation would consist of asking for transparency and openness of the algorithm's source code as well as inputs and ouputs that are used to make relevant algorithmic decisions. However, transparency alone is not sufficient to provide accountability in all cases. First of all, it is often necessary to keep secret certain elements of an algorithimic decision policy, the way how the policy is implemented, the key inputs, or the outcome. This is a way to help prevent strategic gaming of the system. Furthemore, when the decision being regulated is a commercial one -such as a bank decision to give a loan-, a legitimate business interest in protecting proprietary information or algorithms may be incompatible with full transparency. Again, an algorithmic decision-making system may use as input or may create as output sensitive data that should be not shared to protect business interests, privacy, etc. In some domains, such as finance and healthcare, disclosure may be limited by regulations.A strategy for verifying and making transparent the behavior of an algorithmic decision-making process is auditing. An auditing strategy deals with the decision process as a black box, whose inputs and outputs are visible, while inner workings are not Furthermore, some algorithmic decision-making processes aim to determine variables that are not measurable in a direct way, for example the risk of credit default. For this reason, these values are computed by means of proxy variables such as the consumer's credit history, the consumer's income, some consumer's personal characteristics or even their mobile phone usage patterns Ultimately, we need accountability in decision-making algorithms such that there is clarity regarding who holds the responsibility of the decisions made by them or with algorithmic support. Transparency is generally thought as a key enabler of accountability. However, transparency and auditing do not necessarily suffice for accountability. In fact, in a recent paper Kroll et al. Another approach to provide transparency in algorithmic decision-making entails providing explanations regarding the processes that lead to the decisions such that they are interpretable by humans. In a recent work, Ribeiro et al. A recent challenging contribution by Lipton However, the concept of interpretability is often vaguely defined. The learning model's properties that enable or that compromise interpretability broadly fall into two categories. The first one relates to transparency, that is how does the model work. The second one consists of post-hoc interpretations, that is what else can the model tell.As distinct notion of interpretability, post-hoc interpretations consist of explanations that need not elucidate the exact process by which models work. These interpretations include natural language explanations Algorithmic decision-making processes have the potential to lead to fairer and more objective decisions, grounded in data that are representative of the community where the decisions apply. However, as explained in the previous sections, algorithmic decision-making might lead to discrimination, information asymmetry and lack of transparency. Hence, we believe that it is not only important but also urgent to engage multi-disciplinary teams of researchers, practitioners and policy makers to propose, implement and evaluate in the real-world algorithmic decision-making processes that are designed to maximize their fairness and transparency.In the next section, we describe one of such proposals, the OPAL project.The OPAL projectThe Open Algorithms (OPAL) project 7 , is a multi-partner socio-technological platform led by Data-Pop Alliance, Imperial College London, the MIT Media Lab, Orange S.A., and the World Economic Forum, that aims to leverage private sector data for public good purposes by \"sending the code to the data\" in a privacy preserving, predictable, participatory, scalable and sustainable manner. The project came out of the recognition that accessing data held by private companies -including Call Detail Records (CDRs) collected by telecom operators for billing purposes, and banking data-for research and policy purposes has been a conundrum. To date for example, CDRs have been accessed and analyzed either internally, or externally through ad-hoc data challenges or through bilateral arrangements with a limited number of groups under 7 http://opalproject.org/ Non-Disclosure Agreements. These types of engagements have offered ample evidence of the promise and demand, but they do not scale nor address some of the most critical challenges discussed above. Building on the lessons of the past, OPAL is a key milestone towards a vision where data is at the heart of societal development around the globe, by providing a far better picture of human conditions to official statisticians, policy makers, planners, businesses leaders, and citizens, while enabling greater inclusion and inputs of all members of societies on the kinds and uses of analyses performed on data about themselves. As such, OPAL will reflect and foster the double objective to turn Big Data on its head and \"save it from itself\" OPAL's first core feature is technological: the platform allows sending queries (i.e. running algorithms) on the partner companies' servers, behind their firewalls, and not the other way around, so that raw data are never exposed to theft and misuse. The second one is socio-political: it involves codesigning the algorithms so that they are not only open but also serve local needs and respect local standards. For example, a key idea of OPAL is that algorithms should be verified by experts, policy makers, citizens to be as free as possible from biases and unintended side effects such as discrimination. To this end, OPAL makes use of vetting the algorithms that are permitted to run on a given data-set within a specific data repository. Once an algorithm has been vetted, it becomes a template that is digitally signed by the issuers (e.g. expert themselves, public institutions, representatives of the communities that will be affected by algorithmic decisions, etc.). This template algorithm can be shared among a group of entities (e.g. within a consortium) or even be published on a public site. Note that this vetting does not guarantee the quality of the output, which is a function of the quality of the input data.The OPAL model of moving the algorithm to the data and of using vetting allows a data repository to choose whether or not it is willing to accept a submitted OPAL algorithm (query). If the data repository accepts a given vetted algorithm, it also has the option to impose additional filtering on the resulting data prior to being returned as answer to the querier (e.g. defining the degree of personal information within a given answer). Moreover, each repository can introduce machine learning algorithms as additional mechanisms to protect privacy. Such algorithms allow a repository to detect if multiple accesses from the same entity may result in compromising Personal Identifiable Information (PII).Finally, blockchain technology can be used to capture and log both vettedqueries and safe-answers, thus providing a mechanism to support post-event audit and accountability. One easy way would be for the querier to compute a cryptographic hash of the query sent, and for the data repository to compute the hash of the response. In addition, the technology may be used by data owners when they want to monetize their data, including the ability to link money and data flows, or micro-payments with small transaction costs.OPAL is currently being deployed through pilots in Senegal and Colombia, where it has been endorsed by and benefits from the support of their National Statistical Offices and major local telecom operators. Local engagement and empowerment will be central to the development of OPAL: needs, feedback and priorities have been collected and identified through local workshops and discussions, and their results will feed into the design of future algorithms. These algorithms will be fully open, therefore subject to public scrutiny and redress. A local advisory board is being set up to provide guidance and oversight to the project. In addition, trainings and dialogues will be organized around the project to foster its use and diffusion as well as local capacities and awareness more broadly. OPAL aims to be deployed in 2 more countries by the end of 2018.Initiatives such as OPAL have the potential to enable more human-centric accountable and transparent data-driven decision-making and governance, by involving a wide range of stakeholders in and through their design and implementation. Note, however, that OPAL does not fully address the issues of algorithmic fairness and transparency. It does not address internal uses of data and potentially discriminatory behavior by corporations. It does not advance an open data agenda, which may be unrealistic when working with corporateowned data carrying high competititve value and posing severe privacy risks. It also does not in itself enable control for bias in the data itself.Despite these limitations, we believe that OPAL will provide an avenue for an array of users, from official statisticians to community organizers, to openly query data and have those queries and results examined through a fairness and anti-discrimination lens. It is an important concrete step towards a world where data and algorithms can be leveraged through participatory processes for societal development and democracy around the globe.Conclusions", "conclusions": "We live in an unprecedented historic moment where the availability of vast amounts of human behavioral data, combined with advances in machine learning are enabling us to tackle complex problems through algorithmic decisionmaking. The opportunities to have positive social impact through fairer and more transparent decisions are paramount. However, algorithmic decisionmaking processes might lead to discrimination, information asymmetry and lack of transparency.In this paper we have provided an overview of both existing limitations and proposed solutions regarding fairness, accountability and transparency in algorithmic decision-making. We have highlighted open challenges that would still need to be addressed and have described the OPAL project as an exemplary effort that aims to maximize algorithmic fairness and transparency to support decision-making for social good. We would like to emphazise the importance and the urgency to engage multi-disciplinary teams of researchers, practitioners and policy makers to propose, implement and evaluate in the real-world algorithmic decision-making processes that are designed to maximize their fairness and transparency.The opportunity to significantly improve the processes leading to decisions that affect millions of lives is huge. As researchers and citizens we believe that we should not miss on this opportunity. Hence, we would like to encourage the larger community -e.g. researchers, practitioners, policy makers-in a variety of fields -e.g. computer science, sociology, economics, ethics, law-to join forces so we can address today's limitations in data-driven decision-making and contribute to fairer and more transparent decisions with clear accountability and significant positive impact.", "SDG": [8]}, "revisiting_u": {"name": "NBER WORKING PAPER SERIES REVISITING U. S. PRODUCTIVITY GROWTH OVER THE PAST CENTURY WITH A VIEW OF THE FUTURE", "abstract": " This paper is dedicated not only to the 80th birthday of Angus Maddison but also to the memory of Robert McGuckin of the Conference Board. The 2006 conference version of this paper was made possible by the creative research assistance of Robert Krenn. Jesse Wiener brought new insights to this revision and extension of the data. The views expressed herein are those of the author and do not necessarily reflect the views of the National Bureau of Economic Research. NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications.", "keywords": "", "introduction": "", "body": "Figure In parallel with Figure Components of the Output Identity Between Benchmark Quarters, 1954 to 2007We can now look more closely at the growth rates of components of the output identity over the period from 1954 to 2007. The benchmark quarters are chosen to be those in each business cycle recovery when the unemployment rate first declines below the natural rate of unemployment (or time-varying NAIRU) as identified in my ongoing research on the U. S. inflation process. In the final quarter (2007:Q4) the actual unemployment rate was roughly equal to the natural rate of unemployment. The rows of TableProductivity Growth Over the Last Century, page 6The resumption of the decline in hours per employee, especially after 2001, has been partly blamed on pressure from employers for employees to work on a part-time basis which deprives employees of expensive pensions and health-care benefits. The 2001-06 decline in the LFPR has been the subject of much discussion, see especially Interpretations of the Post-1995 U. S. Productivity Growth RevivalThus far we have looked only at labor productivity and have not yet examined the decomposition of trend growth in labor productivity between capital deepening and MFP growth. Using lower-case letters to represent growth rates, the trend growth in multifactor productivity (m) equals output growth minus input growth, which in turn equals the income share of each input times its trend growth rate. Distinguishing only the growth rates of capital (k) and labor (n) as inputs, and using the notation b for the income share of capital, we obtain the standard relationship between MFP growth, labor productivity growth, and the contribution of capital deepening.Thus MFP growth can be written either as the growth rate of output minus the weighted growth rate of the two inputs, labor and capital, or alternatively as the growth rate of labor productivity (y-n) minus the growth rate of the capital-deepening effect (b(k-n)).Growth Accounting for the 1995-2000 Revival and the Post-2000 AftermathThe literature on the post-1995 productivity growth revival goes beyond equation ( Starting at the top of Table The contribution of ICT to overall productivity growth also includes the contribution of growth of MFP in the ICT industries. This also surged after 1995 and after 2000 remained substantially higher than before 1995. In particular the contribution of semiconductors and of software to overall MFP growth was substantially higher after 2000 than before 1995, while communications was the same and the contribution of computer hardware actually lower.Oliner and Sichel's headline result is that the sum of the ICT contribution to overall productivity growth fell from 74 percent in 1995-2000 to 43 percent during 2000-07, a lower percentage contribution than before 1995 (see line 17). However, the absolute contribution of ICT for 2000-07 on line 16 was substantially greater than in the pre-1995 interval. The total contribution of the ICT sector to explaining the post-2000 productivity growth was negative in the sense that the ICT contribution both to capital deepening and to MFP growth declined substantially while the overall growth rate of productivity, the phenomenon to be explained, was basically flat. By definition, this implies that the continuing rapid rate of labor productivity growth after 2000 resulted from a combination of faster growth in labor quality (line 8), of capital deepening in non-ICT capital (line 7), and especially in MFP growth in the non-ICT industries.Dale The most important difference between Tables 2 and 3 is that the Jorgenson team estimates the total effect of capital deepening to have been much higher in 1995-2000 (their 1.51 percent vs. 1.09 percent in Table Whatever the reason for Oliner and Sichel's larger estimates of MFP growth and the Jorgenson et al. larger estimates of the capital-deepening effect outside of the ICT sector, there remains the task of explaining why productivity growth remained robust after 2000 despite the collapse of the ICT investment boom. What caused MFP growth in the non-ICT part of the economy to grow so rapidly in 2000-2006/7 relative to its much slower growth prior to 1995?Substantive Explanations of What Happened after 2000The upsurge of productivity growth between late 2001 and mid-2004, and its much more moderate growth rate during 2004-07, supports the interpretation offered in The unusual trajectory of S&P reported profits in 1998-2001 placed unusual pressure on corporate managers to cut costs and reduce employment after 2000. During the 1990s corporate compensation had shifted to relying substantially on stock options, leading first to the temptation to engage in accounting tricks during 1998-2000 to maintain the momentum of earnings growth, and then sheer desperation to cut costs in response to the post-2000 collapse in reported S&P earnings and in the stock market. The stock market collapse had an independent effect on the pressure for corporate cost cutting, beyond its effect on the stock-option portion of executive compensation, by shifting many corporate-sponsored defined-benefit pension plans from overfunded to underfunded status.A plausible interpretation of the unusual upsurge of productivity growth in 2001-03, then, is that it was the counterpart of an unusual degree of pressure for corporate cost cutting, in turn caused by the role of accounting scandals and corporate write-offs that led to the unusual trajectory of reported S&P profits relative to NIPA profits. The unprecedented nature of corporate cost cutting was widely recognized at the time. As the Wall Street Journal put it:The mildness of the recession masked a ferocious corporate profits crunch that has many chief executives still slashing jobs and other costs . . . Many CEOs were so traumatized by last year's profits debacle that they are paring costs rather than planning plant expansions After I had suggested the \"savage cost-cutting hypothesis\" in my 2003 paper, The idea that some of the benefits of ICT investment were delayed is compatible with the influential Paul Subsequent research after David's insightful paper has suggested that the full impacts of great inventions like electricity, the internal combustion engine, and in this case the marriage of personal computers with the internet, may take a substantial time to come to fruition (see Implications of Explanations for Future GrowthTo speculate in 2003 about the causes of the then-evolving productivity growth explosion was risky, but as the data evolved between 2003 and 2007 those hypotheses seem more plausible. The pressure on costs and to layoff workers reached a peak in 2001-2002, and productivity growth exploded. After 2003 profits have increased rapidly and productivity growth slowed dramatically. The \"intangible capital\" hypothesis is also inherently temporary unless a second era of rapid increases in ICT investment occurs, but that did not happen after 2001.Viewed in retrospect the 2001-04 productivity growth explosion increasingly looks like a one-off, unique, temporary phenomenon. In particular, I find ever less plausible the view, common in the late 1990s, that ICT investment had a magic quality that would break through all previous boundaries set by the scarcity of economic resources. Doubtless the invention of the internet has revolutionized industries ranging from university libraries to bookshops, but that revolution is over. University libraries have already replaced card catalogues by computer terminals, and Amazon (together with Barnes and Noble and Borders) has already achieved its goal of pushing independent bookshops out of business. In turn, Amazon has come close to pushing Borders out of business.My interpretation of the ICT revolution is that it is increasingly burdened by diminishing returns. The push to ever-smaller devices runs up against the fixed size of the human finger that must enter information on the device. Most of the innovations since 2000 have been directed to consumer enjoyment rather than business productivity, including video games, DVD players, and I-pods. I-phones are nice, but the ability to reschedule business meetings and look up corporate documents while on the road already existed by 2003. Readers of this paper who actively create economic research might ask themselves what has happened to improve their productivity since the invention of \"Office 97\" (including MS Word and Excel), other than the arrival in their home offices of broadband internet in contrast to the feeble dialup connections of the late 1990s? 4. See Productivity Growth Over the Last Century, page 11And what about Europe? Europeans use cell phones and web connections as much or more than Americans (admittedly with substantial variation between Scandinavia and the Olive Belt). But what good has this done? As they have adopted their desktops and laptops equipped with Intel processors and Microsoft software, Europeans have experienced a steady slowing in the growth rate of their output per hour, and indeed of their output per capita. The failure of Europeans to achieve accelerated growth in productivity after 1995, as occurred in the U. S., shifts the focus of causal explanations away from universal technological advances such as the internet, to country-specific environmental factors that encourage or discourage growth. The Groningen research (see especially The goal of this paper is to make forecasts 20 years ahead of growth in US output per hour and potential GDP. So far interesting issues have been identified without resolution. The first section devoted to the quarterly data and output identity leaves open whether the unique 2008-09 gap between a repeat explosion of actual productivity growth and the constrained trend growth in productivity will be followed by a subsequent radical slowdown, just as the 2001-04 explosion was followed by the subsequent 2004-07 slowdown. The second section which identified the disconnect between ICT investment and the post-2000 U. S. productivity growth experience makes the 2001-04 \"explosion\" look temporary rather than permanent and adds urgency to our need to determine which part of pre-1995 history is relevant to forecasting productivity growth into the future. This suggests a strong motivation to re-examine the 20 th century history of growth in output, labor productivity, and MFP.A Re-Examination of 20 th Century HistoryWe now turn to a longer-term perspective on economic growth since the late nineteenth century. We begin with a simple calculation of output, conventional inputs, and MFP growth for 1891-2007, dividing up those 115 years into nine subintervals between benchmark years that are chosen to be cyclically neutral and thus to skip over years influenced by war, recessions, or depressions. These data are then compared for the postwar period with the BLS computations of MFP growth that implement adjustments for capital composition, the so-called \"capital quality\" correction that has long been advocated by Dale Jorgenson and his co-authors. Building on earlier research in Productivity Growth Over the Last Century, page 12 and labor quality back before 1948 and in addition to make additional adjustments to the quantity of capital input.BEA and Kendrick Data for MFP since 1891The results of our initial data exercise are displayed in Table The top section of Table BEA Data Compared with BLS MFP Data, with and without Composition AdjustmentsIf we were interested only in the growth of labor productivity and of MFP for the postwar years, we would not need to put together data series from the BEA and Kendrick sources, because the BLS has already done the job for us for the years since 1948. Our 6. The NIPA revisions for 1929-48 were introduced in the benchmark revision of August, 1999. They combine the influence of chain-weighted deflators that increase more slowly than the previous fixedweight deflators, and in addition the growth rate of nominal GDP was revised upwards.Productivity Growth Over the Last Century, page 13 previous examination of the BEA-Kendrick series originates in our interest in what happened to growth in labor productivity and in MFP for the years before 1948.The BLS calculates MFP by subtracting from output growth not only the growth of labor hours and growth in the capital stock, but it also subtracts the contribution of changes in labor and capital composition. Its labor composition adjustment is based on subdividing the labor force into cells by age, gender, and educational attainment, and then weighting by the labor compensation earned in each cell. Thus a shift in composition toward more educated workers yields a positive labor composition adjustment, while the increase in the share of teenagers in the 1960s and 1970s caused by the baby boom creates a negative labor composition adjustment. The BLS capital composition adjustment results from aggregating different components of the capital stock (structures, ICT equipment, non-ICT equipment, inventories, and land) using weights based on the user cost of capital. Because of rapid depreciation, one dollar of ICT equipment receives a higher weight than one dollar of non-ICT equipment, which in turn receives a higher weight than one dollar of structures.The basic series released by the BLS contain data only on their composition-adjusted \"labor input\" (as contrasted to labor hours) and \"capital services\" (as contrasted to the capital stock). However, the BLS web site has extensive information that allows us to create Table The three right-hand columns of the top section report the contributions to output growth of the labor and capital composition adjustments. Note that the labor composition adjustment is essentially zero during 1964-79, when the baby-boomers entered the labor force as inexperienced teenagers. The labor composition adjustment was also held down between 1964 and 1988 by the entry of females into the labor force. Given the increasing importance of computers in recent years, it is somewhat surprising that the capital composition contribution was roughly similar in each of the six periods. 7 Presumably the shift from structures to equipment in the early part of the postwar years was as important as the shift from long-lived non-ICT equipment to short-lived ICT equipment in the period since 1988.The final column in the top section reports the \"adjusted\" growth rate of MFP, that is, the officially reported BLS series on MFP growth that incorporates the composition 7. The numbers shown in the top right section of Table The bottom section of Table Adjustments for Labor Composition Prior to 1950This section develops composition adjustments for labor and capital in the period prior to 1950 and also calculates additional adjustments to the quantity of capital. Each of the adjustments made in this section is based on the analysis of The first adjustment is for labor quality and is based on the extensive analysis carried out by Edward F. Denison in his well-known books on the sources of US economic growth. We reject two of The first line in Table Productivity Growth Over the Last Century, page 15Our alternative labor composition adjustment includes the alternative education component and Denison's age-gender effect, that is, line 5 is the sum of lines 3 and line 4. For comparison we show the BLS labor composition adjustments for the post-1950 and note that they are uniformly lower, perhaps because their data (obviously more recent than Denison's) places more importance on the negative age-sex adjustment and less importance on the positive correction for educational attainment. 9Adjustments for Capital Composition before 1950The BLS makes adjustments for the composition of capital in its postwar MFP data by weighting each type of capital by its user cost. Pending further research on the user cost of capital prior to 1950, we calculate a capital composition adjustment in this section by a rough approximation. The BLS capital composition effect reflects both the rising share of short-lived equipment to structures, and the rising share of short-lived ICT equipment within the total of capital equipment. To determine the relative importance of the composition effect before 1950, we have recalculated our BEA capital stocks alternatively with equal dollar weights and with a weight of three dollars on each dollar of equipment and one dollar for each dollar of structures.In Table Revising the Quantity of CapitalAn important error in standard measures of capital input is the assumption that service lifetimes are fixed. Yet equipment which made such a contribution to America's production achievement during World War II.The method developed in Two types of capital financed by the government enter the production function of the private business economy and should be included in private capital input. Part of the sharp rise in US output during World War II was made possible by government-owned privatelyoperated (GOPO) capital. Initially the government did not keep track of this stock of capital, but after I studied this phenomenon and estimated its magnitude A related problem is that a substantial part of government-owned infrastructure serves as an unmeasured input to production in the private sector. Over the twentieth century there was a gradual shift over time in the transportation sector from privately-owned railroad capital to government-owned highways, airports, and air-traffic control facilities. We rely on The total adjustment to capital input varies over time but reaches by far its highest impact in the \"big wave\" period of 1928-50, due to the combined effects of capital composition, variable retirement, GOPO capital, and highway capital. The effect was smallest in 1950-64 when the variable retirement effect was being reversed. The capital adjustment was substantially higher in 1988-2007 than in 1950-88, mainly because the negative impact of variable retirement and GOPO capital by then had faded away.Alternative Measures of MFP GrowthThe end result of the research in this section has been to develop several adjustments to the growth of labor and capital input. For the period since 1950 the labor and capital composition effects are taken from the BLS, and we develop composition effects by similar methods that are applied to the period 1891-1950. In addition we make three adjustments to the growth of capital input that go beyond the BLS methodology by adjusting for variable retirement, GOPO capital, and highway capital.Table These successful periods for adjusted MFP growth contrast with the dismal slowdown years of 1972-96 when adjusted MFP growth was only 0.33 percent per year. A difficult problem in forecasting into the next decade or two is to determine which of the previous time intervals are relevant.What do these results for MFP growth imply for labor productivity growth? A glance back at the BLS data in Table The results on adjusted MFP growth raise numerous questions about which vast amounts have been written. Why was adjusted MFP growth so low between 1972 and 1996 and so high between 1928 and 1950? Should we accept a \"normal rate\" of adjusted MFP growth of 1.12 percent as occurred in five of our nine intervals? These issues are discussed in the next section as we attempt to forecast the main magnitudes for the next 20 years.  How does the adjusted BLS growth rate of MFP for the post-1972 period inLearning from the Past to Forecast the Future: Productivity and Real GDP Growth 2007-2027One stands on relatively solid ground when talking about the past, but at least initially feels as if floating in a haze when looking out 20 years into the future. But an examination of data in this paper going back to 1891 yields several possible criteria to bound the likely growth rates of labor productivity and of MFP in the future.Aspects of the Record, 1987-2007 as Contrasted with 2000-2007Our forecasts are developed in steps within the format of Table Interest in the seven-year interval 2000:Q4 to 2007:Q4 originates in the fact that both quarters were at or near business cycle peaks. This interval can be viewed as more \"normal\" than 1987-2000, which combined a period when total-economy productivity growth was slower than anyone could explain (1.19 percent for the total economy), and then productivity revived more rapidly than anyone had predicted (2.42 percent). The 1987-95 interval was best described by Solow's famous paradox that \"we can see the computers everywhere but in the productivity statistics.\" But the 1995-2000 revival period was unusual as well. This was the famous \"dot.com\" era, when by general consensus there was too much investment in computers and communication infrastructure, and this five-year interval may also have been unique in that one could only invent the internet once.The 20-year period 1987-2007 combines the inexplicably slow productivity growth of 1987-95, the temporarily ebullient period 1995-2000, and the interesting 2000-07 period that in some dimensions looks like more normal behavior. The seven years between 2000:Q4 and 2007:Q4 were neatly divided in half, with extremely rapid productivity growth between 2000:Q4 and 2004:Q2 (2.68 percent), and much slower growth from 2004:Q2 to 2007:Q4 (1.36 percent), averaging out to 2.02 percent for the seven-year interval. As argued above the productivity growth \"explosion\" of 2001-04 rested on a combination of savage corporate cost cutting and delayed learning from the internet revolution. Once profits had recovered the pressure for cost cutting disappeared, and eventually the delayed learning subsided as well.Unfortunately, the relevance of the 2000-07 period is tainted by the bizarre behavior of the labor-market variables. Given that the unemployment rates were similar at the beginning and end, there was an unprecedented drop in hours per capita, with aggregate hours (H) growing at only 0.36 percent per year while the working-age population grew at 1.24 percent per year. Hours per capita fell by 0.88 percent per year, or a cumulative exponential 6.4 percent decline between 2000 and 2007.The sharp decline in hours per capita in 2007 to some extent reduced output growth but it also raised productivity growth, in that some of the reduction of labor hours represented a permanent transition to a more aggressive stance of management against labor Reconciling Labor Productivity and MFP in the ForecastsOne possible criterion lies in a coincidence that two key numbers are the same. As shown in Table The lower section of Table Our forecasts for the capital-deepening effect are almost identical to the actual record of 2000-07, and we expect the share of ICT investment in GDP to be roughly stable. This share, displayed in Figure The next line in the bottom section of Table Since the continuation of the labor quality adjustment depends largely on increasing educational attainment, yet this increase seems to have disappeared, Table Productivity Growth Over the Last Century, page 21What growth rate should we choose for MFP? This is the wild card in any forecast of future productivity behavior. As summarized above, the average growth rate of adjusted MFP for the U. S. economy since 1891 has been 1.19 percent. The respective figures for 1987-2007 and 2000-07 from Table 10 are 0.96 and 1.26 percent respectively. My inclination is to lean toward a number closer to the 1987-2007 experience than the unusual 2000-07 period. The fundamental driver of adjusted MFP growth is innovation, which exhibits ebbs and flows over time. The \"Great Inventions\" of the late nineteenth century, especially electricity and the internal combustion engine, propelled MFP growth through most of the 20 th century until those inventions had been exploited, at least in the US, around 1970. By the early 1970s the country was electrified, air conditioning had allowed manufacturing and service industries to spread across the south and southwest, consumer appliances had liberated women, radio and TV had ended isolation and brought the world into the living room, the interstate highway system was largely built, and the population had moved to the suburbs.Perhaps it is not so surprising that the opportunities to exploit technology were limited between 1972 and 1995. Then came the upsurge of the contribution of ICT capital to economic growth, and according to the results in Tables The MFP forecast for 2007-27 in Table Implications for Future Growth in Real GDP and Output per Hour for the Total EconomyWhen the components of NFPB labor productivity growth in the lower right section of Table Once the future growth rate of NFPB labor productivity is established, it becomes relatively easy to set out the future growth rates of the other components of the output identity. We take our forecasts of future growth in population and the other labor variables from recent articles projecting to the period 2008-18 by Conclusion", "conclusions": "This paper provides a novel combination of perspectives on the behavior of growth in both output per hour and of MFP at high frequencies for the past 50 years, at low frequencies dating back to 1891, and in forecasts of the likely outcome over the twenty-year period 2007-2027. The history of U. S. productivity behavior in the past 116 years is characterized by significant upward and downward shifts in growth rates. To make sensible forecasts, we must understand the past. This paper contributes to that understanding initially by decomposing the growth of total-economy output, aggregate hours, and labor productivity from 1952-2007 into actual changes, changes in a statistical trend that is defined to exclude business-cycle movements, and the residual or deviation from trend growth. The growth rate of trend real GDP has fluctuated in a narrow range between 3 and 4 percent since 1952, while the trend in aggregate hours for the total economy has ranged from 0.7 to 1.9 percent. The peak period for output growth was in the 1950s and 1960s, while the peak period for hours growth was in the late 1970s as teenagers and females entered the labor force at a rapid rate. The low 0.7 trend growth rate of hours reached in 2002-03 reflected an unprecedented reduction in work hours that was the counterpart of superheated productivity growth in this interval. This paper interprets both as inherently temporary.The statistical trend for growth in total economy labor productivity ranged from 2.75 percent in early 1962 down to 1.25 percent in late 1979 and recovered to 2.45 percent in 2002. Our results on productivity trends identify a problem in the interpretation of the 2008-09 recession. The unprecedented decline in labor hours, employment, and labor-force participation in 2008-09 lead the statistical detrending technique to the conclusion that productivity was doing quite well, given the historical weakness of productivity in recessions. As discussed in  this represents a shift in labor-market behavior rather than an unexplained upsurge in the productivity growth trend, and for the discussion of short-term responses we hold constant the trend of real GDP, aggregate hours, and total economy labor productivity after 2007 at the trend growth rate reached in 2007:Q4.Gordon (2010b)The medium-term analysis of changes in U. S. labor productivity and MFP begins with the familiar decompositions of labor productivity growth into the contributions of capitaldeepening, labor quality, and MFP, further divided into the contributions of ICT and non-ICT investment. These results show that the 1995-2000 productivity growth revival was primarily driven by ICT investment, while the subsequent relatively rapid growth in productivity during 2000-07 was primarily driven by an upsurge of MFP growth in non-ICT industries. We have provided an interpretation of the temporary productivity growth explosion of 2001-04 based on Gordon (2003): a sharp decline in profits and in the stock market in 2000-01 motivated firms to cut all costs, and particularly labor costs, more deeply than had previously occurred for a given change in the output gap. Employment continued to decline while output rose in 2001-04 not only because the pace of the output recovery was weak, but because delayed learning how to use the late-1990s invention of the internet caused some of the benefits of that invention to spill over from the 1990s to the 2001-04 interval. Once the cost-cutting had been digested and the delayed benefits from the internet invention had been realized, productivity growth slowed in 2004-07 to a pace little faster than in 1972-95.Gordon ( , 2010bAn important contribution of the paper is to provide an updated decomposition of productivity growth over the \"long century\" extending between 1891 and 2007. By far the most rapid MFP growth in U. S. history occurred in 1928-50, a phenomenon that I have previously dubbed the \"one big wave.\" This paper presents numerous corrections to the growth of labor quality and to capital quantity and quality, leading to significant rearrangements of the growth pattern of MFP, generally lowering the unadjusted MFP growth rates during 1928-50 and raising them after 1950.The paper approaches the task of forecasting 20 years into the future by extracting relevant precedents from the growth in labor productivity and in MFP over the last seven years, the last 20 years, and the last 116 years. Its conclusion is that over the next 20 years  growth in real GDP will be 2.4 percent (the same as in 2000-07), growth in total economy labor productivity will be 1.7 percent, and growth in the more familiar concept of NFPB sector labor productivity will be 2.05 percent. The implied forecast 1.50 percent growth rate of per-capita real GDP falls far short of the historical achievement of 2.17 percent between 1929 and 2007 and represents the slowest growth of the measured American standard of living recorded during the past two centuries.Despite their pessimism, the conclusions of this paper are not wild guesses but rather emerge from discipline in distinguishing between the NFPB sector and the total economy, in distinguishing between labor productivity and MFP, and in providing a unique interpretation of twentieth history based on a revision to conventional measures of pre-1948 MFP growth.And the conclusions of this paper, even if they are bolstered with a greater reliance on historical precedents, are not wildly deviant from contemporary forecasts.  predicts that growth in U. S. real GDP will be at a rate of 2.5 percent until 2030, not far from our estimate of 2.4 percent through 2027. Maddison (2009) reports several estimates of future real GDP growth at 2.5 percent, including mine in the original 2006 version of this paper. Further consideration has required me to reduce the 2.5 number to 2.4, not a major change. For those who may disagree about the pessimistic conclusions of this paper, there is an obligation to pick out the numbers that are wrong or dubious and to suggest a more convincing set of numbers to propel the discussion.Jorgenson et al. (2008)", "SDG": [8]}, "the_future_of_employment_how_susceptible_are_jobs_to_computerisation": {"name": "THE FUTURE OF EMPLOYMENT: HOW SUSCEPTIBLE ARE JOBS TO COMPUTERISATION? *", "abstract": " We examine how susceptible jobs are to computerisation. To assess this, we begin by implementing a novel methodology to estimate the probability of computerisation for 702 detailed occupations, using a Gaussian process classifier. Based on these estimates, we examine expected impacts of future computerisation on US labour market outcomes, with the primary objective of analysing the number of jobs at risk and the relationship between an occupation's probability of computerisation, wages and educational attainment. According to our estimates, about 47 percent of total US employment is at risk. We further provide evidence that wages and educational attainment exhibit a strong negative relationship with an occupation's probability of computerisation.", "keywords": "Occupational Choice,Technological Change,Wage Inequality,Employment,Skill Demand JEL Classification: E24,J24,J31,J62,O33", "introduction": "In this paper, we address the question: how susceptible are jobs to computerisation? Doing so, we build on the existing literature in two ways. First, drawing upon recent advances in Machine Learning (ML) and Mobile Robotics (MR), we develop a novel methodology to categorise occupations according to their susceptibility to computerisation. 1 Second, we implement this methodology to estimate the probability of computerisation for 702 detailed occupations, and examine expected impacts of future computerisation on US labour market outcomes.Our paper is motivated by John Maynard Keynes's frequently cited prediction of widespread technological unemployment \"due to our discovery of means of economising the use of labour outrunning the pace at which we can find new uses for labour\" . Indeed, over the past decades, computers have substituted for a number of jobs, including the functions of bookkeepers, cashiers and telephone operators (Keynes, 1933, p. 3)(Bresnahan, 1999;. More recently, the poor performance of labour markets across advanced economies has intensified the debate about technological unemployment among economists. While there is ongoing disagreement about the driving forces behind the persistently high unemployment rates, a number of scholars have pointed at computer-controlled equipment as a possible explanation for recent jobless growth (see, for example, MGI, 2013). 2  The impact of computerisation on labour market outcomes is well-established in the literature, documenting the decline of employment in routine intensive occupationsi.e. occupations mainly consisting of tasks following well-defined procedures that can easily be performed by sophisticated algorithms. For example, studies by Brynjolfsson and McAfee, 2011) and Charles, et al. (2013) emphasise that the ongoing decline in manufacturing employment and the disappearance of other routine jobs is causing the current low rates of employment. 3 In ad- 1 We refer to computerisation as job automation by means of computer-controlled equipment.Jaimovich and Siu (2012)2 This view finds support in a recent survey by the McKinsey Global Institute (MGI), showing that 44 percent of firms which reduced their headcount since the financial crisis of 2008 had done so by means of automation . 3 Because the core job tasks of manufacturing occupations follow well-defined repetitive procedures, they can easily be codified in computer software and thus performed by computers (MGI, 2011). dition to the computerisation of routine manufacturing tasks, (Acemoglu and Autor, 2011) document a structural shift in the labour market, with workers reallocating their labour supply from middle-income manufacturing to low-income service occupations. Arguably, this is because the manual tasks of service occupations are less susceptible to computerisation, as they require a higher degree of flexibility and physical adaptability Autor and Dorn (2013)(Autor, et al., 2003;Goos and Manning, 2007;.Autor and Dorn, 2013)At the same time, with falling prices of computing, problem-solving skills are becoming relatively productive, explaining the substantial employment growth in occupations involving cognitive tasks where skilled labour has a comparative advantage, as well as the persistent increase in returns to education (Katz and Murphy, 1992;Acemoglu, 2002;. The title \"Lousy and Lovely Jobs\", of recent work by Autor and Dorn, 2013), thus captures the essence of the current trend towards labour market polarization, with growing employment in high-income cognitive jobs and low-income manual occupations, accompanied by a hollowing-out of middle-income routine jobs.Goos and Manning (2007)According to , the pace of technological innovation is still increasing, with more sophisticated software technologies disrupting labour markets by making workers redundant. What is striking about the examples in their book is that computerisation is no longer confined to routine manufacturing tasks. The autonomous driverless cars, developed by Google, provide one example of how manual tasks in transport and logistics may soon be automated. In the section \"In Domain After Domain, Computers Race Ahead\", they emphasise how fast moving these developments have been. Less than ten years ago, in the chapter \"Why People Still Matter\", Brynjolfsson and McAfee (2011) pointed at the difficulties of replicating human perception, asserting that driving in traffic is insusceptible to automation: \"But executing a left turn against oncoming traffic involves so many factors that it is hard to imagine discovering the set of rules that can replicate a driver's behaviour [. . . ]\". Six years later, in October 2010, Google announced that it had modified several Toyota Priuses to be fully autonomous Levy and Murnane (2004).(Brynjolfsson and McAfee, 2011)To our knowledge, no study has yet quantified what recent technological progress is likely to mean for the future of employment. The present study intends to bridge this gap in the literature. Although there are indeed existing useful frameworks for examining the impact of computers on the occupational employment composition, they seem inadequate in explaining the impact of technological trends going beyond the computerisation of routine tasks. Seminal work by , for example, distinguishes between cognitive and manual tasks on the one hand, and routine and non-routine tasks on the other. While the computer substitution for both cognitive and manual routine tasks is evident, non-routine tasks involve everything from legal writing, truck driving and medical diagnoses, to persuading and selling. In the present study, we will argue that legal writing and truck driving will soon be automated, while persuading, for instance, will not. Drawing upon recent developments in Engineering Sciences, and in particular advances in the fields of ML, including Data Mining, Machine Vision, Computational Statistics and other sub-fields of Artificial Intelligence, as well as MR, we derive additional dimensions required to understand the susceptibility of jobs to computerisation. Needless to say, a number of factors are driving decisions to automate and we cannot capture these in full. Rather we aim, from a technological capabilities point of view, to determine which problems engineers need to solve for specific occupations to be automated. By highlighting these problems, their difficulty and to which occupations they relate, we categorise jobs according to their susceptibility to computerisation. The characteristics of these problems were matched to different occupational characteristics, using O * NET data, allowing us to examine the future direction of technological change in terms of its impact on the occupational composition of the labour market, but also the number of jobs at risk should these technologies materialise. The present study relates to two literatures. First, our analysis builds on the labour economics literature on the task content of employment Autor, et al. (2003)(Autor, et al., 2003;Goos and Manning, 2007;. Based on defined premises about what computers do, this literature examines the historical impact of computerisation on the occupational composition of the labour market. However, the scope of what computers do has recently expanded, and will inevitably continue to do so Autor and Dorn, 2013)(Brynjolfsson and McAfee, 2011;.MGI, 2013)Drawing upon recent progress in ML, we expand the premises about the tasks computers are and will be suited to accomplish. Doing so, we build on the task content literature in a forward-looking manner. Furthermore, whereas this literature has largely focused on task measures from the Dictionary of Occupational Titles (DOT), last revised in 1991, we rely on the 2010 version of the DOT successor O * NET -an online service developed for the US Department of Labor. 4  Accordingly, O * NET has the advantage of providing more recent information on occupational work activities.Second, our study relates to the literature examining the offshoring of information-based tasks to foreign worksites (Jensen and Kletzer, 2005;Blinder, 2009;Jensen and Kletzer, 2010;Oldenski, 2012;. This literature consists of different methodologies to rank and categorise occupations according to their susceptibility to offshoring. For example, using O * NET data on the nature of work done in different occupations, Blinder and Krueger, 2013) estimates that 22 to 29 percent of US jobs are or will be offshorable in the next decade or two. These estimates are based on two defining characteristics of jobs that cannot be offshored: (a) the job must be performed at a specific work location; and (b) the job requires face-to-face personal communication. Naturally, the characteristics of occupations that can be offshored are different from the characteristics of occupations that can be automated. For example, the work of cashiers, which has largely been substituted by self-service technology, must be performed at specific work location and requires face-to-face contact. The extent of computerisation is therefore likely to go beyond that of offshoring. Hence, while the implementation of our methodology is similar to that of Blinder (2009), we rely on different occupational characteristics. The remainder of this paper is structured as follows. In Section II, we review the literature on the historical relationship between technological progress and employment. Section III describes recent and expected future technological developments. In Section IV, we describe our methodology, and in Section V, we examine the expected impact of these technological developments on labour market outcomes. Finally, in Section VI, we derive some conclusions.Blinder (2009)", "body": "In this paper, we address the question: how susceptible are jobs to computerisation? Doing so, we build on the existing literature in two ways. First, drawing upon recent advances in Machine Learning (ML) and Mobile Robotics (MR), we develop a novel methodology to categorise occupations according to their susceptibility to computerisation. 1 Second, we implement this methodology to estimate the probability of computerisation for 702 detailed occupations, and examine expected impacts of future computerisation on US labour market outcomes.Our paper is motivated by John Maynard Keynes's frequently cited prediction of widespread technological unemployment \"due to our discovery of means of economising the use of labour outrunning the pace at which we can find new uses for labour\" 2 This view finds support in a recent survey by the McKinsey Global Institute (MGI), showing that 44 percent of firms which reduced their headcount since the financial crisis of 2008 had done so by means of automation At the same time, with falling prices of computing, problem-solving skills are becoming relatively productive, explaining the substantial employment growth in occupations involving cognitive tasks where skilled labour has a comparative advantage, as well as the persistent increase in returns to education According to To our knowledge, no study has yet quantified what recent technological progress is likely to mean for the future of employment. The present study intends to bridge this gap in the literature. Although there are indeed existing useful frameworks for examining the impact of computers on the occupational employment composition, they seem inadequate in explaining the impact of technological trends going beyond the computerisation of routine tasks. Seminal work by Drawing upon recent progress in ML, we expand the premises about the tasks computers are and will be suited to accomplish. Doing so, we build on the task content literature in a forward-looking manner. Furthermore, whereas this literature has largely focused on task measures from the Dictionary of Occupational Titles (DOT), last revised in 1991, we rely on the 2010 version of the DOT successor O * NET -an online service developed for the US Department of Labor. 4  Accordingly, O * NET has the advantage of providing more recent information on occupational work activities.Second, our study relates to the literature examining the offshoring of information-based tasks to foreign worksites II. A HISTORY OF TECHNOLOGICAL REVOLUTIONS AND EMPLOYMENTThe concern over technological unemployment is hardly a recent phenomenon.Throughout history, the process of creative destruction, following technological inventions, has created enormous wealth, but also undesired disruptions.As stressed by That guilds systematically tried to weaken market forces as aggregators to maintain the technological status quo is persuasively argued by the Glorious Revolution of 1688, declined and lost most of its political clout There are at least two possible explanations for the shift in attitudes towards technological progress. First, after Parliamentary supremacy was established over the Crown, the property owning classes became politically dominant in Britain is contradictory evidence suggesting that capital owners initially accumulated a growing share of national income Together with developments in continuous-flow production, enabling workers to be stationary while different tasks were moved to them, it was identical interchangeable parts that allowed complex products to be assembled from mass produced individual components by using highly specialised machine tools to a sequence of operations. 10 Yet while the first assembly-line was documented in 1804, it was not until the late nineteenth century that continuous-flow processes started to be adopted on a larger scale, which enabled corporations such as the Ford Motor Company to manufacture the T-Ford at a sufficiently low price for it to become the people's vehicle The modern pattern of capital-skill complementarity gradually emerged in the late nineteenth century, as manufacturing production shifted to increasingly mechanised assembly lines. This shift can be traced to the switch to electricity from steam and water-power which, in combination with continuous-process 10 These machines were sequentially implemented until the production process was completed. Over time, such machines became much cheaper relative to skilled labor. As a result, production became much more capital intensive 11 While educational wage differentials in the US narrowed from 1915 to 1980 Although there are clearly several variables at work, consensus is broad that this can be ascribed to an acceleration in capital-skill complementarity, driven by the adoption of computers and information technology The early twentieth century office machines increased the demand for clerking workers The Computer Revolution can go some way in explaining the growing wage inequality of the past decades. For example, The expansion in high-skill employment can be explained by the falling price of carrying out routine tasks by means of computers, which complements more abstract and creative services. Seen from a production function perspective, an outward shift in the supply of routine informational inputs increases the marginal productivity of workers they are demanded by. For example, text and data mining has improved the quality of legal research as constant access to market information has improved the efficiency of managerial decision-making i.e. tasks performed by skilled workers at the higher end of the income distribution. The result has been an increasingly polarised labour market, with growing employment in high-income cognitive jobs and low-income manual occupations, accompanied by a hollowing-out of middle-income routine jobs. This is a pattern that is not unique to the US and equally applies to a number of developed economies (Goos, et al., 2009). 14  How technological progress in the twenty-first century will impact on labour market outcomes remains to be seen. Throughout history, technological progress has vastly shifted the composition of employment, from agriculture and the artisan shop, to manufacturing and clerking, to service and management occupations. Yet the concern over technological unemployment has proven to be exaggerated. The obvious reason why this concern has not materialised relates to Ricardo's famous chapter on machinery, which suggests that laboursaving technology reduces the demand for undifferentiated labour, thus leading to technological unemployment Although the capitalisation effect has been predominant historically, our discovery of means of economising the use of labour can outrun the pace at which we can find new uses for labour, as III. THE TECHNOLOGICAL REVOLUTIONS OF THE TWENTY-FIRST CENTURYThe secular price decline in the real cost of computing has created vast economic incentives for employers to substitute labour for computer capital. 15 Yet the tasks computers are able to perform ultimately depend upon the ability of a programmer to write a set of procedures or rules that appropriately direct the technology in each possible contingency. Computers will therefore be relatively productive to human labour when a problem can be specified -in the sense that the criteria for success are quantifiable and can readily be evaluated Our analysis builds on the task categorisation of Recent technological breakthroughs are, in large part, due to efforts to turn non-routine tasks into well-defined problems. Defining such problems is helped by the provision of relevant data: this is highlighted in the case of handwriting recognition by As such, technological progress has been aided by the recent production of increasingly large and complex datasets, known as big data. 16 For instance, with a growing corpus of human-translated digitalised text, the success of a machine translator can now be judged by its accuracy in reproducing observed translations. Data from United Nations documents, which are translated by hu-man experts into six languages, allow Google Translate to monitor and improve the performance of different machine translation algorithms Further, ML algorithms can discover unexpected similarities between old and new data, aiding the computerisation of tasks for which big data has newly become available. As a result, computerisation is no longer confined to routine tasks that can be written as rule-based software queries, but is spreading to every non-routine task where big data becomes available III.A. Computerisation in non-routine cognitive tasksWith the availability of big data, a wide range of non-routine cognitive tasks are becoming computerisable. That is, further to the general improvement in technological progress due to big data, algorithms for big data are rapidly entering domains reliant upon storing or accessing information. The use of big data is afforded by one of the chief comparative advantages of computers relative to human labor: scalability. Little evidence is required to demonstrate that, in performing the task of laborious computation, networks of machines scale better than human labour Computerisation of cognitive tasks is also aided by another core comparative advantage of algorithms: their absence of some human biases. An algorithm can be designed to ruthlessly satisfy the small range of tasks it is given. Humans, in contrast, must fulfill a range of tasks unrelated to their occupation, such as sleeping, necessitating occasional sacrifices in their occupational performance Fraud detection is a task that requires both impartial decision making and the ability to detect trends in big data. As such, this task is now almost com-pletely automated In health care, diagnostics tasks are already being computerised. Oncologists at Memorial Sloan-Kettering Cancer Center are, for example, using IBM's Watson computer to provide chronic care and cancer treatment diagnostics. Knowledge from 600,000 medical evidence reports, 1.5 million patient records and clinical trials, and two million pages of text from medical journals, are used for benchmarking and pattern recognition purposes. This allows the computer to compare each patient's individual symptoms, genetics, family and medication history, etc., to diagnose and develop a treatment plan with the highest probability of success In addition, computerisation is entering the domains of legal and financial services. Sophisticated algorithms are gradually taking on a number of tasks performed by paralegals, contract and patent lawyers Sensor data is often coupled with new ML fault-and anomaly-detection algorithms to render many tasks computerisable. A broad class of examples can be found in condition monitoring and novelty detection, with technology substituting for closed-circuit TV (CCTV) operators, workers examining equipment defects, and clinical staff responsible for monitoring the state of patients in intensive care. Here, the fact that computers lack human biases is of great value: algorithms are free of irrational bias, and their vigilance need not be interrupted by rest breaks or lapses of concentration. Following the declining costs of digital sensing and actuation, ML approaches have successfully addressed condition monitoring applications ranging from batteries Advances in user interfaces also enable computers to respond directly to a wider range of human requests, thus augmenting the work of highly skilled labour, while allowing some types of jobs to become fully automated. For example, Apple's Siri and Google Now rely on natural user interfaces to recognise spoken words, interpret their meanings, and act on them accordingly. Moreover, a company called SmartAction now provides call computerisation solutions that use ML technology and advanced speech recognition to improve upon conventional interactive voice response systems, realising cost savings of 60 to 80 percent over an outsourced call center consisting of human labour Occupations that require subtle judgement are also increasingly susceptible to computerisation. To many such tasks, the unbiased decision making of an algorithm represents a comparative advantage over human operators. In the most challenging or critical applications, as in ICUs, algorithmic recommendations may serve as inputs to human operators; in other circumstances, algorithms will themselves be responsible for appropriate decision-making. In the financial sector, such automated decision-making has played a role for quite some time. AI algorithms are able to process a greater number of financial announcements, press releases, and other information than any human trader, and then act faster upon them Although the extent of these developments remains to be seen, estimates by MGI (2013) suggests that sophisticated algorithms could substitute for approximately 140 million full-time knowledge workers worldwide. Hence, while technological progress throughout economic history has largely been confined to the mechanisation of manual tasks, requiring physical labour, technological progress in the twenty-first century can be expected to contribute to a wide range of cognitive tasks, which, until now, have largely remained a human domain. Of course, many occupations being affected by these developments are still far from fully computerisable, meaning that the computerisation of some tasks will simply free-up time for human labour to perform other tasks. Nonetheless, the trend is clear: computers increasingly challenge human labour in a wide range of cognitive tasks III.B. Computerisation in non-routine manual tasksMobile robotics provides a means of directly leveraging ML technologies to aid the computerisation of a growing scope of manual tasks. The continued technological development of robotic hardware is having notable impact upon employment: over the past decades, industrial robots have taken on the routine tasks of most operatives in manufacturing. Now, however, more advanced robots are gaining enhanced sensors and manipulators, allowing them to perform non-routine manual tasks. For example, General Electric has recently developed robots to climb and maintain wind turbines, and more flexible surgical robots with a greater range of motion will soon perform more types of operations The big data provided by these improved sensors are offering solutions to many of the engineering problems that had hindered robotic development in the past. In particular, the creation of detailed three dimensional maps of road networks has enabled autonomous vehicle navigation; most notably illustrated by Google's use of large, specialised datasets collected by its driverless cars Expanding technological capabilities and declining costs will make entirely new uses for robots possible. Robots will likely continue to take on an increasing set of manual tasks in manufacturing, packing, construction, maintenance, and agriculture. In addition, robots are already performing many simple service tasks such as vacuuming, mopping, lawn mowing, and gutter cleaningthe market for personal and household service robots is growing by about 20 percent annually III.C. The task model revisitedThe task model of Computer capital can now equally substitute for a wide range of tasks com-monly defined as non-routine The task model assumes for tractability an aggregate, constant-returns-toscale, Cobb-Douglas production function of the formwhere L S and L NS are susceptible and non-susceptible labor inputs and C is computer capital. Computer capital is supplied perfectly elastically at market price per efficiency unit, where the market price is falling exogenously with time due to technological progress. It further assumes income-maximizing workers, with heterogeneous productivity endowments in both susceptible and non-susceptible tasks. Their task supply will respond elastically to relative wage levels, meaning that workers will reallocate their labour supply according to their comparative advantage as in The above described simple model differs from the task model of Hence, in short, while the task model predicts that computers for labour substitution will be confined to routine tasks, our model predicts that computerisation can be extended to any non-routine task that is not subject to any engineering bottlenecks to computerisation. These bottlenecks thus set the boundaries for the computerisation of non-routine tasks. Drawing upon the ML and MR literature, and a workshop held at the Oxford University Engineering Sciences Department, we identify several engineering bottlenecks, corresponding to three task categories. According to these findings, non-susceptible labor inputs can be described as,(2)where L PM , L C and L SI are labour inputs into perception and manipulation tasks, creative intelligence tasks, and and social intelligence tasks.We note that some related engineering bottlenecks can be partially alleviated by the simplification of tasks. One generic way of achieving this is to reduce the variation between task iterations. As a prototypical example, consider the factory assembly line, turning the non-routine tasks of the artisan shop into repetitive routine tasks performed by unskilled factory workers. A more recent example is the computerisation of non-routine manual tasks in construction. On-site construction tasks typically demand a high degree of adaptability, so as to accommodate work environments that are typically irregularly laid out, and vary according to weather. Prefabrication, in which the construction object is partially assembled in a factory before being transported to the construction site, provides a way of largely removing the requirement for adaptability. It allows many construction tasks to be performed by robots under controlled conditions that eliminate task variability -a method that is becoming increasingly widespread, particularly in Japan Perception and manipulation tasks. Robots are still unable to match the depth and breadth of human perception. While basic geometric identification is reasonably mature, enabled by the rapid development of sophisticated sensors and lasers, significant challenges remain for more complex perception tasks, such as identifying objects and their properties in a cluttered field of view. As such, tasks that relate to an unstructured work environment can make jobs less susceptible to computerisation. For example, most homes are unstructured, requiring the identification of a plurality of irregular objects and containing many cluttered spaces which inhibit the mobility of wheeled objects. Conversely, supermarkets, factories, warehouses, airports and hospitals have been designed for large wheeled objects, making it easier for robots to navigate in performing non-routine manual tasks. Perception problems can, however, sometimes be sidestepped by clever task design. For example, Kiva Systems, acquired by Amazon.com in 2012, solved the problem of warehouse navigation by simply placing bar-code stickers on the floor, informing robots of their precise location The difficulty of perception has ramifications for manipulation tasks, and, in particular, the handling of irregular objects, for which robots are yet to reach human levels of aptitude. This has been evidenced in the development of robots that interact with human objects and environments. While advances have been made, solutions tend to be unreliable over the myriad small variations on a single task, repeated thousands of times a day, that many applications require. A related challenge is failure recoveryi.e. identifying and rectifying the mistakes of the robot when it has, for example, dropped an object. Manipulation is also limited by the difficulties of planning out the sequence of actions required to move an object from one place to another. There are yet further problems in designing manipulators that, like human limbs, are soft, have compliant dynamics and provide useful tactile feedback. Most industrial manipulation makes uses of workarounds to these challenges Creative intelligence tasks. The psychological processes underlying human creativity are difficult to specify. According to In principle, such creativity is possible and some approaches to creativity already exist in the literature. In these and many other applications, generating novelty is not particularly difficult. Instead, the principal obstacle to computerising creativity is stating our creative values sufficiently clearly that they can be encoded in an program Social intelligence tasks. Human social intelligence is important in a wide range of work tasks, such as those involving negotiation, persuasion and care.To aid the computerisation of such tasks, active research is being undertaken within the fields of Affective Computing Hence, in short, while sophisticated algorithms and developments in MR, building upon with big data, now allow many non-routine tasks to be automated, occupa tions that involve complex perception and manipulation tasks, creative intelligence tasks, and social intelligence tasks are unlikely to be substituted by computer capital over the next decade or two. The probability of an occupation being automated can thus be described as a function of these task In addition, we exclude any six-digit SOC occupations for which O * NET data was missing. 20 Doing so, we end up with a final dataset consisting of 702 occupations.To assess the employment impact of the described technological developments in ML, the ideal experiment would provide two identical autarkic economies, one facing the expanding technological capabilities we observe, and a secular decline in the price of computerisation, and the other not. By comparison, it would be straightforward to examine how computerisation reshapes the occupational composition of the labour market. In the absence of this experiment, the second preferred option would be to build on the implementation strategy of Instead, our implementation strategy builds on the literature examining the offshoring of information-based tasks to foreign worksites, consisting of different methodologies to rank and categorise occupations according to their susceptibility to offshoring To work around some of these drawbacks, we combine and build upon the two described approaches. First, together with a group of ML researchers, we subjectively hand-labelled 70 occupations, assigning 1 if automatable, and 0 if not. For our subjective assessments, we draw upon a workshop held at the Oxford University Engineering Sciences Department, examining the automatability of a wide range of tasks. Our label assignments were based on eyeballing the O * NET tasks and job description of each occupation. This information is particular to each occupation, as opposed to standardised across different jobs. The hand-labelling of the occupations was made by answering the question \"Can the tasks of this job be sufficiently specified, conditional on the availability of big data, to be performed by state of the art computer-controlled equipment\". Thus, we only assigned a 1 to fully automatable occupations, where we considered all tasks to be automatable. To the best of our knowledge, we considered the possibility of task simplification, possibly allowing some currently non-automatable tasks to be automated. Labels were assigned only to the occupations about which we were most confident.Second, we use objective O * NET variables corresponding to the defined bottlenecks to computerisation. More specifically, we are interested in variables describing the level of perception and manipulation, creativity, and social intelligence required to perform it. As reported in Table Finger DexterityThe ability to make precisely coordinated movements of the fingers of one or both hands to grasp, manipulate, or assemble very small objects.Manual DexterityThe ability to quickly move your hand, your hand together with your arm, or your two hands to grasp, manipulate, or assemble objects.Cramped Work Space, Awkward PositionsHow often does this job require working in cramped work spaces that requires getting into awkward positions?Creative IntelligenceOriginalityThe ability to come up with unusual or clever ideas about a given topic or situation, or to develop creative ways to solve a problem.Fine Arts Knowledge of theory and techniques required to compose, produce, and perform works of music, dance, visual arts, drama, and sculpture.Social Intelligence Social PerceptivenessBeing aware of others' reactions and understanding why they react as they do.NegotiationBringing others together and trying to reconcile differences.PersuasionPersuading others to change their minds or behavior.Assisting and Caring for OthersProviding personal assistance, medical attention, emotional support, or other personal care to others such as coworkers, customers, or patients.the level of \"Manual Dexterity\" computer-controlled equipment would require to perform a specific occupation. An exception is the \"Cramped work space\" variable, which measures the frequency of unstructured work. Hence, in short, by hand-labelling occupations, we work around the issue that O * NET data was not gathered to specifically measure the automatability of jobs in a similar manner to IV.B. Classification methodWe begin by examining the accuracy of our subjective assessments of the automatability of 702 occupations. For classification, we develop an algorithm to provide the label probability given a previously unseen vector of variables.In the terminology of classification, the O * NET variables form a feature vector, denoted x \u2208 R 9 . O * NET hence supplies a complete dataset of 702 such feature vectors. A computerisable label is termed a class, denoted y \u2208 {0, 1}.For our problem, y = 1 (true) implies that we hand-labelled as computerisable the occupation described by the associated nine O * NET variables contained inx \u2208 R 9 . Our training data is D = (X, y), where X \u2208 R 70\u00d79 is a matrix of variables and y \u2208 {0, 1} 70 gives the associated labels. This dataset contains information about how y varies as a function of x: as a hypothetical example, it may be the case that, for all occupations for which x 1 > 50, y = 1. A probabilistic classification algorithm exploits patterns existent in training data to return the probability P (y * = 1 | x * , X, y) of a new, unlabelled, test datum with features x * having class label y * = 1. We achieve probabilistic classification by introducing a latent function f : x \u2192 R, known as a discriminant function. Given the value of the discriminant f * at a test point x * , we assume that the probability for the class label is given by the logistic(3),andFor f * > 0, y * = 1 is more probable than y * = 0. For our application, f can be thought of as a continuousvalued 'automatability' variable: the higher its value, the higher the probability of computerisation. We test three different models for the discriminant function, f , using the best performing for our further analysis. Firstly, logistic (or logit) regression, which adopts a linear model for f , f (x) = w \u22ba x, where the un-known weights w are often inferred by maximising their probability in light of the training data. This simple model necessarily implies a simple monotonic relationship between features and the probability of the class taking a particular value. Richer models are provided by Gaussian process classifiers A GP is defined as a distribution over the functions f : X \u2192 R such that the distribution over the possible function values on any finite subset of X (such as X) is multivariate Gaussian. For a function f (x), the prior distribution over its values f on a subset x \u2282 X are completely specified by a covariance matrixThe covariance matrix is generated by a covariance function \u03ba : X \u00d7 X \u2192 R; that is, K = \u03ba(X, X). The GP model is expressed by the choice of \u03ba; we consider the exponentiated quadratic (squared exponential) and rational quadratic. Note that we have chosen a zero mean function, encoding the assumption that P (y * = 1) = 1 2 sufficiently far from training data. Given training data D, we use the GP to make predictions about the function values f * at input x * . With this information, we have the predictive equationswhereInferring the label posterior p(y * | x * , D) is complicated by the non-Gaussian form of the logistic (3). In order to effect inference, we use the approximate Expectation Propagation algorithm We tested three Gaussian process classifiers using the GPML toolbox (Rasmussen and Nickisch, 2010) on our data, built around exponentiated quadratic, rational quadratic and linear covariances. Note that the latter is equivalent to logistic regression with a Gaussian prior taken on the weights w. To validate these classifiers, we randomly selected a reduced training set of half the available data D; the remaining data formed a test set. On this test set, we evaluated how closely the algorithm's classifications matched the hand labels according to two metrics (see e.g. Having validated our approach, we proceed to use classification to predict the probability of computerisation for all 702 occupations. For this purpose, we introduce a new label variable, z, denoting whether an occupation is truly computerisable or not: note that this can be judged only once an occupation is computerised, at some indeterminate point in the future. We take, again, a logistic likelihood, ( .We implicitly assumed that our hand label, y, is a noise-corrupted version of the unknown true label, z. Our motivation is that our hand-labels of computerisability must necessarily be treated as such noisy measurements. We thus acknowledge that it is by no means certain that a job is computerisable given our labelling. We define X * \u2208 R 702\u00d79 as the matrix of O * NET variables for all 702 occupations; this matrix represents our test features.We perform a final experiment in which, given training data D, consisting of our 70 hand-labelled occupations, we aim to predict z * for our test features X * . This approach firstly allows us to use the features of the 70 occupations about which we are most certain to predict for the remaining 632. Further, our algorithm uses the trends and patterns it has learned from bulk data to correct for what are likely to be mistaken labels. More precisely, the algorithm provides a smoothly varying probabilistic assessment of automatability as a function of the variables. For our Gaussian process classifier, this function is non-linear, meaning that it flexibly adapts to the patterns inherent in the training data. Our approach thus allows for more complex, non-linear, interactions between variables: for example, perhaps one variable is not of importance unless the value of another variable is sufficiently large. We report P (z * | X * , D) as the probability of computerisation henceforth (for a detailed probability ranking, see Appendix). Figure II illustrates that this probability is non-linearly related to the nine O * NET variables selected.V. EMPLOYMENT IN THE TWENTY-FIRST CENTURYIn this section, we examine the possible future extent of at-risk job computerisation, and related labour market outcomes. The task model predicts that recent developments in ML will reduce aggregate demand for labour input in tasks that can be routinised by means of pattern recognition, while increasing the demand for labour performing tasks that are not susceptible to computerisation. However, we make no attempt to forecast future changes in the occupational composition of the labour market. While the 2010-2020 BLS occupational employment projections predict US net employment growth across major occupations, based on historical staffing patterns, we speculate about technology that is in only the early stages of development. This means that historical data on the impact of the technological developments we observe is unavailable. 21 We therefore focus on the impact of computerisation on the mix of jobs that existed in 2010. Our analysis is thus limited to the substitution effect of future computerisation.Turning first to the expected employment impact, reported in Figure The distribution of BLS 2010 occupational employment over the probability of computerisation, along with the share in low, medium and high probability categories. Note that the total area under all curves is equal to total US employment.probability of computerisation (thresholding at probabilities of 0.7 and 0.3). According to our estimate, 47 percent of total US employment is in the high risk category, meaning that associated occupations are potentially automatable over some unspecified number of years, perhaps a decade or two. It shall be noted that the probability axis can be seen as a rough timeline, where high probability occupations are likely to be substituted by computer capital relatively soon.Over the next decades, the extent of computerisation will be determined by the pace at which the above described engineering bottlenecks to automation can be overcome. Seen from this perspective, our findings could be interpreted as two waves of computerisation, separated by a \"technological plateau\". In the first wave, we find that most workers in transportation and logistics occupations, together with the bulk of office and administrative support workers, and labour in production occupations, are likely to be substituted by computer capital. As computerised cars are already being developed and the declining cost of sensors makes augmenting vehicles with advanced sensors increasingly cost-effective, the automation of transportation and logistics occupations is in line with the technological developments documented in the literature. Furthermore, algorithms for big data are already rapidly entering domains reliant upon storing or accessing information, making it equally intuitive that office and administrative support occupations will be subject to computerisation. The computerisation of production occupations simply suggests a continuation of a trend that has been observed over the past decades, with industrial robots taking on the routine tasks of most operatives in manufacturing. As industrial robots are becoming more advanced, with enhanced senses and dexterity, they will be able to perform a wider scope of non-routine manual tasks. From a technological capabilities point of view, the vast remainder of employment in production occupations is thus likely to diminish over the next decades.More surprising, at first sight, is that a substantial share of employment in services, sales and construction occupations exhibit high probabilities of computerisation. Yet these findings are largely in line with recent documented technological developments. First, the market for personal and household service robots is already growing by about 20 percent annually In short, our findings suggest that recent developments in ML will put a substantial share of employment, across a wide range of occupations, at risk in the near future. According to our estimates, however, this wave of automation will be followed by a subsequent slowdown in computers for labour substitution, due to persisting inhibiting engineering bottlenecks to computerisation. The relatively slow pace of computerisation across the medium risk category of employment can thus partly be interpreted as a technological plateau, with incremental technological improvements successively enabling further labour substitution. More specifically, the computerisation of occupations in the medium risk category will mainly depend on perception and manipulation challenges. This is evident from Table Our model predicts that the second wave of computerisation will mainly depend on overcoming the engineering bottlenecks related to creative and social intelligence. As reported in Table The low susceptibility of engineering and science occupations to computerisation, on the other hand, is largely due to the high degree of creative intelligence they require. The O * NET tasks of mathematicians, for example, involve \"developing new principles and new relationships between existing mathematical principles to advance mathematical science\" and \"conducting research to extend mathematical knowledge in traditional areas, such as algebra, geometry, probability, and logic.\" Hence, while it is evident that computers are entering the domains of science and engineering, our predictions implicitly suggest strong complementarities between computers and labour in creative science and engineering occupations; although it is possible that computers will fully substitute for workers in these occupations over the long-run. We note that the predictions of our model are strikingly in line with the technological trends we observe in the automation of knowledge work, even within occupational categories. For example, we find that paralegals and legal assistants -for which computers already substitute -in the high risk category. At the same time, lawyers, which rely on labour input from legal assistants, are in the low risk category. Thus, for the work of lawyers to be fully automated, engineering bottlenecks to creative and social intelligence will need to be overcome, implying that the computerisation of legal research will complement the work of lawyers in the medium term.To complete the picture of what recent technological progress is likely to mean for the future of employment, we plot the average median wage of occupations by their probability of computerisation. We do the same for skill level, measured by the fraction of workers having obtained a bachelor's degree, or higher educational attainment, within each occupation. Figure IV reveals that both wages and educational attainment exhibit a strong negative relationship with the probability of computerisation. We note that this prediction implies a truncation in the current trend towards labour market polarization, with growing employment in high and low-wage occupations, accompanied by a hollowing-out of middle-income jobs. Rather than reducing the demand for middle-income occupations, which has been the pattern over the past decades, our model predicts that computerisation will mainly substitute for low-skill and low-wage jobs in the near future. By contrast, high-skill and high-wage occupations are the least susceptible to computer capital.Our findings were robust to the choice of the 70 occupations that formed our training data. This was confirmed by the experimental results tabulated in Table V.A. LimitationsIt shall be noted that our predictions are based on expanding the premises about the tasks that computer-controlled equipment can be expected to perform. Hence, we focus on estimating the share of employment that can potentially be substituted by computer capital, from a technological capabilities point of view, over some unspecified number of years. We make no attempt to estimate how many jobs will actually be automated. The actual extent and pace of computerisation will depend on several additional factors which were left unaccounted for.First, labour saving inventions may only be adopted if the access to cheap labour is scarce or prices of capital are relatively high Second, regulatory concerns and political activism may slow down the process of computerisation. The states of California and Nevada are, for example, currently in the process of making legislatory changes to allow for driverless cars. Similar steps will be needed in other states, and in relation to various technologies. The extent and pace of legislatory implementation can furthermore be related to the public acceptance of technological progress. 23 Although resistance to technological progress has become seemingly less common since the Industrial Revolution, there are recent examples of resistance to technological change. 24 We avoid making predictions about the legislatory process and the public acceptance of technological progress, and thus the pace of computerisation.Third, making predictions about technological progress is notoriously difficult 24 Uber, a start-up company connecting passengers with drivers of luxury vehicles, has recently faced pressure from from local regulators, arising from tensions with taxicab services. Furthermore, in 2011 the UK Government scrapped a 12.7 billion GBP project to introduce electronic patient records after resistance from doctors.25 Marvin Minsky famously claimed in 1970 that \"in from three to eight years we will have a machine with the general intelligence of an average human being\". This prediction is yet to materialise.Although it is clear that the impact of productivity gains on employment will vary across occupations and industries, we make no attempt to examine such effects.VI. CONCLUSIONS", "conclusions": "While computerisation has been historically confined to routine tasks involving explicit rule-based activities (Autor, et al., 2003;Goos, et al., 2009;, algorithms for big data are now rapidly entering domains reliant upon pattern recognition and can readily substitute for labour in a wide range of non-routine cognitive tasks Autor and Dorn, 2013)(Brynjolfsson and McAfee, 2011;. In addition, advanced robots are gaining enhanced senses and dexterity, allowing them to perform a broader scope of manual tasks MGI, 2013)(IFR, 2012b;Robotics-VO, 2013;. This is likely to change the nature of work across industries and occupations.MGI, 2013)In this paper, we ask the question: how susceptible are current jobs to these technological developments? To assess this, we implement a novel methodology to estimate the probability of computerisation for 702 detailed occupations.Based on these estimates, we examine expected impacts of future computerisation on labour market outcomes, with the primary objective of analysing the number of jobs at risk and the relationship between an occupation's probability of computerisation, wages and educational attainment. We distinguish between high, medium and low risk occupations, depending on their probability of computerisation. We make no attempt to estimate the number of jobs that will actually be automated, and focus on potential job automatability over some unspecified number of years. According to our estimates around 47 percent of total US employment is in the high risk category. We refer to these as jobs at riski.e. jobs we expect could be automated relatively soon, perhaps over the next decade or two.Our model predicts that most workers in transportation and logistics occupations, together with the bulk of office and administrative support workers, and labour in production occupations, are at risk. These findings are consistent with recent technological developments documented in the literature. More surprisingly, we find that a substantial share of employment in service occupations, where most US job growth has occurred over the past decades , are highly susceptible to computerisation. Additional support for this finding is provided by the recent growth in the market for service robots (Autor and Dorn, 2013) and the gradually diminishment of the comparative advantage of human labour in tasks involving mobility and dexterity (MGI, 2013).(Robotics-VO, 2013)Finally, we provide evidence that wages and educational attainment exhibit a strong negative relationship with the probability of computerisation. We note that this finding implies a discontinuity between the nineteenth, twentieth and the twenty-first century, in the impact of capital deepening on the relative demand for skilled labour. While nineteenth century manufacturing technologies largely substituted for skilled labour through the simplification of tasks (Braverman, 1974;Hounshell, 1985;James and Skinner, 1985;, the Computer Revolution of the twentieth century caused a hollowing-out of middle-income jobs Goldin and Katz, 1998)(Goos, et al., 2009;. Our model predicts a truncation in the current trend towards labour market polarisation, with computerisation being principally confined to low-skill and low-wage occupations. Our findings thus imply that as technology races ahead, low-skill workers will reallocate to tasks that are non-susceptible to computerisationi.e., tasks requiring creative and social intelligence. For workers to win the race, however, they will have to acquire creative and social skills.Autor and Dorn, 2013)", "SDG": [8, 9]}, "the_impact_of_artificial_intelligence_on_innovation": {"name": "NBER WORKING PAPER SERIES THE IMPACT OF ARTIFICIAL INTELLIGENCE ON INNOVATION", "abstract": " At least one co-author has disclosed a financial relationship of potential relevance for this research. Further information is available online at http://www.nber.org/papers/w24449.ack NBER working papers are circulated for discussion and comment purposes. They have not been peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications.", "keywords": "", "introduction": "Rapid advances in the field of artificial intelligence have profound implications for the economy as well as society at large. These innovations have the potential to directly influence both the production and the characteristics of a wide range of products and services, with important implications for productivity, employment, and competition. But, as important as these effects are likely to be, artificial intelligence also has the potential to change the innovation process itself, with consequences that may be equally profound, and which may, over time, come to dominate the direct effect.Consider the case of Atomwise, a startup firm which is developing novel technology for identifying potential drug candidates (and insecticides) by using neural networks to predict the bioactivity of candidate molecules. The company reports that its deep convolutional neural networks \"far surpass\" the performance of conventional \"docking\" algorithms. After appropriate training on vast quantities of data, the company's AtomNet product is described as being able to \"recognize\" foundational building blocks of organic chemistry, and is capable of generating highly accurate predictions of the outcomes of real-world physical experiments . Such breakthroughs hold out the prospect of substantial improvements in the productivity of early stage drug screening. Of course, Atomwise's technology (and that of other companies leveraging artificial intelligence to advance drug discovery or medical diagnosis) is still at an early stage: though their initial results seem to be promising, no new drugs have actually come to market using these new approaches. But whether or not Atomwise delivers fully on its promise, its technology is representative of the ongoing attempt to develop a new innovation \"playbook\", one that leverages large datasets and learning algorithms to engage in precise prediction of biological phenomena in order to guide design effective interventions. Atomwise, for example, is now deploying this approach to the discovery and development of new pesticides and agents for controlling crop diseases.(Wallach et al., 2015)Atomwise's example illustrates two of the ways in which advances in artificial intelligence have the potential to impact innovation. First, though the origins of artificial intelligence are broadly in the field of computer science, and its early commercial applications have been in relatively narrow domains such as robotics, the learning algorithms that are now being developed 2 suggest that artificial intelligence may ultimately have applications across a very wide range.From the perspective of the economics of innovation (among others, ), there is an important distinction between the problem of providing innovation incentives to develop technologies with a relatively narrow domain of application, such robots purposebuilt for narrow tasks, versus technologies with a wide-advocates might say almost limitlessdomain of application, as may be true of the advances in neural networks and machine learning often referred to as \"deep learning.\" As such, a first question to be asked is the degree to which developments in artificial intelligence are not simply examples of new technologies, but rather may be the kinds of \"general purpose technologies\" (hereafter GPTs) that have historically been such influential drivers of long-term technological progress.Bresnahan and Trajtenberg (1995)Second, while some applications of artificial intelligence will surely constitute lower-cost or higher-quality inputs into many existing production processes (spurring concerns about the potential for large job displacements), others, such as deep learning, hold out the prospect of not only productivity gains across a wide variety of sectors but also changes in the very nature of the innovation process within those domains. As articulated famously by , by enabling innovation across many applications, the \"invention of a method of invention\" has the potential to have much larger economic impact than development of any single new product.Griliches (1957)Here we argue that recent advances in machine learning and neural networks, through their ability to improve both the performance of end use technologies and the nature of the innovation process, are likely to have a particularly large impact on innovation and growth. Thus the incentives and obstacles that may shape the development and diffusion of these technologies are an important topic for economic research, and building an understanding of the conditions under which different potential innovators are able to gain access to these tools and to use them in a pro-competitive way is a central concern for policy. This essay begins to unpack the potential impact of advances in artificial intelligence on innovation, and to identify the role that policy and institutions might play in providing effective incentives for innovation, diffusion, and competition in this area. We begin in Section II by highlighting the distinctive economics of research tools, of which deep learning applied to R&D problems is such an intriguing example. We focus on the interplay between the degree of generality of application of a new research tool and the role of research tools not simply in enhancing the efficiency of research activity but in creating a new \"playbook\" for innovation itself. We then turn in Section III to briefly contrasting three key technological trajectories within AI-robotics, symbolic systems, and deep learning. We propose that these often conflated fields will likely play very different roles in the future of innovation and technical change. Work in symbolic systems appears to have stalled and is likely to have relatively little impact going forwards. And while developments in robotics have the potential to further displace human labor in the production of many goods and services, innovation in robotics technologies per se has relatively low potential to change the nature of innovation itself. By contrast, deep learning seems to be an area of research that is highly general-purpose and that has the potential to change the innovation process itself.We explore whether this might indeed be the case through an examination of some quantitative empirical evidence on the evolution of different areas artificial intelligence in terms of scientific and technical outputs of AI researchers as measured (imperfectly) by the publication of papers and patents from 1990 through 2015. In particular, we develop what we believe is the first systematic database that captures the corpus of scientific paper and patenting activity in artificial intelligence, broadly defined, and divides these outputs into those associated with robotics, symbolic systems, and deep learning. Though preliminary in nature (and inherently imperfect given that key elements of research activity in artificial intelligence may not be observable using these traditional innovation metrics), we find striking evidence for a rapid and meaningful shift in the application orientation of learning-oriented publications, particularly after 2009. The timing of this shift is informative, since it accords with qualitative evidence about the surprisingly strong performance of so-called \"deep learning\" multi-layered neural networks in a range of tasks including computer vision and other prediction tasks. Supplementary evidence (not reported here) based on the citation patterns to authors such as Geoffrey Hinton who are leading figures in deep learning suggests a striking acceleration of work in just the last few years that builds on a small number of algorithmic breakthroughs related to multi-layered neural networks.Though not a central aspect of the analysis for this paper, we further find that, whereas research on learning-oriented algorithms has had a slow and steady upward swing outside of the United States, US researchers have had a less sustained commitment to learning-oriented research prior to 2009, and have been in a \"catch up\" mode ever since.Finally, we begin to explore some of the organizational, institutional and policy consequences of our analysis. We see machine learning as the \"invention of a method of invention\" whose application depends, in each case, on having access not just to the underlying algorithms but also to large, granular datasets on physical and social behavior. Developments in neural networks and machine learning thus raise the question of, even if the underlying scientific approaches (i.e., the basic multi-layered neural networks algorithms) are open, prospects for continued progress in this field-and commercial applications thereof-are likely to be significantly impacted by terms of access to complementary data. Specifically, if there are increasing returns to scale or scope in data acquisition (there is more learning to be had from the \"larger\" dataset), it is possible that early or aggressive entrants into a particular application area may be able to create a substantial and long-lasting competitive advantage over potential rivals merely through the control over data rather than through formal intellectual property or demandside network effects. Strong incentives to maintain data privately has the additional potential downside that data is not being shared across researchers, thus reducing the ability of all researchers to access an even larger set of data that would arise from public aggregation. As the competitive advantage of incumbents is reinforced, the power of new entrants to drive technological change may be weakened. Though this is an important possibility, it is also the case that, at least so far, there seems to be a significant amount of entry and experimentation across most key application sectors.", "body": "Rapid advances in the field of artificial intelligence have profound implications for the economy as well as society at large. These innovations have the potential to directly influence both the production and the characteristics of a wide range of products and services, with important implications for productivity, employment, and competition. But, as important as these effects are likely to be, artificial intelligence also has the potential to change the innovation process itself, with consequences that may be equally profound, and which may, over time, come to dominate the direct effect.Consider the case of Atomwise, a startup firm which is developing novel technology for identifying potential drug candidates (and insecticides) by using neural networks to predict the bioactivity of candidate molecules. The company reports that its deep convolutional neural networks \"far surpass\" the performance of conventional \"docking\" algorithms. After appropriate training on vast quantities of data, the company's AtomNet product is described as being able to \"recognize\" foundational building blocks of organic chemistry, and is capable of generating highly accurate predictions of the outcomes of real-world physical experiments Atomwise's example illustrates two of the ways in which advances in artificial intelligence have the potential to impact innovation. First, though the origins of artificial intelligence are broadly in the field of computer science, and its early commercial applications have been in relatively narrow domains such as robotics, the learning algorithms that are now being developed 2 suggest that artificial intelligence may ultimately have applications across a very wide range.From the perspective of the economics of innovation (among others, Second, while some applications of artificial intelligence will surely constitute lower-cost or higher-quality inputs into many existing production processes (spurring concerns about the potential for large job displacements), others, such as deep learning, hold out the prospect of not only productivity gains across a wide variety of sectors but also changes in the very nature of the innovation process within those domains. As articulated famously by Here we argue that recent advances in machine learning and neural networks, through their ability to improve both the performance of end use technologies and the nature of the innovation process, are likely to have a particularly large impact on innovation and growth. Thus the incentives and obstacles that may shape the development and diffusion of these technologies are an important topic for economic research, and building an understanding of the conditions under which different potential innovators are able to gain access to these tools and to use them in a pro-competitive way is a central concern for policy. This essay begins to unpack the potential impact of advances in artificial intelligence on innovation, and to identify the role that policy and institutions might play in providing effective incentives for innovation, diffusion, and competition in this area. We begin in Section II by highlighting the distinctive economics of research tools, of which deep learning applied to R&D problems is such an intriguing example. We focus on the interplay between the degree of generality of application of a new research tool and the role of research tools not simply in enhancing the efficiency of research activity but in creating a new \"playbook\" for innovation itself. We then turn in Section III to briefly contrasting three key technological trajectories within AI-robotics, symbolic systems, and deep learning. We propose that these often conflated fields will likely play very different roles in the future of innovation and technical change. Work in symbolic systems appears to have stalled and is likely to have relatively little impact going forwards. And while developments in robotics have the potential to further displace human labor in the production of many goods and services, innovation in robotics technologies per se has relatively low potential to change the nature of innovation itself. By contrast, deep learning seems to be an area of research that is highly general-purpose and that has the potential to change the innovation process itself.We explore whether this might indeed be the case through an examination of some quantitative empirical evidence on the evolution of different areas artificial intelligence in terms of scientific and technical outputs of AI researchers as measured (imperfectly) by the publication of papers and patents from 1990 through 2015. In particular, we develop what we believe is the first systematic database that captures the corpus of scientific paper and patenting activity in artificial intelligence, broadly defined, and divides these outputs into those associated with robotics, symbolic systems, and deep learning. Though preliminary in nature (and inherently imperfect given that key elements of research activity in artificial intelligence may not be observable using these traditional innovation metrics), we find striking evidence for a rapid and meaningful shift in the application orientation of learning-oriented publications, particularly after 2009. The timing of this shift is informative, since it accords with qualitative evidence about the surprisingly strong performance of so-called \"deep learning\" multi-layered neural networks in a range of tasks including computer vision and other prediction tasks. Supplementary evidence (not reported here) based on the citation patterns to authors such as Geoffrey Hinton who are leading figures in deep learning suggests a striking acceleration of work in just the last few years that builds on a small number of algorithmic breakthroughs related to multi-layered neural networks.Though not a central aspect of the analysis for this paper, we further find that, whereas research on learning-oriented algorithms has had a slow and steady upward swing outside of the United States, US researchers have had a less sustained commitment to learning-oriented research prior to 2009, and have been in a \"catch up\" mode ever since.Finally, we begin to explore some of the organizational, institutional and policy consequences of our analysis. We see machine learning as the \"invention of a method of invention\" whose application depends, in each case, on having access not just to the underlying algorithms but also to large, granular datasets on physical and social behavior. Developments in neural networks and machine learning thus raise the question of, even if the underlying scientific approaches (i.e., the basic multi-layered neural networks algorithms) are open, prospects for continued progress in this field-and commercial applications thereof-are likely to be significantly impacted by terms of access to complementary data. Specifically, if there are increasing returns to scale or scope in data acquisition (there is more learning to be had from the \"larger\" dataset), it is possible that early or aggressive entrants into a particular application area may be able to create a substantial and long-lasting competitive advantage over potential rivals merely through the control over data rather than through formal intellectual property or demandside network effects. Strong incentives to maintain data privately has the additional potential downside that data is not being shared across researchers, thus reducing the ability of all researchers to access an even larger set of data that would arise from public aggregation. As the competitive advantage of incumbents is reinforced, the power of new entrants to drive technological change may be weakened. Though this is an important possibility, it is also the case that, at least so far, there seems to be a significant amount of entry and experimentation across most key application sectors.II.The Economics of New Research Tools: The Interplay between New Methods ofInvention and the Generality of InnovationAt least since First, consider the challenge in providing appropriate innovation incentives when an innovation has potential to drive technological and organizational change across a wide number of distinct applications. Such \"general purpose technologies\" As emphasized by Various aspects of artificial intelligence can certainly be understood as a GPT, and learning from examples such as the microprocessor are likely to be a useful foundation for thinking about both the magnitude of their impact on the economy, and associated policy challenges.A second conceptual framework for thinking about AI is the economics of research tools.Within the research sectors, some innovations open up new avenues of inquiry, or simply improve productivity \"within the lab\". Some of these advances appear to have great potential across a broad set of domains, beyond their initial application: as highlighted by Rather than being a means of creating a single a new corn variety, hybrid corn represented a widely applicable method for breeding many different new varieties. When applied to the challenge of creating new varieties optimized for many different localities (and even more broadly, to other crops) the invention of double-cross hybridization had a huge impact on agricultural productivity.One of the important insights to be gained from thinking about IMIs, therefore, is that the economic impact of some types of research tools is not limited to their ability to reduce the costs of specific innovation activities-perhaps even more consequentially they enable a new approach to innovation itself, by altering the \"playbook\" for innovation in the domains where the new tool is applied. For example, prior to the systematic understanding of the power of \"hybrid vigor,\" a primary focus in agriculture had been improved techniques for self-fertilization (i.e., allowing for more and more specialized natural varietals over time). Once the rules governing hybridization (i.e., heterosis) were systematized, and the performance advantages of hybrid vigor demonstrated, the techniques and conceptual approach for agricultural innovation was shifted, ushering in a long period of systematic innovation using these new tools and knowledge.Advances in machine learning and neural networks appear to have great potential as a research tool in problems of classification and prediction. These are both important limiting factors in a variety of research tasks, and, as exemplified by the Atomwise example, application of \"learning\" approaches to AI hold out the prospect of dramatically lower costs and improved performance in R&D projects where these are significant challenges. But as with hybrid corn, AI based learning may be more usefully understood as an IMI than as a narrowly limited solution to a specific problem. One the one hand, AI based learning may be able to substantially \"automate discovery\" across many domains where classification and prediction tasks play an important role. On the other, they may also \"expand the playbook\" is the sense of opening up the set of problems that can be feasibly addressed, and radically altering scientific and technical communities' conceptual approaches and framing of problems. The invention of optical lenses in the 17 th century had important direct economic impact in applications such as spectacles. But optical lenses in the form of microscopes and telescopes also had enormous and long-lasting indirect effects on the progress of science, technological change, growth, and welfare: by making very small or very distant objects visible for the first time, lenses opened up entirely new domains of inquiry and technological opportunity. Of course, many research tools are neither IMIs nor GPTs, and their primary impact is to reduce the cost or enhance the quality of an existing innovation process. For example, in the pharmaceutical industry, new kinds of materials promise to enhance the efficiency of specific research processes. Other research tools can indeed be thought of as IMIs but are nonetheless relatively limited in application. For example, the development of genetically engineered research mice (such as the Oncomouse) is an IMI that has had a profound impact on the conduct and \"playbook\" of biomedical research, but has no obvious relevance to innovation in areas such as information technology, energy, or aerospace. The challenge presented by advances in AI is that they appear to be research tools that not only have the potential to change the method of innovation itself but also have implications across an extraordinarily wide range of fields.Historically technologies with these characteristics-think of digital computing-have had large and unanticipated impacts across the economy and society in general. From a policy perspective, a further important feature of research tools is that it may be particularly difficult to appropriate their benefits. As emphasized by providing appropriate incentives for an upstream innovator that develops only the first \"stage\" of an innovation (such as a research tool) can be particularly problematic when contracting is imperfect and the ultimate application of the new products whose development is enabled by the upstream innovation is uncertain. Scotchmer and her co-authors emphasized a key point about a multi-stage research process: when the ultimate innovation that creates value requires multiple steps, providing appropriate innovation incentives are not only a question of whether and how to provide property rights in general, but also of how best to distribute property rights and incentives across the multiple stages of the innovation process. Lack of incentives for earlystage innovation can therefore mean that the tools required for subsequent innovation do not even get invented; strong early-stage property rights without adequate contracting opportunities may result in \"hold-up\" for later-stage innovators and so reduce the ultimate impact of the tool in terms of commercial application.The vertical research spillovers created by new research tools (or IMIs) are not just a challenge for designing appropriate intellectual property policy. 1 They are also exemplars of the core innovation externality highlighted by endogenous growth theory Of course, these frameworks cover only a subset of the key informational and competitive distortions that might arise when considering whether and how to provide optimal incentives for the type of technological change represented by some areas of AI. But these two areas in particular seem likely to be important for understanding the implications of the current dramatic advances in AI supported learning. We therefore turn in the next section to a brief outline of the ways in which AI is changing, with an eye towards bringing the framework here to bear on how we might outline a research agenda exploring the innovation policy challenges that they create.III. The Evolution of Artificial Intelligence: Robotics, Symbolic Systems, and NeuralNetworksIn his omnibus historical account of AI research, Though often grouped together, the intellectual history of AI as a scientific and technical field is usefully informed by distinguishing between three interrelated but separate areas: robotics, neural networks, and symbolic systems. Perhaps the most successful line of research in the early years of AI-dating back to the 1960s-falls under the broad heading of symbolic systems. Although early pioneers such as Turing had emphasized the importance of teaching a machine as one might a child (i.e., emphasizing AI as a learning process), the \"symbol processing hypothesis\" A second influential trajectory in AI has been broadly in the area of robotics. While the concepts of \"robots\" as machines that can perform human tasks dates back at least to the 1940s, the field of robotics began to meaningfully flourish from the 1980s onwards through a combination of the advances in numerically controlled machine tools and the development of more adaptive but still rules-based robotics that rely on the active sensing of a known environment. Perhaps the most economically consequential application of AI to date has been in this area, with large scale deployment of \"industrial robots\" in manufacturing applications.These machines are precisely programmed to undertake a given task in a highly controlled environment. Often located in \"cages\" within highly specialized industrial processes (most notably automobile manufacturing), these purpose-built tools are perhaps more aptly described as highly sophisticated numerically controlled machines rather than as robots with significant AI content. Over the past twenty years, innovation in robotics has had an important impact on manufacturing and automation, most notably through the introduction of more responsive robots that rely on programmed response algorithms that can respond to a variety of stimuli. This approach, famously pioneered by Rod These advances are important, and the most advanced robots continue to capture public imagination when the term AI is invoked. But innovations in robotics are not, generally speaking, IMIs. The increasing automation of laboratory equipment certainly improves research productivity, but advances in robotics are not (yet) centrally connected to the underlying ways in which researchers themselves might develop approaches to undertake innovation itself across multiple domains. There are of course counterexamples to this proposition: robotic space probes have been a very important research tool in planetary science, and the ability of automated remote sensing devices to collect data at very large scale or in challenging environments may transform some fields of research. But robots continue to be used principally in specialized end-use \"production\" applications.Finally, a third stream of research that has been a central element of AI since its founding can be broadly characterized as a \"learning\" approach. Rather than being focused on symbolic logic, or precise sense-and-react systems, the learning approach attempts to create reliable and accurate methods for the prediction of particular events (either physical or logical) in the presence of particular inputs. The concept of a neural network has been particularly important in this area. A neural network is a program that uses a combination of weights and thresholds to translate a set of inputs into a set of outputs, measures the \"closeness\" of these outputs to reality, and then adjusts the weights it uses to narrow the distance between outputs and reality. In this way, neural networks can learn as they are fed more inputs techniques that further enhance their potential for supervised learning.After being initially heralded as having significant promise, the field of neural networks has come in and out of fashion, particularly within the United States. From the 1980s through the mid-2000s, their challenge seemed to be that there were significant limitations to the technology that could not be easily fixed by using larger training datasets or through the introduction of additional layers of \"neurons.\" However, in the mid-2000s, a small number of new algorithmic approaches demonstrated the potential to enhance prediction through back propagation through multiple layers. These neural networks increased their predictive power as they were applied to larger and larger datasets, and were able to scale to an arbitrary level (among others, a key reference here is Hinton and Salakhutdinov ( IV. How Might Different Fields within Artificial Intelligence Impact Innovation?Distinguishing between these three streams of AI is a critical first step towards developing a better understanding of how AI is likely to influence the innovation process going forward, since the three differ significantly in their potential to be either GPTs or IMIs-or both.First, though a significant amount of public discussion of AI focuses on the potential for AI to achieve super-human performance over a wide range of human cognitive capabilities, it is important to note that, at least so far, the significant advances in AI have not been in the form of the \"general problem solver\" approaches that were at the core of early work in symbolic systems (and that were the motivation for considerations of human reasoning such as the Turing test).Instead, recent advances in both robotics and in deep learning are by and large innovations that require a significant level of human planning and that apply to a relatively narrow domain of problem-solving (e.g., face recognition, playing Go, picking up a particular object, etc.) While it is of course possible that further breakthroughs will lead to a technology that can meaningfully mimic the nature of human subjective intelligence and emotion, the recent advances that have attracted scientific and commercial attention are well removed from these domains.13 Second, though most economic and policy analysis of AI draws out consequences from the last two decades of automation to consider the future economic impact of AI (e.g., in job displacement for an ever-increasing number of tasks), it is important to emphasize that there is a sharp difference between the advances in robotics that were a primary focus of applications of AI research during the 2000s and the potential applications of deep learning which have come to the fore over the last few years.As we suggested above, current advances in robotics are by and large associated with applications that are highly specialized and that are focused on end-user applications rather than on the innovation process itself and these advances do not seem as of yet to have translated to a more generally applicable IMI. Robotics is therefore an area where we might focus on the impact of innovation (improved performance) and diffusion (more widespread application) in terms of job displacement versus job enhancement. We see limited evidence as yet of widespread applications of robotics outside industrial automation, or of the scale of improvements in the ability to sense, react to, and manipulate the physically environment that the use of robotics outside manufacturing probably requires. But there are exceptions: developments in the capabilities of \"pick and place\" robots and rapid progress in autonomous vehicles point to the possibility for robotics to escape manufacturing and become much more broadly used.Advances in robotics may well reveal this area of AI be a GPT, as defined by the classic criteria.Some research tools/IMIs based on algorithms have transformed the nature of research in some fields, but have lacked generality. These types of algorithmic research tools, based on a static set of program instructions, are a valuable IMI, but do not appear to have wide applicability outside a specific domain and do not qualify as GPTs. For example, while far from perfect, powerful algorithms to scan brain images (so-called functional MRI imaging) have transformed our understanding of the human brain, not only through the knowledge they have generated but also by establishing an entirely new paradigm and protocol for brain research.However, despite its role as a powerful IMI, fMRI lacks the type of general-purpose applicability that has been associated with the most important GPTs. In contrast, the latest advances in deep learning have the potential to be both a general-purpose IMI and a classic GPT.The following table summarizes these ideas: Deep LearningHow might the promise of deep learning as a general-purpose IMI be realized? Deep learning promises to be an enormously powerful new tool that allows for the unstructured \"prediction\" of physical or logical events in contexts where algorithms based on a static set of program instructions (such as classic statistical methods) perform poorly. The development of this new approach to prediction enables a new approach to undertaking scientific and technical research. Rather than focusing on small well-characterized datasets or testing settings, it is now possible to proceed by identifying large pools of unstructured data which can be used to dynamically develop highly accurate predictions of technical and behavioral phenomena. In pioneering an unstructured approach to predictive drug candidate selection that brings together a vast array of previously disparate clinical and biophysical data, for example, Atomwise may fundamentally reshape the \"ideas production function\" in drug discovery.If advances in deep learning do represent the arrival of a general-purpose IMI, it is clear that there are likely to be very significant long-run economic, social, and technological consequence. First, as this new IMI diffuses across many application sectors, the resulting explosion in technological opportunities and increased productivity of R&D seem likely to generate economic growth that can eclipse any near-term impact of AI on jobs, organizations, and productivity. A more subtle implication of this point is that \"past is not prologue\": even if automation over the recent past has resulted in job displacement (e.g., Acemoglu and Restrepo, 2017a), AI is likely to have at least as important an impact through its ability to enhance the potential for \"new tasks\" (as in Acemoglu and Restrepo, 2017b).Second, the arrival of a general-purpose IMI is a sufficiently uncommon occurrence that its impact could be profound for economic growth and its broader impact on society. There have been only a handful of previous general-purpose IMIs and each of these has had an enormous impact not primarily through their direct effects (e.g., spectacles, in the case of the invention of optical lenses) but through their ability to reshape the ideas production function itself (e.g. telescopes and microscopes). It would therefore be helpful to understand the extent to which deep learning is, or will, causing researchers to significantly shift or reorient their approach in order to enhance research productivity (in the spirit of Jones ( Finally, if deep learning does indeed prove to be a general-purpose IMI, it will be important to develop institutions and a policy environment that is conductive to enhancing innovation through this approach, and to do so in a way that promotes competition and social welfare. A central concern here may be the interplay between a key input required for deep learning-large unstructured databases that provide information about physical or logical events-and the nature of competition. While the underlying algorithms for deep learning are in the public domain (and can and are being improved on rapidly), the data pools that are essential to generate predictions may be public or private, and access to them will depend on organizational boundaries, policy and institutions. Because the performance of deep learning algorithms depends critically on the training data that they are created from, it may be possible, in a particular application area, for a specific company (either an incumbent or start-up) gain a significant, persistent innovation advantage through their control over data that is independent of traditional economies of scale or demand-side network effects. This \"competition for the market\" is likely to have several consequences. First, it creates incentives for duplicative racing to establish a data advantage in particular application sectors (say, search, autonomous driving, or cytology) followed by the establishment of durable barriers to entry that may be of significant concern for competition policy. Perhaps even more importantly, this kind of behavior could result in a balkanization of data within each sector, not only reducing innovative productivity within the sector, but also reducing spillovers back to the deep learning GPT sector, and to other application sectors. This suggests that the proactive development of institutions and policies that encourage competition, data sharing, and openness is likely to be an important determinant of economic gains from the development and application of deep learning.Our discussion so far has been largely speculative, and it would be useful to know whether our claim that deep learning may be both a general-purpose IMI and a GPT, while symbolic logic and robotics are probably not, have any empirical basis. We turn in the next section to a preliminary examination of the evolution of AI as revealed by bibliometric data, with an eye towards answering this question.V. DataThis analysis draws upon two distinct datasets, one that captures a set of AI publications from Thompson Reuters Web of Science, and another that identifies a set of AI patents issued by the U.S. Patent and Trademark Office. In this section, we provide detail on the assembly of these datasets and summary statistics for variables in the sample.. As previously discussed, peer-reviewed and public-domain literature on AI points to the existence of three distinct fields within AI: robotics, learning systems and symbol systems, each comprised of numerous subfields. To track development of each of these using this data, we began by identifying the publications and patents falling into each of these three fields based on keywords. Appendix 1 lists the terms we used to define each field and identify the papers and patents belonging to it. . 2 In short, the robotics field includes approaches in which a system engages with and responds to environmental conditions; the symbolic systems field attempts to represent complex concepts through logical manipulation of symbolic representations, and the learning systems field processes data through analytical programs modeled on neurologic systems.Publication Sample and Summary Statistics", "conclusions": "Our analysis focuses on journal articles and book publications through the Web of Science from 1955 to 2015. We conducted a keyword search utilizing the keywords described in Appendix A (we tried several variants of these keywords and alternative algorithmic approaches but this did not result in a meaningful difference in the publication set). We are able to gather detailed information about each publication, including publication year, journal information, topical information, as well as author and institutional affiliations.This search yields 98,124 publications. We then code each publication into one of the three main fields of AI, as described above. Overall, relative to an initial dataset of 98,124, we are able to uniquely classify 95,840 publications as symbolic systems, learning systems, robotics, or \"general\" AI (we drop papers that involve combinations of these three fields). Table  reports the summary statistics for this sample.1AOf the 95,840 publication in the sample,   We identify organization type as academic if the organization of one of the authors on the publication is an academic institution. 81,998 publications (85.5 percent) and 13,842 (14.4 percent) are produced by academic and private sector authors, respectively. We identify publication location as US domestic if one of the authors on the publication lists the United States as his or her primary location. 22,436 publications (25 percent of the sample) are produced domestically.11,938 (12.5 percent)We also differentiate between subject matter. 44 percent of the publications are classified as Computer Science, with 56 percent classified as other applications. Summary statistics on the other applications are provided in Table . The other subjects with the largest number of publications in the sample include Telecommunications (5.5 percent), Mathematics (4. 2A", "SDG": [8]}, "assessing_bank_performance_with_operational": {"name": "Assessing Bank Performance with Operational Research and Artificial Intelligence Techniques: A Survey", "abstract": " This paper presents a comprehensive review of 179 studies which employ operational research (O.R.) and Artificial Intelligence (A.I.) techniques in the assessment of bank performance. We first discuss numerous applications of data envelopment analysis which is the most widely applied O.R. technique in the field. Then we discuss applications of other techniques such as neural networks, support vector machines, and multicriteria decision aid that have also been used in recent years, in bank failure prediction studies and the assessment of bank creditworthiness and underperformance.", "keywords": "Artificial Intelligence,Banks,Data Envelopment Analysis,Operational Research,Literature review", "introduction": "Banks play a central role in the economy. They keep the savings of the public and finance the development of business and trade. Furthermore, numerous studies argue that the efficiency of financial intermediation affects economic growth while others indicate that bank insolvencies can result in systemic crises which have adverse consequences for the economy as a whole. 1   Thus, the performance of banks has been an issue of major interest for various stakeholders such as depositors, regulators, customers, and investors. While bank performance has been traditionally evaluated on the basis of financial ratios, advances in operational research (O.R.) and artificial intelligence (A.I.) have resulted in a shift towards the use of such state-of-the-art techniques. Of course, this is not surprising, since O.R. has been extensively applied to finance during the last half century . This paper presents a comprehensive review of the use of OR and AI techniques in the assessment of bank performance.(Board et al., 2003)The rest of the paper is structured as follows. Section 2 positions the survey within the existing literature and discusses our framework. Section 3 discusses applications of data envelopment analysis (DEA) in the estimation of bank efficiency and productivity growth. Section 4 presents applications of other O.R. and A.I.techniques in the prediction of bank failure and the assessment of bank creditworthiness and underperformance. Section 5 summarizes our conclusions.", "body": "Banks play a central role in the economy. They keep the savings of the public and finance the development of business and trade. Furthermore, numerous studies argue that the efficiency of financial intermediation affects economic growth while others indicate that bank insolvencies can result in systemic crises which have adverse consequences for the economy as a whole. 1   Thus, the performance of banks has been an issue of major interest for various stakeholders such as depositors, regulators, customers, and investors. While bank performance has been traditionally evaluated on the basis of financial ratios, advances in operational research (O.R.) and artificial intelligence (A.I.) have resulted in a shift towards the use of such state-of-the-art techniques. Of course, this is not surprising, since O.R. has been extensively applied to finance during the last half century The rest of the paper is structured as follows. Section 2 positions the survey within the existing literature and discusses our framework. Section 3 discusses applications of data envelopment analysis (DEA) in the estimation of bank efficiency and productivity growth. Section 4 presents applications of other O.R. and A.I.techniques in the prediction of bank failure and the assessment of bank creditworthiness and underperformance. Section 5 summarizes our conclusions.Scopus and frameworkThere are several interesting reviews that are related to our survey. For example, Finally, techniques in bankruptcy prediction. The applications that they survey were published until 2005, and although a few of them focus on the banking sector most of the studies deal with non-financial firms.We differentiate our review from the above surveys by discussing applications of O.R. and A.I. techniques over the period 1998-2008 while focusing on bank performance. 2 We searched for papers in Scopus, which is considered to be one of the largest abstract and citation databases. We consider only journal articles and we do not include working papers, monographs, dissertations, or other publication outcomes. Furthermore, our search is limited to articles written in English. We use a combination of various keywords such as \"bank efficiency\", \"bank and data envelopment analysis\", \"bank performance\", \"bank and neural networks\", \"bank and artificial intelligence\", \"bank and operational (or operations) research\". A few additional studies were identified from cross-referencing and were manually collected.We reviewed a total of 179 studies. DEA is by far the most commonly used O.R./A.I. technique in assessing bank performance and we identified 136 studies that use DEA-like techniques to estimate various measures of bank efficiency and productivity growth, and 28 studies that provide similar estimates at the branch level. 3   We also identified 15 studies that use classification techniques such as neural networks, support vector machines, multicriteria decision aid, decision trees, nearest neighbours to predict bank failure or assess bank creditworthiness, and bank underperformance. These studies were published in a total of 67 journals, however, around 56% of them appeared in just eleven journals. As shown in Table frequent sources of publication are the European Journal of Operational Research (18) and the Journal of Banking and Finance (15), followed by Applied Financial Economics (13), Applied Economics (9), Expert Systems with Applications (9), and the Journal of Productivity Analysis (9). In the sections that follow we discuss various issues surrounding these studies, while additional information is available in Appendices I to III.[Insert Table DEA and bank-efficiencyDEA is a mathematical programming technique for the development of production frontiers and the measurement of efficiency relative to these frontiers One of the well-known advantages of DEA is that it works relatively well with small samples. Other advantages of DEA are that it does not require any assumptions to be made about the distribution of inefficiency and it does not require a particular functional form on the data in determining the most efficiency banks. However, DEA is also subject to few limitations. Two of the best known shortcomings are that DEA assumes data to be free of measurement error, and that it is sensitive to outliers. Our survey shows that recent DEA studies have examined almost all the banking sectors around the world. A few recent studies provide cross-country evidence. Most of them examine banks from the large EU banking sectors Methodological issuesEfficiency measuresMost of the studies focus on the technical efficiency of banks (e.g. However, when price data for the inputs and/or outputs are available one can also estimate cost and/or profit efficiency measures. 4 Cost efficiency is the product of technical efficiency and allocative efficiency. The latter refers to the ability of a bank to use the optimum mix of inputs given their respective prices. Consequently, cost efficiency shows the ability of a bank to provide services without wasting resources as a result of technical or allocative inefficiency. Appendix I shows that around 35 studies present measures of DEA cost efficiency (e.g. Estimations of profit efficiency with DEA are rather limited in the literature.One potential reason is the difficulty in collecting reliable and transparent information for output prices. Furthermore, the decomposition of profit efficiency into technical and allocative efficiency is not straightforward Finally, around 30% of the studies obtain estimates of total factor productivity (TFP) growth (e.g. Constant vs Variable Returns to scaleDEA can be implemented by assuming either constant returns to scale (CRS) or variable returns to scale (VRS). In their seminal study, The former relates to the ability of managers to utilize firms' given resources, while the latter refers to exploiting scale economies by operating at a point where the production frontier exhibits CRS.In most of the recent papers, DEA models are estimated using the assumption of VRS, while arguing that CRS is only appropriate when all firms are operating at an optimal scale. 6 Nevertheless, other studies argue in favour of CRS rather than VRS.For example, Hence, the assumption of VRS may be more suitable for large samples. Output-input orientationTechnical Efficiency can be estimated under either an input-oriented or outputoriented approach. 7 As In contrast, the output-oriented measures of technical efficiency address the question:\"By how much can output quantities be proportionally expanded without altering the input quantities used?\" (p. 137).By far, studies in banking obtain efficiency estimates under the input-oriented approach. 8 This is most likely due to the assumption that bank managers have higher 6 Reasons that may not allow a firm to operate at optimal scale include among others imperfect competition, government regulations, constrains on finance, etc. (Coelli et al., 2005).  7 If price data are available, then under the under cost minimization objective (i.e. cost efficiency) one obtains input-oriented measures of technical efficiency and input-mix allocate efficiency. Revenue maximization (i.e. revenue efficiency), results is output-oriented technical efficiency and output-mix allocative efficiency measures. In the case of profit maximization (profit efficiency), technical efficiency can be obtained under either the input or output-oriented assumption Selection of Inputs and OutputsThere is an on-going discussion in the banking literature regarding the proper definition of inputs and outputs. In the words of Bergendahl (1998): There have been almost as many assumptions of inputs and outputs as there have been applications of DEA\" (p. 235).Berger and Humphrey (1997) identify two main approaches for the selection of inputs and outputs. These are the \"production approach\" and the \"intermediation approach\". The first assumes that banks produce loans and deposits account services, using labour and capital as inputs, and that the number and type of transactions or documents processed measure outputs. The second approach perceives banks as financial intermediaries between savers and investors. However, there is a controversy even within this approach concerning the role of deposits Around thirty studies use interest expenses as an input without using the stock of deposits (e.g. Furthermore, around five studies use time deposits and saving deposits as input and demand deposits as output (e.g. More recently, some studies have adopted another variation of the intermediation approach. This is the so-called profit-oriented (or operating) approach which defines revenue components (e.g. interest income, non-interest income, etc.) as outputs and cost components (e.g. personnel expenses, interest expenses, etc.) as inputs. 10 Drake et al. (2006) mention that \"from the perspective of an input-oriented DEA relative efficiency analysis, the more efficient units will be better at minimizing the various costs incurred in generating the various revenue streams and, consequently, better at maximizing profits\" (p. 1451). They also argue that this approach can be more appropriate in capturing the diversity of strategic responses by financial firms in the face of dynamic changes in competitive and environmental conditions. Furthermore, As discussed before, with the exception of deposits there is a general agreement about the main categories of inputs and outputs, however, this does not 9 This discussion refers to banks as a whole and not branches. The number of applications does not match the number of studies as in several cases there are numerous models developed in each study. Furthermore, in some cases deposits or non interest expenses are not considered in the analysis. In one case necessarily imply that there is consistency with respect to the specific inputs/outputs used in various studies. For instance, the traditional inputs are fixed assets, personnel 11 , and in many cases deposits (e.g. Several studies use two outputs, usually, loans and other earning assets (e.g. Finally, recent studies include non-interest income or off-balance-sheet items as additional outputs (e.g. 3.1.5. Adjusting for the environment Under the third method, the environmental variables are included directly into the DEA problem as non-discretionary inputs (if it is believed to have a positive effect on efficiency) or outputs (if they have a negative effect on efficiency). The disadvantage of this approach is that one must know a priori the direction of the influence, a shortcoming that is also applicable in the case of the first method.Alternatively, the environmental variables can be included as non-discretionary neutral variables using an equality form. The shortcoming of this approach is that it can reduce the reference set for each firm. Recent applications of the above approaches in banking can be found in The fourth method that is discussed in Therefore, it is more suitable when the objective is to examine the correlations of efficiency with various factors (see Section 3.2.1.) rather than provide the basis for absolute comparisons across different environments.Finally, Topics of interestDeterminants of efficiencySeveral studies attempt to investigate the factors that influence the efficiency of banks. Some studies examine only bank-specific factors and others examine both bank-specific attributes and environmental determinants.Commonly found bank-specific factors are size, profitability, capitalisation, loans to assets Country-specific factors include market concentration, presence of foreign banks, ratio of private investments to GDP, fiscal deficits to GDP, GDP growth, These studies use the two-stage approach discussed above. First, they use DEA to obtain efficiency estimates. Then, in a second stage, the DEA scores are regressed on a number of explanatory variables using Tobit (e.g. Whether or not censored regression is used, the most important argument comes from However, Stock returns and efficiencyFollowing the work of Furthermore, Bank ownershipA number of studies compare the efficiency of banks across different ownership types. One approach used in the literature is to split the sample and compare the means of the different ownership groups. Another approach is to incorporate dummy variables in a second stage analysis as the one described in Section 3.2.1. Some studies compare domestic and foreign banks. Other studies examine the efficiency of state-owned banks. Many of them find that state-owned banks are less efficient than other banks. For instance, Garcia-Cestona and Surroca Corporate events and efficiencyAnother part of the literature relates DEA efficiency estimates to corporate events such as mergers and acquisitions and/or bankruptcy. Some of the studies compare the efficiency of acquirers and their targets and examine whether mergers improve efficiency. Regulatory reform/liberalization and efficiencyA number of studies examine the impact of regulatory reform and liberalization initiatives on bank efficiency and productivity. The studies that we review seem to indicate a positive relationship in most countries including India and Pakistan However, Comparison of frontier techniquesA few studies compare alternative frontier techniques over the period of our survey. Efficiency of bank branchesWhile most of the above studies focus on the efficiency of banking institutions as a whole, a related strand of the literature examines the efficiency of bank branches. 16 In general, branches are predominantly regarded as production units (see There are studies which consider the changing role of bank branches from a predominantly transaction-based one to a sales-oriented role. Two different ways are developed in empirical applications to consider service quality. For instance, The diversity of environments in which the branches are operating is also considered in the literature. Other O.R. and A.I. techniques and bank performanceIn this section, we review 15 recent studies that develop classification models used in the prediction of bank failure (9), bank underperformance (2), and credit ratings (4). MethodologiesNeural network (NN) modelling is an intelligence technique that follows a process similar to the human brain. As in other classification models, the parameters of a NN model need to be estimated before the network can be used for prediction purposes. There are numerous NN architectures, learning methods, and parameters.Generally speaking, NN architectures can be either feedback or feedforward. In a feedback network, nodes can receive inputs from nodes in any other layer in the network, whereas in a feedforward network inputs are only received from previous layers. The multi-layer preceptor (MLP-NN), competitive learning neural networks (CL-NN), self-organizing map neural networks (SOM-NN), back-propagation neural networks (BP-NN), and probabilistic neural networks (PNN) are alternative approaches employed in the studies under review (e.g. The approach of Support Vector Machines (SVMs), used in Boyacioglu et al. Nearest Neighbours is a non-parametric estimation method that has been applied in various problems in finance. The nearest neighbour rule classifies an object (i.e. bank) to the class of its nearest neighbour in the measurement space using some kind of distance measure like the local metrics, the global metrics, the Mahalanobis or the Euclidean distance. The modification of the nearest neighbour rule, the k-nearest neighbour (k-NN) method that is employed in In the case of decision trees that originate from machine learning, instead of developing classification functions or network architectures, a binary decision tree is developed. This can be accomplished through a set of if-then split conditions that lead to the accurate classification of cases (e.g. banks). Commonly used algorithms are CART (e.g. The UTilit\u00e9s Additives DIScriminantes (UTADIS) multicriteria decision aid (MCDA) method, used in The Multi-Group Hierarchical Discrimination (MHDIS) method, used in As discussed in Multiple discriminant analysis (MDA) and logistic regression analysis (LRA) are two traditional techniques that are commonly used as benchmarks in classification studies (e.g. VariablesIn most of the cases, the variables are selected on the basis of the CAMEL model and include measures for Capital strength, Asset quality, Earnings, and Liquidity. 19 The number of variables differs significantly among studies. Some researchers start from a large list of variables and then use statistical screening (e.g. Kruskal-Wallis) or dimension reduction (e.g. factor analysis) to end up with a reduced set of variables. In addition to the financial variables, some studies include additional characteristics related to ownership (e.g. % of shares held by the government), market information (e.g. stock price in previous year), auditing, and country (e.g. Heritage index).Classification resultsMany of the studies in Appendix III report quite satisfactory classification accuracies, and in several cases, the proposed O.R. and A.I. methods outperform the traditional techniques (i.e. MDA, LRA, Cox regression). Furthermore, the studies that focus on bankruptcy prediction, in general, perform more accurately than those that deal with credit ratings. However, this is not surprising for two reasons. First, while classifying the banks in more groups is more informative, it becomes more difficult to discriminate among groups in the intermediate categories. Second, credit ratings may incorporate additional qualitative information and human judgment that cannot be captured by the conventional variables used in most studies.In any case, comparisons across different studies should be treated with extreme caution. One reason is that different studies make different assumptions about the prior probabilities of group membership as well as misclassification costs. A second reason is that different studies use different approaches to validate the developed models. For instance, some researchers simply split the total sample in training and holdout datasets without accounting for the stability of models over time; others use resampling techniques (e.g. k-fold cross-validation), and others use a holdout sample from a future period. However, it should kept in mind that classification ability is likely to overstate predictive ability. Thus, a superior approach would require that the model be validated against a future period, as this approach more closely reflects a \"real world\" setting.Concluding remarks", "conclusions": "We have presented a comprehensive review of applications of operational research and artificial intelligence techniques in the assessment of bank performance, by discussing a total of 179 studies published between 1998 and 2008. We have classified the studies in two main categories.The first uses DEA and the DEA-like Malmquist index to estimate the efficiency and productivity growth of banks and bank branches. We have discussed various methodological issues such as the estimated measures of efficiency, the underlying assumptions of the estimated models, and the selection of inputs and outputs. We also discussed the main topics of interest including the relationship between ownership and efficiency, stock returns and efficiency, the determinants of efficiency, the efficiency of bank branches, amongst others. Three of the main conclusions are that: (i) profit efficiency and capacity efficiency have received quite limited attention in DEA studies in banking (ii) most studies that use a two-stage DEA do not employ appropriate bootstrapping techniques, and their results may be biased (iii) there is much diversity among studies with respect to the selection of input and outputs. As in the case banks as a whole, cost and profit efficiency has received considerably less attention in branch efficiency studies. Furthermore, an area of research deserving attention would be the estimation of bank branches efficiency over successive time periods.Studies falling into the second category, attempt to develop classification models to predict failure, the credit ratings of banks, and identify underperformers.We have found both country-specific and cross-country studies and applications of techniques that originate from various disciplines. Most of the studies rely heavily on financial information although in some cases non-financial variables are also used.Given the differences in the approaches employed to validate the models, comparisons of classification accuracies across studies should be treated with caution. We find only a few studies that propose the combination of the predictions of individual models into integrated meta-classifiers, and we believe that this is an area of research that is worthy of further attention.Bergendahl G., (1998), DEA and benchmarks -an application to Nordic banks, Annals of Operations , Evaluating the performance of Swedish savings banks according to service efficiency, European Journal of Operational Research, 82, 233-249. Bergendahl G., Lindblom T., (2008)Research, 185, 1663-1673. Berger A.N., (2007 Boyacioglu M.A., Kara Y., Baykan OmK., ), International Comparisons of Banking Efficiency, Financial Markets, Institutions & Instruments, 16, 119-144. Berger A.N., Humphrey D.B., (1997), Predicting bank financial failures using neural networks, support vector machines and multivariate statistical methods: A comparative analysis in the sample of savings deposit insurance fund (SDIF) transferred banks in Turkey, Expert Systems with Applications, doi:10.1016/j.eswa. (2008) Brissimis S.N., Delis M.D., Papanikolaou N.I., 2008.01.003, Exploring the nexus between banking sector reform and performance: Evidence from newly acceded EU countries, Journal of (2008)Banking and Finance, 32, 2674-2683 Camanho A.S., Dyson R.G., . Camanho A.S., Dyson R.G., (1999), A generalisation of the Farrell cost efficiency measure applicable to non-fully competitive settings, (2008), Financial liberalization and efficiency in Tunisian banking industry: DEA test, International Journal of Information Omega, 36, 147-162. Cook, W.D., Hababou, M., Liang, L., (2005), Multicomponent efficiency measurement and shared inputs in Data Envelopment Analysis: an application to sales and service performance in bank branches, Journal of Productivity Technology & Decision Making, 4, 3, 455-475. Cook, W.D., Hababou, M., Tuenter, H.J.H, (2000), The effects of shared ATM networks on the efficiency of Turkish banks, Applied Analysis, 14, 209-224. Damar H.E., (2006), Financial deregulation and efficiency: An empirical analyis of Indian banks during the post reform period, Review of Financial Economics, 38, 683-697. Das A., Ghosh S., (2006) Zhao H., Sinha A.P, Ge W., Economics, 15, 193-221. Das, A., Ray, S.C., Nag, A., (2007), Effects of feature construction on classification performance: An empirical study in bank failure prediction, Expert Systems with Applications, doi:10.1016/j.eswa.2008.01.053 (2008), Deregulation and productivity growth: a study of the Indian commercial banking industry, International Journal of Business Performance Management, 10, 4, 318-343. Zhou p., Ang B.W., Poh K.L., Zhao, T., Casu, B., Ferrari, A., (2008), A survey of data envelopment analysis in energy and environmental studies, European Journal of Operational Research, 189, 1-18.   (2008)", "SDG": [9]}, "daily_water_level_forecasting_using_wavelet_decomposition_and_artificial_intelligence_techniques": {"name": "Daily Water Level Forecasting Using Wavelet Decomposition and Artificial Intelligence Techniques", "abstract": " Reliable water level forecasting for reservoir inflow is essential for reservoir operation. The objective of this paper is to develop and apply two hybrid models for daily water level forecasting and investigate their accuracy. These two hybrid models are wavelet-based artificial neural network (WANN) and wavelet-based adaptive neuro-fuzzy inference system (WANFIS). Wavelet decomposition is employed to decompose an input time series into approximation and detail components. The decomposed time series are used as inputs to artificial neural networks (ANN) and adaptive neuro-fuzzy inference system (ANFIS) for WANN and WANFIS models, respectively. Based on statistical performance indexes, the WANN and WANFIS models are found to produce better efficiency than the ANN and ANFIS models. WANFIS7-sym10 yields the best performance among all other models. It is found that wavelet decomposition improves the accuracy of ANN and ANFIS. This study evaluates the accuracy of the WANN and WANFIS models for different mother wavelets, including Daubechies, Symmlet and Coiflet wavelets. It is found that the model performance is dependent on input sets and mother wavelets, and the wavelet decomposition using mother wavelet, db10, can further improve the efficiency of ANN and ANFIS models. Results obtained from this study indicate that the conjunction of wavelet decomposition and artificial intelligence models can be a useful tool for accurate forecasting daily water level and can yield better efficiency than the conventional forecasting models.", "keywords": "Water level forecasting,wavelet decomposition,artificial neural network,adaptive neuro-fuzzy inference system", "introduction": "The temporal and spatial variability of precipitation is very high in South Korea. About 70% of annual precipitation occurs in the summer season between June and September . In addition to precipitation characteristics, overpopulation, urbanization, industrialization and change of farming practices further complicate operational hydrology.(Bae et al., 2007)The control and management of reservoir systems are essential in South Korea, since they play an important role in water supply and flood prevention. Accurate forecasting of reservoir inflow is critical for enhancing reservoir operation, water supply, flood prevention, hydropower generation, water resources management and decision support system.Conventionally, reservoir inflow forecasting has been done using statistical models based on time series analysis, including AR (autoregressive), ARMA (autoregressive moving average), ARIMA (autoregressive integrated moving average), FGN (fractional Gaussian noise), BL (broken line), TF (transfer function), TFN (transfer function noise) and ARMAX (autoregressive moving average with exogenous terms). Since most current models can be classified as linear models, they have limited capability to forecast the inflow pattern that is highly nonlinear and non-stationary.Over the past years, artificial intelligence (AI) techniques have been successfully developed for modeling non-linear hydrologic systems. In particular, artificial neural networks (ANNs) and adaptive neuro-fuzzy inference system (ANFIS) have been accepted as effective tools for modeling complex hydrologic systems (Bae et al., 2007;Cheng et al., 2005b;Coulibaly et al., 2000;El-Shafie et al., 2007;Figueiredo et al., 2007;Jain et al., 1999;Jeong and Kim, 2005;Jothiprakash and Magar, 2012;Karimi-Googhari and Lee, 2011;Kim et al., 2013b;Othman and Naseri, 2011;Razavi and Araghinejad, 2009;Seo et al., 2013aSeo et al., , 2013bSeo et al., , 2013c;;.Wu et al., 2009)ANNs are parallel computational models that resemble biological neural network and have better generalization capabilities. ANFIS, on the other hand, combines the advantages of both ANN and fuzzy inference system  and is becoming more popular in hydrological applications. It is reported to perform better than ANNs for flood forecasting and long-term discharge prediction (Okkan, 2012)(Chau et al., 2005;Cheng et al., 2005a. Although ANN and ANFIS have been extensively used for prediction of hydrological variables, they have also some problems when dealing with non-stationary data.Cheng et al., , 2005b))Since hydrological time series includes several frequency components and have nonlinear relationships, hybrid model approaches have been used to improve the performance of models forecasting . These approaches include chaotic neural networks (Okkan, 2012), neural networks based on set pair analysis (SPA) and principle component analysis (PCA) (Karunasinghe and Liong, 2006)(Wang et al., 2006a;Wang et al., 2006b;, threshold neural networks Wu et al., 2009), cluster-based hybrid neural networks (Wang et al., 2006b), and bootstrapped artificial neural networks (Cigizoglu and Kisi, 2005)(Han et al., 2007;Jeong and Kim, 2005;Jia and Culver, 2006;Kim et al., 2013a;Seo et al., 2013aSeo et al., , 2013c;;Sharma and Tiwari, 2009;Srivastav et al., 2007;Tibshirani, 1996;Tiwari andChatterjee, 2010a, 2010b;Twomey and Smith, 1998;.Zio, 2006)In the last years, the conjunction of wavelet transform and AI techniques has been successfully implemented in hydrological applications (Abiyev, 2011;Adamowski and Chan, 2011;Adamowski and Prasher, 2012;Adamowski and Sun, 2010;Anctil and Tape, 2004;Belayneh and Adamowski, 2012;Cannas et al., 2006;Khanghah et al., 2012;Kisi, 2008Kisi, , 2011;;Kisi et al., 2011;Nejad and Nourani, 2012;Nourani et al., 2012;Okkan, 2012;Okkan and Serbes, 2013;Rajaee, 2010;Rajaee et al., 2011;Tiwari and Chatterjee, 2010b;Wang and Ding, 2003;Wang et al., 2009;. The wavelet transform is another datapreprocessing technique which can analyze a signal in both time and frequency so that it can overcome the drawbacks of conventional Fourier transform. The wavelet transform provides an effective decomposition of time series so that the decomposed data can increase the performance of hydrological forecasting models by capturing the useful information on different resolution levels Wei et al., 2012)(Nourani et al., 2009. (Nourani et al., , 2011)) proposed a method based on coupling discrete wavelet transforms and ANN for streamflow forecasting for non-perennial rivers in semi-arid watersheds. The performance of the coupled wavelet-neural network models (WA-ANN) was compared with the ANN models for streamflow forecasting. They found that the WA-ANN models provided more accurate streamflow forecasting than the ANN models. Adamowski and Chan (2011) proposed a method coupling the discrete wavelet transform and ANN for monthly groundwater level forecasting. Comparing the proposed coupled wavelet-neural network models (WA-ANN) with ANN and ARIMA models for groundwater level forecasting, they found that the WA-ANN models provided more accurate average groundwater level forecasts than the ANN and ARIMA models. Adamowski and Sun (2010) proposed a simple wavelet regression (WR) approach for modeling daily reference evapotranspiration. The accuracy of the WR models was compared with that of the single linear regression (LR) and the empirical models including CIMIS Penman, Hargreaves, Ritchie and Turc. Comparison of these models showed that the WR models performed better than the LR and empirical models for modeling daily reference evapotranspiration. Kisi (2011) proposed a new method combining wavelet and gene expression programming (WGEP) methods for forecasting long-term air temperature. This method combines the discrete wavelet and genetic programming methods. Comparing the accuracy of single GEP and WGEP models, they found that the WGEP model performed much better than the single GEP model. They also found that the WGEP model significantly increased the accuracy of single GEP model especially for forecasting long-term air temperatures.Kisi et al. (2011)Adamowski and Prasher (2012) compared support vector regression (SVR) and wavelet networks (WN) for daily runoff forecasting in a mountainous watershed. They found that the best WN model performed slightly better than the best SVR model.  applied wavelet-based global soft thresholding method to denoise daily time series of streamflow. The denoised time series was imposed on an ANN model to forecast streamflow. They found that the results of the ANN model for streamflow forecasting could be improved by 11% when the wavelet-based denosing approach, as a preprocessing method, was applied to the data. Nejad and Nourani (2012) developed a hybrid model using discrete wavelet transform (DWT) and feed forward neural networks (FFNNs) for monthly runoff prediction. It was found that the hybrid models successfully predicted monthly runoff series and gave good prediction performance as compared with conventional methods, including FFNN, multiple linear regression (MLR), wavelet-MLR combined model, and PCA-based neural networks. Okkan (2012) developed different hybrid models combining DWT and different black box techniques, including multiple linear regression (MLR), feed forward neural networks (FFNN), and least square-support vector machines (LS-SVM) for reservoir inflow modeling using weather data. They found that the DWT-FFNN model performed better than other models in terms of mean square errors (MSE) and determination coefficients (R 2 ). They also found that DWT increased the accuracy of MLR and LS-SVM.Okkan and Serbes (2013)This study develops and applies two different hybrid models, wavelet-based artificial neural network (WANN) and wavelet-based adaptive neuro-fuzzy inference system (WANFIS), for daily water level forecasting in the Andong dam watershed, South Korea. The statistical performance measures are employed to evaluate the developed models. The performance of WANN and WANFIS models is compared with that of ANN and ANFIS models, respectively. The effect of different mother wavelets such as Daubechies (db1,db2,db4,db6,db8 and db10),Symmlet (sym2,sym4, and Coiflet (coif6, coif12 and coif18) wavelets on accuracy of the WANN and WANFIS models is also investigated in this study.sym8 and sym10)", "body": "The temporal and spatial variability of precipitation is very high in South Korea. About 70% of annual precipitation occurs in the summer season between June and September The control and management of reservoir systems are essential in South Korea, since they play an important role in water supply and flood prevention. Accurate forecasting of reservoir inflow is critical for enhancing reservoir operation, water supply, flood prevention, hydropower generation, water resources management and decision support system.Conventionally, reservoir inflow forecasting has been done using statistical models based on time series analysis, including AR (autoregressive), ARMA (autoregressive moving average), ARIMA (autoregressive integrated moving average), FGN (fractional Gaussian noise), BL (broken line), TF (transfer function), TFN (transfer function noise) and ARMAX (autoregressive moving average with exogenous terms). Since most current models can be classified as linear models, they have limited capability to forecast the inflow pattern that is highly nonlinear and non-stationary.Over the past years, artificial intelligence (AI) techniques have been successfully developed for modeling non-linear hydrologic systems. In particular, artificial neural networks (ANNs) and adaptive neuro-fuzzy inference system (ANFIS) have been accepted as effective tools for modeling complex hydrologic systems ANNs are parallel computational models that resemble biological neural network and have better generalization capabilities. ANFIS, on the other hand, combines the advantages of both ANN and fuzzy inference system Since hydrological time series includes several frequency components and have nonlinear relationships, hybrid model approaches have been used to improve the performance of models forecasting In the last years, the conjunction of wavelet transform and AI techniques has been successfully implemented in hydrological applications Adamowski and Prasher (2012) compared support vector regression (SVR) and wavelet networks (WN) for daily runoff forecasting in a mountainous watershed. They found that the best WN model performed slightly better than the best SVR model. This study develops and applies two different hybrid models, wavelet-based artificial neural network (WANN) and wavelet-based adaptive neuro-fuzzy inference system (WANFIS), for daily water level forecasting in the Andong dam watershed, South Korea. The statistical performance measures are employed to evaluate the developed models. The performance of WANN and WANFIS models is compared with that of ANN and ANFIS models, respectively. The effect of different mother wavelets such as MethodologyWavelet decompositionWavelet analysis is a multi-resolution analysis in time and frequency domains. The wavelet transform decomposes a time series signal into different resolutions by controlling scaling and shifting. It provides a good localization properties in both time and frequency domains x t is defined as where s = the scale parameter, \u03c4 = the translation parameter, * = the complex conjugate, and ( ) t \u03a8 = mother wavelet. CWT necessitates a large amount of computation time and resources, while DWT requires less computation time and is simpler to implement than CWT.DWT involves choosing scales and positions, which are called dyadic scales and positions, based on powers of two. This is achieved by modifying the wavelet representation as where j and k = integers that control the wavelet dilation and translation, respectively. 0 1 s > is a fixed dilation step, and 0 \u03c4 = the location parameter. The most common and simplest choice for parameters are 0 2 s = and 0 1 \u03c4 = (Nourani et al., 2009). Using the wavelet discretization, the time-scale space can be sampled at discrete levels.A fast DWT algorithm developed by These filters used in Mallat's algorithm are determined according to the selection of mother wavelets signal. An approximation holds the general trend of the original signal, while a detail depicts high-frequency components of it. A multilevel decomposition process (Figure There are many types of wavelets, including Daubechies, Symmlet, Coiflet, Meyer, Gaussian, Mexican hat, Morlet, and Shannon wavelets, and so on, which can be used for wavelet-based time series analysis The least-asymmetric wavelets are also known as Symmlets. The Symmlets are proposed to improve symmetry as a modification to Daubechies wavelets. The Symmlets are compact supported, orthogonal, continuous, but only nearly symmetric mother wavelets. Their construction is very similar to the construction of Daubechies wavelets, but the symmetry of Symmlets is stronger than that of Daubechies wavelets. Coifman wavelets, which also called Coiflets, are discrete wavelets which are compactly supported wavelets and designed to be more symmetrical than Daubechies wavelets Artificial neural networkANN is parallel computing systems that were developed originally, based on the structure and functional aspects of biological neural networks. A feed-forward ANN comprises a system of units, analogous to neurons, that are arranged in layers The simplest MLP consists of an input layer with n covariates and an output layer with one output neuron. It calculates the function where 0is the vector consisting of all synaptic weights without the intercept, and 1 ( , , )is the vector of all covariates.Hidden layers can be included to increase the model flexibility. However, MLP with a hidden layer and J hidden neurons calculates the following function where 0 w = the intercept of the output neuron, 0 j w = the intercept of the j th hidden neuron, j w = the synaptic weight corresponding to the synapse starting at the j th hidden neuron and leading to the output neuron, 1 ( , , )is the vector of all synaptic weights corresponding to the synapses leading to the jth hidden neuron, and 1 ( , , )is the vector of all covariants.All hidden neurons and output neurons calculate an output of 0 1 ( ( , , , ))from the outputs of all preceding neurons, 0 1 , , , k z z z , where g = the integration function, f = the activation function, and the neuron 0 1 z \u2261 is the intercept.The integration function is often defined asThe activation function is usually a bounded non-decreasing nonlinear and differentiable function, including the logistic function or the hyperbolic tangent The training of a neural network model is a process of adjusting the connection weights and biases so that its output can match the desired output best. Specifically, at each setting of the connection weights, it is possible to calculate the error committed by the networks simply by taking the difference between the desired and actual responses Adaptive neuro-fuzzy inference systemANFIS is a combination of an adaptive neural network and a fuzzy inference system (FIS). Parameters of FIS are determined by neural network learning algorithms. Since this system is based on FIS, reflecting vague knowledge, an important aspect is that the system should always be interpretable in terms of fuzzy IF-THEN rules. ANFIS can approximate any real continuous function on a compact set to any degree of accuracy ANFIS identifies a set of parameters through a hybrid learning rule combining the backpropagation gradient descent method and least-square method. There are two approaches for FIS, namely the approaches of Mamdani The procedure described by The output of the ith node in layer 1 is denoted as 1,i O . Every node i in layer 1 is an adaptive node, with ( ), where x (or y) is the input to node i, and i A (or 2 i B \u2212 ) is a linguistic label (e.g., LOW or HIGH) associated with this node. The Gaussian MFs for A and B can be written as:where [ ] , c \u03c3 = the parameter set. Any continuous and piecewise differential functions, such as commonly used triangular-shaped or bell-shaped MFs, are also qualified candidates for node function in layer 1 Layer 2 consists of nodes labeled \u03a0 , that multiply the incoming signals and sends the product out. The output of layer 2 comprises the membership values of the premise part and can be written as:The nodes labeled N calculate the ratio of the ith rule's firing strength to the sum of all rules' firing strengths in layer 3. Each node output represents the firing strength of a rule and can be written as:where i w is the output of layer 3. The outputs of layer 3 are called normalized firing strengths. The nodes of layer 4 are adaptive with node functions and can be written as:where [ ] , ,p q r is the parameter set. Parameters of layer 4 are referred to as consequent parameters. The single fixed node of layer 5 labeled \u03a3 computes the final output as the summation of all incoming signals that can be written as:ANFIS is trained using a hybrid learning algorithm, combining the least-square method and the backpropagation gradient descent method, to adjust the premise and consequent parameters. In forward pass, the least-square method is used to identify consequent parameters. In backward pass, the gradient descent method is used to propagate the errors backward and adjust premise parameters. Detailed information for ANFIS can be found in WANN and WANFISWANN Step 1. Multilevel wavelet analysis using DWT decomposes a signal into details (D1, D2, \u2022\u2022\u2022, Dk) and approximation (Ak), where k is decomposition level.Step 2. ANN and ANFIS are trained and tested using the details and approximation as input and the model performance is evaluated.Figure Performance evaluationThe performance of water level forecasting models (ANN, ANFIS, WANN, and WANFIS) was evaluated using seven performance indexes including the coefficient of efficiency (CE), the index of agreement (d), the coefficient of determination (r 2 ), the root mean squared error (RMSE), the mean absolute error (MAE), the mean squared relative error (MSRE), and the mean higher order error (MS4E). These performance indexes (CE, d, r 2 , RMSE, MAE, MSRE and MS4E) can be written as Case StudyThe Andong dam watershed (Figure Application and ResultsIn this study, we used various computer programs such as MATLAB AnalysisAppropriate input variables must be selected in advance for developing and applying ANN, ANFIS, WANN, and WANFIS models for forecasting daily water level. In this study, since daily water level data from two gauging stations are strongly auto-correlated and the influence of rainfall is relatively marginal, simple models with only the daily water level as inputs were selected. In addition to the current values of daily water level, daily water level values at various lag times were also considered. This study used a statistical approach suggested by To obtain the appropriate set of input variables, several models were trained with different set of input variables. Table The input data were decomposed by DWT to develop and apply WANN and WANFIS models. The optimal decomposition level must be selected in advance to determine the performance of the model in the wavelet domain. where L = the decomposition level, N = the number of time series data, and int[\u2022] = the integer-part function. In this study, three decomposition levels were obtained. Thus, input times series were decomposed using different mother wavelets, and sub-time series of 2-day mode (D1), 4-day mode (D2), 8-day mode (D3) and approximation mode (A3) for each input set were obtained for the training and testing periods. In case of Set 2 ( This study also aims at examining the effects of different mother wavelets on the efficiency of developed models. For this purpose, the performance of applied models were investigated for different mother wavelets, including Haar wavelet (db1), Daubechies-2 (db2), Daubechies-4 (db4), Daubechies-6 (db6), Daubechies-8 (db8), Daubechies-10 (db10), Symmlet-2 (sym2), Symmlet-4 (sym4), Symmlet-8 (sym8), Symmlet-10 (sym10), Coiflet-6 (coif6), Coiflet-12 (coif12), and Coiflet-18 (coif18). For discrete wavelet analysis, Daubechies wavelets have been commonly used mother wavelets, and Symmlets and Coiflets have been also applied in hydrologic wavelet-based studies Daubechies, Symmlet and Coiflet wavelets provide compact support Figure The selection of effective wavelet components is important for the performance of developed models. Previous studies selected effective wavelet components using the correlation coefficients between wavelet components and observed values To construct new input time series from the wavelet components, several methods have been used, including summing the effective components MLP neural network model was used, which has an input layer, an output layer and a hidden layer between the input and output layers, to forecast daily water level using ANN and WANN models. The number of nodes in the hidden layer was determined using a trial-anderror approach following the previous studies The ability of ANFIS model to achieve the performance goal depends on the predefined internal ANFIS parameters, including the number and shape of MFs and the step size Based on the modeling strategies, a total of 196 models was developed in this study.Seven ANN and ANFIS models were developed by using seven input sets (Table WANN and WANFIS models were developed by using the combination of 13 mother wavelets and seven input sets (Table Evaluation of model performanceThe performance indexes used in this study for evaluation of the applied models' Figure Models with higher values for CE, d and r 2 have higher ranking, while models with higher values for RMSE, MAE, MSRE and MS4E have lower ranking. Overall rankings were determined by summing rankings for performance indexes and sorting the added ranking values. The numbers (Figure Model performance for different mother wavelets from each input set was also evaluated.Figure It can be seen from figure for Set 3; db8, db10 and sym8 for Set 4; db8, db10 and sym10 for Set 5; db8, db10 and coif18 for Set 6; db10, sym10 and coif18 for Set 7. In particular, WANFIS models with db10 yielded the highest efficiency for Set 2, Set 4 and Set 6, and the second highest efficiency for Set 3, Set 5 and Set 7. This indicates that wavelet decomposition using mother wavelet, db10, can further improve the efficiency of ANFIS models as compared with the other mother wavelets.Table Figure shows the water level hydrographs of total testing period, whereas each lower figure shows the zoom part of the rectangle box for the upper figure. It is clear from the figures that the estimates of the WANN and WANFIS models are closer to the corresponding observed water level values than those of the ANN and ANFIS models. From all these graphs, it can be said that the wavelet decomposition can significantly improve the efficiency of the ANN and ANFIS models in forecasting daily water levels. It was also observed from the scatter plots that, for WANFIS models, two straight lines were almost consistent and standard deviations were lower compared with other models. Especially, the WANFIS models produced better forecasting for high water level values, while other models were slightly overestimated or underestimated. This indicates that the WANFIS models are more accurate than the other models.Conclusions", "conclusions": "This paper investigates the accuracy of two different hybrid models, wavelet-based artificial neural networks and wavelet-based neuro-fuzzy inference system, for forecasting daily water level in the Andong dam watershed, South Korea. The specific objectives are to develop and evaluate the methods for improving daily water level forecasting, comparing with the single ANN and ANFIS models, and evaluating the performance of these models based on various performance indexes, including the coefficient of efficiency (CE), the index of agreement (d), the coefficient of determination (r 2 ), the root mean squared error (RMSE), the mean absolute error (MAE), the mean squared relative error (MSRE), and the mean higher order error (MS4E).The WANN and WANFIS models perform better results than the ANN and ANFIS models, respectively.       Discrete Wavelet Decomposition )   -WANN and WANFIS models produce better efficiency than ANN and ANFIS models.(db1, db2, db4, db6, db8, db10, sym2, sym4, sym8, sym10, coif6, coif12, coif18", "SDG": [9]}, "electricity_demand_estimation_using_an_adaptive_neuro_fuzzy_network_a_case_study_from_the_ontario_province_e_canada": {"name": "Electricity demand estimation using an adaptive neuro-fuzzy network: A case study from the Ontario province e Canada", "abstract": " Electricity is an important asset that influences not only the economy, but political or social security of a country. Reliable and accurate planning and prediction of electricity demand for a country are therefore vital. In this paper, electricity demand in Ontario province of Canada from the year 1976e2005 is modeled by using an (adaptive neuro fuzzy inference system) ANFIS. A neuro fuzzy structure can be defined as an ANN (artificial neural network) which is trained by experimental data to find the parameters of (fuzzy inference system) FIS. Inputs for the model include number of employment, (gross domestic product) GDP, population, dwelling count and two meteorological parameters related to annual weather temperature. The data were collected and screened using statistical methods. Then, based on the data, a neuro-fuzzy model for the electricity demand is built. It was found that electricity demand is most sensitive to employment.", "keywords": "Electricity,demand,Neuro-fuzzy,Forecasting", "introduction": "Several investigations have been carried out to find important parameters affecting electricity demand and also the interaction between these parameters. Designed and calculated model will help us to manage energy consumption and distribution efficiently. Most studies have focused on the relationship between electricity demand and economical parameters such as (gross domestic product) GDP, (gross national product) GNP, national income, and the rate of employment as well as unemployment. Sari and Soytas  studied the relationship between different sources of electricity consumption, employment and national income growth in Turkey. Narayan and Smyth [1] carried out the same study in Australia. They evaluated both long and short term relationship between electricity consumption, employment and real income. Relationships between GDP and electricity consumption in ten newly industrialized Asian countries were estimated by Chen et al. [2]. They studied long run relationship in China, Hong Kong, India, Indonesia, Korea, Malaysia, Philippines, Singapore, Taiwan and Thailand. In another attempt, German Institute for Economic Research (DIW) was commissioned by \"German Advisory Group on Economic Reform in Ukraine\" in 1998 to predict electricity demand in Ukraine until the year 2010. A comparison of the relationship between renewable and non-renewable electricity consumption and real GDP in the US using annual data from 1949 to 2006 was done by Payne [3]. Bowden and Payne [4] used these data in 2008 to check the causal relationship between electricity consumption and real GDP. Studying the time series properties of electricity consumption of G-7 countries was the subject of Soytas and Sari [5]. In Pakistan, Aqeel and Butt [6] found out that economic growth affects the total electricity consumption. They also discovered that economic growth leads to growth in petroleum consumption but however electricity consumption leads to economic growth without feedback. De Vita et al. [7] found the same results for Namibia. Their research for the period between the year 1980e2002 showed that electricity consumption respond positively to changes in GDP and negatively to changes in electricity price and air temperature. Hainoun et al. [8] found that both electricity and electricity demand growth rates are lower than the corresponding GDP growth rates in Syria. In some literature, other parameters which are not economical are also selected. For example Valor et al. [9] tried to analyze the relationship between electricity load and daily air temperature in Spain. More recently many studies have been conducted on short/long term electricity demand/load forecasting [11e25], but application of neuro fuzzy logic for forecasting electricity demand is still unexplored. In this paper, an ANFIS network (adaptive neuro fuzzy inference system) was designed to map six parameters as input data (i.e. employment, GDP, dwelling, population, HDD and CDD) to electricity demand as output variable.[10]", "body": "Several investigations have been carried out to find important parameters affecting electricity demand and also the interaction between these parameters. Designed and calculated model will help us to manage energy consumption and distribution efficiently. Most studies have focused on the relationship between electricity demand and economical parameters such as (gross domestic product) GDP, (gross national product) GNP, national income, and the rate of employment as well as unemployment. Sari and Soytas Problem statementAll of the referred literature have applied classical methods and statistical techniques to predict electricity demand, but this paper employs a new approach (neuro-fuzzy) in order to achieve a model which can present more accurate prediction. Some researchers like Abraham and Nath In this paper, a neuro fuzzy network has been designed to model the electricity demand of Ontario province in Canada based on six input variables. These variables are employment, GDP, population, dwelling and two other variables to indicate how hot or cold the weather is. All these data were collected from CANSIM II, Statistics Canada's key socio-economic database, and Environment Canada (2005), with statistical analysis. Electricity demand data from 1976 to 2004 is obtained from IESO (Independent Electricity System Operators).In the literature review, the neuro-fuzzy modeling is described. The section is divided into three sub-sections i.e. description of ANN, fuzzy logic inference systems, and adaptive neuro fuzzy inference system description. In research methodology section, Ontario electricity demand forecasting is presented followed by the designed and validation of ANFIS model in the next section. In the last section, the sensitivity analysis of the designed network is evaluated.Neuro fuzzy system: ANFISFuzzy logic is a system that can be applied to transform linguistic concepts to mathematical and computational structure for many purposes. But fuzzy systems do not have good ability to learn and adapt to changing the conditions ANFIS applies a combination of error back propagation algorithm and least squares method as a hybrid algorithm to adjust the membership functions of a fuzzy logic system optimally There are two fuzzy style inferences which are mostly used, Mamdani-style inference and Sugeno-style inference. Mamdanistyle is based on Lotfi Zadeh's 1973 paper In Fig. The first layer of this structure contains nodes which generate output of each membership functions for inputs.a i \u00bc m Bj \u00f0y\u00de i \u00bc 5; 6 for j \u00bc 1 and 7; 8 for j \u00bc 2 Nodes labeled N, in third layer calculates the ratio of a rule firing strength to the sum of all rules firing strength:After calculating this ratio, the outputs of fourth layer are obtained by equation ( Using a Sugeno fuzzy modeling style in this network, z i is calculated by equation ( The last single node in the fifth layer, computes overall output as summation of all incoming signals, which is expressed as below:Research methodologyIn this paper, an ANFIS has been employed to estimate electricity demand in Ontario, Canada. A statistical study has been carried out on available data to find the affecting factors and prepare data for model building.Statistical preprocessingAvailable dataSeven parameters in the period from 1976 to 2011 were available. These data included the number of employment, GDP, population, dwelling count, (degree days) DD, 1 number of new housing and bank of Canada interest rate. The first five parameters are obtained from CANSIM II, Statistics Canada's key Socio-economic database, and Environment Canada and two others are obtained from Ref. Data analysis using Pearson coefficientFrancis Galton in the 1880s introduced a factor to show measure of correlation or linear dependence between two variables X and Y where X 0 and Y 0 are average of variables X and Y respectively. Pearson correlation factor is widely used in sciences as a measure of strength of linear dependence between two variables. Measure of this coefficient between each seven parameters and electricity demand was calculated. The results are listed in Table It is obviously that employment; GDP, population and dwelling count have high correlation with electricity demand. But the remaining parameters do not have good correlation to be selected as an independent variable for model building.All these four parameters are social or economic and none of them are climatic variables. Environmental conditions that affect electricity demand is described by Rodgers where Tb 1 is the base temperature, which was set as 10 C Tb 2 is another base temperature which was chosen to be 20 C. Td is the mean temperature of a given day and d is the number of days in the year (365 or 366). In this study, summation of HDDd during a year (HDD) and summation of CDDd during a year (CDD) were selected as new parameters for model development. Measure of correlation between these new variables and electricity demand was calculated. The results are listed in Table ANFIS model resultsDesigned network structureAs mentioned in the previous section, six parameters that had the most important effects on the actual electricity demand were selected for input variables (i.e. employment, GDP, dwelling, population, HDD and CDD). Based on these data, an ANFIS network with Sugeno-style inference system has been designed which maps six independent variables as input data to electricity demand as output. MATLAB 7.6 was employed for model building. Three Gaussian membership functions have been considered for each input data. Fig. From 36 available data sets, 30 sets were selected to train the network. Six remaining data sets were applied to validate the trained network. The procedure ensures that the designed network produces good results for any range of data.After training the network, a (mean square error) MSE of 8.9251 \u00c2 10 \u00c013 was obtained for training the data. The low training error enabled the trained network to estimate unseen data with high precision. The best obtained network MSE is 0.0016 for test data.Model validationCensuses in Canada are conducted in a five-yearly interval. The latest census run by IESO provides exact data for employment, GDP, population and dwelling count. In order to build a forecasting model to the year 2015, a linear trend was assumed (Table These equations can be used to find the average values for the next 15 years period from 2012 to 2026. Having the values of all input parameters for the year 2006 the electricity demand for the year can be predicted. The calculated value is 165.46 TWh where the actual data reported by IESO, is 151 TWh. It means a good validation for the model and confirms its power to predict electricity demand in future years.Prediction of electricity demand until 2015After validating the model, it can be used to forecast the electricity demand in future. Linear lines were fitted to the data based on the linear trend of the data. Table By using these equations, it is possible to predict the values of these parameters in the future (in this paper 2012e2015). The trend of change for HDD and CDD is not liner. In this case, the average values for every 5 years have been calculated to find out whether they have a special trend. To compare the annual changes of parameters, changes of these parameters needs to be performed step by step.In this case, using the fitted equations, all six independent variables for future (2012e2015) were obtained (Table Sensitivity analysisOne of the important benefits of having a forecasting model is to find the effects of independent parameters (in this study: employment, GDP, dwelling, population, HDD and CDD) on electricity demand. To investigate sensitivity of employment, amount of GDP, dwelling population and also HDD and CDD were fixed in their 2005 values. Amount of employment changed based on trend obtained in previous section (equation of employment in Table The GDP, amount of employment, dwelling, population, HDD and CDD were fixed at 2005 value and GDP was varied using the second equation in Table Sudden fall and rise in energy demand have been experienced in year 1980 as shown in Fig. The dwelling, amount of employment GDP, population, HDD and CDD were fixed at 2005 value and the dwelling was varied using the third equation in Table Finally, the population, amount of employment, GDP, dwelling, HDD and CDD were set at the year 2005 value and the population   was changed was varied using the last equation in Table For the purpose of comparison, all sensitivity analysis results have been plotted in Fig. It is clear that employment yields the biggest slope and it confirms our conclusion that it is the most important parameter affecting electricity demand.Conclusion and remarks", "conclusions": "In this paper, an ANFIS network (adaptive neuro fuzzy inference system) was designed to map six parameters as input data (i.e. employment, GDP, dwelling, population, HDD and CDD) to electricity demand as output variable. To reduce the number of independent variables, input parameters were selected using statistical analysis in order to determine the parameter that has the highest impact on electricity demand. The network had excellent forecasting capacity with MSE of 0.0016.Electricity demand until 2015 was predicted. By analyzing sensitivity of electricity demand based on changes of independent parameters, it was found out that employment affects electricity demand the most.In term of econometric systems, euro fuzzy systems are more accurate than regression models. Compared to neural network models the neuro fuzzy models are robust in future energy estimations while ANN models fail in such extrapolations. Also neuro fuzzy models require less data compared to ANN models. Developing neuro fuzzy models are time consuming which is a drawback of this method compared to regression methods or Fourier series. Also for operators and technicians it is easy and understandable to work with regressions which are feasible. If the system is not nonlinear it is recommended the regression models be employed in demand predictions. For non-linear systems, neuro fuzzy, Fourier or semi empirical models can be used for forecasting.In this paper, all inputs were considered as independent variables. The inputs can be further improved by using hybrid models such as the use of fuzzy inference systems or neural networks (or even statistical methods like regression or time series) to find values of these parameters before entering them into the ANFIS network. It is suggested that this case to be studied by using hybrid methods in future researches.   ", "SDG": [9]}, "household_power_optimisation_and_monitoring_system": {"name": "Household Power Optimisation and Monitoring System", "abstract": " The user has requested enhancement of the downloaded file.", "keywords": "Power optimization,Household power saving,Electricity saving,Electricity shortage,SDGs,IT4D,ICT4D", "introduction": "The importance of electricity in any economy cannot be underestimated [1,, hence the need to efficiently use it. Zimbabwe currently has a shortage of electricity and since 2007 the nation has experienced load shedding due to inadequate generation of electricity by the national power utility company 2]. [3]has it that there is generally shortage of electricity globally, and Zimbabwe is no exception. According to [4] Zimbabwe will continue to have electricity shortages for upto 8 more years due to the incapacity to generate sufficient electricity. Several attempts, such as the use of energy savers, electricity importation and use of alternative, natural power sources such as gas and solar, have been made to ease pressure on the insufficient electricity in Zimbabwe. However, the problem of electricity shortage in Zimbabwe still persists, and there is a call to everyone to contribute towards solving the problem [3]. In light of this, the researchers developed a household power optimization and monitoring system for optimizing the usage of the inadequate electricity that is currently generated in the country while not inconveniencing the users.[3]Africa's electricity shortage is hugely characterised by continuing power cuts and a complete deficiency of electricity infrastructure . This has resulted in negative effects to human and socioeconomic development across the continent [5]. According to [5], only an average of 40 per cent of Africans enjoy a consistent electricity supply; while only 69 per cent of the electrified homes really have electricity that works most or all of the time. 62 per cent of Zimbabwe's population has access to an electricity grid [5]. [5] claim that only 30 per cent of Zimbabweans have electricity that works reliably, 26 per cent have electricity that works half the time while 44 per cent have electricity that either works ocassionally or not at all. Zimbabwe Electricty Supply Authority (ZESA) is the sole producer, distributor and seller of electricity. [5] states that the electricity industry in Zimbabwe has operated as a controlled monolopoly for about five decades. [6] has it that they will reduce electricity generation from 750MW to 475MW due to reduced dam levels since most of the electricity in the country in hydro generated. Zimbabwe has had an 80 per cent urban electrification, 20 per cent rural electrification, and 41 per cent overall elecrification growth from 1980 to 2007 [3]. An unmatched increasing population and balooning number of electric appliances has created an electricity shortage in Zimbabwe, resulting in substantial load shedding [7]. This electricity shortage is despite several efforts that have been made to increase electricity supply and reduce electricity consumption in Zimbabwe, including power importation, use of energy savers and use of alternative energy sources such as solar and biogas [3]. All over the world, several IT based systems have been developed in an attempt to reduce excessive power demand, such as the Green Building in Italy [3]. In Zimbabwe, little has been done to optimise power usage through the use of ICTs [8]. In 2012 ZESA introduced pre-paid meters as to enable customers to manage their electricity bills and encourage them to use electricity wisely. Despite all these efforts, Zimbabwe still faces electricity shortage [1], hence the need to come up with a solution for optimisation[3]", "body": "The importance of electricity in any economy cannot be underestimated Africa's electricity shortage is hugely characterised by continuing power cuts and a complete deficiency of electricity infrastructure Problem StatementThere is generally a serious shortage of electricity in the whole world in general and Zimbabwe in particular Research Objectives1. To design an automated system that optimises electricity usage in households. 2. To design an android application that enables remote manipulation and monitoring of plugged on household electric appliances.Significance of the StudyThis study seeks to come up with a solution for optimising electricity usage in households; hence reducing the load on the national grid. Reducing electricity consumed by households reduces the national demand for electricity and may save the country foreign currency in reducing electricity imports. Moreover, if domestic electricity consumption is reduced, it increases the amount of electricity available for industrial use, which in turn may improve employment creation. According to RELATED WORKSPower optimization refers to reducing the amount of power consumed by devices (such as home appliances, while preserving their functionality) through designing automation tools that minimise power wastage [8] designed an automated power management system called the GreenBuilding. This system used sensors to intelligently monitor power usage and automatically control the behaviour of devices in a building. The system provides a dashboard through which a user can view power consumption statistics by each appliance The Smart Grid is another power optimisation solution which is an amalgamation of communication and electric infrastructure through IT in the current electrical networks to boost efficiency [21] designed a simple system for remotely controlling and monitoring lights, using the Global System for Mobile Communication for long range communication and Bluetooth technology for short range communication.The system sought to reduce electricity consumed by household devices through the use of infrared sensor. Apart from reducing electricity usage, the system also notified users of any irregular situations (like high temperatures and intrusions) through Short Messaging System or Bluetooth technology. Upon receiving a notification on a mobile phone the user initiates appropriate action which will be implemented by the system [22] developed a Smart Power Saving System in smart homes for controlling appliances with the aim of saving power. The system comprises two modules namely fingerprint electronic door-locking and electricity saving. It uses GSM for interaction between the microcontroller and the phone. A user scans their fingerprint on the door-lock and if it matches, the electricity saving module will be turned on. The electricity saving sub-unit controls household electric devices in the home in response to the relative conditions from different sensors installed in the room. Fan and lights are switched on/off in response to the temperature and light intensity inside the home Artificial Intelligent-based systems have also been proposed for power usage optimization. These learn about the behavior of an inhabitant in a smart house to self-adjust the system so that it can be independent and easy to personalize METHODOLOGYThe design science research methodology (Improvement Research) was adopted for this research. The approach focuses on creation, invention or design of some new artifacts, while deriving or obtaining suggestions to solving the problem from current knowledge or theory base for the problem domain Figure 2. System Flow ChartThe motion sensor is used to check whether there is anyone in the house. If there is no one yet lights and/ or fan is on, the system will automatically turn them off. The assumption is someone might have forgotten to switch them off before leaving the room. The system will check again whether there is anyone in the room after ten seconds. This ten seconds delay can be set to another value as determined by the user in line with their requirements. If motion is detected, the system checks whether there is enough light intensity and heat as determined by the user. If light intensity is too low, lights will automatically be turned on. Conversely, if light intensity is too high, lights will be automatically turned off. The fan will also be turned on if temperature is higher than a user set value and will be turned off if temperature rises to a maximum desirable value. When 5 minutes have elapsed, the system will check again whether there is anyone in the room by reading a motion sensor status. This iterates as long as the system is up and running. The system was implemented using the Java Programming language, which was used to link the user interface and the SQLite database. In addition to Java code, XML was used to create the interfaces for the android application. Eclipse Indigo IDE was used to implement the application. The Android SDK and ADT were also used. The researchers created, compiled, debugged and deployed the android application from the Eclipse IDE using the android ADT. The Android SDK was integrated into the Eclipse IDE to help create and test the system during different iterations of the application. SQLite was used for the database.The AlgorithmThe following hardware components are required in the development of the prototype and testing of the prototype: Arduino UNO (R3), GSM Module with an unlocked SIM card, 4 Channel 5 volt Relay, Connecting wires, Bread board, 16x2 LCD, Power supply, An Android mobile phone for hosting the user application, Sensors (PIR motion sensor, ACS712 current sensor, LM35 temperature sensor and LDR light intensity sensor), and Resistors. Android Studio, Arduino Development Tool, Eclipse IDE and Proteus must also be installed on the development computer.RESULTSThe system was evaluated in terms of its ability to optimise power usage by domestic appliances. While a prototype was developed and tested using a fan, light bulb, stove and water heater as the household appliances, its effectiveness in terms of power consumption optimisation was measured for the light bulb only. The researchers calculated light bulb power consumption over 24 hours for best and worst cases. The researchers then ran a 24 hours long experiment using the same light bulb on a prototype of the proposed system. The results of these experiments are shown in Table The results indicate a significant drop in power usage when using the system being proposed herein. Monthly figures are derived from the average daily figures obtained from the experiments. The light bulb used was a 230V, 100W bulb which consumes 0.1 kW per hour.Table 1. Power Consumption Comparison for a light bulb before and after installation of the systemThe worst case scenario is when an appliance remains on for the whole day and night. Given that the bulb used consumed 0.1kW per hour, if left on for 24 hours it will consume 2.4kW. This worst case scenario is only possible if no power optimisation system is implemented. In this experiment, the researchers defined the best case as the case when consumption time is at least 12 hours but less than 24 hours per day. Taking the lower bound of 12 and upper bound of 24 hours per day and calculating the average of the two, it gives 18 hours as the best case scenario's hours when the light will be on per day. The assumption is that the user will be turning the lights on and off when necessary. For 18 hours at a consumption of 0.1 kW per hour, the light bulb will consume 1.8kW per day. The optimal case is was when the power  Cost of electricity is directly proportional to usage, hence reducing electricity consumption results in reduced cost of electricity to domestic electricity consumers. In terms of percentages, reduction in cost is equal to reduction in the amount of power consumed. The results indicate that implementing the household power optimisation and monitoring system resulted in cost saving of 33.3 per cent and 50 per cent for the best and worst case scenarios respectively. Consequently, it means the amount of power consumed was reduced by the same margins. The system resulted in optimal power usage and thus reduced demand for electricity. Apart from power usage optimisation, the system improves comfort levels for users as they remotely monitor and control their household devices. The ability of users to monitor and control household electric devices from a distance is also useful for people living with disabilities as they can control and monitor appliances in the home without having to physically move around to power switches which are usually mounted on different points on the walls of houses.A number of authors who have been engaged in making smart homes systems concentrated more on improving the comfort for inhabitants than electricity saving. CONCLUSIONS", "conclusions": "The Household Power Optimisation and Monitoring System proposed herein focused mainly on reducing the amount of electricity consumed by households and hence reducing stress on the national power grid. The results of the system indicate that the system can reduce power consumption in households by up to 50 per cent. This 50 per cent reduction in electricity consumed translates to 50 per cent savings in electricity costs to households. It is important to save electricity since electrical power is scarce in developing countries like Zimbabwe . Saving electricity in households increases the amount of electricity available for industrial use, which in turn increases employment creation. [11] claims that every single occupation in the manufacturing sector generates more than two million occupations in other sectors of the economy, hence it is imperative to make sure that there is enough electricity for the manufacturing industry. The system also has other benefits of convenience and comfort since users can remotely manipulate appliances on their phones. This feature makes this system an inclusive solution as it also helps people living with disabilities to manipulate appliances on their own without having to move around to different power switch points around the home to power on or off appliances. However, the system could be improved by incorporating voice commands to control appliances. It could also be improved by adding a functionality of predicting future consumption of an appliance based on past and present consumption patterns. The system is designed on the assumptions that supply of electricity is always less than demand; the users are not using the available electricity optimally and all policies pertaining electrical usage are held constant. The performance of this system depends on the performance of the sensors. Moreover, remote manipulation of electric gadgets will depend on the availability of network, hence remote manipulation and monitoring may not work if there is no network coverage, unless the user is within the Bluetooth range. The focus of the study is to optimise power usage in households only.[11]", "SDG": [9, 11]}, "intelligent_cities_r_d_offshoring_web_2": {"name": "Intelligent Cities: R&D offshoring, web 2.0 product development and globalization of innovation systems", "abstract": " In the cities and regions of the twentieth first century a radical turn is taking place as information and communication technologies are converging with the innovation-led regional economies, innovative clusters and agglomerations. Intelligent cities are part of the orientation towards the creation of environments that improve our cognitive skills, our ability to learn, foresee, and innovate. The paper discusses the driving forces sustaining the rise of intelligent cities, such as the globalization of innovation clusters and networks, open innovation, and web-based collaborative environments. Then we look at the movements shaping them -local initiatives around the world, European Living Labs, and applications developed by large companies like IBM, MS and CISCO. The last part of the paper focuses on the planning challenges for building intelligent cities and interactive systems of innovation. We discuss the problem of integration among the physical, institutional and digital dimensions of intelligent cities and the 'bridges' that connect these three spatialities.", "keywords": "Intelligent cities,innovation systems,global innovation,collective intelligence,web 2.0", "introduction": "Cities are changing. A new paradigm of city development and planning has arisen from the actual wave of globalization, emerging technologies, virtuality, and the collective intelligence of the web. Cities in Europe, USA, and Asia respond to these trends by a set of new strategies, namely intelligent city strategies. Well known cases are Living Labs in Europe, Singapore's iN 2015 strategy, Malaysia Multimedia Super Corridor, Florida's high tech corridor, and a series of innovation clusters / global hubs such as Arabianranta, Zaragoza Milla Digital, Seoul Digital Media City.Intelligent cities highlight a key aspect of this new paradigm relating to the creation of environments that improve the cognitive and learning skills of the population and the knowledge and innovation capabilities of organizations located within them. Intelligent cities are territories in which the local system of innovation is enhanced by digital collaboration spaces, interactive tools, and embedded systems. Digital spaces, electronic devices, information systems and online services sustain a series of new urban functions related to knowledge creation, technology transfer, innovation, and global marketing and delivery. Virtual spaces and embedded systems are generating a wave of hybrid environments (global digital ecosystems, Living Labs, i-hubs, COINs, smart cities, e-gov, digital cities, U-communities, intelligent environments, etc.) which in turn amplify local creativities, networking, experimentation and innovation. The city gains innovation capability, which is translated into increased competitiveness, a better environment, more jobs and wealth.", "body": "Cities are changing. A new paradigm of city development and planning has arisen from the actual wave of globalization, emerging technologies, virtuality, and the collective intelligence of the web. Cities in Europe, USA, and Asia respond to these trends by a set of new strategies, namely intelligent city strategies. Well known cases are Living Labs in Europe, Singapore's iN 2015 strategy, Malaysia Multimedia Super Corridor, Florida's high tech corridor, and a series of innovation clusters / global hubs such as Arabianranta, Zaragoza Milla Digital, Seoul Digital Media City.Intelligent cities highlight a key aspect of this new paradigm relating to the creation of environments that improve the cognitive and learning skills of the population and the knowledge and innovation capabilities of organizations located within them. Intelligent cities are territories in which the local system of innovation is enhanced by digital collaboration spaces, interactive tools, and embedded systems. Digital spaces, electronic devices, information systems and online services sustain a series of new urban functions related to knowledge creation, technology transfer, innovation, and global marketing and delivery. Virtual spaces and embedded systems are generating a wave of hybrid environments (global digital ecosystems, Living Labs, i-hubs, COINs, smart cities, e-gov, digital cities, U-communities, intelligent environments, etc.) which in turn amplify local creativities, networking, experimentation and innovation. The city gains innovation capability, which is translated into increased competitiveness, a better environment, more jobs and wealth.Intelligent cities and globalization of innovationFor more than 20 years innovation has been a central driving force of urban and regional development. A rich literature corroborates this orientation; already the 6 th Periodic Report on the social and economic situation and development of the regions of the European Union (1999) documented that the actual regional development of Europe is based on factors of knowledge, innovation, and geographical accessibility. Innovation-led or knowledge-based development of cities and regions has become the model which most cities and regions try to adopt and adapt it to their particular conditions. Central element in linking innovation and regional development is the concept of the regional system of innovation, which denotes the cooperation nexus among R&D, However, recent trends reveal a profound transformation of regional systems of innovation towards more outward and global profiles. Several factors contribute to this change: a new geographical mobility of R&D, R&D offshoring, new supply chain architectures shaped by flagship networks of multinational companies, rise of global clusters of excellence, people-led product innovation, web 2.0 and participatory product development. Altogether these changes create a new spatiality of innovation systems, shaped by global innovation networks and digital collaboration networks, and reveal a new way of innovation, more global, open, and participatory. 'Intelligent cities\" is a planning paradigm which corresponds to this type of innovation spatiality shaped by global innovation networks and web-based collaboration.The globalization of innovation networks is a contemporary trend deeply influencing local innovation clusters and regional systems of innovation. Decentralizing business units and operations to every corner of the world has become routine practice, but now companies are also redistributing their product innovation, even basic and applied research, across global R&D networks The global decentralization of R&D and innovation has a direct impact on local and innovation clusters and regional innovation systems as well. It is well documented that innovation activities tend to cluster. In Europe, for instance, R&D laboratories and companies active in R&D are concentrated to a great extent in a series of urban islands of innovation and innovative regions in north-west Europe and Scandinavia. This spatial polarization of innovation is explained by the horizontal and vertical knowledge interaction within the clusters, local knowledge spillovers, and the 'embedded tacit knowledge\" thesis. Recent evidence however maintains that international relationships and global knowledge flows are crucial sources of creativity and innovativeness within local innovation clusters. Successful clusters are building and managing resources from around the globe Confronting these trends of intense R&D mobility and innovation globalization, many communities and cities have launched 'intelligent city' strategies. Public authorities in Singapore, Taipei (China), Spokane (US), Seoul and Songdo (S. Korea), Cyberjaya and Putrajaya (Malaysia), in many cities of Europe, and in 'smart communities' in the US have implemented plans to make their cities more 'intelligent Movements shaping intelligent citiesFragments of intelligent cities are emerging all over the world, but still we are very far from the creation of amazing intelligent environments that open minds and transform radically human skills and mental capabilities. This is a weakness both of technology in the field of intelligent environments and of integration of technologies with innovation institutions and city activities. However, some major shaping movements have already appeared.Local initiatives -The ICF awards: An extremely valuable source of current applications and local experimentations in the field is to be found in the Intelligent Community Forum and the cities selected by ICF since 2001 as top intelligent cities Planning challengesThe above movements bring on the surface a number of planning challenges linked to the multi-dimensional spatiality and architecture of intelligent cities, which is simultaneously social, physical, institutional, and digital.Integration: Intelligent cities are organized as multi-layer territorial systems of innovation, bringing together knowledge-intensive activities, innovation institutions, and digital communication spaces. These layers reflect both the different dimensions of intelligence (human, collective, artificial) and the deployment of innovation on physical, institutional and digital spaces. \uf0b7 The first layer includes the city's knowledge-intensive activities in manufacturing and services that are usually organized into clusters. The population of the city, knowledge workers, and innovative companies are the fundamental elements upon which intelligent cities are constructed. Proximity in physical space is important, integrating enterprises, production units, and service providers into a coherent innovation system. Critical factor at this level is the intellectual capital of the city population. A major challenge for building intelligent cities is integration among the above three layers and the making of 'bridges' that connect their physical, institutional, and digital spatiality. Analysis of intelligent places shows that most 'bridges' are organizational and institutional in nature and highly dependent on the digital technologies implemented.Technology: Computer Aided Design (CAD) and Geographical Information Systems (GIS) are major technologies enabling a digital management of urban space. Intelligent cities, however, rely on a different set of technologies, web-based applications, virtual collaboration tools, and u-communities The dominant software stacks used in such collaborative environments are open source. The integrated, optimized, open-source Apache, MySQL, PHP/Perl/Python (AMP) stack seems to be the preferred platform for building and deploying new Web applications and services.Today cloud computing makes a new step of efficiency and economy to this environment, delivering IT resources on demand and opening up new business models and market opportunities. In some ways cloud computing is a metaphor for Internet-based services and the increasing movement of virtual computing and hosting data resources onto the Web. It abstracts the software application platform from the underlying hardware infrastructure, freeing developers and users from the need to poses hardware. In cloud computing, the user's data and software execution are in the cloud (the Internet) and the network becomes the computer, while the use of resources follows the model a utility pricing Sectoral and district-based strategies:In a previous paper Measuring: Defining metrics in the field of intelligent cities is driven by two principles: (1) to compare localities between themselves and learn from the best, and (2) to the internal dynamics of intelligent cities, define weaknesses, and recognize the effort needed to overcome them. Out of these metrics four axis of intelligent city development can be defined. Three of them deal with input factors (skills, knowledge institutions, digital spaces), while the fourth measures outputs (innovation). A 4dimensions radar chart thus may be defined measuring the progress made in each of the four fundamental dimensions of an intelligent city. We should, however, keep in mind that intelligent city strategies successfully followed within a particular region may not necessarily generate the relevant results if copied to another region.Future directions: Today intelligent cities offer an attractive prospect, a strategy and a vision for the future, rather than an actuality that has been realized. Key issue in making such environments is to understand and manage the linkages between the physical, institutional and digital aspects of innovation and how these interconnections activate knowledge functions, release creativities and transform knowledge into new products.Intelligent cities can achieve more global and interactive systems of innovation enabling, through the digital interaction, an extension of innovation collaboration networks and the participation of users. These are two novel elements (global innovation networks / user participation to innovation) that broadband communication and digital collaborative spaces bring into local / regional systems of innovation.However, the precise way that digital collaboration enables the participation of overseas researchers, suppliers, innovators, customers, and end-users to innovation processes has to be defined with respect to the functional differentiation and complexity of the city. Different forms of IT applications, virtual spaces and novel e-services have to be defined in different city districts of industry, technology, university, CBD, shopping, port, and airport areas. Intelligent cities are emerging as dynamic re-arrangement of networks, nodes and clusters. The creation of intelligent innovation ecosystems is a major challenge for the future.Table 1 :", "conclusions": "", "SDG": [9, 11]}, "intelligent_infrastructure_for_sustainable_potable_water_a_roundtable_for_emerging_transnational_research_and_technology_development_needs": {"name": "Intelligent infrastructure for sustainable potable water: a roundtable for emerging transnational research and technology development needs", "abstract": " Problem statement: Recent commercial and residential development have substantially impacted the fluxes and quality of water that recharge the aquifers and discharges to streams, lakes and wetlands and, ultimately, is recycled for potable use. Whereas the contaminant sources may be varied in scope and composition, these issues of urban water sustainability are of public health concern at all levels of economic development worldwide, and require cheap and innovative environmental sensing capabilities and interactive monitoring networks, as well as tailored distributed water treatment technologies. To address this need, a roundtable was organized to explore the potential role of advances in biotechnology and bioengineering to aid in developing causative relationships between spatial and temporal changes in urbanization patterns and groundwater and surface water quality parameters, and to address aspects of socioeconomic constraints in implementing sustainable exploitation of water resources. Workshop outcomes: An interactive framework for quantitative analysis of the coupling between human and natural systems requires integrating information derived from online and offline point measurements with Geographic Information Systems (GIS)-based remote sensing imagery analysis, groundwater -", "keywords": "Sustainability,Potable water,Sensors,Information technology,Point-of-use treatment", "introduction": "The sustainability of groundwater resources in coastal zones bounded by freshwater and saltwater bodies is affected by human activities as the result of residential, commercial, agricultural and industrial land use, which has resulted in overuse as well as microbial and chemical contamination (Francy et al., 2000;Thomas, 2000;. The increased urbanization has affected the hydrological cycle in coastal regions whereby the increase in impervious surfaces has impacted aquifer recharge, and consequentially the flow of rivers and streams, groundwater -surface water interactions and, ultimately, the chemical and microbial quality of recycled drinking water Daughton and Ternes, 1999). As a result, several national efforts have been initiated to quantify the cause-and-effect relationships between human activities, groundwater and surface water quality, and the recycling of potable water reserves. For example, the U.S. Geological Survey (USGS) National Water Quality Assessment Program (NAWQA) has during the last 10 years (Cycle I) investigated relationships between the occurrence and distribution of water quality constituents to natural and human factors, and has provided interpreted qualitative information to policy makers and resource planners (National Research Council, 1998). The Cycle II Program objectives are to focus on trend assessment and understanding of processes controlling water quality. The complementary efforts guided by the National Research Council and the American Chemical Society (ACS) on the recycling of potable water have highlighted the concerns and needs associated with the detection, treatment and removal of effluent-derived microbial and chemical contaminants (e.g. (Francy et al., 2000)NRC, 1998;.Sedlak et al., 2000)The development of quantitative correlations between population settlement patterns and potable water quality will require the integration of: (i) a proper description and definition of the systems boundaries (remote sensing and geostatistics), (ii) approaches to quantify systems interaction (modeling and trend analysis), (iii) selection of water quality indicators and sensing strategies, and (iv) appropriate methods for data interpretation, integration and use.Interactive frameworks for quantitative analysis of the coupling between human and natural systems in urban coastal environments, by integrating information derived from Geographic Information Systems (GIS)-based remote sensing imagery analysis with groundwater -surface water hydrologic fluxes and water quality data to assess the vulnerability of coastal aquifers to the deterioration of potable water supplies (Fig. ). The concept incorporates the following elements: (1) online and offline spatially referenced microbial and chemical point measurements (sensing); (2) geostatistical integration of point measurements with GIS layers (uncertainty analysis); (3) distributed parameter models to describe the watershed in terms of connectivity and fluxes; (3) unbiased environmental forensics techniques for source apportionment (coding); and (4) spatially referenced distributed control technology networks (management).1By adopting an adaptive modeling approach coupled to geostatistical validation, it is expected that watershed-specific rankings of stressors and receptors will guide researchers and policymakers in the development of targeted sensing and monitoring technologies, as well as tailored applications for risk mitigation of potable water from microbial and chemical environmental contamination. Information dissemination on innovative sensing and source control strategies for microbial and chemical contamination in urban environments is a major sanitation and developmental priority in emerging economies and developing countries, with the spread of infectious diseases policy and response agendas. Considering the scarce resources available in less prosperous urban environments, regulators and city engineers would significantly benefit from decision support systems capable of targeting the spatial distribution of contaminant input into the raw water resources, and thus the implementation of early warning monitoring networks (for fecal or other input), source control systems and need/breakdown of water treatment strategies.The diffusion of innovative technologies for environmental sustainability from developed nations to emerging economies was the topic of an Organization for Economic Cooperation and Development (OECD)-sponsored conference in , and of the World Association of Industrial and Technological Research Organizations (WAITRO; www.oecd.org/dataoecd/19/60/2432617.pdf) conference in The Hague Seoul, 2000 (www. waitro.org). It was reported that the main impediments to closer integration are perceived to be issues of technological competition, economics of implementation and effective communication. Hence, early engagements through a roundtable discussion at smaller but focused venues such as International Society for Environmental Biotechnology (ISEB) 2002 will stimulate information dissemination and exploration of eventual partnerships in this effort on economically feasible technologies for sustainable potable water supplies. This venue is particularly relevant since attendance at ISEB conferences tends to include substantial participation of members from the Asian subcontinent, South and Central America, and European participants, as well as members of multilateral organizations. (2000)", "body": "The sustainability of groundwater resources in coastal zones bounded by freshwater and saltwater bodies is affected by human activities as the result of residential, commercial, agricultural and industrial land use, which has resulted in overuse as well as microbial and chemical contamination The development of quantitative correlations between population settlement patterns and potable water quality will require the integration of: (i) a proper description and definition of the systems boundaries (remote sensing and geostatistics), (ii) approaches to quantify systems interaction (modeling and trend analysis), (iii) selection of water quality indicators and sensing strategies, and (iv) appropriate methods for data interpretation, integration and use.Interactive frameworks for quantitative analysis of the coupling between human and natural systems in urban coastal environments, by integrating information derived from Geographic Information Systems (GIS)-based remote sensing imagery analysis with groundwater -surface water hydrologic fluxes and water quality data to assess the vulnerability of coastal aquifers to the deterioration of potable water supplies (Fig. By adopting an adaptive modeling approach coupled to geostatistical validation, it is expected that watershed-specific rankings of stressors and receptors will guide researchers and policymakers in the development of targeted sensing and monitoring technologies, as well as tailored applications for risk mitigation of potable water from microbial and chemical environmental contamination. Information dissemination on innovative sensing and source control strategies for microbial and chemical contamination in urban environments is a major sanitation and developmental priority in emerging economies and developing countries, with the spread of infectious diseases policy and response agendas. Considering the scarce resources available in less prosperous urban environments, regulators and city engineers would significantly benefit from decision support systems capable of targeting the spatial distribution of contaminant input into the raw water resources, and thus the implementation of early warning monitoring networks (for fecal or other input), source control systems and need/breakdown of water treatment strategies.The diffusion of innovative technologies for environmental sustainability from developed nations to emerging economies was the topic of an Organization for Economic Cooperation and Development (OECD)-sponsored conference in Materials and methodsThe roundtable involved five participants from leading organizations to provide a genesis for discussion in the ISEB forum. The lecturers were instructed to highlight key elements relevant to the enabling technologies required in this framework for 20 min each, followed by 2 h of discussion by the general audience. Specifically, the aim of the workshop was to frame questions and discussion within the following context: Results and discussion3.1. Recent advances toward automated microbial analysis in natural and engineered aqueous systems Solutions to the technological challenges of achieving automated, low-cost, and robust detection of microbial parameters of interest in field applications are currently under research in numerous laboratories around the world. The technological challenges are many and can be divided into three subcategories: (1) sample preparation, (2) sample detection, and (3) data analysis and interpretation. Current research approaches and advances in all three of these areas based upon a review of recent academic and industrial research activity indicate that distributed microbial detection and quantification is highly dependent on near-term technological capabilities for online microbial detection, quantification and analysis.On-going research at The University of Michigan directed toward the development of an early warning microsensor infrastructure for the detection of microorganisms in aqueous systems has indicated that technology under development based on the principle of flow cytometry is promising for distributed microbial sensing. Rapid optical detection methods, based on proven technology such as flow cytometry, offer high-speed multiparametric data acquisition, are compatible with molecular detection and quantification methodologies, and are amenable to miniaturization. The overall requirements and constraints of distributed microbial sensing and control technology include: very low cost, large span, low maintenance, automated, compact, fast, versatile, artificially intelligent, networkable (desired). Whereas flow cytometry technology is standard bench-top methodology, our application is novel in that only the basic functions necessary for microbial detection and quantification have been maintained. As these functions have been miniaturized and integrated in accordance with recent advances in micro-electromechanical systems (MEMS) technology, the instrument is named the Micro Integrated Flow Cytometer (MIFC) (Fig. Distributed chemical sensing capabilities based on microarrays (Edwards)Wastewater effluents, municipal sewage and agricultural runoff following field applications of manure have been shown to introduce trace levels of endocrine-disrupting compounds (EDCs). The endocrine system controls cellular activities throughout the body, enabling the many cells and tissues of the body to work together as a single organism. Almost all actions undergone by animals are in some way regulated or influenced by the endocrine system, which also guides the organism's growth, development, and behavior. The endocrine system can be influenced by a wide range of chemical compounds. Bioactive compounds are found in most major classes of pollutants, including dioxins and furans, halogenated organic compounds, poly-chlorinated biphenyls, phthalate esters, pesticides (both banned substances such as DDT and others currently in use, such as atrazine) and a number of other pollutants, such as polyaromatic hydrocarbons, tri-butyl tin and heavy metals. Many of these compounds are highly persistent in the environment and are capable of bioaccumulation and biomagnification in living organisms (e.g. The detection of these trace compounds and their activity in the environment, and their potential for amplification in humans presents an enormous challenge to the scientific community due to the lack of understanding on their persistence and causal relationships in living systems. A variety of testing methods have been developed to investigate EDCs (for a review, see The approach presented here was illustrated using research conducted at the University of Toronto to assess the endocrine-disrupting capacity of bleached Kraft mill effluents of the pulp and paper industry using 19,000 immortalized human cell lines, including the breast cancer cell lines T-47-D and MCF-7 Geostatistical integration of monitoring data and GIS layers (Goovaerts)One of the main issues pertaining to the measurement of water quality indicators is the uncertainty of quantitative point measurements, and its propagation in space and time. In other words, whereas the false positive/false negative specifications of monitoring tools may be validated, its propagation in spatially referenced domains may be much more significant than that of instrument specificity. Recent years have witnessed the develop-Fig. The first step to linking urbanization and water quality will require characterization of the main spatial patterns displayed by the organic constituents as well as their cross-correlation across the study area. The geostatistical analysis The second step necessitates the development and comparison of models that allow the prediction of water quality parameters identified in Step 1 from GIS-based layers of information. Following recent USGS work The approaches used at The University of Michigan (Fig. In practice, it is commonly found that the statistical accuracy of physics-based models is poor because natural systems are too complex for deterministic modeling method, which was the reason Distributed technology for water treatment in developing countries (Egli)In recent years, the concept of point-of-use water treatment technologies has gained wide support at the research and policy levels, as this approach allows for technology development and implementation depending on water quality and water use characteristics, as well as depending on socioeconomic boundaries. Distributed optimal technology networks (DOTNet) concepts have been advanced to separate treatment requirements depending on end use (e.g, industrial, commercial, domestic and drinking water) from a range of waste streams (e.g. blackwater, recycled water, surface water and groundwater). Unproven at this time to be economically viable, the inherent assumptions made in the approach seek to optimize energetic benefit for optimal treatment of lesser amounts of material (water). In its technology optimization, DOTNet incorporates the entire suite of water treatment strategies available to consumers, ranging from desalinization, to membrane filtration, activated carbon filtration and biological treatment systems. The DOTNet approach also exhibits significant potential for technology export to emerging economies and developing countries as relevant to Africa.One technology developed by the Swiss Federal Institute for Water Science and Technology (EAWAG), and endorsed by the World Health Organization (WHO) for developing countries is SODIS, or Solar Water Disinfection Process (Fig. Preliminary field studies in Bolivia, Burkina Faso, China and Colombia have shown that the process works, and indicate that the cases of diarrhoea may be reduced up to 80% by treating the water with SODIS before use. For example, it was determined that SODIS requires relatively clear water with turbidity of less than 30 NTU, and exposure length is dependent on cloud cover (6 h to 2 days). The most favorable region for SODIS lies between latitudes 15jN and 35jN. These semiarid regions are characterized by high solar radiation and limited cloud coverage and rainfall (3000 h of sunshine per year). The second most favorable region lies between the equator and latitude 15jN, the scattered radiation in this region is quite high (2500 h of sunshine per year). Various types of transparent plastic materials are good transmitters of light in the UV and visible range of the solar spectrum. Whereas in concept this approach is promising, the specifications will need to be adapted to the actual quality of the raw water supply under consideration, and hence, the dependence on the spatially referenced watershed quality parameters.Conclusions", "conclusions": "Raw water resources are increasingly stressed by chemical and microbial contamination due to urban development, industrialization, agricultural practice and overdrawing of aquifers, and require monitoring and data interpretation to meet the sanitation and drinking water needs of the global population. Whereas this issue is at the forefront of public and political debate, and solutions have been advanced in the realm of economics and public health policy, there is a role for professional organizations such as ISEB to help define the potential role of biotechnology and bioengineering in the scientific and educational debate.This roundtable brought an applied research vision to capitalize on the integration of information technology and molecular-based sensing technology platforms such as microflow cytometers and DNA arrays to ultimately develop real-time data collection, interpretation and dissemination infrastructure across socioeconomic boundaries. Decision support systems, to help the regulatory and environmental public health communities manage stressed water resources, depend on distributed sensing platforms, and a robust means to integrate and interpret the data from a chemical and microbial perspective to allow for the implementation of point-of-use treatment technology. Emphasis was placed on technological innovations, which could be adapted for remote data collection, as a chemical and microbial baseline condition needs to be established against which anomalies are benchmarked. Hence, point measurements need to be integrated with GIS layers to develop a risk framework applicable to local or regional spatial (and temporal) scales. This emphasis is a truism whether drinking water supplies in rich or poor economies are considered; the difference is mainly based on how to approach the problem, once properly circumscribed. The availability and socioeconomic implementation of pointof-use treatment technologies have to be considered in light of the real and perceived threats on water resources and their associated risk characterization (e.g. diarrhoea, dehydration, etc.). Technology transfer to developing nations was illustrated by means of low-tech solar disinfection technology. Criticisms that high-tech detection platforms have no place in the developing world emphasize the need for technology implementation with grass-roots input of the population, and considerations for technology adaptation to the local infrastructure. It should be noted that remote sensing technology is developing to the point where some of these local infrastructure limitations can be overcome.A second possible role for ISEB is in the advancement of biotechnology curricula and technology transfer courses relevant to sustainable potable water issues. The education barrier for implications and applications of biotechnology for sustainable watershed management is relevant across socioeconomic boundaries. For example, at The University of Michigan, curriculum development in this area takes place at three levels: graduate programs (e.g. Concentrations in Environmental Sustainability, ConsEnSus), development of user-friendly interactive mapping software for decision support systems and the development of a remote interactive system for microbial analysis to be used in graduate courses. NSF and NATO have recently supported technology transfer workshops for environmental assessment and remediation of contaminated sites with the aim to discuss areas of technology diffusion within the context of Eastern Europe (e.g.  and Latin America. Of particular relevance to this workshop are the efforts of the Swiss Federal Institute for Water Research and Technology in the area of water and sanitation in developing countries (SANDEC; http://www.sandec.ch/). Its mandate is to assist in developing appropriate and sustainable water and sanitation concepts and technologies adapted to the different physical and socioeconomic conditions prevailing in developing countries. Perhaps, ISEB could capitalize on the participation of leading institutions in biotechnology-based applications for sustainable water issues, by developing a niche and role as a clearinghouse for expertise in this area.Reible and Demnerova, 2002)", "SDG": [9, 11]}, "maintenance_optimization_for_heterogeneous_infrastructure_systems_evolutionary_algorithms_for_bottom_up_methods": {"name": "Maintenance Optimization for Heterogeneous Infrastructure Systems: Evolutionary Algorithms for Bottom-Up Methods", "abstract": " This chapter presents a methodology for maintenance optimization for heterogeneous infrastructure systems, i.e., systems composed of multiple facilities with different characteristics such as environments, materials and deterioration processes. We present a two-stage bottom-up approach. In the first step, optimal and near-optimal maintenance policies for each facility are found and used as inputs for the system-level optimization. In the second step, the problem is formulated as a constrained combinatorial optimization problem, where the best combination of facility-level optimal and near-optimal solutions is identified. An Evolutionary Algorithm (EA) is adopted to solve the combinatorial optimization problem. Its performance is evaluated using a hypothetical system of pavement sections. We find that a near-optimal solution (within less than 0.1% difference from the optimal solution) can be obtained in most cases. Numerical experiments show the potential of the proposed algorithm to solve the maintenance optimization problem for realistic heterogeneous systems.", "keywords": "", "introduction": "Infrastructure management is a periodic process of inspection, maintenance policy selection and maintenance activities application. Maintenance, Rehabilitation, and Reconstruction (MR&R) policy selection is an optimization problem where the objective is to minimize the expected total life-cycle cost of keeping the facilities in the system above a minimum service level while satisfying agency budget constraints.MR&R optimization can be performed using one of two approaches: top-down and bottom-up. In a pavement management system, a top-down approach provides a simultaneous analysis of an entire roadway system. It first aggregates pavement segments having similar characteristics such as structure, traffic loading and environmental factors into mutually exclusive and collectively exhaustive homogeneous groups. The units of policy analysis are the fractions of those groups in specific conditions, and individual road segments are not represented in the optimization. As a result, much of the segment-specific information (history of construction, rehabilitation, and maintenance; materials; structure) is lost.One of the main advantages of a top-down approach is that it enables decision makers to address the trade-off between rehabilitation of a small number of facilities and maintenance of a larger number of facilities, given a budget constraint. On the other hand, the top-down approach does not specify optimal activities for each individual facility, and mapping system-level policies to facility-level activities is left to the discretion of district engineers. One of the early examples of a top-down formulation is Arizona Department of Transportation (ADOT) Pavement Management System (PMS), which selects maintenance and rehabilitation strategies that minimize life-cycle cost. The ADOT PMS saved $200 million over five years . However, the Arizona DOT PMS is designed for homogeneous systems where all facilities are assumed to have same characteristics, and cannot be applied to a heterogeneous system where individual facility characteristics are different.(OECD, 1987)For a heterogeneous system, composed of facilities with different material, deterioration process, and environmental characteristics, it is necessary to specify optimal maintenance activities at the facility-level. For example, a system of bridges usually consists of facilities of different materials, structural designs and traffic loads. For a heterogeneous system maintenance optimization, a bottom-up approach is appropriate to determine maintenance policies at the facility level.In formulating heterogeneous system optimization,  proposed a bottom-up approach as follows. First, identify a set of optimal (or near optimal) sequences of MR&R activities for each facility over the desired planning horizon. Then, find the optimal combination of MR&R activity sequences for entire system given a budget constraint.Robelin and Madanat (2007)The main advantage of the bottom-up approach is that the identity of individual facilities is preserved as we maintain the information associated with each facility such as structure, materials, history of construction, MR&R, traffic loading, and environmental factors. However, preserving individual details leads to high combinatorial complexity in the system optimization step. The methodology proposed herein is an attempt to overcome such shortcoming of the bottom-up formulation. We propose a two-stage bottom-up approach to address MR&R planning for an infrastructure system composed of dissimilar facilities undergoing stochastic state transitions over a finite planning horizon. This chapter consists of five sections. In Section 2, state-of-the-art methods for MR&R planning are reviewed. In Section 3, a new two-stage approach for solving the heterogeneous system maintenance problem is presented. In Section 4, a parametric study is presented to illustrate and evaluate the new approach. Finally, Section 5 presents conclusions.", "body": "Infrastructure management is a periodic process of inspection, maintenance policy selection and maintenance activities application. Maintenance, Rehabilitation, and Reconstruction (MR&R) policy selection is an optimization problem where the objective is to minimize the expected total life-cycle cost of keeping the facilities in the system above a minimum service level while satisfying agency budget constraints.MR&R optimization can be performed using one of two approaches: top-down and bottom-up. In a pavement management system, a top-down approach provides a simultaneous analysis of an entire roadway system. It first aggregates pavement segments having similar characteristics such as structure, traffic loading and environmental factors into mutually exclusive and collectively exhaustive homogeneous groups. The units of policy analysis are the fractions of those groups in specific conditions, and individual road segments are not represented in the optimization. As a result, much of the segment-specific information (history of construction, rehabilitation, and maintenance; materials; structure) is lost.One of the main advantages of a top-down approach is that it enables decision makers to address the trade-off between rehabilitation of a small number of facilities and maintenance of a larger number of facilities, given a budget constraint. On the other hand, the top-down approach does not specify optimal activities for each individual facility, and mapping system-level policies to facility-level activities is left to the discretion of district engineers. One of the early examples of a top-down formulation is Arizona Department of Transportation (ADOT) Pavement Management System (PMS), which selects maintenance and rehabilitation strategies that minimize life-cycle cost. The ADOT PMS saved $200 million over five years For a heterogeneous system, composed of facilities with different material, deterioration process, and environmental characteristics, it is necessary to specify optimal maintenance activities at the facility-level. For example, a system of bridges usually consists of facilities of different materials, structural designs and traffic loads. For a heterogeneous system maintenance optimization, a bottom-up approach is appropriate to determine maintenance policies at the facility level.In formulating heterogeneous system optimization, The main advantage of the bottom-up approach is that the identity of individual facilities is preserved as we maintain the information associated with each facility such as structure, materials, history of construction, MR&R, traffic loading, and environmental factors. However, preserving individual details leads to high combinatorial complexity in the system optimization step. The methodology proposed herein is an attempt to overcome such shortcoming of the bottom-up formulation. We propose a two-stage bottom-up approach to address MR&R planning for an infrastructure system composed of dissimilar facilities undergoing stochastic state transitions over a finite planning horizon. This chapter consists of five sections. In Section 2, state-of-the-art methods for MR&R planning are reviewed. In Section 3, a new two-stage approach for solving the heterogeneous system maintenance problem is presented. In Section 4, a parametric study is presented to illustrate and evaluate the new approach. Finally, Section 5 presents conclusions.Literature ReviewInfrastructure maintenance optimization problems can be classified into single facility problems and multi-facility problems (also known as system-level problems).The single facility problem is concerned with finding the optimal policy, the set of MR&R activities needed for each state of the facility that achieves the minimum expected life-cycle cost. Optimal Control For the system-level problem, the objective is to find the optimal set of MR&R policies for all facilities in the system, which minimizes the expected sum of lifecycle cost within the budget constraint for each year. The optimal solution at the system-level will not coincide with the set of optimal policies for each facility if the budget constraint is binding. Homogeneous system problems have been solved by using linear programming Durango-Cohen et al. (2007) proposed a quadratic programming platform for multi-facility MR&R problem. While the quadratic programming (QP) formulation successfully captures the effect of MR&R interdependency between facility pairs, the applicability of QP is limited to situations when the costs are quadratic. The numerical example in the chapter is limited to facilities with the same deterministic deterioration process, where each facility is a member of either a 'substitutable' or a 'complementary' network. Although intuitively sensible, the determination of 'substitutable' or 'complementary' networks might not be evident in large scale networks. The approach used in Robelin and Madanat ( MethodologiesConsider an infrastructure system composed of N independent facilities, with different attributes such as design characteristics, materials, traffic loads: this system is a heterogeneous system. We assume that a managing agency has to find the best combination of maintenance activities within a budget constraint of the current year. This optimization process is repeated at the start of every year using the outputs of facility inspections. As the optimization is an annual process and the future budgets are unknown, future budget constraints are not considered in the current year optimization.The objective is to find an optimal combination of facility-level maintenance activities, minimizing the total system-level cost. We assume that two variables, cost and activity, can be defined for all facilities, regardless of individual characteristics.We assume that inspections are performed at the beginning of the year, and the current state of each facility is known. In our two-stage bottom-up approach, we first solve the facility-level optimization to find a set of best and alternative MR&R activities and costs for each facility. In the second stage, we solve the system-level optimization to find the best combination of MR&R activities across facilities by choosing among the optimal and sub-optimal alternative activities found in the first step. Fig. In this chapter, we develop a general methodology for heterogeneous system optimization with emphasis on a pavement system as it is one of the most widely researched systems. This has a common problem structure for infrastructure management, i.e. probabilistic state transition, time discounting, and multiple MR&R activities. Therefore, the methodology developed here can be modified and applied to other types of facilities such as bridge systems.In a Pavement Management System (PMS), the state of pavement can be represented by discrete numbers such as the Pavement Serviceability Rating (PSR), ranging from 1 (worst condition) to 5 (best condition). If pavement deterioration can be represented as a Markovian process, the serviceability (PSR) changes over time depend only on the current state and the maintenance activity applied at the beginning of the time period after inspection. The transition probability matrix, P a (i,j) specifies the probabilities of a state change from state i to j after applying maintenance activity a. An MR&R program X =[x 1 , \u2026, x N ] is a set of activities that will be applied to the N facilities in the system in the current year. We assume a finite planning horizon of length T. The vector X must be feasible, i.e., it must satisfy the budget constraint for the current year.Facility-Level OptimizationThe facility-level optimization solves for the optimal activity and its cost pair (action cost and expected-cost-to-go) without accounting for the budget constraint. It also identifies suboptimal alternative policies and their cost pairs. The facilitylevel optimization for a PMS can be formulated as a dynamic program to obtain an optimal policy and the alternative policies. The dynamic programming formulation that solves for optimal activity a * and its expected cost-to-go V * is:Where, Note that when k=0, the result of Equation (3), * 1 a is the optimal activity, and * 1 V , the result of Equation ( }, and their costs { ,... , ,} can be solved for the current year. Although the facility-level optimization can also be formulated and solved as a linear program (for the infinite horizon case), we used dynamic programming because it also produces the alternative policies and costs used as inputs for the system-level optimization without additional calculations. System-Level OptimizationThe facility-level optimizations yield a set of activities { ,... , ,} and their expected cost-to-go { ,... , ,} for each facility. Given the agency cost for each activity, the objective is to find the combination of activities (one for each facility) that minimizes the system-wide expected cost-to-go while keeping the total agency cost within the budget. We refer to this combination of activities as the optimal program. Assuming that all facilities are independent, and given a budget constraint, the system-level optimization can be formulated as a constrained combinatorial optimization problem.Let n M ={0, 1, 2,\u2026} be an alternative activity set for facility n, where 0 represents the optimal activity and i represents i-th alternative activity. The systemlevel optimal activity n n M x \u2208 will be determined given state s n for facility n. Let Such that:Where,is the optimal program, TEC represents the total system expected cost-to-go from current year to year T and AC the total activity cost.There exist various methods for solving the constrained combinatorial optimization problem including integer programming and heuristic search algorithms. As the constraints and object function may include nonlinear equations as in Durango-Cohen et al. ( Two-Facility ExampleTo develop a system level solution, consider a simple case with only two facilities. Fig. inside the feasible region, defined by the budget constraint.To guarantee global optimality, the solution path has to include every point for which the total cost (TEC) is below the optimal solution as illustrated in Fig. Evolutionary AlgorithmIn this section, we discuss the formulation and application of Evolutionary Algorithms (EAs) to heterogeneous infrastructure system management optimization. The complexity of the system level optimization for a heterogeneous infrastructure system arises from the large number of combinations of possible MR&R activities. Unlike traditional optimization where the solution search is conducted candidate by candidate, Evolutionary Algorithms (EAs) are optimization techniques to search and evaluate a group of solution candidates, or population of solutions, with a goal to converge to a space that contains solutions satisfying pre-set criteria.Starting with a group of candidates, EAs select only competitive solutions in the group to generate the parent. Solutions are then mutated or recombined to produce the next generation of solutions. The process of selection, mutation and crossover are repeated until a certain set of control criteria is satisfied. EAs are not always guaranteed to find the global optimum, and they require a careful planning in selecting the parent selection process and control parameters.Among several EAs techniques, we apply Genetic Algorithms (GAs), which are widely studied and used, and provide the most suitable platform for combinatorial optimization problems like ours. The details of our implementation are discussed below.Fig. Stage 1: Mutant Offspring GenerationTo generate mutant offspring, we use the current solution vector X as a single parent. Let dX be a movement vector. A number of movement vectors are randomly generated according to the normal distribution. The initial solution vector is the optimal one found in the facility-level optimization without budget constraint. After generating movement vectors, they are rounded to one of the discrete values: -3, -2, -1, 0, 1, 2, 3. The smaller the value of s, the more components of the movement vector have zero value, resulting in smaller search space. The number of offspring can be used for controlling the precision of search.Stage 2: Offspring Evaluation and SelectionAt this stage, generated offspring are evaluated to find the best movement from the current solution point. When the current agency activity cost (AC) is greater than the budget assigned, the offspring satisfying the following conditions is selected.( )When the solution is inside the feasible region, we select an offspring satisfying the following condition:The stage 2 procedure is repeated until there is no solution improvement.Stage 3: Optimality CheckChecking optimality, the search range is expanded to find a solution closer to the global optimal solution. In each step when no improved solution is found, s is increased by multiplication factor w. Therefore, if k steps pass without solution improvement, offspring with movement vector dx n ~ Normal (0, w 2k s 2 ) are evaluated for optimality checking. If an improved solution is found, k is reset to its initial value, and from the new point, the stage 3 is repeated until no improved solution can be found within a predefined number of iterations. Numerical ExamplesTo evaluate the proposed optimization algorithm and show the applicability of the suggested approach to a realistic problem, we created highway pavement systems with random Transition Probability Matrices and action costs, and compared the optimality of the solutions and the algorithm execution speeds.Test System CreationVirtual highway pavement systems based on realistic data were created. To evaluate the optimality, a 20-facility system was created, and the number of facilities was increased to 2000 to assess the algorithm performance. The planning horizon was set to 40 years, and the interest rate to 5%. The agency activity costs and Transition Probability Matrices were generated randomly with the mean values suggested in Table Table P PAlgorithm VerificationFig. Algorithm EvaluationOptimality EvaluationTo evaluate the EA solutions, 1000 random experiments were executed for a twenty-facility system. In each experiment, facilities were randomly generated with random activity costs and transition probabilities as described in the previous Section. The value of s for offspring randomization was set to 0.15, the multiplication factor w to 1.1. The number of offspring for each iteration was set to 100. Real optimal costs TEC opt were calculated by exhaustive search for comparison. Table Conclusions", "conclusions": "We proposed a two-stage bottom-up methodology to solve the MR&R optimization problem for a heterogeneous infrastructure system. To overcome the computational complexity of the brute force search, we developed a method that utilizes the optimal and alternative solutions at the facility-level. This approach makes the search process more efficient by specifying the search order for the alternatives. The system-level problem is formulated as a constrained combinatorial optimization, and solutions are found by applying the Evolutionary Algorithm. Evaluation results suggest that the Evolutionary Algorithm is effective in identifying close-to-optimal solutions in relatively short time for large scale network problems. Numerical experiments showed that we obtain near-optimal solutions (within less than 0.1% difference from the optimal solution) in most cases, and also showed the potential of the proposed algorithms to solve the maintenance optimization problem for realistic heterogeneous systems. One extension of the proposed method is the optimization of a system composed of diverse types of infrastructures of bridges, pavements and other types of facilities. Such an extension would be useful for optimizing DOT maintenance expenditure in a multi-asset management framework.", "SDG": [9, 11]}, "the_forthcoming_artificial_intelligence_(ai)_revolution_its_impact_on_society_and_firms": {"name": "The Forthcoming Artificial Intelligence (AI) Revolution: Its Impact on Society and Firms", "abstract": " The impact of the industrial and digital (information) revolutions has, undoubtedly, been substantial on practically all aspects of our society, life, firms and employment. Will the forthcoming AI revolution produce similar, far-reaching effects? By examining analogous inventions of the industrial, digital and AI revolutions, this article claims that the latter is on target and that it would bring extensive changes that will also affect all aspects of our society and life. In addition, its impact on firms and employment will be considerable, resulting in richly interconnected organizations with decision making based on the analysis and exploitation of \"big\" data and intensified, global competition among firms. People will be capable of buying good and obtaining services from anywhere in the world using the Internet, and exploiting the unlimited, additional benefits that will open through the widespread usage of AI inventions. The paper concludes that significant competitive advantages will continue to accrue to those utilizing the Internet widely and willing to take entrepreneurial risks in order to turn innovative products/services into worldwide commercial success stories. The greatest challenge facing societies and firms would be utilising the benefits of availing AI technologies, providing vast opportunities for both new products/services and immense productivity improvements while avoiding the dangers and disadvantages in terms of increased unemployment and greater wealth inequalities.", "keywords": "Artificial Intelligence (AI),Industrial Revolution,Digital Revolution,AI", "introduction": "", "body": "First, 190 years is a brief period by historical standards and during this period we went from horses being the major source of transportation to self-driving cars and from the abacus and slide rules to powerful super computers in our pockets. Secondly, the length of time between technological inventions and their practical, widespread use is constantly being reduced. For instance, it took more than 200 years from the time Newcomen developed the first workable steam engine in 1707 to when Henry Ford built a reliable and affordable car in 1908. It took more than 90 years between the time electricity was invented and its extensive use by firms to substantially improve factory productivity. It took twenty years, however, between ENIAC, the first computer, and IBM's 360 system that was mass produced and was affordable by smaller business firms while it took only ten years between the invention of the mobile phone in 1973 by Dr Martin Cooper and its public launch by Motorola. The biggest and most rapid progress, however, took place with smartphones which first appeared in 2002 and saw a stellar growth with the release of new versions possessing substantial improvements every one or two years by the likes of Apple, Samsung and several Chinese firms. Smartphones, in addition to their technical features, now incorporate artificial intelligence characteristics that include understanding speech, providing customized advice in spoken language, completing words when writing a text and several other functions requiring embedded AI, provided by a pocket computer smaller in size than a pack of cigarettes.From smart machines to clever computers and to Artificial Intelligence (AI) programs:A thermostat is a simple mechanical device exhibiting some primitive but extremely valuable type of intelligence by keeping temperatures constant at some desired, pre-set level.Computers are also clever as they can be instructed to make extremely complicated decisions taking into account a large number of factors and selection criteria, but like thermostats such decisions are pre-programmed and based on logic, if-then rules and decision trees that produce the exact same results, as long as the input instructions are alike. The major advantage of computers is their lightning speed that allows them to perform billions of instructions per second. AI, on the other hand, goes a step further by not simply applying pre-programmed decisions, but instead exhibiting some learning capabilities.The reading of handwritten digits (first utilized to determine the written amount on bank checks) by the neural net device in 1990 (see Table The story of the Watson computer beating Jeopardy's two most successful contestants is more complicated, since retrieving the most appropriate answer out of the 200 million pages of information stored in its memory is not a sign of real intelligence as it relied on its lightning speed to retrieve information in seconds. What is more challenging according to Jennings, one of Jeopardy's previous champions, is \"to read clues in a natural language, understand puns and the red herrings, to unpack just the meaning of the clue\" (May, 2013). Similarly, it is a sign of intelligence to improve its performance by \"playing 100 games against past winners\". Computers and real learning: According to its proponents, \"the main focus of AI research is in teaching computers to think for themselves and improvise solutions to common problems\" From digital computers to AI tools:The Intel Pentium microprocessor, introduced in 1993, incorporated graphics and music capabilities and opened computers up to a large number of affordable applications extending beyond just data processing. Such technologies signalled the beginning of a new era that now includes intelligent personal assistants understanding and answering natural languages, robots able to see and perform a intelligent functions, self-driving vehicles and a host of other capabilities which were until then an exclusive human ability. The tech optimists ascertain that in less than 25 years computers went from just manipulating 0 and 1 digits, to utilizing sophisticated neural network algorithms that enable vision and the understanding and speaking of natural languages among others. Technology optimists therefore maintain there is little doubt that in the next twenty years, accelerated AI technological progress will lead to a breakthrough, based on deep learning that imitates the way young children learn, rather than the laborious instructions by tailor-made programs aimed for specific applications and based on logic, if-then rules and decision trees For instance, DeepMind is based on a neural program utilizing deep learning that teaches itself how to play dozens of Atari games, such as Breakout, as well or better than humans, without specific instructions for doing so, but by playing thousands of games and improving itself each time. This program, trained in a different way, became the AlphaGo that defeated GO champion Lee Sodol in 2016. Moreover, it will form the core of a new project to learn to play Starcraft, a complicated game based on both long term strategy as well as quick tactical decisions to stay ahead of an opponent, which DeepMind plans to be its next target for advancing deep learning Google had two deep learning projects underway in 2012. Today it is pursuing more than 1,000, according to their spokesperson, in all its major product sectors, including search, Android, Gmail, translation, maps, YouTube, and self-driving cars (The Week, 2016). IBM's Watson system used AI, but not deep learning, when it beat the two Jeopardy champions in 2011. Now though, almost all of Watson's 30 component services have been augmented by deep learning. Venture capitalists, who did not even know what deep learning was five years ago, today are wary of start-ups that do not incorporate it into their programs. We are now living in an age when it has become mandatory for people building sophisticated software applications to avoid click through menus by incorporating natural-language processing tapping deep learning How far can deep learning go? There are no limits according to technology optimists for two reasons. First as progress is available to practically everyone to utilize through Open Source software, researchers will concentrate their efforts on new, more powerful algorithms leading to cumulative learning. Second and equally important, in the future intelligent computer programs will be capable of writing new programs themselves, initially perhaps not so sophisticated ones, but improving with time as learning will be incorporated to be part of their abilities. For some people these predictions are startling, with far-fetched implications should they come true. In the next section, four scenarios associated with the AI revolution are presented and their impact on our societies, life work and firms is discussed.The Four AI ScenariosUntil rather recently, famines, wars and pandemics were common, affecting sizable segments of the population, causing misery and devastation as well as a large number of deaths. The industrial revolution considerably increased the standards of living while the digital one maintained such rise and also shifted employment patterns, resulting in more interesting and comfortable office jobs. The AI revolution is promising even greater improvements in productivity and further expansion in wealth. Today more and more people, at least in developed countries, die from overeating rather than famine, commit suicide instead of being killed by soldiers, terrorists and criminals combined and die from old age rather than infectious disease The Optimists: Kurzweil and other optimists predict a \"science fiction\", utopian future with Genetics, Nanotechnology and Robotics (GNR) revolutionizing everything, allowing humans to harness the speed, memory capacities and knowledge sharing ability of computers and our brain being directly connected to the cloud. Genetics would enable changing our genes to avoid disease and slow down, or even reverse aging, thus extending our life span considerably and perhaps eventually achieving immortality. Nanotechnology would enable us to create virtually any physical product from information and inexpensive materials bringing an unlimited creation of wealth. Finally, robots would be doing all the actual work, leaving humans with the choice of spending their time performing activities of their choice and working, when they want, at jobs that interest them.The Pessimists: In a much quoted article from Wired magazine in 2000, Bill Joy Harari is the newest arrival to the ranks of pessimists. His recent book Harari admits that nobody really knows how technology will evolve or what its impact will be. Instead he discusses the implications of each of his three questions: \uf0b7 If indeed organisms are algorithms then thinking machines utilizing more efficient ones than those by humans will have an advantage. Moreover, if life is just data processing then there is no way to compete with computers that can consult/exploit practically all available information to base their decisions. \uf0b7 The non-conscious algorithms Google search is based on the consultation of millions of possible entries and often surprise us by their correct recommendations,. The implications that similar, more advanced algorithms than those utilized by Google search will be developed (bearing in mind Google search is less than twenty years old) in the future and be able to access all available information from complete data bases are far reaching and will \"provide us with better information than we could expect to find ourselves\". \uf0b7 Humans are proud of their consciousness, but does it matter that self-driving vehicles do not have one, but still make better decisions than human drivers, as can be confirmed by their significantly lower number of traffic accidents?When AI technologies are further advanced and self-driving vehicles are in widespread use, there will come a time that legislation will be passed forbidding human driving. Clearly, selfdriving vehicles do not exceed speed limits, do not drive under the influence of alcohol or drugs, do not get tired, do not get distracted by talking on the phone or sending emails and in general make fewer mistakes than human drivers, causing fewer accidents. There are two implications if humans are not allowed to drive. First, there will be a huge labour displacement for the 3.5 million unionized truck drivers in the USA and the 600 thousand ones in the UK (plus the additional number of non-unionized ones) as well as the more than one million taxi and Uber drivers in these two countries. Second, and more importantly, it will take away our freedom of driving, admitting that computers are superior to us. Once such an admission is accepted there will be no limits to letting computers also make a great number of other decisions, like being in charge of nuclear plants, setting public policies or deciding on optimal economic strategies as their biggest advantage is their objectivity and their ability to make fewer mistakes than humans.One can go as far as suggesting letting computers choose Presidents/Prime Ministers and elected officials using objective criteria rather than having people voting emotionally and believing the unrealistic promises that candidates make. Although such a suggestion will never be accepted, at least not in the near future, it has its merits since people often choose the wrong candidate and later regret their choice after finding out that pre-election promises were not only broken, but they were even reversed. Critics say if computers do eventually become in charge of making all important decisions there will be little left for people to do as they will be demoted to simply observing the decisions made by computers, the same way as being a passenger in a car driven by a computer, not allowed to take control out of the fear of causing an accident. As mentioned before, this could lead to humans eventually becoming computers' pets.The Pragmatists: At present the vast majority of views about the future implications of AI are negative, concerned with its potential dystopian consequences (Elon Musk, the CEO of Tesla, says it is like \"summoning the demon\" and calls the consequences worse than what nuclear weapons can do). There are fewer optimists and only a couple of pragmatists like Sam Altman and Michio Kaku The doubters: The doubters do not believe that AI is possible and that it will ever become a threat to humanity. A more sophisticated attack comes from doubters who state that it is wrong to believe that once computers have been provided with sufficiently advanced algorithms, they will be able to improve and then replicate the way our mind works. According to them The timing and expected impact of the AI Revolution: Kurzweil predicted that computers will reach human intelligence around 2029 The second prediction being asked was: \"I believe that AGI (however I define it) will be a net positive event for humankind\". Among the 60 participants, 51, that is 85% answer \"Yes\" while the remaining 15% said \"No\".In a similar survey The median answer for the 10% probability was 2022, the 50% probability was 2040 and the 90% probability the year 2075. The timing from the answers of the two surveys is not far apart from those of Kurzweil (although they may have been influenced by them), agreeing that AGI or HLMI are not so far off and with the majority of scientists believing that AI will have a positive effect for humankind.In a new survey conducted in early March 2016, Etzioni (2016) posed the following question:\"In his book, Nick Bostrom has defined Superintelligence as 'an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.' When do you think we will achieve Superintelligence?\"The answers of the 80 responders (a 41% rate) are summarized below: 0%In the next 10 years 7.5%In the next 10-25 years 67.5%In more than 25 years 25.0% NeverThese dates put the start of superintelligence later than that of Kurzweil or those of the previous surveys but the answers refer to \"superintelligence\" rather than AI. It is also interesting that only 25% of respondents answer \"never\".Firms and employment: From the Industrial and digital to the AI revolutionThe industrial revolution brought far reaching changes to firms and employment while those of the digital continued the decrease of employment in agriculture and manufacturing while contributing to strong increases in services, in particular in computers, the Internet and the mobile phone markets. The digital revolution also resulted in the decrease of the large industrial firm. The expected changes being brought by AI technologies will be just as, or even more significant as those of the Industrial revolution and much harder to predict for two reasons. First, they will depend on the speed that AI technologies will succeed in automating mental tasks currently performed by humans and replacing them in the process, and secondly the extend of the accelerated process of technological change as intelligent computer programs will become available and capable of developing new programs on their own. There is no doubt, therefore, that AI technologies coupled with the exponential growth of the Internet will affect how firms operate, how they sell their products/services as well as how they are managed, influencing employment patterns.In 1995 when the digital revolution was in its infancy, in a Newsweek article in February of this year, Clifford Stoll wrote \"Baloney. Do our computer pundits lack all common sense? The truth is no online database will replace your daily newspaper, no CD-ROM can take the place of a competent teacher and no computer network will change the way government works\" What is even more interesting than just \"largeness\" is the market capitalization of digital versus traditional firms. Table The four digital firms are at the forefront of AI, investing huge amounts of money for internal AI research as well as buying promising start ups The successful, dominant firm of the AI revolution and its managementRevolutions, by definition, are associated with major changes. The Industrial one brought the large, industrial firm that exploited the power of machines to substitute, supplement and amplify the manual work performed by humans, increasing productivity considerable and offering affordable products to consumers, significantly increasing market size and living standards. The digital revolution exploited the power of computers to substitute, supplement and amplify the routine mental tasks performed by humans also improving productivity and further contributing to reduced prices. As previously mentioned, the AI revolution aims to substitute, supplement and amplify practically all tasks currently performed by humans, becoming in effect, for the first time, a serious competitor to them.  Accepting this watershed reality, setting aside criticisms about AI similar to those levied in 1995 against the Internet In 1995 from the four digital firms listed in Table First and foremost all four firms have been extremely innovative, each one in its own unique way. Second, they have all used the Internet in a super effective manner to provide their services, sell their products and streamline their operations. Third, they have been successful in hiring top talent and motivate their top employees with a pleasant work environment, high salaries and generous stock options. Finally, these four firms have grown significantly by acquiring other companies, often promising start-ups in the selective areas they want to expand or attain expertise. Interestingly none of these four characteristics can be automated, or become part of an algorithm, at least during the next twenty years. In my view, they will continue to remain critical factors for succeeding in the future and will depend greatly on people's decisions and actions to implement them. Furthermore, algorithms will have to be modified as competitive, market, environment and other factors will be changing and only humans will be able to identify when such changes have occurred (otherwise, the algorithms will be dysfunctional). Below each of these four success factors is discussed in further detail.Innovative breakthroughs: There are different types of innovation from minor, marginal ones to distinctive breakthroughs as those implemented by the four digital firms of Table It cannot be assured that by identifying interesting innovative ideas they will turn into commercially successful applications. The road is long and full of risks as many things can go wrong, including technological problems, inability to ensure adequate financing, competitors developing similar ideas and possible delays until people realize their importance and decide to use them. Moreover, it is worth mentioning that the great majority of innovative ideas do not succeed in a big way, that new competitors can improve the original idea and capture market shares and that many inventions can become obsolete by more novel ones often introduced by garage start-ups (see below). Creative destruction is at the heart of market economies and the major contributor to productivity increases as well as the offering of a greater choice of product/services to consumers. No concrete advice on how to innovate, not to mention achieve breakthroughs, can be provided beyond saying that some organizations are better than others, at least for some period of time, in creating the right environment to cultivate innovation and exploit its benefits. It must be emphasized, therefore, that success is unpredictable and that the more valuable the innovative idea the harder it is to conceive and implement.Technologies and their Usage:The digital revolution has provided significant productivity improvements for the back office operations of firms and allowed consumers to buy goods and services online without having to physically go to a store or office. In the process it has eliminated a large number of repetitive mental tasks performed by people and shifted employment patterns. This is evident in Table The uniqueness of the AI technologies is their potential to supplement, substitute and amplify practically all tasks currently performed by humans with critical consequences for firms that must achieve significant productivity improvements to stay competitive, but at the same time raising the possibility of increased unemployment (see below). Extrapolating from the impact of the Industrial and digital revolutions it seems that technology has created more jobs than it has destroyed Managing people:It is also highly likely that the trend towards a smaller payroll will continue as AI technologies will accelerate the number of tasks that can be performed by machines and robots. The more jobs being automated however, the greater the skills that would be required to adequately perform the remaining tasks, for both the efficient operation of firms as well as for utilizing AI and other technologies in the best possible way. This would require hiring talented employees and motivating them to get the most out of their performance in order to attain and maintain competitive advantages over other firms. The major differentiator would therefore come from these talented individuals conceiving and implementing innovative ideas and winning strategies that would steer the organization onto a successful path. Hiring, motivating and successfully managing talented individuals would probably be one of the most critical success factors for firms in the AI era and would also be impossible to put into an algorithm.Growth by acquisition:The four digital firms of Table Together they have acquired more than 400 firms Competitive Requirements: Big Data, Algorithmic Decisions and Operational ExcellenceLet us assume that an advanced algorithm is developed to predict exactly what customers want. What will the result be? Even if it is proprietary, competitors will inevitably figure out how it works and also utilize its predictions after some period of time. If it is available to everyone at a price, all competitors will swiftly adopt this algorithm to stay competitive. Those who do not utilize it will probably go out of business given its superiority to predict customers' wants. There will be no special benefit as the advanced algorithm would simply become a competitive requirement for staying in the race. Big data The 2037 Successful FirmThe successful 2037 AI era firms cannot be predicted but they will probably be closer to the digital than the traditional ones of Table The successful firms during the AI revolution will have in addition to the Chief Innovation Officer (CIO) to also appoint a Chief Artificial Intelligence Officer (CAIO) to be in charge of evaluating and exploiting AI technologies to gain the most out of their implementation in all aspects of the firm.Garage start-ups, crowd sourcing and VC FundingThe way the four digital firms of Table The widespread utilization of the Internet has provided two significant tools for start-ups that can utilize crowd funding and crowd creativity to improve their chances of success. Such tools coupled with VC funding will increase the number of new firms aimed at exploiting AI technologies and the chance to succeed in big ways like Google, Amazon or Facebook. Breakthrough ideas can come from anywhere and they do not require expensive laboratories or huge financing to develop and market. This means that garage operations, small offices in incubators or accelerators, as well as cubicles in universities, can provide an initial base for start-ups that can emerge everywhere outside Silicon Valley as their value in wealth creation is becoming obvious. Today a large number of countries and individuals understand the importance of innovation and entrepreneurship for their economies and encourage their youth to implement their innovative ideas by starting their own business. Israel has been extremely successful in doing so Employment patternsThe 1995 paper showed the continuous decline in the agricultural and manufacturing (since it peaked in the 1950s) employment and the growth in that of services. Table AgricultureManufacturingServicesWhat critics fear is the speed of job obsolescence through AI technologies in particular in the service sector. They say that while it took more than two centuries to witness the full impact of the Industrial revolution and three or four decades to experience that of the digital one, it may be no more than a decade until we observe the full effects of the AI revolution In addition to the truck and taxi drivers already mentioned that could be replaced by selfdriving vehicles, there are many other jobs at stake. According to PBS A recent McKinsey study entitled \"Where machines could replace humans -and where they can't (yet)\" In an article entitled \"Will Humans Go the Way of Horses? Labour in the Second Machine Age\" In addition to specialized AI professions, employment opportunities will exist by moving from traditional jobs susceptive to automation, to those demanding social and interpersonal skills as well as creativity and innovation. There will also be a demand for novel jobs that will aim to satisfy the needs of higher income people that can afford personal trainers, coaches, beauty advisors, diet consultants, nutritionists and teachers for their children, among others. These and similar jobs will offer employment opportunities and adequate payment, and will compensate for those lost when AI technologies will replace existing jobs.Finally, the impact of the AI revolution will probably be more pronounced in developing countries than in advanced ones for two reasons. First, as unskilled and semiskilled labour will be replaced by computers and robots there will be no reason for firms to move their production to developing nations to exploit their cheap supply of labour as they can achieve the same or cheaper costs utilizing AI technologies, thus increasing the trend towards \"reshoring\" back to advanced countries Work and LeisureMachines, electrification, cars, computers, the Internet and smartphones among others inventions have affected our lives and shaped our work and social environment. The impact of AI technologies can be even more profound than that of both the Industrial and digital revolutions put together, as it holds the potential to affect practically all tasks currently performed by humans, diminishing the amount of work left for people and increasing wealth inequalities and their free time. Proponents of the AI technologies see this as a positive development liberating people from routine work, allowing them to pursue their own interests. The critics say that this will further increase inequalities as fewer people will hold the well-paying jobs and the majority will depend on part time work or limited employment opportunities and therefore receive a lower income. There are no obvious answers to this argument and it will all depend as Brynjolfsson and McAfee say \"what kind of society we should construct around a labor-light economy? How should the abundance of such an economy be shared?\" At the beginning of the Industrial revolution people used to work fifteen hours a day, five days a week. Today a standard work week is less than half and there is no reason that it cannot be halved in the future as productivity increases.Aristocrats did not work at all in the past, devoting all their time to leisurely activities, hobbies, holidays and travel and things they were interested in. The citizens of ancient Athens spent practically all their time philosophising, exercising and concerned with democracy, while slaves did all the work. The optimists dream is that we can all become the \"new\" aristocrats, or the \"modern\" Athenians, with computers and robots as our slaves doing all the housework, the shopping and working at the office, the factory or land. Whether this dream is a utopian or dystopian future is left up to the reader to decide, not underestimating however, that intelligent machines will eventually become at least as smart as us and a serious competitor to the human race if left unchecked and if their great potential to augment our own abilities is not exploited to the maximum.Conclusions", "conclusions": "The societal impact of the digital revolution has been significant as it has affected most aspects of our lives and work, having molded the dominant firm, shaped our shopping and entertainment habits as well as our employment patterns. This paper argues that the AI revolution is on target and will come into full force within the next twenty years as did the digital one since 1995 and will probably have an even greater impact than both the Industrial and digital ones combined. What is uncertain is if such an impact will lead to a utopian or dystopian future, or somewhere in between, which according to the optimists will happen when some mutually beneficial coexistence is achieved by accepting our limitations and seeking ways to augment our own decision making abilities in a similar way that the world chess champion is now a human utilizing the help of a computer to determine his moves. The pessimists worry that when AI machines become smarter than us they will be making all our decisions for us, from the less important ones like SIRI or Cortana choosing a restaurant and ordering food and wine for their owners, to important ones like driving cars or managing nuclear plants. Pessimists are also concerned with social discontent as the amount of work available for people will diminish, and with the increasing wealth inequality between those employed or owning AI applications and the rest.This paper has described the substantial uncertainty about the future impact of AI technologies and their potential to create a utopian or dystopian world. As we head towards unchartered waters and uncertain choices the obvious challenge is what can be done to maximize the chances of exploiting the benefits while avoiding the negative consequences of AI technologies? There are two positive aspects in dealing with this challenge. First, the dangers are well understood and secondly, there is plenty of time to debate the issues and take wise actions to deal with them effectively. Increased unemployment and greater wealth inequality are debatable given that the Industrial and digital revolutions decreased rather than increased unemployment , although some claim that this may change drastically with the widespread introduction of AI technologies leading to massive job reductions and bringing us towards \u0397uxley's Brave New World. Scientists like Etzioni (2016), however, do not believe that AI is a threat to humanity, maintaining that it would be a great pity to forfeit its great advantages out of unsubstantiated fears that it may get out of control. There is little doubt that new technological breakthroughs similar to those of the Internet and smartphones of the digital revolution will probably emerge during the next twenty years and that will greatly impact our societies, lives and work in general. The challenge will be to identify such breakthroughs as early as possible in order to exploit their benefits.(Stewart, Debapratim and Cole, 2015)Concerns about the AI related risks are not unique. Possible catastrophes, in particular those that threaten to destroy the environment or end human civilization attract people's attention and are popular among both scientists seeking to publicize their work and journalists looking for \"cool\" stories. Nuclear wars, global warming, accidents in nuclear plants, unchecked epidemics, and geoengineering, among others, worry people who are concerned about their implications. At the same time it is clear that AI risks cannot be ignored even though the probability of their occurrence may be extremely small as their potential impact, according to critics, can be devastating, ending human supremacy when machines smarter than people are developed. At the same time progress cannot be halted which means that the only rational alternative is to identify the risks involved and devise effective actions to avoid their negative consequences. In ending this paper I would like to quote  saying: \"Thinking about the risks associated with emerging AI technology is hard work, engineering potential solutions and safeguards is harder work, and collaborating globally on implementation and monitoring of initiatives is the hardest work of all. But considering all that's at stake, I would place all my bets on the table and argue that the effort is worth the risk many times over\".Faggella (2016)", "SDG": [9]}, "detecting_price_manipulation_in_the_financial_market": {"name": "Detecting Price Manipulation in the Financial Market", "abstract": " Market abuse has attracted much attention from financial regulators around the world but it is difficult to fully prevent. One of the reasons is the lack of thoroughly studies of the market abuse strategies and the corresponding effective market abuse approaches. In this paper, the strategies of reported price manipulation cases are analysed as well as the related empirical studies. A transformation is then defined to convert the time-varying financial trading data into pseudostationary time series, where machine learning algorithms can be easily applied to the detection of the price manipulation. The evaluation experiments conducted on four stocks from NASDAQ show a promising improved performance for effectively detecting such manipulation cases. I.", "keywords": "", "introduction": "Surveillance of the financial exchange market for monitoring market abuse activities has attracted much attention from financial regulators across different exchange markets in recent years especially since the flash crash in 2010. However, the lack of research in effective and efficient detection algorithms, in both industry and academia, causes challenges for regulators in their ability to monitor huge amounts of trading activities in real-time. A major concern to financial market abuse is price manipulation, where the manipulated target is the bid (or ask) price of certain financial instruments . There is a large amount of literature regarding stock market manipulation theories [1] [2] [1] and a few empirical studies of real manipulation cases [3]. However, an effective detection model of price manipulation is yet to be developed due to the lack of understanding of strategic spoofing tactics.[4]In this paper, we summarize and further analyse the price manipulation strategies by examining actual reported cases as well as the empirical studies in existing literature. We define two key characteristics of price manipulation strategies, which enable us to propose a transformation procedure, converting the original market trading data to a comparable metric, where the non-stationary nature of the financial data is demonstrated to be \"nearly removed\" and the machine learning techniques can then be effectively applied as detection models. Our proposed detection approach is evaluated based on real trading records of selected stocks from NASDAQ.The remainder of this paper is organized as follows: Section II provides a brief review of price manipulation and the corresponding detection methods. In Section III, the price manipulation tactics are thoroughly analysed. A data transform procedure is then proposed and illustrated with real trading data. Section IV evaluates the proposed machine learning based detection models and the obtained promising performance is reported. Finally Section V concludes the paper and discusses potential improvements and future work.", "body": "Surveillance of the financial exchange market for monitoring market abuse activities has attracted much attention from financial regulators across different exchange markets in recent years especially since the flash crash in 2010. However, the lack of research in effective and efficient detection algorithms, in both industry and academia, causes challenges for regulators in their ability to monitor huge amounts of trading activities in real-time. A major concern to financial market abuse is price manipulation, where the manipulated target is the bid (or ask) price of certain financial instruments In this paper, we summarize and further analyse the price manipulation strategies by examining actual reported cases as well as the empirical studies in existing literature. We define two key characteristics of price manipulation strategies, which enable us to propose a transformation procedure, converting the original market trading data to a comparable metric, where the non-stationary nature of the financial data is demonstrated to be \"nearly removed\" and the machine learning techniques can then be effectively applied as detection models. Our proposed detection approach is evaluated based on real trading records of selected stocks from NASDAQ.The remainder of this paper is organized as follows: Section II provides a brief review of price manipulation and the corresponding detection methods. In Section III, the price manipulation tactics are thoroughly analysed. A data transform procedure is then proposed and illustrated with real trading data. Section IV evaluates the proposed machine learning based detection models and the obtained promising performance is reported. Finally Section V concludes the paper and discusses potential improvements and future work.II. REVIEW OF RELATED LITERATURETheoretical studies of the stock price manipulation were presented in a number of existing works. A model of transaction-based manipulation was developed, showing that price manipulation is profitable Research regarding the detection of the stock price manipulation is comparably limited in both academia and the financial industry. The appropriateness of a sample entropy methodology as a measure for the detection was evaluated in To date, existing research has mainly focused on either empirical studies of certain price manipulation cases or the detection techniques based on abnormalities of the market features during the manipulation period. An effective classification algorithm was shown in In this paper, the manipulative strategies are analysed with no assumptions on unusual changes of market features. The proposed detection approaches are aimed at learning and modelling the trading behaviours and further identifying the manipulative actions by the learned model. Our approach is evaluated in a real data context.III. CHARACTERISING PRICE MANIPULATIONPrice manipulation activities affect price fluctuation in capital markets, where the returns, volatilities and liquidities, unexpectedly rise then decline during the manipulation period A. Price manipulation strategy characteristicsA generic price manipulation tactic is defined as artificially pushing up (or down) the bid (or ask) price of a security and taking advantage of the shifted price so as to make a profit The definition of the primary manipulation tactic, spoofing trading, summarized from the real manipulation cases, is given as: an order with a size at least twice the previous day's average order size, with a price at least 6 basis points (bps) 1  away from the current bid (or ask) price and with a cancellation time longer than 30 minutes In September 2012, a price manipulation case was reported and documented by the Financial Industry Regulatory Authority of the USA Spoofing trading and quote stuffing suggest two primary strategies of price manipulation. The former utilises a large volume and a passive quote for inducing the impact and reducing the risk while an aggressive quote and a tiny cancellation time are used in the latter, respectively. Both formats can be depicted by two key conditions defined in our argument.The two strategies are graphically illustrated in Fig. B. Market Data TransformationFinancial data is usually considered as a non-stationary time series Here, we propose to transform the original data to a stationary domain while maintaining the desired features, which may bring an alternative computationally efficient solution. This idea is also reflected in a time series analysis, such as the autoregressive integrated moving average (ARIMA) model which is usually applied in cases where the non-stationary feature of the data can be removed by an initial differencing step Inspired by the differencing step and log-return methods, we define a transformation procedure as converting the order data to a consistent and comparable metric. On one hand, this procedure transforms the original data to pseudo-stationary; on the other hand, it enables the evaluation of analytic relationships amongst stocks despite the original unequal values of the order series.Denoting as a time index, indicating all order book activities, i.e., submission, cancellation and execution, a buy (or sell) order (or ) is described as a three dimensional vector, , ,), where p, v and t represent order price, volume and submission time (physical time) respectively. Furthermore, and denote the best bid and ask price instantaneously before the order activity. is denoted as the length of a sliding window and is set to one trading day, corresponding with the spoofing trading definition. Thus, \u0305 and \u0305 define the moving average volume of the buy and sell orders in the previous period of time excluding the current data point . The cancellation times and execution times of order (or ) are denoted by and (or and ) and the lifecycle of an order / can then be defined asfor the cancelled or executed order respectively. Thus the average lifecycle of orders in the prior period are calculated asThe transformation is then defined in Equations ( The stationary nature of a time series ( , ) is usually tested by the weak definition of stationarity To evaluate the stationarity, we calculate the mean, variance and AutoCor for the original and transformed data of the trading orders. In the calculation, we consider each order attribute as a single time series thus the original and the transformed data are represented as three time series respectively:The top four stocks in NASDAQ in terms of the total market capital, Apple, Google, Intel and Microsoft, are selected for evaluation. The datasets, obtained from LOBSTER project The calculated mean and variance of three attributes, price, volume and time, for the Intel dataset are shown as an example in Fig. When further observing the dispersion of the sequences of calculated mean and variance, the measure of the dispersion of a given sequence, the coefficient of variation (CV), defined as the ratio of standard deviation to the mean of a data sequence, is further calculated. The CV of the mean and variance sequences under different lag values for three attributes, price, volume and time are calculated for both the original data and the transformed data across four datasets and shown in Table The significantly smaller CV values of the mean and variance sequences of the transformed price, volume and time compared with the original data show far lower level of dispersion in the transformed data sequences, which indicates \"nearly constant\" mean and variance values. The \"nearly constant\" mean and variance partially conform to the weak definition of stationarity. As a time series matches up perfectly with itself ('zerolag'), the figures in Fig. C. Strategic behaviour illustrationGiven the \"non-stationarity\" removing transformation and the characteristics of the manipulation strategy previously discussed, we illustrate the converted data by denoting the x, y and z-axis by transformed price p , volume v and time t respectively. The trading orders are then represented in a new 3-dimensional domain as shown in the example in Fig. Fig. Similarly, the normal orders data of other selected stocks shows a similar pattern to Fig. Since the manipulation cases are apparently located apart from the cluster (as Fig. IV. DETECTING PRICE MANIPULATIONThe concept of describing the shape of a cluster of normalities is often referred to as novelty detection, where the abnormalities are subsequently identified by testing for novelty against the model of the normality. The mass of normal trading data and the scarceness of the manipulation cases, due to the regulatory rules prohibiting the disclosure of illegitimate market cases, are another reason that the novelty detection paradigm suits the detection of such manipulation. K-Nearest Neighbour (kNN) based novelty detection is one of the simplest non-parameter methods. It simply computes the distances between the test data and all training samples and uses the lowest distance score plus a threshold (a radius around that example) to make the novelty decision. In general, the Euclidean distance is used between two samples In this paper, we examine the price manipulation detection problem using the above-mentioned two machine learning models on the transformed time series as well as the original market data. We argue that both models work effectively on the underlying detection problem and the proposed transformation procedure significantly improves the detection performance.A. Application of OCSVM and kNN to price manipulation detectionWhen applying the novelty detection approaches to the price manipulation detection problem, a set of normal data vectors = { , , \u2026 , } is collected as the training dataset. The vector is from either the vector of the original market data , , ( : a buy or sell order), or the transformed data , , , calculated by Equations ( The evaluation of a detection model is usually reliant upon the labelled benchmarks of both normality and abnormality. Due to a few real manipulation cases being reported, we needed to synthesize a number of abnormal cases based on our study of the characteristics of the manipulation strategy. Synthetic exploratory financial data is accepted in academia for evaluating the proposed model when real market data is hard to collect The generated manipulation cases are then randomly injected into the corresponding order records, creating a mixture of both \"normal\" and \"abnormal\" patterns in the testing datasets. In order to ensure comprehensive assessment of the approach, 5000 synthesized cases are injected to each dataset with each type containing 2500 examples. For the Apple stock dataset, the reported real cases are also injected for evaluation.In our experiments, LIBSVM Performance evaluation is based on the Receiver Operating Characteristic (ROC), which is calculated according to the confusion matrix, where false positive (FP), is defined as manipulation cases detected as normal, false negative (FN) is defined as normal cases detected as manipulation, true positive (TP) is defined as normal cases detected as normal and true negative (TN) is defined as manipulation cases detected as manipulation. The ROC curve is a widely used metric for evaluating and comparing binary classifiers. The ROC curve plots the true positive rate against the false positive rate while the discrimination threshold of the binary classifier is varied. In order to assess the overall performance of a novelty detector, one can measure the area under the ROC curve (AUC). Larger AUC values are generally an indication of better classification performance.The ROC curves of two models on four stock datasets (original and transformed) with 5000 injected novelties in each dataset are illustrated in Fig. Meanwhile, as shown in Fig. It is also noted that the OCSVM outperforms the kNN across all four different datasets. The higher performance of OCSVM may be due to a better description of the clusters of normal cases through a more accurate description of the boundary by support vectors.V. CONCLUSION AND FUTURE WORK", "conclusions": "This paper analysed the price manipulation in the financial market and defined its key features. A transformation procedure is proposed for mapping non-stationary market data to pseudo stationary attributes, to which the machine learning algorithms can easily be applied as a detection model. The evaluation, which has been conducted on top four stocks from NASDAQ in terms of the market capital, shows promising performance in terms of area under the ROC curve.In the proposed method, the stationary nature of the data is tested separately on three attributes, which however, have been modelled by OCSVM and kNN as a feature vector. The study of the stationarity of the order vector and the corresponding detection model updating (re-training) will be the focus of our future work. Furthermore, in recent years, the market manipulation tends to be carried out in more than one exchange market by some tricky manipulators. Detection within any single market hardly achieves a complete and accurate result. This requires a cross-market detection model, which is also one of our primary future works. ", "SDG": [10]}, "measuring_discrimination_in_algorithmic_decision_making": {"name": "Measuring discrimination in algorithmic decision making", "abstract": " Society is increasingly relying on data-driven predictive models for automated decision making. This is not by design, but due to the nature and noisiness of observational data, such models may systematically disadvantage people belonging to certain categories or groups, instead of relying solely on individual merits. This may happen even if the computing process is fair and well-intentioned. Discriminationaware data mining studies of how to make predictive models free from discrimination, when the historical data, on which they are built, may be biased, incomplete, or even contain past discriminatory decisions. Discrimination-aware data mining is an emerging research discipline, and there is no firm consensus yet of how to measure the performance of algorithms. The goal of this survey is to review various discrimination measures that have been used, analytically and computationally analyze their performance, and highlight implications of using one or another measure. We also describe measures from other disciplines, which have not been used for measuring discrimination, but potentially could be suitable for this purpose. This survey is primarily intended for researchers in data mining and machine learning as a step towards producing a unifying view of performance criteria when developing new algorithms for non-discriminatory predictive modeling. In addition, practitioners and policy makers could use this study when diagnosing potential discrimination by predictive models.", "keywords": "Discrimination-aware data mining,Fairness-aware machine learning,Accountability,Predictive modeling,Indirect discrimination", "introduction": "Nowadays, increasingly many decisions for people and about people are made using predictive models built on historical data, including credit scoring, insurance, personalized pricing and recommendations, automated CV screening of job applicants, profiling of potential suspects by the police, and many more cases. The penetration of data mining and machine learning technologies, as well as decisions informed by big data has raised public awareness that data-driven decision making may lead to discrimination against groups of people (Angwin and Larson 2016;Burn-Murdoch 2013;Corbett-Davies et al. 2016;Dwoskin 2015;Nature Editorial 2016;The White House 2016;. Such discrimination may often be unintentional and unexpected, assuming that algorithms must be inherently objective. Yet decision making by predictive models may discriminate against people, even if the computing process is fair and well-intentioned Miller 2015)(Barocas and Selbst 2016;Calders and Zliobaite 2013;. This is because most data mining methods are based upon assumptions that historical datasets are correct, and accurately represent population, which often appears to be far from reality.Citron and Pasqualle 2014)Discrimination-aware data mining is an emerging discipline that studies how to prevent potential discrimination due to algorithms. It is assumed that non-discrimination regulations prescribe which personal characteristics are considered sensitive, or which groups of people are to be protected. The regulations are assumed to be defined externally, typically by national or international legislation. The research goal in discrimination-aware data mining is to translate those regulations mathematically into non-discrimination constraints, and develop predictive modeling algorithms that would be able to take into account those constraints, and at the same time be as accurate as possible. These constraints prescribe how much of differences between groups can be considered explainable. In a broader perspective, research needs to be able to computationally explain the roots of such discrimination events before increasing public concerns lead to unnecessarily restrictive regulations against data mining.In the last few years researchers have been developing discrimination-aware data mining algorithms using a variety of performance measures. Yet there is a lack of consensus of how to define the fairness of predictive models, and how to measure their performance in terms of non-discrimination. Often research papers propose new ways to quantify discrimination, and new algorithms that would optimize that measure. The existing variety of evaluation approaches makes it difficult to compare results and assess progress in the discipline; furthermore, the variety of measures makes it difficult to recommend computational strategies to practitioners and policy makers.The goal of this survey is to develop a unifying view towards discrimination measures in data mining and machine learning, and analyze the implications of optimizing one or another measure in predictive modeling. Therefore, it is essential to develop a coherent view early in the development of this research field, in order to present task settings in a systematic way for follow up research, to enable systematic comparison of approaches, and to facilitate a discussion hopefully aimed at reaching a consensus among researchers in terms of the fundamentals of the discipline. For this purpose we review and categorize measures that have been used in data mining and machine learning, and also discuss measures from other disciplines, such as feature selection, which in principle could be used for measuring discrimination. We complement the review by experimental analysis of core measures.Several surveys on different aspects of discrimination-aware data mining already mentary to this survey. A previous review ) presents a multi-disciplinary context for discrimination-aware data mining. The review (Romei and Ruggieri 2014 focuses on approaches to solutions across different disciplines (Romei and Ruggieri 2014), rather than analysis and comparison of measures. A yet earlier study (law, economics, statistics, computer science)) discusses a number of measures in relation to association rule discovery task, which in principle can be applied to any classification algorithm. This study discussed four measures that we current categorize under Absolute measures. A recent review (Pedreschi et al. 2012 discusses the legal aspects of potential discrimination by machine learning, mainly focusing on American anti-discrimination laws in the context of employment, as well as discussing how big data and machine learning can lead to discrimination attributable to algorithmic effects regardless of jurisdiction. A classical handbook on measuring racial discrimination (Barocas and Selbst 2016)) focuses on surveying and collecting evidence for discrimination discovery. The book does not consider discrimination by algorithms, it only considers discrimination by human decision makers, and therefore presents inspiring ideas, but not solutions for measuring algorithmic discrimination, which is the focus of our survey. Interactions between human and algorithmic decision making is experimentally investigated in a recent study (Blank et al. 2004.(Berendt and Preibusch 2014)", "body": "Nowadays, increasingly many decisions for people and about people are made using predictive models built on historical data, including credit scoring, insurance, personalized pricing and recommendations, automated CV screening of job applicants, profiling of potential suspects by the police, and many more cases. The penetration of data mining and machine learning technologies, as well as decisions informed by big data has raised public awareness that data-driven decision making may lead to discrimination against groups of people Discrimination-aware data mining is an emerging discipline that studies how to prevent potential discrimination due to algorithms. It is assumed that non-discrimination regulations prescribe which personal characteristics are considered sensitive, or which groups of people are to be protected. The regulations are assumed to be defined externally, typically by national or international legislation. The research goal in discrimination-aware data mining is to translate those regulations mathematically into non-discrimination constraints, and develop predictive modeling algorithms that would be able to take into account those constraints, and at the same time be as accurate as possible. These constraints prescribe how much of differences between groups can be considered explainable. In a broader perspective, research needs to be able to computationally explain the roots of such discrimination events before increasing public concerns lead to unnecessarily restrictive regulations against data mining.In the last few years researchers have been developing discrimination-aware data mining algorithms using a variety of performance measures. Yet there is a lack of consensus of how to define the fairness of predictive models, and how to measure their performance in terms of non-discrimination. Often research papers propose new ways to quantify discrimination, and new algorithms that would optimize that measure. The existing variety of evaluation approaches makes it difficult to compare results and assess progress in the discipline; furthermore, the variety of measures makes it difficult to recommend computational strategies to practitioners and policy makers.The goal of this survey is to develop a unifying view towards discrimination measures in data mining and machine learning, and analyze the implications of optimizing one or another measure in predictive modeling. Therefore, it is essential to develop a coherent view early in the development of this research field, in order to present task settings in a systematic way for follow up research, to enable systematic comparison of approaches, and to facilitate a discussion hopefully aimed at reaching a consensus among researchers in terms of the fundamentals of the discipline. For this purpose we review and categorize measures that have been used in data mining and machine learning, and also discuss measures from other disciplines, such as feature selection, which in principle could be used for measuring discrimination. We complement the review by experimental analysis of core measures.Several surveys on different aspects of discrimination-aware data mining already mentary to this survey. A previous review BackgroundThe root of the word 'discrimination' is the Latin for distinguishing. While distinguishing is not undesirable as such, discrimination has a negative connotation when referring to adversary treatment of people based on belonging to some group rather than their individual merits. Initially associated with racism, nowadays discrimination may refer to a wide range of grounds, such as, race, ethnicity, gender, age, disability, sexual orientation, religion and more. Data mining is not aiming to decide what is the right or wrong reason for distinguishing, but considers sensitive characteristics to be externally decided by social philosophers, policy makers and society itself. The notion of sensitive characteristics can depend on the context and can change from case to case. The role of data mining is to understand generic principles and provide technical expertise on how to guarantee non-discrimination in algorithmic decision making.Discrimination and lawPublic attention to discrimination prevention is increasing, national and international anti-discrimination legislation are expanding the scope of protection against discrimination, and extending discrimination grounds. For instance, the EU is developing a unifying \"Council Directive on implementing the principle of equal treatment between persons irrespective of religion or belief, disability, age or sexual orientation\".Adversary discrimination is undesired from the perspective of basic human rights, and in many areas of life non-discrimination is enforced by international and national legislation, to allow all individuals an equal prospect to access opportunities available in a society (European Union Agency for Fundamental Rights 2011). Enforcing nondiscrimination is not only for the benefit of individuals. Considering individual merits rather than group characteristics is expected to benefit decision makers leading to more informed, and likely more accurate decisions.From the regulatory perspective discrimination can be described by three main concepts: (1) what actions, (2) in which situations, and (3) towards whom are actions considered to be discriminatory. Actions are forms of discrimination, situations are areas of discrimination, and grounds of discrimination describe the characteristics of the people who may be discriminated against.The EU legal framework for anti-  Discriminatory actions may take different forms, the two main being known as direct discrimination and indirect discrimination. Direct discrimination occurs when a person is treated less favorably than another person would be treated in a comparable situation on protected grounds. For example, property owners not renting to a racial minority tenant. Indirect discrimination occurs where an apparently neutral provision, criterion or practice would put persons of a protected ground at a particular disadvantage compared with other persons. For example, the requirement to produce ID in the form of a driver's license for entering a club may discriminate against visually impaired people, who cannot have a driver's license. A related term statistical discrimination Data-driven decision making refers to using predictive models learned on historical data for decision support. Data-driven decision making is prone to indirect discrimination, since data mining and machine learning algorithms produce decision rules or decision models, which then may put persons of some groups at a disadvantage as compared to other groups. When decisions are made by human judgement, biased decisions may occur on a case-by-case basis. Rules produced by algorithms are applied to every case, and hence may discriminate more systematically and on a larger scale than human decision makers. Discrimination due to algorithms is sometimes referred to as digital discrimination The current non-discrimination legislation has been set up to guard against discrimination by human decision makers. The basic principles of the non-discrimination legislation generally apply to algorithmic decision making as well, the specifics of algorithmic decision making are yet to be taken into national and international legislation. Ideally, algorithmic discrimination measures should be universal in a sense that they would not be tied to any specific legislation.The current EU directives do not specify particular discrimination measures or tests to be used to judge whether there has been a discrimination. Rather, statistical measures of discrimination are used on case-by-case bases to establish prima facie evidence, which then shifts the responsibility of proving discrimination from the person who is being discriminated against to the discriminating party.The general population, and even some data scientists may think that since data mining is based on data, models produced by data mining algorithms must be objective by nature. In reality models are as objective as the data on which they are built, and as long as the assumptions behind the models are perfectly matched in the data. In practice, assumptions are rarely perfectly matched. Historical data may be biased, incomplete, or record past discriminatory decisions that can easily be transferred to predictive models, and reinforced in new decision making Discrimination-aware data miningDiscrimination-aware data mining is a discipline at an intersection of computer science, law and the social sciences. It has two main research directions: discrimination discovery, and discrimination prevention. Discrimination discovery aims at finding discriminatory patterns in data using data mining methods. A data mining approach for discrimination discovery typically extracts association and classification rules from data, and then evaluates those rules in terms of potential discrimination Discrimination prevention algorithms have been developed to produce nondiscriminatory predictive models with respect to externally given sensitive characteristics. The objective is to build a model or a set of decision rules that would obey non-discrimination constraints. Typically, such constraints directly relate to some selected discrimination measure. Algorithmic solutions for discrimination prevention fall into three categories: data preprocessing, model post-processing, and model regularization. Data preprocessing modifies historical data such that it no longer contains unexplained differences across the protected and the unprotected groups, and then uses standard learning algorithms with this modified data. Data preprocessing may modify the target variable Defining coherent discrimination measures is fundamental for both lines of research: discrimination discovery and discrimination prevention. Discrimination discovery requires some measure that can be used to judge whether there is any discrimination in data. Discrimination prevention requires some measure for use as an optimization criterion in order to sanitize predictive models. Direct discrimination by algorithms can be avoided by excluding the sensitive variable from decision making, but this unfortunately does not prevent the risk of indirect discrimination. In order to aid in establishing a basis for further research in the field, especially in algorithmic discrimination prevention, our main focus in this survey is to review indirect discrimination measures. While measuring direct discrimination is based on comparing individual to individual, measuring indirect discrimination is based on comparing group characteristics.Definition of fairness for data miningIn the context of data mining and machine learning non-discrimination can be defined as follows: (1) people that are similar in terms of non-protected characteristics should receive similar predictions, and (2) differences in predictions across groups of people can only be as large as justified by their non-protected characteristics. To the best of our knowledge, in the data mining context these two conditions, expressed as Lipschitz condition and statistical parity, have been first formally discussed by The first condition is necessary but not sufficient for ensuring non-discrimination in decision making, because even though similar people are treated in a similar way, groups of similar people may be treated differently from other groups. The first condition relates to direct discrimination, which occurs when a person is treated less favorably than another would be treated in a comparable situation, and can be illustrated by the twin test. Suppose gender is the protected attribute, and there are two identical twins who share all the characteristics, but gender. The test is passed if both individuals receive identical predictions by the model.The second condition ensures that there is no indirect discrimination, which occurs when apparently neutral provision, criteria or practice would put persons of a protected ground at a particular disadvantage compared with other persons. The so called redlining practice More formally, let X be a set of variables describing non-protected characteristics of a person (a complete set of characteristics may not always be known or available, in such a case X denotes a set of available characteristics), S be a set of variables describing the protected characteristics, and \u0177 be the model output. A predictive model can be considered fair if: (1) the expected value for model output does not depend on the protected characteristics E( \u0177|X, S) = E( \u0177|X ) for all X and S, that is, there is no direct discrimination; and (2) if non-protected characteristics and protected characteristics are not independent, that is if E(X |S) = E(X ), then the expected value for model output within each group should be justified by some fairness model, that is E( \u0177|X ) = F( \u0177|X ), where F is a fairness model. Defining and justifying F is not trivial, that is where a lot of ongoing effort in discrimination-aware data mining currently is concentrated.Discrimination by predictive models can occur only when the target variable is polar, that is, some predictions outcomes are considered superior to others. For example, getting a loan is better than not getting a loan, or the \"golden client\" package is better than the \"silver\", and \"silver\" is better than \"bronze\", or an assigned interest rate of 3% is better than 5%. If the target variable is not polar, there is no discrimination, because no treatment is superior or inferior to another treatment.The protected characteristic (also referred to as the protected variable or sensitive attribute) may be binary, categorical or numeric, and it does not need to be polar. For example, gender can be encoded with a binary protected variable, ethnicity can be encoded with a categorical variable, and age can be encoded with a numerical variable. In principle, any combination of one or more personal characteristics may be required Fig. Principles for making predictive models non-discriminatoryFigure Learning algorithms as such cannot discriminate, because they are not used for decision making. The resulting predictive models (decision rules) would discriminate. Yet algorithms may be discrimination-aware by employing procedures to enforce non-discrimination constraints into the models. Hence, one of the main goals of discrimination-aware data mining is to develop discrimination-aware algorithms, that would guarantee that non-discriminatory models are produced.There is a debate in the discrimination-aware data mining community about whether models should or should not use protected characteristics as inputs. For example, a credit risk assessment model may use gender as input, or may leave the gender variable out. Our position Ensuring that there is no indirect discrimination (the second fairness condition) is more tricky. In order to verify to what extent non-discrimination constraints are obeyed and enforce fair allocation of predictions across groups of people, learning algorithms must have access to the protected characteristics in the historical data. We argue that if protected information (e.g. gender or race) is not available during the model learning building process, the learning algorithm cannot be discrimination-aware, because it cannot actively control non-discrimination. The resulting models produced without access to sensitive information may be discriminatory, they may be not, but that is a chance rather than discrimination-awareness property of the algorithm. Non-discrimination can potentially be measured in input data, on predictions made by models, or in models themselves. Measuring requires access to the protected characteristic. Yet this does not mean that algorithmic discrimination is always direct. The distinction between direct and indirect discrimination refers to using the protected characteristic in decision making, not to measuring discrimination. The following section presents a categorized survey of measures used in discrimination-aware data mining and the machine learning literature, and discusses other existing measures that could in principle be used for measuring the fairness of algorithms. the magnitude of discrimination, but the spread of discrimination, that is, the share of people in the dataset that are affected by direct discrimination.The following notation summarized in Table Statistical testsStatistical tests are the earliest measures that are focused on indirect discrimination discovery in data. Statistical tests are formal procedures to accept or reject statistical hypotheses, which check how likely the result is to have occurred by chance. Typically, in discrimination analysis the null hypothesis is that there is no difference between the treatment of the general group and the protected group. The test checks how likely the observed difference between groups could have occurred by chance. If chance is unlikely then the null hypothesis is rejected and discrimination is declared.Two limitations of statistical tests need to be kept in mind when using them for measuring discrimination. tistical methods include methods for determining the effect size, which can in principle be translated to algorithmic discrimination measures and optimization constraints. The absolute measures, discussed in the next section (such as the mean difference), often derived from the statistical approaches for computing test statics.Regression slope testThe test fits Ordinary Least Squares (OLS) regression to the data including the protected variable, and tests whether the regression coefficient of the protected variable is significantly different from zero. A basic version for discrimination discovery considers only the protected characteristic s and the target variable y The test statistic is t = b/\u03c3 , where b is the estimated regression coefficient of s, and \u03c3 is the standard error, computed aswhere n is the number of observations, f (.) is the regression model, . indicates the mean. The t-test with n \u2212 2 degrees of freedom is applied.Difference of means testThe null hypothesis is that the means of the two groups are equal. The test statistic iswhere n 0 is the number of individuals in the unprotected group, n 1 is the number of individuals in the protected group,where \u03b4 2 0 and \u03b4 2 1 are the sample target variances in the respective groups. The t-test with n 0 \u2212n 1 \u22122 degrees of freedom is applied. The test assumes independent samples, normality and equal variances. Difference of means, although not formally used as a statistical test, has been used in the data mining literature, for instance by Difference in proportions for two groupsThe null hypothesis is that the rates of positive outcomes within the two groups are equal. The test statistic iswhereThe z-test is applied. Difference in proportions, although not formally used as a statistical test, has been used in a number of data mining studies Difference in proportions for many groupsThe null hypothesis is that the probabilities or proportions are equal for all the groups. This can be used for testing many groups at once. For example, equality of decisions for different ethnic groups, or age groups. If the null hypothesis is rejected that means at least one of the groups has statistically significantly different proportion. The text statistic iswhere k is the number of groups. The Chi-Square test is used with k \u2212 1 degrees of freedom.Rank testThe Mann-Whitney U test Absolute measuresAbsolute measures are designed to capture the magnitude of differences between (typically two) groups of people. The groups are determined by the protected characteristic (e.g. one group is males, another group is females). If more than one protected group is analyzed (e.g. different nationalities), typically each group is compared separately to the most favored group.Mean differenceThe mean difference measures the difference between the means of the targets of the protected group and the general group,If there is not difference then it is considered that there is no discrimination. The measure relates to the difference of means, and difference in proportions test statistics, except that there is no correction for the standard deviation.The mean difference for binary classification with a binary protected variable,is also known as the discrimination score The mean difference has been the most popular measure in early work on discrimination-aware data mining and machine learning ", "conclusions": "", "SDG": [10]}, "preventing_discrimination_in_the_automated_targeting_of_job_advertisements": {"name": "Preventing discrimination in the automated targeting of job advertisements", "abstract": " On the background of the increasing amount of discriminatory challenges facing artificial intelligence applications, this paper examines the requirements that are needed to comply with European non-discrimination law to prevent discrimination in the automated online job advertising business. This paper explains under which circumstance the automated targeting of job advertisements can amount to direct or indirect discrimination. The paper concludes with technical recommendations to dismantle the dangers of automated job advertising. Various options like influencing the pre-processing of big data and altering the algorithmic models are evaluated. This paper also examines the possibilities of using techniques like data mining and machine learning to actively battle direct and indirect discrimination. The European non-discrimination directives 2000/43/EC, 2000/78/EC, and 2006/ 54/EC which prohibit direct and indirect discrimination in the field of employment on the grounds of race or ethnic origin, sex, sexual orientation, religious belief, age and disability are used as a legal framework.", "keywords": "2000,43,EC", "introduction": "With the rise of artificial intelligence (AI) and the accompanying subfields of big data, data mining and machine learning, a lot of human tasks can be successfully performed by AIdriven software. A White House report on big data warns that such innovations can root discrimination deeply into society and reinforce prejudice and bias.  An example of discriminatory AI is the computer program which is used in some jails to determine which prisoners are eligible for parole. The program generates a risk assessment score to determine which prisoners are likely to re-offend. According to a research done by ProPublica, the system is biased against prisoners of colour. 2 Such technologies are applied to automate decisions in multiple other fields such as online advertising and employment. 1 Imagine that 50 years ago, a newspaper gave the option to advertise vacancies only in copies that went to male readers. Advertising like this belongs to the possibilities that platforms like Facebook provide nowadays. With countless targeting settings, people can be excluded until the advertiser has reached the perfect audience. When AI is used to control and apply these settings it is vital that it does not do so in a discriminatory way, especially in the field of employment. The chances for job seekers are seriously diminished when they are excluded from seeing job advertisements because this gives them a false start. 3 This undermines the principle of equality from which follows that every individual should have the same opportunities, including equal access to employment. 4 A good example of discriminatory job advertising can be found in a research that analysed advertising placements. 5 It discovered that an advertisement for a highpaying executive position was shown almost six times more to men than to women. 6 However, when used in the right way, AI can efficiently identify and reach the candidates possessing the required skills for a job while avoiding individual biases. 7 On the background of the increasing amount of discriminatory challenges facing AI applications, this paper (or hereafter \"we\") examines discrimination in the automated online job advertising business in Europe. Because this topic is so extensive, this paper examines how discrimination can be prevented in the automated online targeting of job advertisements. This question is answered in this paper in four steps. First, the technical elements that come into play and may cause discrimination when using AI to target advertisements are presented in Section 2. Secondly, the scope and effect of European non-discrimination law are established in Section 3. Thirdly, Section 4 examines in which ways the targeting of job advertisements can be discriminatory. Fourthly, technical recommendations on how to prevent discrimination when the targeting is done by AI are presented in Section 5. Various options like influencing the pre-processing of big data, altering the algorithmic models are evaluated. Section 5 also examines the possibilities of using techniques like data mining and machine learning to actively battle direct and indirect discrimination. Finally, Section 6 concludes the paper.8", "body": "With the rise of artificial intelligence (AI) and the accompanying subfields of big data, data mining and machine learning, a lot of human tasks can be successfully performed by AIdriven software. A White House report on big data warns that such innovations can root discrimination deeply into society and reinforce prejudice and bias. Using artificial intelligence in automated online advertisingIn order to scrutinize automated online job advertising, an examination of the advertising process and a delimitation of the subject is needed. This section explains the practice of online advertising and which factors play a role in the outcome of an online advertising campaign.Online job advertisingTraditionally, advertising took place through billboards, magazines, television or radio. Advertisers could play with what they would show, and more generally to whom, by changing the content, channel and timing of the advertisement. For this reason, toy commercials were not shown on MTV but rather on Nickelodeon. Since the invention of the internet, online advertising has become a booming business and experts estimate that by 2019 online advertising will become the biggest segment of advertising. 9The best known online advertising channels are Facebook and Google. They let other companies advertise to their users on their websites. Next to that, they provide advertising banner space on all kinds of websites or telephone apps. These platforms have their own advertising software. Facebook's software is called Facebook Business Manager and Google has AdWords. When creating a new campaign in Facebook Business Manager, there is one particular goal that has to be reached. In an employment opportunity case, the campaign needs to result in large numbers of applications by suitable candidates. want to see. In other words, the advertisement needs to be targeted to the right people and the content needs to be appealing enough to click on. As said above, we focus on the former. Facebook provides the option for advertisers to target their advertisements based on inter alia location, age, gender, sexual orientation, language, education, job, relationship status, financial status, home ownership, income, political interest and ethnic affinity. 12 These targeting settings are used to narrow down the audience and can be used to include or exclude people. Some of the targeting options have a potential for discriminatory usage. This can be the case when they are used to exclude people of a certain ethnic or racial affinity. In the case of job advertising, these settings can be used to target a job advertisement for marketers at people who studied marketing. However, they can also be used to further narrow down the audience within the population of marketers. Further narrowing down the audience is convenient, especially in large populations, to separate the more suitable candidates from the less suitable candidates. Ergo, targeting is the key element in reaching the right candidates for a job offer because it can be used to alter which people get to see the advertisement. The next section establishes how AI is involved in the targeting process.Using artificial intelligenceTargeting in the online advertising process is normally done by a human advertiser. In the automated process this is done by artificial intelligence. This section explains how this works by elaborating on the definitions of AI and some of its sub elements. AI can be defined as 'any device that perceives its environment and takes actions that maximize its chance of success at some goal'. It does so by a technique called machine learning. Machine learning addresses the question of how to build a computer system that improves automatically through its experience. Data mining uses algorithmic models to identify relationships between different attributes with the purpose of extracting useful information from big data. 17 Big data is unstructured and the amount of data is extremely excessive. An example of the use of data mining with big data can be found with an employer who used these methods to discover that one predicting factor of strong coding skills is an affinity for a particular Japanese cartoon site. So, when targeting is done by AI, the factors that influence its targeting decisions are the big data sets, the algorithmic models that mine the data, the targeting rules that are learned from the found correlations and the use of those rules by the The answer could lie in non-discrimination law which is set out in Section 3 and applied in Section 4.Non-discrimination lawProvisions on non-discrimination and equality are strongly integrated in international law. The concept of equality has been expressed explicitly in most human rights instruments as a preambular objective, as an implicit descriptive function in the understanding of the scope and application of human rights, and it has been codified in substantive provision of human rights treaties. Equality and non-discrimination in European Union lawThe focus in this paper lies on European Union nondiscrimination law since that is the most specific and comprehensive non-discrimination law. Another reason for focussing on EU non-discrimination law is the fact that it has lots of elaborating case law and that it is applicable in twentyeight countries. The wide reach of Union law is relevant because companies that use artificial intelligence to advertise online do business in many different states at once.The concept of equality and non-discrimination can be found in multiple places in primary Union law. According to article 2 of the Treaty on European Union (TEU), the European Union is founded on the values of respect for human dignity, freedom, equality, the rule of law, and respect for human rights. The characteristics of Union law have been set out in multiple cases. The scope and effect of Union law was first addressed in the Van Gend & Loos case. The European non-discrimination directivesThe Union has adopted three directives which address discrimination in multiple fields. In this part, their substance, interpretation, implementation and their direct effect are analysed. The Directives prohibit discrimination based on multiple discrimination grounds and in multiple fields. The Racial Equality Directive prohibits discrimination based on race or ethnicity in the context of employment and occupation, social protection, social advantages, education and access to goods and services. The interpretation of the directives is not a clear-cut case since the directives itself lack a clarification on the scope and meaning of the grounds. The CJEU had to make up for this defect in its case law. The directives affect the member states in two ways. First, the directives provide objectives which need to be achieved by all member states. These objectives are result-based obligations of the member states. States are free to determine which means they want to use to achieve these results. The directives are addressed to the member states so, in principle, they only bind the member states. The striking effect of the judgements in Mangold and K\u00fcc\u00fckdeveci is the de facto horizontal exclusion effect of the nondiscrimination directives. Since all member states have to implement the Directives, all national law will be or at least should be in compliance with the Directives. If national law is not completely in line with the directives, it must be interpreted in the light of EU law. Even if it is not in line at all, it can be excluded since by analogy, all non-discrimination directives reflect general principles of Community law and thus all these principles have horizontal direct effect. This means that the Directives can be used autonomously to review horizontal private disputes between job advertising companies and candidates. For these reasons, from the perspective of businesses using automated job advertising, it is most efficient to comply with the EU directives on non-discrimination.Recognising discrimination in the targeting of job advertisementsThis section aims to determine in which ways the targeting of a job advertisement can be discriminatory. The concepts of direct and indirect discrimination are set out and applied to the automated targeting of job advertisements in Sections 4.1 and 4.2.In online job advertising, advertisements are shown based on the information the social media channel has collected about its users. Facebook creates an advertisement profile of its users based on information provided by them and their behaviour on Facebook. 51 This profile does not necessarily represent the real characteristics of its users. In other words, Facebook assumes certain characteristics, and associates its users with certain groups. However, discrimination can be based upon an assumed or associated ground, whether true or not in reality, and, as such, is prohibited under the directives. There is another relevant concept that applies to the automated online advertising of jobs. This concept entails that there does not necessarily have to be an identifiable victim or claimant for sanctions to be applied. This concept was introduced in the Feryn case. Direct discriminationEstablishing direct discriminationIn order to prevent discrimination in online job advertising it is important to first establish in which ways it can be discriminatory. Targeting can be directly and indirectly discriminating. But what does direct discrimination exactly entail? The Gender Equality Directive defines direct discrimination as follows: ''direct discrimination': where one person is treated less favourably on grounds of sex than another is, has been or would be treated in a comparable situation'. While this interpretation seems straightforward, there are two issues that are connected with direct discrimination. First, the question whether direct discrimination addresses individuals or groups and second the question whether it should be seen from the perspective of the alleged victim or from the perspective of the alleged perpetrator. effect or the existence of a victim does not influence the existence of direct discrimination.In principle, a neutral practice will never amount to direct discrimination but it can possibly amount to indirect discrimination. So, in principle, targeting based on the protected grounds constitutes direct discrimination with regard to access to employment. Direct discrimination also occurs when a prima facie neutral targeting setting is inextricably linked to a protected ground and is only capable of affecting persons of that protected group. So, targeting settings which narrow down an audience by excluding people based upon protected characteristics amount to direct discrimination. For instance, if a job advertising campaign is set to only reach Caucasian people, Christian people or straight people, discrimination is a fact.Justifying direct discriminationNow that it is clear how targeting can be directly discriminating, it is time to delve into the justification methods provided by the directives. The directives provide limited and only specific defences to direct discrimination. This means that direct discrimination can only be justified when it is in pursuit of the particular aims which are set out in the directives. Two things to note about the scope of GORs are relevant for automated job advertising. First, the GOR only needs to be related to a protected characteristic and does not have to be a protected characteristic itself. There is an important difference in the way GORs and targeting settings are linked to the job. GORs are inseparable from the job, without them the work cannot be performed the way it should be. Targeting settings are used on the one hand to filter out the people who cannot comply to those genuine requirements of the job. On the other hand, targeting is used to further narrow down the audience of suited people in order to reach only the best performing candidates. Reaching the best performing candidates is an economically efficient way of spending advertising budget and the employer will be more satisfied when he/she only receives applications from high quality candidates.Indirect discriminationEstablishing indirect discriminationBesides direct discrimination, the Directives include the concept of indirect discrimination which is not aimed at the practice itself but at the outcome. This section describes the concept and its justifications. The definition set out below is the same in all three directives.'indirect discrimination': where an apparently neutral provision, criterion or practice would put persons of one sex at a particular disadvantage compared with persons of the other sex, unless that provision, criterion or practice is objectively justified by a legitimate aim, and the means of achieving that aim are appropriate and necessary 84Hence, indirect discrimination requires an apparently neutral provision, criterion or practice that puts a certain protected group at a particular disadvantage unless it can be objectively justified. The terms 'provision, criterion or practice' show that measures in the broadest meaning of the word fall within the scope of the definition. To establish indirect discrimination, a comparison between the effect on the protected group and on others needs to be made. It is vital that meaningful statistics are available for making this comparison. The Seymour Smith case elaborates when an adverse impact is present in the case of sex discrimination. A practical example: Imagine if in an audience of waiters, a certain neighbourhood which contains a lot of people of colour is excluded, the neutral rule has a discriminating effect when it removes a considerable percentage of the protected group. However, the term considerable percentage needs to be defined in order to draw a practical conclusion.The percentage of people removed should be compared to the effects of the neutral rule to the other protected groups. If 95% of the women are excluded but only 30% of the men, the neutral rule obviously has a discriminating effect. On the contrary, if a targeting setting removes 80% of the women, but also 80% of the men, it narrows the reached audience in an equal way. But what happens when 80% of the women are removed and 79% of the men? Is there a considerably smaller percentage of women that is left in this case? In order to answer this, we need to know when the scale tips from considerable to inconsiderable and vice-versa. One indication is that the degree in disparate impact must be quite high. However, numbers that define \"quite high\" are not available. \"Justifying\" indirect discriminationNot in all cases where there is an adverse effect or disparate impact, can indirect discrimination be established. Since the justification of indirect discrimination is embedded in its definition, indirect discrimination does not occur at all when the practice can be justified. Unlike direct discrimination, indirect discrimination has an open-ended justification. The nondiscrimination directives stipulate in the definition of indirect discrimination that it does not occur when the practice can be 'objectively justified by a legitimate aim, and the means of achieving that aim are appropriate and necessary'. On the other hand, objective justification can be seen as a causal link test. This causation approach can be used to rebut the assumption that there is a link between the practice and the detrimental effect. According to the non-discrimination directive, a practice can be objectively justified if it has a legitimate aim and at the same time is necessary and appropriate. So, the first question is: when does a practice have a legitimate aim? General indications can be found in the CJEU's case law on sex discrimination. Targeting settings that reflect occupational requirements serve a legitimate aim since they are necessary to perform the job. This means that targeting settings based on educational level, educational subject, professional background, job title or work industry cannot amount to indirect discrimination. So, a targeting setting which narrows an audience down to people who have studied economics at university level and who work in management in the steel industry can always be used. Any discriminatory effect of such setting is justified by the needs of the employer.The conclusion is that even with the use of neutral targeting settings, there still is a risk of discrimination. However, if this neutral setting is objectively justified, the practice will not constitute indirect discrimination. This is the case when the targeting setting has a legitimate aim, is appropriate and necessary. This means that the setting does not excessively prejudice the legitimate interests of the persons concerned, in this case the opportunity to apply to jobs. Furthermore, there should be no other targeting settings available that cause a less discriminatory effect and achieve the same result.Preventing discrimination in the automated targeting of job advertisementsThis section provides recommendations on how direct and indirect discrimination can be prevented when using AI in the automated job advertising process and it builds on the concepts discussed in sections two and four. When the targeting is done by AI, the factors that influence its targeting decisions are the data contained in the big data sets, the algorithms used for the data mining, the rules that are learned from this and the use of those rules by the AI. Consequently, these are also the factors that determine if an AI-driven job advertising campaign is discriminatory. By manipulating these factors discrimination can be prevented. It is important to prevent discrimination and to allow for the exceptions that justify discrimination at the same time.Preventing direct discriminationWith regard to direct discrimination, this means that the use of the protected grounds as targeting settings must be prevented while allowing the use of protected grounds that are genuine occupational requirements. So, if a protected ground is not explicitly required to fulfil the job, it should not be used as a targeting setting. Furthermore, the use of characteristics which are inextricably linked to a protected ground must be prevented. Consequently, the AI must recognize these characteristics in order to prevent them from being used. The first factor that could be influenced is big data. If the discrimination grounds are excluded from the big data, the algorithms can never pick them up. The solution to the problem of recognising characteristics which are inextricably linked to the protected grounds can be found in the use of data mining. When using the big data for more than the single purpose of discovering correlations which result in the most successful targeting rules, other correlations which are useful for the prevention of discrimination can be found. The big data can be used to determine which neutral grounds are inextricably linked to protected grounds. In order to do so, both the big data and the algorithms must include the protected grounds. When this is done, the algorithms could find, for instance, that 100% of the pregnancies are correlated to women, that 100% of the people with facial hair are men or that 100% of the people that wear burkas are Muslim. The fact that something correlates with a protected ground for 100% proves that it is inextricably linked to that ground because it is connected to the ground in every case. With this information, a list of prohibited targeting settings can be established, e.g., a blacklist. This blacklist contains the grounds protected by the directives and all characteristics which are inextricably linked to it. This blacklist must be used as a filter between the established rules or correlations and the output of the AI. In this way, the AI is able to identify discriminatory targeting settings and it will not use settings that are directly discriminating.The exception of GORs should also be added. This means that the AI must know in what circumstances it should allow the use of an otherwise directly discriminating targeting setting. For the AI to determine if the use of a protected ground can be justified because it constitutes a genuine occupational requirement, it must have knowledge of the context and the nature of the work that has to be performed. Since every job vacancy is different it is hard to draft general rules to assess when a protected characteristic is a GOR. Even if this is technically feasible it would still require the AI to have access to each employer's system to gather information about the context and nature of the work. It is unlikely that every employer will give this access and if they would, it would be a huge operation to connect the AI to every employer's system and to preprocess all their data to the right format to be used. The third option is the use of machine learning and rule induction. With these techniques, the AI can learn to recognise real life situations and predict their outcomes based on observational data from previous events. Preventing indirect discriminationAs for indirect discrimination, the use of neutral targeting settings which have an indirectly discriminating effect must be prevented while allowing the use of such settings if they can be objectively justified. The definition of indirect discrimination entails that neutral targeting settings that are objectively justifiable cannot amount to indirect discrimination. When this rule is applied to targeting, a distinction between two different types of targeting needs to be made. First, there are targeting settings that entail occupational requirements which are determined by the employer. This is a different concept than the GORs connected to the justification of direct discrimination. In the context of indirect discrimination, targeting based upon occupational requirements should be seen as neutral rules that reflect the genuine needs of the employer. Occupational requirements are objectively justifiable because they serve the legitimate aim of employing a qualified person. So, as long as these requirements are necessary and appropriate, targeting settings based on educational level, educational subject, professional background, job title or work industry cannot amount to indirect discrimination. It is therefore the responsibility of the employer to provide the advertising company that uses the AI with legitimate requirements. Facebook provides categories of targeting settings in the exact categories as mentioned above. These categories should be put on a green list and should always be allowed. So, when the rule picking element of the AI is manipulated in this way, there is no need to assess the effect of these settings since discrimination cannot occur either way. For instance, an advertisement for a programmer targeted at people who studied at university level and have programmer as current job title should always be allowed.The second type of targeting settings are the additional settings that further narrow down the qualified audience established with the use of occupational targeting setting. The second type of settings is determined by the AI based on correlations found in the big data. The big advantage of this is that the AI will come up with effective targeting settings which humans would never think of. For instance, AI found through data mining that an affinity for a Japanese cartoon site is a solid predictor of good coding skills. 117 So the AI could target the programmer advertisement at people who studied at university level and have programmer as current job title and like the particular Japanese cartoon site. In this situation, the first two settings are occupational requirements which are necessary to perform the job, the last setting is not. Consequently, this last targeting setting cannot be objectively justified since it is not necessary to perform the job and thus not objectively justifiable. Therefore, it can only be used if it is not indirectly To predict the discriminatory effects of a setting, the AI needs to understand the concept of detrimental effect or disparate impact. Such effect or impact exists when a considerably larger percentage of a protected group is excluded than others by the targeting setting. From the Seymour Smith case, it can be inferred that indirect discrimination does not occur when the relative difference in exclusion is 12.3% or less. So, the AI should compare the predicted impact of the targeting settings on protected groups and others and use this as a rule when comparing the effects. Through its APIs it can see what kind of effect each targeting setting will have on the audience. If the setting has an adverse effect on a protected group, it may not be used.The following simplified examples will make this clear. If an advertisement for a management function is targeted at people who work as managers in the Netherlands, 92.000 women and 108.000 men are reached. Conclusion", "conclusions": "On the background of the increasing amount of discriminatory challenges facing AI applications, this paper examined the requirements needed in order to comply to European nondiscrimination law to prevent discrimination in the automated online job advertising business in Europe. The factors that influence the occurrence of discrimination are the big data, the algorithms that mine the big data, the correlations that are found and the accompanying targeting rules, and the way AI uses these rules. Job advertising companies using artificial intelligence to target their job advertisement should comply to the European non-discrimination directives since member states have to implement the directives, national courts must interpret national law in light of the directives and individuals can directly invoke the principle of non-discrimination. From the directives follows that targeting may not be directly or indirectly discriminating on the grounds of race, ethnicity, sex, sexual orientation, religious belief, age and disability. Direct dis-crimination occurs when a targeting setting is used that contains such ground or a characteristic that is inextricably linked to it. Indirect discrimination occurs when a neutral setting excludes a considerable larger percentage of a protected group compared to others. Direct discrimination can only be justified by a genuine occupational requirement while indirect discrimination has an open justification. So, targeting settings which reflect occupational requirements cannot amount to discrimination. These justifications can be implemented within an AI by establishing a black list with targeting settings that cannot be used at all to prevent direct discrimination. A green list with settings which can always be used needs to be established to allow for neutral settings which are justified. To prevent indirect discrimination the effect of a setting that is not justifiable needs to be predicted. Only if the setting does not exclude a considerably bigger percentage of a protected group, it cannot amount to indirect discrimination and it can be used. When these requirements as guided by the EU directives and with the proved conditions of either the black or green lists are implemented in the AI that makes the targeting decision, discrimination in the automated online targeting of job advertisements can be prevented.", "SDG": [10]}, "simulating_norms_social_inequality_and_functional_change_in_artificial_societies": {"name": "Simulating Norms, Social Inequality, and Functional Change in Artificial Societies", "abstract": " In this paper, we compare the computational and sociological study of norms, and resimulate previous simulations (Conte and", "keywords": "Simulation of norms,Social inequality,Functions of norms", "introduction": "In recent years, the computational study of norms through simulation has grown in importance (Shoham and Tenneholtz 1992a, Shoham and Tenneholtz 1992b, Conte and Castelfranchi 1995a, Conte and Castelfranchi 1995b, Walker and Wooldridge 1995. Most interestingly, the theoretical input to this field of research seems to originate in game theory, but not in sociology itself, although the sociological study of norms has a tradition going back more than a hundred years. In this paper we claim that the computational study of norms could gain from a wider sociological perspective on norms., Castelfranchi, Conte and Paolucci 1998)", "body": "In recent years, the computational study of norms through simulation has grown in importance 1.2In the next section we compare the computational and the sociological study of norms and make some suggestions to the further development of the computational study of norms. In the third section we present and discuss the Conte and Castelfranchi model from a wider sociological perspective. In the fourth section we present two resimulations of the Conte and Castelfranchi model under slightly different conditions. First, we analyze the relation between norms, social inequality, and the functions of norms more closely. Our results suggest that the hypothesis stating that the 'finder-keeper' norm while controlling aggression efficaciously reduces social inequality holds only in egalitarian predator-collector societies. In the majority of inegalitarian societies, it instead increases social inequality. Second, we remodel normative behaviour from a sociological point of view and repeat the Conte and Castelfranchi experiments. Finally, some conclusions will be drawn, and suggestions for further studies will be discussed.The Computational vs the Sociological Study of Norms2.1Social norms are a classical topic of research in sociology. The sociological study of norms aims at the appropriate definition of norms, the explanation of how norms affect social behaviour, and the explanation how norms emerge, become established and internalized and change. Like game theory, the computational study of norms is a formal approach to theory building in the field of norms. It is hoped that the computational study of norms will advance our knowledge about the interrelation between cognitive and social processes. Both can be modeled with the help of intelligent agent technology.The Computational Study Of Norms2.2In the Artificial Intelligence literature a norm is operationalized as a behavioural constraint on agents which interact with each other in a multi-agent society. Previous studies have taken the theoretical input from game theory.Conceptualization Of Norms2.3Within game theory three different conceptualizations of norms have been developed (Ullman-Margalit 1977):1. Norms as solutions to problems of co-ordination: Here, norms are seen essentially as conventions, behavioural conformities that do not presuppose explicit agreements among agents. They emerge from the individual interests of the agents 2.4Following 2.5Conte 2.6The advantages of norm-governed systems are avoiding useless, stupid, and self-destructive behaviour favoured by the rigid execution of routines, as well as the spreading of errors and deviations produced by pure imitation. Therefore, it is promising to construct autonomous artificial agents with a capacity for applying norms. Conte and Castelfranchi put forward the thesis that normgoverned systems are more useful than norm-abiding systems. It has to be tested to see which of norm-governed or norm-abiding systems perform better. They state that 'all these hypotheses should be tested computationally. Unfortunately, we are far from being able to execute such a test' 2.7Conte 2.8As far as implemented models are concerned, the agents in Conte and Castelfranchi's study are the most advanced ones (Castelfranchi, The Functions Of Norms 2.9 Implemented computational studies of norms have explored two different functions of norms: to permit or improve co-ordination among agents 2.10Conte and Castelfranchi state that 'this type of norm [the finder-keeper norm], while controlling aggression efficaciously, also reduces the variance of strength among the agents; that is, their inequality' The concept of partial tutoriality seems to be derived from Ullman-Margalit's 'norms of partiality'. It is complemented by distributive tutoriality (actions prescribed are in the interests of all members of the system) and collective tutoriality (actions prescribed are in the interest of the MAS as a collective, but independent of and outside the interest of its members). Do norms have a function in relation to (in)equality? Remember Ullman-Margalit's third argument: Norms are solutions to problems of inequality.The Sociological Study Of NormsConceptualization Of Norms2.11The sociological study of norms has developed four different conceptualizations of norms 1. The statistical conceptualization of norms originates in behaviourism. A behavioural pattern becomes a norm if the majority of actors in fact behave due to this pattern. Thus, norms are objectively observeable and measureable. From this perspective any regular behaviour becomes a norm. One cannot distinguish between regular and rule-obeying behaviour, or conditioned and rule-obeying behaviour, or sanction-avoiding and rule-obeying behaviour.Examples of this approach are the theories of the formation of social norms by Piaget and The Functions Of Norms2.12Functional analysis asks for the objective effect that one element of a social system produces for the system as a whole. There is not necessarily common knowledge about the objective effect of a social element. Functional analysis in general does not concentrate on where the causal relation lies. Cause effect-relations are a special case of functional relations. Different causes are functionally equivalent if they produce the same effect.2.13From the beginning, the functions of norms in society as a whole have been a central field of research in sociology. There are two positions in this debate. First, norms have been analysed as solutions to social problems (Marx, Durkheim, Parsons). According to Comparison And SuggestionsConceptualization Of Norms2.14Comparing the computational (Shoham and Tenneholtz 1992a, Shoham and Tenneholtz 1992b, 2.15As far as implemented models are concerned, the agents in the Castelfranchi, Conte and Paolucci's (1998: 5.1-5.9) study are the most advanced. In order to put forward our theoretical argument here, we sketch the critical characteristics of their model without going into details. In their model of normative action, agents move and try to eat food items that are distributed randomly to the world. Food can be possessed and may be stolen. There is a norm, the finder-keeper norm, that prescribes 'attack an eater unless the food item being eaten is marked as 'owned' by that agent'. The multi-agent system is composed out of two different sub-populations: agents either respect the finder-keeper precept (the Respectful) or not (the Cheaters). Either through experience, or through communications the agents learn whether another agent is a Respectful or a Cheater. From the sociological point of view, the division into normative (Respectful) and non-normative agents (Cheaters) is not yet fully convincing. The 'normative' algorithm of the Respectful is modified so that they respect the norm only with agents known to be Respectful. This looks like a sanction towards the Cheaters, but as the finder-keeper precept does not hold for any Cheater -it is in fact not prescribed for them -they cannot violate it, and therefore they cannot be sanctioned.2.16Following Hart (1961), 2.17We suggest two modifications to advance the computational concepts of the development of norms:In order to model normative behaviour more sociologically we suggest imposing norms on all agents of a MAS, introducing a mechanism that decides whether they respect the norms, and introducing a mechanism that decides about the defence of norms.In order to model the conditions of norm violations more sociologically we suggest consulting sociological theories of deviant behaviour.The Functions Of Norms2.18Whereas the computational study of norms has concentrated on distinct functions of norms, from the sociological point of view, we have in mind that the functions of norms are relative. They depend on time and space, like so many other social facts. It would not be surprising to find that certain norms may also impede co-ordination, or increase aggression. The same holds for norms and inequality. A certain norm may increase inequality, but under different conditions the same norm may decrease inequality. We suggest that to advance the computational study of norm functions it would be desirable to implement functional change in simulation models, in order to investingate the relative time and space-dependent functions of norms.The Conte and Castelfranchi Model3.1The following design decisions are made by Twenty-five food items of nutritional value 20 are distributed randomly on the grid. Each food item is replenished at a randomly selected location on the grid after it has been consumed. At the beginning of a match, agents are randomly allocated to locations and are assigned those food items which happen to fall into their own territories (their von-Neumann neighbourhood). Food possessed is flagged and each agent knows to whom it belongs. Several types of action that cost resources are available to each agent in order to find and eat food (see table 2). 3.2Actions are supposed to be simultaneous. It may be that an agent does not get to perform its action because the conditions for performing the action are no longer fulfilled (e.g., two agents planning to move to the same position cannot both achieve their goal). Depending on built-in routines and knowledge, agents may decide to attack one another. Three routines are available: blind aggression ('attack an eater to get its food, unless free food is available at a lower cost'), strategic aggression ('attack an eater whenever you perceive it as no stronger than you, unless free food is available at a lower cost'), and normative aggression ('attack an eater unless the food item being eaten is marked as 'owned' by that agent', i.e. the finder-keeper norm).3.3An experiment consists of 100 matches, each of which includes 2000 games. During each game each agent performs one action. For each experiment, the number of attacks, the average strength, and the standard deviation of individual strength is recorded, and the significance of the differences tested. In the standalone simulation (homogeneous population) the agents using the normative routine do best at controlling aggression, promoting average strength, and keeping inequality low. In mixed populations (two subpopulations containing agents each of which behaves according to one of two routines, ratio 50:50) the normative strategy becomes the worst.Resimulating Norms4.1We have resimulated the Conte and Castelfranchi model in three series of experiments:1. replication 2. resimulating the relation between norms, social inequality, and the functions of norms 3. remodeling normative behaviour from a sociological point of view Replication 4.2 First, we have reimplemented the Conte and Castelfranchi model in JAVA according to the model descriptions in 4.3Second, we reanalyze the relation between norms, social inequality, and functional change. Following our suggestions above we have two goals:1. We want to demonstrate that the thesis that the 'finder-keeper' norm reduces social inequality while controlling aggression efficaciously holds only in egalitarian predator-collector societies. Throughout the majority of inegalitarian societies, it instead increases social inequality. This argument, which can be traced back to Marx, is investigated by use of computer simulations of artificial societies. 2. We want to demonstrate that societal evolution leads to functional change of norms: More concretely, we show that the finder-keeper precept while controlling conflict and establishing an equal distribution of resources in egalitarian predator-collector societies turns to stabilizing and promoting inequality if we allow private property and heritage, or private property, heritage and unequal renewal of resources.First Experiment: Private Property And Heritage4.4To examine the effect of private property and heritage upon inequality in an agent-society we extended the Conte and Castelfranchi model: agents may reproduce and the offspring inherit the sum of the strength of their parents. To keep things simple and the number of agents constant we decided that one in 100 time steps each agent would have the chance to produce offspring. Then, the agent chooses another agent who is next to it within its von Neumann-neighbourhood. They unite their strength, produce two children, divide their total strength and forward it to the children. The parents die immediately and the children take their places in the grid. The whole reproduction process is completed in one time step. We varied the share the children inherit from their parents (alpha i = alpha j = 0.5 in experiment 1a, alpha i = 0.9, alpha j = 0.1 in experiment 1b). The results are given in tables 5 and 6. The results are gathered from sets of 100 matches for each Strategy. The values shown are from left to right: The average of the agent's strength (Str) at the end of the match, the standard deviation of Str, the average of the standard deviation (St. Dev) of the agents' strength values at the end of the match, the standard deviation of St. Dev, the average number of aggressions (Agg) occuring during one match, and the standard deviation of Agg. Alpha i (j) represents the share that child i (j) inherits from the total strength of its parents.4.5As can be seen from table 5, neither the average strength of the agents, nor the number of aggressions change qualitatively. With variance, or inequality, however, the pattern varies. Private property and equal heritage prove to be extremely equalizing. The redistribution of strength due to the equal heritage reduces the inequality among the agents dramatically. This effect has no feedback on the number of aggressions or the average strength of the agents. The normative strategy is found to do best at increasing the average strength of the agents, reducing inequality among them, and reducing aggression.4.6As can be seen from table 6, when there is unequal heritage neither the average strength of the agents, nor the amount of aggressions change qualitatively. With variance, or inequality, however, the pattern varies. Private property and unequal heritage lead to extreme inequality. The redistribution of strength due to the unequal heritage increases the inequality among the agents dramatically. This effect has no feedback on the aggressions or the average strength of the agents.The normative strategy is found to do best at increasing the average strength of the agents, and reducing aggression, but now, it proves to be worst in producing inequality. Most interestingly, blind aggression now leads to the highest degree of equality!Second Experiment: Unequal Renewal Of Resources4.7To examine the effect of unequal renewal of resources upon inequality in an agent-society we extended the Conte and Castelfranchi model. In human societies, the unequal renewal of resources is a social fact. Merton has called this phenomenon the Matthew effect (Merton 1968), whereas Zuckerman (1993) used but did not introduce the term cumulative advantage to describe the same phenomenon. According to the Matthew effect (derived from the New Testament according to St. Matthew 25,29) those who already own a portion of something shall be given more.4.8In our model, the nutritional value of food is no longer constant. When a food item is replenished and happens to fall at the same location as an agent, the nutritional value is set to depend on the strength (s) of the agent. The higher the strength of the agent during the previous time step the larger is the nutritional value (v) of the replenishing food item:To avoid hopelessly deprived agents the minimum nutritional value of a food item is set to 20. Food landing on an empty position has the nutritional value of 20.4.9With this model, the nutritional value of food items may become very large. The nutritional value increases with time, as the agents' strength increases with time. For example, a food item landing on the cell already occupied by an agent of strength 5,000 would have a nutritional value 49,620 (with beta = 10.0). In order to avoid the dramatic increase of the average strength of the agents (the results would hardly be comparable to the results of previous experiments with constant nutritional value of 20 units) we introduce a time restriction. An agent cannot consume food items of more than 20 nutritional units at once. Instead, it eats away a portion of 20 in two rounds. Each food item is split into n portions of size 20, and to consume the portions the agent will need n * 2 time steps. For instance, it will need 6 rounds to consume a food item of 42 (42 is split into 2 portions of 20 units and one of 2 units). What we have in mind as the social equivalent in a primitive human society which respects the finder-keeper norm is a hunter having found a very big resource, e.g. a mammoth. The hunter and his family will be satiated very soon, but most of the mammoth will remain. The hunter and his family will need some weeks to consume the whole mammoth before hunting again. We tried to implement this logic in our model. In the experiments we varied the extent of the Matthew effect (parameter beta: beta = 0.2 in experiment 2a; beta = 10.0 in experiment 2b, and beta = 0.0375, removed time restriction in experiment 2c). The results are given in tables 7, 8, and 9.  4.10As can be seen from tables 7, 8 and 9, the average strength of the agents is slightly greater. The normative MAS has the highest average strength and the lowest degree of aggression. Compared to the original model (see Table Remodeling Normative Behaviour From A Sociological Point Of View4.11Next, we will model normative behaviour from a more sociological point of view. Following our earlier suggestions we impose norms on all agents of a MAS, introduce a mechanism that decides about the respect or disrespect of norms, and introduce a mechanism that decides about the defence of norms. We consult sociological theories on deviant behaviour to specify the theoretical model.Theory4.12We have decided to implement Haferkamp's theory of action approach to deviant behaviour. In his theory of deviant behaviour 4.13Haferkamp defines norms as conceptions which are internalized by the majority of members of a social situation. The conception implies the correct (re)actions to defined situations and the certitude that deviance will be sanctioned 4.14According to The Model4.15In our model we implement Haferkamp's approach. We start from the Castelfranchi, Conte and Paoluccis later study with mixed populations (1998: 5.1-5.3). The modeling of the agents is extended as follows:The agents live in a two-group society, in-group g 1 and out-group g 2 . Members of the ingroup are more resourceful and have more power than members of the out-group.Those individuals who are most powerful rule and therefore decide about the institutionalization of norms in society as a whole. Each time step members of the in-group transfer some of their resources to a redistribution agent on the macro level in exchange for the institutionalization of norm n 1 in situation s 1 on the society level. Institutionalization on the society level means that norm n 1 is saved in the knowledge base of the redistribution agent. The redistribution agent redistributes the resources uniformly to all agents (Here, we deviate from Haferkamp who redistributes these resources only to members of the out-group.The problem is, that the redistribution agent in Haferkamp's theory would have to be omniscient about the membership of each agent with respect to in-group and out-group. This seems unrealistic to us). Each agent has internalized that on the level of the society norm n1 holds in situation s 1 .The agents are able to identify and define social situations. If certain conditions are given agent a 1 (a 2 ) who is a member of the in-group (out-group) will identify and define situation s 1 as situation s 1 . All agents know that situation s 1 implies norm n 1 . Whereas agents of the ingroup comply with norm n 1 and show behaviour b 1 , the members of the out-group transgress the norm and show behavior b 2 . Deviant behaviour is sanctioned by members of the in-group. Agent a 1 will sanction agent a 2 if it observes it reacting by behaviour b 2 to situation s 1 . Whenever an agent of the in-group sanctions an agent of the out-group this will increase its power and decrease its resources.4.16The model was implemented using the finder-keeper norm. Situation s 1 is a situation in which an agent a perceives another agent b who has a claim to the food items which are placed in b's von Neumann-neighbourhood (the food is 'flagged' for b). Each agent has internalized that on the level of the society the finder-keeper norm holds in situation s 1 . The members of the in-group (out-group) comply (not) with the finder-keeper norm and show behavior respect-the-finder (attack-any-weakerfinder, or attack-any-finder). These behaviours are the redefined agent's normative, strategic, or blind aggression strategies introduced by 4.17The results are given in tables 10 and 11. (The experiment blind vs strategic is not of interest here because our argument concentrates on the normative strategy. We have changed the algorithm of the normative strategy. This strategy is not used in the experiment blind vs strategic. If we simulate blind vs strategic we would need two subcases, one in which the strategic are the in-group and one with the blind as the in-group. Furthermore, we would have to introduce sanctioning into the blind as well as the strategic algorithm in case this strategy represents the in-group. The results would not be comparable to the Castelfranchi, Conte and Paolucci experiment.)Results4.18As can be seen from tables 10 and 11, in the mixed population case, the normative strategy no longer beomes the worst (as it was in the Castelfranchi, Conte and Paolucci experiments). The normative strategy is now better than the blind one (table 5.4In the following, we will concentrate on the second question. There are two promising approaches to building a dynamic model:1. Haferkamp's dynamic conception is that groups compete with each other and try to expand their norms to further groups. His explanation is on the macro level and he argues that on different levels of generality there exist different in-groups and out-groups. The in-groups try to extend their norms to levels of higher generality. On levels of higher generality personal contact between the members of the in-group becomes more difficult which leads to the formation of new in-groups on levels of lower generality. 2. Sutherland has given a dynamic explanation of deviant behaviour on the micro level Criminal behaviour is learned in interactions with persons who show criminal behaviour. But the contact with these persons is not the crucial point. It is the contact with the deviant behaviour itself, the observation of the techniques and the motives that leads people to learn deviant behaviour from others. Sutherland assumes that a majority of contacts with deviant behaviour leads a person to learn and finally show deviant behaviour (this is a very simple assumption which was operationalized by Opp (1974: 165)).5.5First experiments have shown that there is a serious problem that has to be overcome before the dynamic model will succed. In contrast to the cellular worlds of Hegselmann and Flache (1998) and 5.6Finally, we would like to encourage further experiments into norm-governed and norm-abiding agents. From the computational point of view, the advantages of norm-governed systems are avoiding useless, stupid, and self-destructive behaviour as favoured by the rigid execution of routines, as well as the spreading of errors and deviations produced by pure imitation. Therefore, it is promising to construct autonomous artificial agents with a capacity of applying norms. Like Castelfranchi, Conte and Paolucci, we have investigated the costs of complying with norms, as norm-governed systems not only have advantages, but also costs. From the computational point of view, like the Conte and Castelfranchi model, even our model represents norm-abiding behaviour using norms as built-in constraints. We should advance to more intelligent representations of normative behaviour, with norms as built-in ends, or built-in obligations.Table 1 : Alternative representations of norms inside the agents' architectures (norm-abiding systems)Table 2 : Types of actions in the Conte and Castelfranchi model Action type Order of preference among these actionsTable 3 : The results of the Castelfranci and Conte modelTable 5 : Private property and equal heritage (alpha i = alpha j = 0.5)Table 6 : Private property and unequal heritage (alpha iTable 7 : Unequal renewal of resources (beta = 0.2)Table 8 : Unequal renewal of resources (beta = 10.0) Strategy Str st.dev. St.Dev st.dev. Agg st.devTable 9 : Unequal renewal of resources (beta = 0.0375), time restriction removedTable 10 : Normative behavior from a sociological point of view; two sub- populations: Blind vs Normative (50:50) Strategy Str st.dev. St.Dev st.dev. Agg st.dev Pow st.devTable 11 : Normative behavior from a sociological point of view; two sub- populations: Strategic vs Normative (50:50) StrategyThe results are gathered from sets of 100 matches for each Strategy. The values shown are from left to right: The average of the agent's strength (Str) at the end of the match, the standard deviation of Str, the average of the standard deviation (St. Dev) of the agents' strength values at the end of the match, the standard deviation of St. Dev, the average number of aggressions (Agg) occuring during one match, the standard deviation of Agg, the average value of power (Pow) and the standard deviation of Pow.4.19However, this model has a serious problem. It is incomplete with respect to the power variable. The power of the in-group agents increases continuously, whereas the power of the out-group agents remains constant. Power is not consumed. The problem can be traced back to Haferkamp who has not presented a closed theory of power within his theory of deviant behaviour. We have to work on this problem and include a theory of power in order to be a convincing functional equivalent to Castelfranchi, Conte and Paolucci's later study with mixed populations and communication.4.20Nevertheless, we are convinced that it is a promising approach to include power in the computational study of norms. Following the game theoretic models, the computational study of norms has up to now ignored the importance of power in explaining how norms affect social behaviour, how norms emerge, become established and internalized, and change. In our model, we have introduced power as an important variable that decides about the institutionalization of norms and the distribution of wealth. At present, as the model has to be systematically tested with respect to the robustness of its results, we would like to stress the advance in theory adequacy that our model represents. The numerical results are also important, but only a sensitivity analysis will reveal systematically the critical values of parameters within the parameter space that lead to a functional change of the finder-keeper precept with respect to equality and aggression.Conclusion and Future Work", "conclusions": "", "SDG": [10]}, "social_area_analysis_data_mining_and_gis": {"name": "Social area analysis, data mining, and GIS", "abstract": " There is a long cartographic tradition of describing cities through a focus on the characteristics of their residents. A review of the history of this type of urban social analysis highlights some persistent challenges. In this paper existing geodemographic approaches are extended through coupling the Kohonen Self-Organizing Map algorithm (SOM), a data-mining technique, with geographic information systems (GIS). This approach allows the construction of linked maps of social (attribute) and geographic space. This novel type of geodemographic classification allows ad hoc hierarchical groupings and exploration of the relationship between social similarity and geographic proximity. It allows one to filter complex demographic datasets and is capable of highlighting general social patterns while retaining the fundamental social fingerprints of a city. A dataset describing 79 attributes of the 2217 census tracts in New York City is analyzed to illustrate the technique. Pairs of social and geographic maps are formally compared using simple pattern metrics. Our analysis of New York City calls into question some assumptions about the functional form of spatial relationships that underlie many modeling and statistical techniques.", "keywords": "Self-Organizing Maps,Geodemographics,New York City,Data mining,GIS", "introduction": "In gearing up for the first United States decennial census in 1790, James Madison argued that the census should be ''extended so as to embrace some other objects besides the bare enumeration of the inhabitants; it would enable them to adapt the public measures to the particular circumstances of the community\" ). Madison's idea, that knowing something about the characteristics of local populations improves local governance is accepted as a basic premise in planning, politics, and policy analysis. However how one understands the particular circumstances of a community is a methodological question that has been evolving for over a century.(Kurland & Lerner, 1987, p. 139Madison's proposal to extend the census to include the occupations of inhabitants was rejected by the United States Senate in 1790. In a letter to Jefferson, Madison reflected that his plan was ''thrown out by the Senate as a waste of trouble and supplying materials for idle people to make a book\" . Unlike in Madison's day, data about cities and the people who live in them is now abundant; in fact data are so abundant and complex that integrating available information into the public planning processes is often difficult. The first census asked five questions; the long form of the questionnaire for the 2000 decennial census of population was 10 pages long and included over 50 questions. Many municipalities now maintain detailed datasets describing crime, traffic, school performance, the built environment, and many other facets of urban life. The volume of data currently available to planners is excellent fodder for urban scholars. Yet, it remains a challenge to communicate the complexity of the urban social landscape in an engaging and efficient manner.(Cohen, 1981, p. 47)In addition to a dramatic increase in the volume of information, new forms of analysis that emphasize an exploratory approach and are based on computational 0198-9715/$ -see front matter \u00d3 2007 Elsevier Ltd. All rights reserved. doi:10.1016/j.compenvurbsys.2007.11.  principles have become commonplace. Data mining is ''the extraction of implicit, previously unknown, and potentially useful information from data\" 004. Machine learning techniques of data mining, while still seldom used in urban analysis, have the potential to help analysts develop detailed differentiation of the urban landscape. In contrast to more conventional multivariate statistical methods such as factor analysis, principal component analysis, and multidimensional scaling, they tend to be less bound by a priori assumptions. Geographic Information Systems, on the other hand, are widely used in urban analysis because they facilitate cartographic visualization and management of geographically referenced data.(Witten & Frank, 2005, p. xxiii)Our goal in this paper is to revisit the problem of describing communities through a focus on the characteristics of residents. The history of residential segregation by race and income in America has supported the use of very general colloquial descriptions of neighborhoods that focus almost exclusively on combinations of these two factors. We present a novel application of geographic information systems by integrating them with a data-mining technique to characterize populations in urban areas using large datasets. The Kohonen Self-Organizing Map algorithm  is used to develop a geodemographic classification of a dataset containing 79 attributes describing census tracts in New York City. The Self-Organizing Map extends current geodemographic practice by allowing the formalization of spatial relationships between physical (geographic) space and social (attribute) space. The result is a typology of census tracts presented as a pair of linked maps -one representing social space and another representing geographic space. These maps capture the complexity of New York's social landscape and provide insight into the relationship between geographic proximity and social similarity at the census tract level. The relationship between proximity and similarity has potentially important implications for modeling and statistical techniques that drawing on (Kohonen, 1997) First Law of Geography make assumptions about the functional form of spatial relationships.Tobler's (1970)", "body": "In gearing up for the first United States decennial census in 1790, James Madison argued that the census should be ''extended so as to embrace some other objects besides the bare enumeration of the inhabitants; it would enable them to adapt the public measures to the particular circumstances of the community\" Madison's proposal to extend the census to include the occupations of inhabitants was rejected by the United States Senate in 1790. In a letter to Jefferson, Madison reflected that his plan was ''thrown out by the Senate as a waste of trouble and supplying materials for idle people to make a book\" In addition to a dramatic increase in the volume of information, new forms of analysis that emphasize an exploratory approach and are based on computational 0198-9715/$ -see front matter \u00d3 2007 Elsevier Ltd. All rights reserved. doi:10.1016/j.compenvurbsys.2007.11. Our goal in this paper is to revisit the problem of describing communities through a focus on the characteristics of residents. The history of residential segregation by race and income in America has supported the use of very general colloquial descriptions of neighborhoods that focus almost exclusively on combinations of these two factors. We present a novel application of geographic information systems by integrating them with a data-mining technique to characterize populations in urban areas using large datasets. The Kohonen Self-Organizing Map algorithm Maps and Neighborhood TypologiesSince the turn of the previous century, advocates and social scientists have been mapping the socioeconomic variation in cities through looking at residential patterns. Charles Booth's poverty maps of London are a classic effort to map this social landscape. Booth, working between 1886 and 1903, classified London's streets as using seven categories: wealthy, well-to-do, fairly comfortable, mixed, poor, very poor, and vicious, semi-criminal In spite of the abundance and complexity of spatial data describing the US population in the planning and policy context, one often finds that we have not moved very far beyond Booth's classification system. Neighborhoods are often differentiated using just a few attributes -the income, race, and occupation of inhabitants and the density of the built environment. Descriptive terms like ''working class suburbs\" and ''poor inner city\" evoke images of prototypical neighborhoods. Among the residents of a given city, neighborhood names are often signifiers of subtle differentiations in social and physical landscape. These subtle distinctions are often hard to communicate to non-residents and may not be commonly understood by residents.In the modern context, the most sophisticated efforts to classify populations are known as geodemographic or market segmentation systems. Geodemographic systems classify small areas into discrete categories using consumer behavior, lifestyle, and demographic data. These tools are widely used in the commercial sector and multivariate social classifications of ''neighborhoods\" has become an international industry As described by In the private sector geodemographers then assign evocative titles to each cluster. Names like ''American Dreams\" and ''Multi-Culti Mosaic\" are used by the PRIZM lifestyle segmentation system in the United States Shevky's early work on social area analysis was instrumental in the emergence of ''factorial ecology\" as a line of inquiry. The term, factorial ecology, emerged in the mid-60s and refers to the use of factor analysis to differentiate areal (ecological) units using the characteristics of residents Factor analysis is a method to reduce a large matrix of units of observation and their attributes to a smaller number of factors. ''If there are n areas and m variables, an n \u00c2 m matrix is used to list the manifest evidence. An atlas comprising m plates could also depict the variations. Factorial methods are brought into play to determine the latent structure of dimensions of variation -the repetitive sequences -underlying the manifest experiences of the atlas.\"The smaller matrix is a more concise description of the economic and demographic variability of census tracts. Factor scores are sometimes described as ''latent\" or ''fundamental\" variables. Interpretation of latent variables is a matter of some debate, some use latent variables to explain urban residential patterns In the explanatory mode factor scores are interpreted as representations of theoretical constructs The goal of commercial and public sector geodemographic packages is to place local areas in some national context based on the characteristics of residents, that is, their primary purpose is descriptive generalization. As national classifications have proven useful for marketing and are widely used as predictors of consumer behavior The problem of assigning labels in the inductive, quantitative, analytical techniques that have been used since Shevky is essentially a problem of designing an interface to the classification system. In addition to labels, geodemographic systems often have a hierarchical structure which allows the user to explore the classification with various levels of detail. The UK 2001 output areas classification was constructed by first creating seven categories, and the subdividing each of those categories further to create a final dataset with 7 high level categories, 21 mid level categories, and 52 classes at the finest level of details Self-Organizing MapsMaps preserve topological relationships among objects in space. In the cartographic context, entities and features that are close to each other in the real world are represented close to each other on a map. There is evidence to suggest that the ability to situate oneself on a map is an innate human ability The concept of a map can also be applied to non-geographic objects; or it can be used to visualize geographic objects (census tracts) in a spatial but non-geographic context. That is, census tracts can be organized in space based upon the similarity of their characteristics rather than their geographic proximity. This is the basic idea behind the Kohonen algorithm that creates Self-Organizing Maps (SOMs) that maps observations with similar attribute patterns onto contiguous areas in output space. The resulting visualizations are called self-organizing feature maps SOM outputs are attribute maps. Unlike thematic maps, SOM feature maps excel at the display of high dimensional datasets. Feature maps are a projection of high dimensional attribute space such that attribute vectors of a particular generalized form are associated with locations in output space Space in a SOM consists of a regular lattice of ''neurons\" each of which stores a vector describing attribute weights. The elements of the lattice generally are square or hexagonal. Through the SOM mapping process, each neuron in the output layer is sensitized to a particular configuration of attributes and observations are ''fit\" to neurons much as a regression model is fit to data. It is useful to think of the neurons on the feature map as buckets for data. Observations that are similar are placed either in the same bucket or in buckets that are topologically close to one another on the feature map. For example, places with many wealthy householders, with high levels of education, high homeownership rates, and low poverty rates would end up in buckets that are near each other and clustered in a region of the feature map. On the other hand, census tracts where poverty is abundant and residents typically have low levels of education would end up clustered in buckets in a different region of the SOM; probably quite far away from the well educated and wealthy people. Places that have both wealthy households and poor households would end up occupying a region of the map somewhere between the two extremes. Training a SOM is an iterative process of defining what types of observations are associated with buckets in different regions of the feature map. By examining the contents of each bucket after the SOM is completely trained, one can get a sense of how different regions of the SOM represent different types of observations. SOM feature maps of different sizes have different characteristics Relatively few geographic applications of SOM have so far been reported in the literature. The SOM has successfully been trained to classify digital satellite images The Kohonen Self-Organizing Map algorithm extends geodemographics, and similar cluster-like methods by constructing topological relationships between classes (Kohonen, 1997). Geodemographic classifications group areas with similar characteristics and apply descriptive labels to these classifications. One of the problems with such classifications is that groups are discrete. It is not clear how similar or dissimilar classes are in a multivariate sense because classes are typically described by comparison to regional or national averages. By constructing topological relationships between classes, the Kohonen algorithm allows the user to understand the degree of similarity or dissimilarity between areas based upon their location in a two dimensional projection of multidimensional attribute space.To explore the efficacy of SOMs and geovisualization as a geodemographic tool, a dataset with 79 variables is used to describe census tracts in New York City. The variables used in the analysis are listed in the Appendix. Variables from the 2000 decennial census were chosen to represent some aspect of New York's social geography. Census tracts are mapped onto a 45 \u00c2 30 cell map of ''social space\" consisting of 1350 buckets (neurons) for 2217 census tracts. Buckets can be interpreted as classes or clusters of similar data. The topological relationships built by the Kohonen algorithm allow the user to examine any number of buckets or classes -selecting a single bucket would be akin to exploring a single geodemographic class at the highest level of disaggregation, selecting groups of contiguous cellsobjectsSmall SOM (7 Cells)Large SOM (24 Cells)Each cell contains approximately 4 observtions on average. It is a more general categorization than the larger SOM below.Each cell contains just over 1 observation on average. Regions of the SOM are more specialized. would be parallel to exploring a geodemographic classification at a higher level aggregation. With an integrated visual data-mining approach, we avoid the use of category labels. Since our approach is visual, we can define a very large number of categories and still present our results in a way that is easy to interpret. Pairing the synthetic map of attribute space with a geographic map of census allows the user to explore how groups of tracts in attribute space map to geographic space, and vice versa. The comparison of map pairs can be simplified and standardized through the use of a statistic to compare map patterns.A simple scaled measure of the average distance between observational units was developed to compare maps of social space to maps of geographic space. To compare maps, we compute the average distance between all pairs of census tracts and all pairs of SOM buckets (neurons) that satisfy some pre-specified criteria (cases). For both maps the distance between cases is compared to the average distance between all observational units to obtain a relative measure of dispersion. This relative dispersion index is formulated as P where d ij is the Euclidean distance between observations i and j. Small numbers indicate that neurons or census tracts satisfying a given criteria form compact regions, while large numbers indicate that the units of interest are further apart than average. The correspondence of dispersion statistics between map pairs allows one to assess the relationship between geographic proximity and social similarity.Mapping New York CityNew York City is an ideal subject for testing for spatial demographic methods. New York is home to what may be the most racially and ethnically diverse zip code in the United States, 11373 in Elmhurst (a neighborhood in Queens County) where the local high school has students from 96 different nations and 59 languages are spoken The SOMPAK code library was used and a SOM was trained using random selection of 50% of the census tracts There are a number of different ways to summarize a SOM. Traditionally, component planes and the unified distance matrix (or U-matrix) are utilized. The U-matrix is a visualization of the SOM that illustrates the distance between adjacent neurons in attribute space (the U-matrix for the SOM described below is shown in Fig. The analysis of Community Board 8 suggests that upper right corner of the SOM feature map represents the more affluent portions of the city (Fig. The areas represented by the upper right are more geographically clustered than the lower left, 0.53 for the upper right versus 0.84 for the lower left (see Table Selecting large contiguous regions of the SOM as shown in Figs. Another approach to exploring and interpreting the trained SOM feature map is to select an area based on a single criterion, say census tracts where more than 90% of residents are not caucasian, Fig. Conclusions", "conclusions": "Representing the complexity of urban populations through cartography has been an area of inquiry since the 1890s. As data became more abundant and statistical techniques more refined, social area analysis and then factor analysis emerged. Modern geodemographic techniques have their roots in the analytic framework of the Chicago School and the methods pioneered by Eshref Shevky. Fac-tor analysis and geodemographic techniques are limited in that when classifications use large high-dimensional datasets it is difficult to assess the multidimensional similarity/difference between classes. Factorial ecology and geodemographics have been critiqued for their use of labels (Goss, 1995;. Self-Organizing Maps belong to a new class of approaches to the problem of describing urban populations. They are noteworthy in that when combined with geographic information systems they allow one to filter the complex demographic reality of New York City and are capable of highlighting general social patterns while retaining the fundamental social fingerprints of the city.Palm & Caruso, 1972)One of the precepts of the human-ecologic approach that underlies geodemographics and urban factorial ecol-Census tracts where more than 90% of residents are not caucasian Census tracts that map to the same regions of the feature map as those places where more than 90% of residents are not caucasian ogy is that populations sort themselves geographically to form socioeconomically differentiated areal units or neighborhoods (Park & Burgess, 1925;. This framework is important to the interpretation of census reporting districts. The SOM method is a powerful tool to extract high-level structures of groupings of census tracts in the multidimensional attribute space. The comparison of patterns or structures in geographic space and attribute space is of interest as it sheds light onto the basic hypothesis of the human-ecologic approach to urban analysis.Robson, 1969)Our analysis of New York City provides insight into Tobler's First Law of Geography which states that proximal things are more similar than distal things. This law seems not to hold when the ''things\" of interest are the census tracts of New York City. We find that things which are often quite similar, that project to same region of the SOM map, are often in very different sections of the city. Welldefined, compact geographic regions are often composed of neighborhoods that are widely distributed in attribute space; as an example, Brooklyn's Community Board 6 is a geographically defined compact region comprised of census tracts that map to many regions of the attribute space (Table ). The ''First Law\" has important implications for spatial analytical techniques where buffers, kernels, or weights matrices are used in estimation or to treat the environment endogenously. It is important to understand that in a multidimensional sense, the assumption of proximity equating to similarity does not hold at the census tract level, in New York City. Whether and to what extent this may also be the case in other metropolitan areas remains to be established through case studies across a representative sample of metropolitan areas.1Self-Organizing Maps share many of the limitations of factor analysis and geodemographic clustering techniques. When these techniques are applied to census divisions they must be interpreted with caution. Any analysis of census tracts in an urban area raises important questions about the nature of tracts. Are tracts a meaningful unit of analysis? An analysis of census tracts is not an analysis of people and one must be careful to limit inference to scale of observation -any statements in this paper are about groups, not individuals. How important is the variability of populations within a tract to the overall classification scheme that results from a particular analytical approach?The ability to visualize SOMs using commercial geographic information systems is limited. Interfaces between the SOM data-mining method and GIS are not widely available however with lots of pointing and clicking or some simple scripting the connections can be made. Customized tools for the visualization of Self-Organizing Maps in a geovisualization context are quickly becoming available (Guo, Chen, MacEachren, & Liao, 2006;Takatsuka, 2001;Thill et al., 2008;).Yan & Thill, forthcoming, in pressFinally, one of the most important aspects of using Self-Organizing Maps in demographic analysis is variable selection. The absence of suitable theory to guide variable selection is a troubling reality; there is no current analogue to the Shevky-Bell hypothesis. Absent theoretical guidance the best a researcher can do is choose variables deemed important to the problem at hand. SOMs are an exploratory technique and as such are not useful for confirming theory. Nor are SOMs easily integrated into traditional statistical modeling techniques. While SOMs are subject to criticism because of their inability to extend urban theory, when used in the exploratory mode they provide insight into the residential population of a city and can shed light on some of the assumptions underlying many urban analytical techniques such as the relationship between proximity and similarity. In sum linking the Kohonen Algorithm with GIS helps in understanding the ''particular circumstances of the community.\" ", "SDG": [10]}, "aidr_artificial_intelligence_for_disaster_response": {"name": "AIDR: Artificial Intelligence for Disaster Response", "abstract": " We present AIDR (Artificial Intelligence for Disaster Response), a platform designed to perform automatic classification of crisis-related microblog communications. AIDR enables humans and machines to work together to apply human intelligence to large-scale data at high speed. The objective of AIDR is to classify messages that people post during disasters into a set of user-defined categories of information (e.g., \"needs\", \"damage\", etc.) For this purpose, the system continuously ingests data from Twitter, processes it (i.e., using machine learning classification techniques) and leverages human-participation (through crowdsourcing) in real-time. AIDR has been successfully tested to classify informative vs. non-informative tweets posted during the 2013 Pakistan Earthquake. Overall, we achieved a classification quality (measured using AUC) of 80%. AIDR is available at http://aidr.qcri.org/.", "keywords": "H.4 [Information Systems Applications]: Miscellaneous,D.2.2 [Software Engineering]: Design Tools and Techniques Stream processing,Crowdsourcing,Classification,Online Machine learning", "introduction": "Information overload during disasters can be as paralyzing to humanitarian response as the absence of information. During disasters, microblogging platforms like Twitter re-Figure : AIDR: overall approach ceive an overwhelming amount of situation-sensitive information that people post in the form of textual messages, images, and videos. Despite the fact that social media streams contain a significant amount of noise, much research 1[9, has shown that these same streams of information also include relevant, tactical information (e.g., regarding infrastructure damage, needs, donations). Because social media communications provide a rich trove of information, it is possible that even a small amount of relevant information can greatly enhance situational awareness and help responders and other concerned parties make more informed decision.4]Finding tactical and actionable information in real time within a rapidly growing stack of information is challenging for many reasons. For instance, performing information extraction on short bursts of text (e.g., on 140-character tweets) is significantly more difficult than performing the same task on large documents such as blog posts pr news articles . Moreover, research has shown that pre-trained classifiers significantly drop in classification accuracy when used in different but similar disasters [6]. This requires learning and training new classifiers using fresh training data every time a disaster strikes.[3]Considering the amount of information that flows on Twitter, it is challenging for emergency managers and other stakeholders to investigate each individual tweet in real-time to look for useful information. Therefore, our goal is to leverage different machine learning techniques (e.g., information classification, and extraction) to perform the job automatically. Moreover, we want humans (i.e. volunteers) to label part of the incoming data to be used for the training purposes of machine learning algorithms. Above all, the whole process must be ingesting, processing and producing only credible information in real-time, or with low latency .[5]The rest of the paper is organized as follows: In the next section, we describe domain challenges in crisis response. In section 3, we present an overview of AIDR from an end-user perspective, as well as an evaluation. Section 4 presents AIDR's architecture and implementation. A demonstration storyboard is described in section 5, followed by the conclusion in section 6.", "body": "Information overload during disasters can be as paralyzing to humanitarian response as the absence of information. During disasters, microblogging platforms like Twitter re-Figure Finding tactical and actionable information in real time within a rapidly growing stack of information is challenging for many reasons. For instance, performing information extraction on short bursts of text (e.g., on 140-character tweets) is significantly more difficult than performing the same task on large documents such as blog posts pr news articles Considering the amount of information that flows on Twitter, it is challenging for emergency managers and other stakeholders to investigate each individual tweet in real-time to look for useful information. Therefore, our goal is to leverage different machine learning techniques (e.g., information classification, and extraction) to perform the job automatically. Moreover, we want humans (i.e. volunteers) to label part of the incoming data to be used for the training purposes of machine learning algorithms. Above all, the whole process must be ingesting, processing and producing only credible information in real-time, or with low latency The rest of the paper is organized as follows: In the next section, we describe domain challenges in crisis response. In section 3, we present an overview of AIDR from an end-user perspective, as well as an evaluation. Section 4 presents AIDR's architecture and implementation. A demonstration storyboard is described in section 5, followed by the conclusion in section 6.DOMAIN CHALLENGES IN CRISIS RESPONSEDuring disasters, social media messages provide real-time or low-latency situational awareness information that can enable crisis responders to be more effective in their relief efforts Below, we discuss the roles of automatic computation, human computation, and the combination of the two in the processing of social media streams.Role of machine intelligence: Traditional information processing cannot be employed in this model, as disaster responders cannot wait to collect information, and then curate and classify it offline. Instead, responders and other stakeholders require real-time insight and intelligence as the disaster unfolds. To this end, we aim to ingest and classify social media streams in real-time through automated means with the help of human intervention.Role of human intelligence: When attempting to perform non-trivial tasks, machines alone are not capable of great accuracy. Human intervention is required to verify, teach, and/or correct the machine output Combined intelligence: Relying solely on humans to investigate each individual message is challenging due to the 1 The United Nations organizes its agencies into clusters: http://business.un.org/en/documents/6852. scale of information posted on Twitter, which goes beyond the processing capacity of humans. To this end, an automatic approach is required that can intelligently crowdsource messages to obtain training examples when needed, and additionally, the system should effectively use crowdsourcing workers both in terms of time (i.e., for volunteers) and cost (i.e., for paid workers).SYSTEMS OVERVIEWThe purpose of AIDR (Artificial Intelligence for Disaster Response), 2 is to filter and classify messages posted to social media during humanitarian crises in real time.Specifically, AIDR collects crisis-related messages from Twitter 3 (\"tweets\"), asks a crowd to label a sub-set of those messages, and trains an automatic classifier based on the labels. It also improves the classifier as more labels become available. Automatic classification using pre-existing training data is not a satisfactory solution because although crises have elements in common, they also have specific aspects which make domain adaptation difficult. Crisis-specific labels lead to higher accuracy than labels from past disasters AIDR in action: end-user perspectiveAIDR users begin by creating a collection process by entering a set of keywords or a geographical region that will be used to filter the Twitter stream, as shown in Figure Finally, an output of messages sorted into categories is generated, which can be collected and used to create crisis maps and other types of reports. An example consumer application is the current version of CrisisTracker, 4 which uses AIDR to enable users to slice the data by categories of interest, which vary by deployment scenario to include for instance eyewitness accounts, reports of violence, or reports of damage infrastructure.EvaluationAIDR was successfully tested during a recent earthquake in Pakistan in 2013. We set AIDR up to collect tweets using the hashtags (#Pakistan, #Awaran, #Balochistan, #earthquake, #ReliefPK) on September 25, 2013 at 20:20:09 AST 5 on a request of UN Office for the Coordination of Humanitarian Affairs (OCHA). Within a few hours, SBTF (Standby Task Force) 6 volunteers were asked to label whether a given tweet was informative (i.e., if the tweet reports infrastructure damage, casualties, donation offered or needed, etc.). They tagged about 1,000 tweets approximately within 6 hours. Though the prevalence of the negative class (\"not informative\") was high, the system was able to learn from \u2248200 informative labeled tweets. In this setup, we achieved  ARCHITECTURE & IMPLEMENTATIONThe general architecture of AIDR is shown in Figure The training examples required by the system can be obtained either using internal web-based interface or by calling an external crowdsourcing platform. The former aims at enabling the collection owner to provide trusted training examples, whereas the latter collects training examples using public crowdsourcing with the help of volunteers. We assume that there is a fixed budget of crowdsourcing work, but even if that is not the case, we see this as a problem of cost effectiveness. To ensure quality, training examples are obtained in a way that maximizes marginal quality gains per human label. The maximization of quality gains per label is done by performing intelligent task generation by selecting a small set of messages to be labeled by humans. Details on AIDR crowdsourcing part and task generation strategies are discussed in detail in our additional research The output of AIDR (i.e., classified tweets) can be accessed through output adapters, which are exposed as an API. To show real-time classified items on a map or any other visualization widget, one can use AIDR's live stream output adapter. Moreover, to fulfill various visualization demands, AIDR includes APIs to retrieve the k-latest items or to subscribe to a live data feed.  DEMONSTRATIONA live demo will be presented starting from an introduction of the crisis computing domain and motivation behind the development of AIDR platform. A guided walk-through of the platform will be presented to introduce how different components of AIDR work. After demonstrating how to create collections, perform training, and enable an automatic classification process, we ask our reader to try the tool and create their own collection and perform classification without using any knowledge of machine learning.CONCLUSIONS", "conclusions": "Social media platforms like Twitter receive an overwhelming amount of situational awareness information. For emergency response, real-time disaster insights are important. Finding actionable and tactical information in real-time poses serious challenges. Effective coordination of human and machine intelligence can improve disaster response efforts. In this paper, we have described AIDR, a platform to classify Twitter messages into a set of user-defined situational awareness categories in real-time. The platform combines human and machine intelligence to obtain labels of a subset of messages and trains an automatic classifier to classify further posts. The platform uses active learning approach to select potential messages to tag, and learns continuously to increase classification accuracy when new training examples are available.", "SDG": [11]}, "an_anfis_\u2013_based_air_quality_model_for_prediction_of_so2_concentration_in_urban_area": {"name": "An AnFIS -BASED AIR QUALITY MODEL FOR PREDICTIOn OF SO 2 COnCEnTRATIOn In URBAn AREA", "abstract": " This paper presents the results of attempt to perform modeling of SO 2 concentration in urban area in vicinity of copper smelter in Bor (Serbia), using ANFIS methodological approach. The aim of obtained model was to develop a prediction tool that will be used to calculate potential SO 2 concentration, above prescribed limitation, based on input parameters. As predictors, both technogenic and meteorological input parameters were considered. Accordingly, the dependence of SO 2 concentration was modeled as the function of wind speed, wind direction, air temperature, humidity and amount sulfur emitted from the pyrometallurgical process of sulfidic copper concentration treatment.", "keywords": "Mathematical modeling,ANFIS,SO 2 air concentration", "introduction": "Sulfur dioxide (SO 2 ) pollution has long been reported to be associated with many adverse health effects (Herbarth et al., 2001;Brunekreef & Holgate, 2002;Biggeri et al., 2005;. The control studies have indicated that some respiration problems could appear exposure to increased SO 2 concentrates during period longer than 10 minutes WHO, 2006). Based on this evidence, it is recommended that a SO 2 concentration of 500 \u00b5g/m 3 should not be exceeded over averaging periods of 10 minutes duration (WHO, 2006). Some evidences of SO 2 harmful effect on human respiration organs are presented in references (WHO, 2006)(Koren, 1995;Wong et al., 2001;Barnet et al., 2005;, this is especially evident with children Biggeri et al., 2005). Also, many studies evidenced existence of increased mortality in regions with increased SO 2 concentration in the air (Herbnarth et al., 2001)(Kan & Chen, 2003;Buringh et al., 2000;.Jerrett, 2005)The European Union (EU) limits the concentration of SO 2 in the air: (1) hourly limit for protection of human health is 350 \u00b5gm -3 and must not be exceeded more than 24 times in a calendar year; (2) daily limit for protection of human health is 125 \u00b5gm -3 and must not be exceeded more than three times in a calendar year; (3) annual limit in order to protect ecosystem is 20 \u00b5gm -3 .All these measures are the result of high concentrations of SO 2 in many regions of the world which seriously endangers human health and vegetation (WHO, 2006;). In the central region of Chile in the period 1997 -1999, in the vicinity of copper smelters Caletones, high concentrations of SO 2 were registered. It is thought that there is a possibility of occurrence of acute injuries (in each year), considering that the concentration of SO 2 was in the range of 500 to 50,000 \u03bcgm -3 EU Directive, 2008. In Istanbul, Turkey, SO 2 concentrations were recorded in the range of 50-170 \u03bcgm -3 (Huidobro-Garcia et al., 2001). In Beijing, during the year 2000, SO 2 concentrations were up to 100 \u03bcgm -3 (Sahin et al., 2011). In one of the regions of Spain, in the period 2004 -2007, SO 2 concentrations ranged up to 100 \u03bcgm -3 (Chak & Yao, 2008). In the vicinity of the copper smelter in Bor, Eastern Serbia, in the period 2005-2008, in the urban part of the city, maximum monthly average SO 2 concentrations were recorded in the range of (Santacatalina et al., 2011)500-2000. In the period 2000-2008, the episodes occurred with a daily average values in the range of 5000 -8000 \u03bcgm -3 in this region 500- \u03bcgm -3 (Nikoli\u0107 et al., 2010)), when the smelter was stopped after intervention of the state for a few days because of the high toxicity of gas, and then continued to work with the same technical parameters. If the SO 2 > 1000 \u00b5gm -3 concentrations occur several times a year, it represent a significant risk to human health and vegetation (Dimitrijevi\u0107 et al., 2008). During 2011 and 2012, episodes of SO 2 contrentations up to 10.000 \u00b5gm -3 were registrated (Garcia-Huidobro et al., 2001).(Djordjevi\u0107 et al., 2013)In the region of Eastern Serbia, in Bor, within the company RTB Bor, one of the largest copper smelters in Europe operates for over 100 years. In the technology of copper production in this company, since 1975 to date, important improvement at the stage of melting, refining gas and production of H 2 SO 4 has not been done. Since 2003, immission of pollutants was monitored at eight measuring points , and after 2010 only at three measuring points, with limited measurement range and transparency of data controlled by the government, and at the two measuring points with the internal character data. In the period 2009-2011, there were episodes of extremely high concentrations of SO 2 with over 9000 \u03bcgm -3 , with fatal consequences for the bees and agricultural crops. In these cases, the company has paid damages to farmers from the surrounding and penalties for the responsible managers, and then company continued to work with the same technical parameters.(Nikoli\u0107 et al., 2010)In order to curb the growing harmful effects of air quality, urgent risk assessment and appropriate risk management tools are essential in order to ensure flexible control of high levels of pollution. For this purpose, mathematical models have become essential in the design of business decisions and engineering management of technological processes . Linear statistical models generally do not produce satisfactory results, which led to the development of nonlinear models of artificial neural networks ANNs (Yetilmezsoy et al., 2011), and more recently several adaptive neuro-fuzzy techniques -Adaptive Neural-Fuzzy Inference System (ANFIS) are developed, which have been applied successfully to control air pollution (Yilmaz & Kaynar, 2011)(Morabito & Versaci, 2003;Yildrim & Bayramogly, 2006;. In order to combine the advantages of fuzzy logic methodology and architecture of the neural network, Ashish & Rashami, 2011) has proposed a brand new hybrid, adaptive neuro-fuzzy inference system (ANFIS). ANFIS has the advantages of neural networks and fuzzy logic, where the ANN has a better ability to learn, parallel processing, adaptation, fault tolerance, while the strategy of fuzzy logic can deal with higher-level reasoning Jang (1993).(Lei & Wan, 2012)The main objective of this study is to define the appropriate mathematical model for predicting the SO 2 content (imission) at the measuring stations around the copper smelter in Bor, which currently emits the greatest amount of sulfur in the SO 2 in the urban environment smelter. Defined model should allow defining the content of SO 2 in urban areas, with acceptable statistical significance, from the amount of concentrate processed and meteorological parameters, which should enable better management of SO 2 immission in the urban environment around the copper smelter.", "body": "Sulfur dioxide (SO 2 ) pollution has long been reported to be associated with many adverse health effects The European Union (EU) limits the concentration of SO 2 in the air: (1) hourly limit for protection of human health is 350 \u00b5gm -3 and must not be exceeded more than 24 times in a calendar year; (2) daily limit for protection of human health is 125 \u00b5gm -3 and must not be exceeded more than three times in a calendar year; (3) annual limit in order to protect ecosystem is 20 \u00b5gm -3 .All these measures are the result of high concentrations of SO 2 in many regions of the world which seriously endangers human health and vegetation In the region of Eastern Serbia, in Bor, within the company RTB Bor, one of the largest copper smelters in Europe operates for over 100 years. In the technology of copper production in this company, since 1975 to date, important improvement at the stage of melting, refining gas and production of H 2 SO 4 has not been done. Since 2003, immission of pollutants was monitored at eight measuring points In order to curb the growing harmful effects of air quality, urgent risk assessment and appropriate risk management tools are essential in order to ensure flexible control of high levels of pollution. For this purpose, mathematical models have become essential in the design of business decisions and engineering management of technological processes The main objective of this study is to define the appropriate mathematical model for predicting the SO 2 content (imission) at the measuring stations around the copper smelter in Bor, which currently emits the greatest amount of sulfur in the SO 2 in the urban environment smelter. Defined model should allow defining the content of SO 2 in urban areas, with acceptable statistical significance, from the amount of concentrate processed and meteorological parameters, which should enable better management of SO 2 immission in the urban environment around the copper smelter.Study area and measuring pointsThe study area is located in southeastern Serbia, Figure l. City of Bor has about 40,000 inhabitants and is situated at a distance of 30 km from the border with Bulgaria, and about 100 km with Romania. The rivers in this region belong to the basin of the Danube River. The whole region has about 200,000 inhabitants. Near to the Romanian border is a national park Djerdap, representative tourist center of the region.Locations of measuring stations shown in Figure Measuring the concentration of SO 2 at these measuring stations are performed in accordance with standard EN 14212 ISO 10498 : 2004. Accuracy of measuring instruments is at the level of 0.4 \u03bcgm -3 .Registered parameters in real time are publicly available on the website of the State Agency for Environmental Protection (http://www.sepa.gov.rs), for measuring stations (3), ( EXPERIMEnTALIn recent years, artificial intelligence (AI) based methods have been proposed as alternatives to traditional linear statistical ones in many scientific disciplines. The literature demonstrates that AI models such as ANN and neuro-fuzzy techniques are successfully used for air pollution modeling If observing the measurement series for variables presented in Table The ANFIS system serve as a basis for constructing a set of fuzzy if-then rules with appropriate membership functions to generate the stipulate input-output pairs. The ANFIS structure is obtained by embedding the fuzzy interference system into the framework of adaptive networks Three main components of a FIS structure are: a rule base, a database, and a reasoning mechanism. The rule base has adequate number of if-then rules for levels of ranges of input variables. For example, one rule might be \"if wind speed is low, than registered SO 2 concentration in the air is high\", where low and high are linguistic variables. The database defines the membership functions applied in fuzzy rules and the reasoning mechanism performs the inference procedure In the case f(x 1 , x 2 ) is a first-order polynomial, then the model is called a firstorder Sugeno fuzzy model.The graphical presentation of general ANFIS network is presented in Figure specifies the degree to which the given X i satisfies the quantifier A i , B i , etc. Usually, membership functions are either bell-shaped with maximum equal to 1 and minimum equal to 0, or Gaussian function.Nodes located in the layer 2 are multipliers, which are multiplying the signals exiting the layer 1 nodes. For1, 2, etc. Output of each node is representing the firing strength of a rule. The i-th node of layer 3 calculates the ratio of i-th rules firing strength to sum of all rules firing strengths.This Training of the parameters in the ANFIS structure is accommodated according to the hybrid learning rule algorithm which is the integration of the gradient descent method and the least square methods. In the forward pass of the algorithm, functional signals go forward until layer 4 and the consequent parameters are identified by the least squares method to minimize the measured error. In the back propagation pass, the premise parameters are updated by the gradient descent method RESULTS AnD DISCUSSIOnThe main motive for investigations presented in this article was to draw conclusions about the possibilities of predicting the SO 2 concentration in the ambient air, under different environmental conditions and based on the influence of sulphur entering the process with the charge. This way, for modeling the dependence of SO 2 concentration, on different predictors, the data obtained from the automated measuring stations were used in combination with the data obtained from the smelting process. The data were collected during the year 2011, in the period of two months (October and November). Measurement of the four input parameters: wind speed (X 1 ); wind direction (X 2 ); air temperature (X 3 ); relative humidity (X 4 ) and the one output (Y) parameter -SO 2 concentration in the air, was facilitated using three of above described measuring stations (4, 5 and 6, in Figure The values of the measured input parameters (X i ) and the air quality indicator, investigated in this work -output of the 30 M.Savi\u0107 / SJM 8 (1) ( According to the results presented in Defining the linear correlation dependence between the output and the input parameters, with significant value of coefficient of correlation (R 2 ), provides the possibility of predicting potential excess of SO 2 concentration in the air, in the investigated area, using linear statistical analysis methods such is multiple linear regression analysis (MLRA). MLRA is one of the most widely used methodologies for expressing the dependence of a response variable on several independent variables According to values presented in Table On the other hand, if the value of correlation between two variables is not high, this doesn't automatically mean that behaviour of one variable do not influence the behaviour of other. This is indicator that their inter correlation cannot be described with linear model, however modeling based on dynamic behaviour of the variables can be used to present their inter dependences According to the number of input variables, their ranges and the variations, presented in Table To apply the ANFIS methodology the assembly of 1,800 input and output samples was divided into two groups. The first group consisted of 1,292 (\u224870 %) at random selected samples, and it was used for training of the model, whereas the second group consisted of 508 (\u224830 %) remaining samples from the starting data set, and it was used for testing the model. The selection of the variables for these two stages was performed by using random number generator. In the gathering data process for the training and the testing stage, the values for each input variable are normalized by the maximum values. This was done because of different nature and measuring units of input variables.During the training phase the correction of the weighted parameters (p i , q i , r i , etc) of the connections is achieved through the necessary number of iterations, until the mean squared error between the calculated and measured outputs of the ANFIS network, is minimal. During the second phase, the remaining 30% of the data is used for testing the ''trained'' network. In this phase, the network uses the weighted parameters determined during the first phase. These new data, excluded during the network training stage, are now incorporated as the new input values (X i ) which are then transformed into the new outputs (Y). For calculation presented in this paper MATLAB ANFIS editor was used (MathWorks, R2012b).In the phase of the network training, the necessary number of iterations was performed until the error between the measured output of the SO 2 concentration in the air -Y and the calculated values wasn't minimized and remained constant. In the case of the investigation presented in this paper, optimal number of iterations (epochs) was 10. The obtained results from the training stage can be evaluated by comparison of the calculated values Y with the measured ones (Figure The ANFIS modeling approach, in the training stage, predicted the SO 2 concentration in the air with a determination coefficient R 2 = 0.526 (Figure Another merit of obtained model is in possibility to assess the influence of single or coupled input variables on values of output variable. Such dependence of SO 2 concentration on different combinations of input variables is presented in Figure According to results presented in Figure COnCLUSIOn", "conclusions": "This paper presents the beginning of investigation of applicability of nonlinear statistical modeling on modeling the SO 2 content in the air dependence on different predictors. It was presented that such approach can be used in general, based on ANFIS method. On the other hand, the model fitting that was obtained was not that high. The reason is in the fact, that the data for only two months were used for analysis. In our future work, the procedure will be repeated with data aquatinted in longer time intervals (a year or more). Also, results of bivariate influence of predictors on output variable will be further studied in subsequent work. ", "SDG": [11]}, "application_of_artificial_intelligence_f": {"name": "Application of Artificial Intelligence for Development of Intelligent Transport System in Smart Cities", "abstract": " This study presents basic concepts and applications of Artificial Intelligence System (AIS) for development of intelligent transport systems in smart cities in India. With growing urbanization the government has now realized the need for developing smart cities that can cope with the challenges of urban living and also be magnets for investment in India. Transport system in smart cities should be accessible, safe, environmentally friendly, faster, comfortable and affordable without compromising the future needs. The Indian cities largely lacks of Intelligent Transport System in India and there are various problems such as inefficient public transport system, severe congestion, increasing incidence of road accidents, inadequate parking spaces and a rapidly increasing energy cost etc. Therefore, development of Intelligent Transportation System is essential for smart cities due to concerns regarding the environmental, economic, and social equity. Artificial Intelligence is a key technology to resolve these issues. Therefore, there is an urgent need to adopt Artificial Intelligence system for development of Intelligent Transport System to better understand and control its operations in smart cities. Hence, the main objective of this study is to present some basic concepts of Artificial Intelligence and its applications for development of Intelligent Transport System in smart cities in India. This study concludes that Artificial Intelligence system needs to be adopted to develop smart public transport system, intelligent traffic management and control, smart traveller information system, smart parking management and safe mobility & emergency system in smart cities. It is expected that this study will pave the way for development of Intelligent Transport System in smart cities in India.", "keywords": "Artificial intelligence,smart city,intelligent transport system", "introduction": "A 'smart city' is an urban region that is highly advanced in terms of overall system, safe mobility, sustainable real estate, communications and market viability. The smart-city management system handles information and controls across the different types of system needed by a city  . The United Nations estimates that the urban population of emerging economies across the world, the stride of migration from rural to urban areas is increasing. In 2050, about 70 per cent of the population will be living in urban areas, and India is no exception [1] . With growing urbanization the government has now realized the need for smart cities in urban area that can cope with the challenges of urban living and also be magnets for investment in India announcement of '100 smart cities' falls in line with this vision [2] . Intelligent transportation system play an essential role in today's world and are a vital extension for development of a smart city due to concerns regarding the environmental, economic, and social equity [1] . Intelligent transport system move people and freight by emphasizing accessible, environmentally friendly, comfortable, affordable and accessible with integrated various transport modes which is safe and operates at suitable speeds without compromising the future needs. However, at present there are various problems involved in Indian cities such as severe congestion, deteriorating air quality, increasing incidence of road accidents and a rapidly increasing energy cost due to present traffic management, the city largely lacks of Intelligent Transport System in India. As the number of vehicles increased everyday on Indian roads, the average motorists spend hours in traffic jam due to due to lack of urban planning, traffic mismanagement, and parking control which leads to billions of rupees wasted every year in major cities of India.[3]Most of the major cities in India, due to lack of Intelligent Transport System faces various problems like accidents, environmental degradation, congestion; overcrowding and parking space etc. Today India leads the world in traffic accidents and also has the highest fatality rate of traffic accidents in the world. Hence there is urgent need to develop intelligent transport systems in smart cities proposed to be developed in India for traffic control management and to tackle the rising menace of road accidents and fatalities. Artificial Intelligence is a key technology for development of Intelligent Transport System in smart citiesto better understand and control its operations and optimize the use of limited resources in smart cities. Table  represents the requirement of Intelligent Transport System in smart cities in India and need for application of Artificial Intelligence system for developing such system.1", "body": "A 'smart city' is an urban region that is highly advanced in terms of overall system, safe mobility, sustainable real estate, communications and market viability. The smart-city management system handles information and controls across the different types of system needed by a city Most of the major cities in India, due to lack of Intelligent Transport System faces various problems like accidents, environmental degradation, congestion; overcrowding and parking space etc. Today India leads the world in traffic accidents and also has the highest fatality rate of traffic accidents in the world. Hence there is urgent need to develop intelligent transport systems in smart cities proposed to be developed in India for traffic control management and to tackle the rising menace of road accidents and fatalities. Artificial Intelligence is a key technology for development of Intelligent Transport System in smart citiesto better understand and control its operations and optimize the use of limited resources in smart cities. Table Table1: Requirements of Intelligent Transportation Systems in Smart Cities.S.No.Requirements of Intelligent Transport System in Smart CitiesNeed for Application of ArtificialIntelligence System 1.Accessible in most of the area of smart cities. Need to develop multi-modal integrated public transport system 2 Minimum travelling time Need to develop intelligent traffic control and management system to reduce congestion.3Real time information system for safe and efficient movementNeed to develop smart traffic information system 4Affordable by all section of the society. Need to develop economical public transport system 5 Environmental friendly and energy efficient. Need to develop intelligent traffic control and management system 6Redesign and management of street as per the requirements of different transport modes. Hence, the main objective of this study is to present some basic concepts of Artificial Intelligence and its applications for development of Intelligent Transport System in smart cities in India. This study presents applications of Artificial Intelligence system in developing smart public transport system, intelligent traffic management and control, smart traveller information system, smart parking management and safe mobility & emergency system in smart cities.Artificial Intelligent system is a key technology for improvement of traffic management & control, traveller information system i.e. real-time traffic information provision route navigation systems, parking information etc. It maximizes the capacity of transport system applying real time traffic data. Traffic signal lights can improve traffic flow significantly, reducing the need to build additional highway capacity, reducing stops by as much as 40%, reducing travel time by 25%, decreasing fuel consumption by 10% and thereby reducing the carbon emission This paper consists of four sections of which this is the first one. The first section introduces the study. The second section highlighted the basic concepts, challenges and advantages of artificial intelligent for development of intelligent transport system. Third section briefly presents an application of artificial intelligent system for development of intelligent transport system in smart cities. The last section presents the important conclusions drawn from this study.Artificial Intelligence SystemArtificial Intelligence (AI) is the science of making machines or systems do things that would require intelligence if done by men Knowledge Based system (KBS)KBS is a computer system capable of giving advice in a particular domain using the knowledge provided by a knowledge base which has data provided from past events in similar domains and feedbacks by a human expert. In this system, the knowledge is represented in several ways such as rules, frames, or cases, and the inference engine or algorithm Table S.No.Component Application 1User Interface It Converts user queries into an internal representation to be processed by the system and converts system's solutions and explanations into a language which the user can understand.Knowledge BaseIt contains an expert's knowledge about a narrow domain of application.The knowledge is represented in several ways such as rules, frames, or cases, and the inference engine or algorithm. The domain expert intervenes to arrive at appropriate decisions 3Inference Engine It manipulates the knowledge base, to give answers to user's queries.4Explanation GeneratorIt provides explanations to the user about how the system arrives at a conclusion so that the user can be convinced (sometimes it is also considered a part of Inference Generator).Computational Intelligence (CI)Computational Intelligence (CI) techniques provide superior analysis of complex real-time data sets that arise within transport systems. According to literature, improved use of existing transport infrastructures can accomplish positive sustainable outcomes, decreasing congestion, improving air quality, providing real-time travel information and supporting low carbon vehicles Classic set theory defines sharp boundaries between sets and an object can only be a member of a given set. Fuzzy set functions allow gradual transitions between sets and also varying degree of membership of a given set. The use of Fuzzy set theory, though not necessarily minimises uncertainty relating to problem objectives or input values, provides a standard way to systematically capture and define ambiguity.GA would yield a near optimum solution.Some of the advantages of the Artificial Intelligence in developing transport system may be summarised as follows:1. Artificial Intelligent is useful solution for design, construction, maintenance, and time scheduling of transport system. 2. It can be used better and faster models for solving complex problems of transport system involving huge volume of data such as airways, roadways, railway and waterways. 3. It can be formulated and adopted to ensure proper use of available resources. 4. It helps to converting traffic sensors into intelligent agents that can automatically detect and report traffic accidents or predict traffic conditions. 5. It is more reliable system for assessing and predicting traffic conditions, 6. It can be used to review and evaluation of transport technology. 7. It can be used to analysis of traffic demands and analysis & simulations of pedestrian and herd behaviour.However, there are a number of challenges involved in developing and deploying Artificial Intelligence in smart cities in India. Artificial Intelligence System face a range of challenges, including system interdependency, network effect, scale, funding, political, institutional and other challenges Application of Artificial Intelligence System in Smart CitiesMost of the major cities in India due to lack of intelligent transport system faces various problems like accidents, environmental degradation, congestion; overcrowding and parking spaces etc. Application for Development of Smart Public Transportation SystemArtificial Intelligence system include applications for development of smart public transport system such as en-route public transit information, automatic vehicle location, smart travel security and smart revenue management which enable transit vehicles, to report their current location, making it possible for traffic operations and revenue management to construct a real-time view. Figure 2. It can be used for monitoring the vehicles which will be useful to reduce traffic congestion and thus, saving the travelling time. 3. It can be used to provide commuters and operators with current information so they can avoid congested routes. 4. It can be used to provide commuters with arrival and departure information of transit vehicles. 5. It can be used to make public transport system more attractiveSmart Public Transportation SystemSmart Revenue Management Smart Transit Safety &SecurityAutomatic Vehicle locationUser Information option for commuters by giving them enhanced visibility. 6. It can be used to represent an emerging new infrastructure system, from which new products and services are likely to emerge. 7. It can be helpful for transit network operators to take decision, for choosing the route when to travel. 8. To deploy and enable a communications infrastructure that supports vehicle to infrastructure as well as vehicle to vehicle.Application for Development of Intelligent Traffic Management and Control SystemArtificial intelligence system includes various applications in traffic control, traffic demand management, emission testing and mitigation, electronic payment management and incident management for development of intelligent traffic management and control system. Figure Intelligent Traffic Management and ControlIncident Management Traffic Demand Management Electronic Payment ManagementIntelligent Traffic Control 9. In many locations, incident data are archived on a regular basis to identify locations of high incident frequency. These locations can be used in planning the responders' routes on the highway and for the identification of reasons for causation of incident to improve the existing roadway characteristics to avoid future incidents at the same location. 10. The most common application of artificial intelligent system is electronic payment collection through which operators and commuters can pay automatically. Implementation of electronic payment collection can help to generate needed resources to fund investments in public transport infrastructures and to reduce traffic congestion and environmental impact of vehicles.Application for Development of Smart Traffic Information SystemArtificial Intelligence system includes various applications in pre-trip travel information, en route travel information, route guidance and archived data function for development of smart traffic information system. Some of the applications of Artificial Intelligence for development of smart traffic information system in smart cities are summarised as follows:1. Artificial Intelligence system can be provide real-time travel and traffic information such as transit routes and schedules, navigation directions, and information about delays due to congestion, accidents, weather conditions, or road repair work. 2. It can be used to inform driver's precise location, current traffic on roadways and empower them with optimal navigation instructions and route selection. 3. It can be used to make parking easier and indicate to drivers where vacant spaces can be found in the city, and even allow drivers to reserve spaces in for parking.Application for Development of Safety Management and Emergency SystemThe traffic accident patterns word wide indicated that it is not enough to have the safest vehicle and road technology to ensure safe mobility but the city structure, modal share split, and exposure of motorists and pedestrians also have a significant role in ensuring safe mobility. Artificialal Intelligence plays an important role for reducing the traffic accidents and increases the safe mobility in smart cities. Some applications of the Artificial intelligence for development of Safety management and emergency system in smart cities are summarized as follows:1. Artificial intelligence used to develop a comprehensive and reliable computerised accident data base which can provide necessary data for analysis of trends of accidents required for safe mobility. 2. Artificial Intelligence system can be used to provide facilitates rapid intervention in an emergency situation which can safeguard the health and safety of the people involved. 3. The main objective of Artificial intelligence system for development of emergency system are to improve the safety of travel, to increase the efficiency of the emergency service by reducing travel time, to optimize trip planning from the emergency vehicle station to final destination and to reset ordinary traffic conditions as soon as the emergency has ceased.Application for Development of Smart Parking ManagementMost of the metro cities of India every third driver is looking for a parking space. This does not necessarily have anything to do with a shortage of parking spaces, but with a lack of information about the location and number of available spaces Application for Development of Smart Pavement Management SystemSome applications of the Artificial Intelligence for development of smart pavement management system in smart cities are summarized as follows: 1. Artificial Intelligence system is best solution for collection and analysis of very large volume of data for evaluation of existing condition of the pavement both structurally and functionally.It helps pavement preservation bymaintaining it to the desired level without resorting to restoration spending huge amount of money. 3. It can be used to develop project level analysis tools based on economic principles to arrive at maintenance standards and maintenance strategies and to optimize the use of funds allocated for pavement maintenance and rehabilitation. 4. It is used to develop expert systems for pavement management i.e. PAVEMENT EXPERT, PAVER, SEVADER and ORAGE which can be used to manage roads, incidence, severity, streets, parking lots and airfield and the extent of range of distress for each road section.The computer engineers can develop appropriate models using his knowledge in Artificial Intelligence and combining them with transport expert's to evolve realistic solution for developing transport systems in smart cities. Though the computer engineer could handle computer part of the Artificial Intelligence System, it will not be appropriate to expect him to deal with transport specific problems and hence the need arises for him to work jointly with transport expert. Similarly, the transport expert is also not expected to have so much exposure to computer applications as to independently handle all the nuances of Artificial Intelligence Systems in solving the complex problems of developing transportation infrastructure in smart cities. The application of Artificial Intelligence in developing transport Systems will therefore involve a joint effort of transportation experts and computer engineers. The application of Artificial Intelligence in developing transport Systems will therefore involve a joint effort of transportation experts and computer engineers.CONCLUSIONS", "conclusions": "The important conclusions based on this study are summarized as follows: 1. There is an urgent need for developing smart cities in India that can cope with the challenges of urban living. 2. Transport system in smart cities should be accessible, safe, environmentally friendly, faster, comfortable and affordable without compromising the future needs. The Indian cities largely lacks of intelligent transport system and there are various problems such as inefficient public transport system, severe congestion, increasing incidence of road accidents, inadequate parking spaces and a rapidly increasing energy cost etc. Therefore, development of intelligent transportation system is essential for smart cities due to concerns regarding the environmental, economic, and social equity. Artificial intelligence system is a key technology to resolve these issues. Therefore, there is an urgent need to adopt artificial intelligence system for development of intelligent transport systems to better understand and control its operations in smart cities. 3. This study reviews the basic concepts of Artificial Intelligence. This study also identifies various sub-systems of Intelligent Transport System i.e. smart public transport system, intelligent traffic management and control system, smart traffic information system, safety management & emergency system, smart parking management and smart pavement management system in smart cities. This study also presents application of Artificial Intelligence for development of intelligent transport systems in smart cities in India. 4. There is an urgent need to develop smart public transport system for smart cities in India. This study identifies various sub components for developing such a system. These components are en-route public transit information, automatic vehicle location, smart travel security and smart revenue management which enable transit vehicles to construct a real-time view. 5. There is an urgent need to develop intelligent traffic control and management system for smart cities in India. This study identifies various sub components for developing such a system. These components are intelligent traffic control, traffic demand management, emission testing and mitigation, electronic payment management and incident management for effectiveness of the use of existing infrastructure. 6. There is an urgent need to develop smart traffic information system for smart cities in India. This study identifies various sub components for developing such a system. These components are pre-trip travel information, en-route travel information, and route guidance to provide real-time travel and traffic information such as transit routes and schedules, navigation directions, and information about delays due to congestion, accidents, weather conditions and road repair. 7. There is an urgent need to develop a smart parking management system for identification of vacant spaces for parking and even allow drivers to reserve spaces in for parking for smart cities in India. 8. There is an urgent need to develop safety management and emergency system for smart cities in India to increase the efficiency of service in an emergency situation and to improve the safety of travel. 9. There is an urgent need to develop smart pavement management system for redesign and management of street as per the requirements of different transport modes in smart cities.The application of Artificial Intelligence in developing intelligent transport system in smart cities will involve a joint effort of transportation experts and computer engineers. It is expected that this study will be useful for planners and engineers so that AI system can be formulated for development of intelligent transport system in smart cities in India.", "SDG": [11]}, "artificial_societies_for_integrated_and_sustainable_development_of_metropolitan_systems": {"name": null, "abstract": "", "keywords": "", "introduction": "", "body": "Second is the experimenting problem. Experimenting with complex systems is difficult-often even infeasible-especially when it involves human or societal subjects. So, artificial systems are helpful for computational experiments, which are a natural extension of computer simulations. Artificial systems and computational experiments are ideal tools to validate goals and objectives or evaluate strategies and decisions for coordinated and sustainable development of metropolitan transportation, logistics, and ecosystems.Next is the decision-making problem. Through the interaction between real and artificial systems, we can construct parallel systems to perform computational experiments, test different strategies and decisions, and evaluate their effects for solving complex problems. Ideas and methods developed in adaptive-control systems can be used effectively in the framework of parallel systems for decision-making in complex systems. Furthermore, artificial systems, computational experiments, and parallel systems provide both data sources and proving grounds for decision support and analysis of complex systems using metasynthesis. Finally, to solve the problems we've described, we must also address the computing problem. How can we best use new and advanced computing architectures and environments, especially networked computing such as grid computing and peer-topeer computing?To address these problems, the team has specified seven major tasks in four areas and assigned them to various team members (see Figure Area 1: Artificial systems for real complex systemsThe first area's goal is to establish a computational theory that uses artificial systems as alternatives to real complex systems for validating or evaluating computational experiments (see Figure The first subtask is the agent-based modeling, design, analysis, and synthesis of artificial systems. This subtask will require\u2022 Cellular automata and their generalization for modeling agents \u2022 Linguistic dynamic systems for describing agent behaviors \u2022 Multiresolution observation and analysis of agent behaviors \u2022 Petri nets for specifying networking and cooperation among agents \u2022 Computational-intelligence methods and game-theoretic strategies for decisionmaking by agentsThe second subtask is to develop methods and procedures for computational experiments, which will require Using artificial systems to study complex systems is a natural extension of computer simulations as computational power grows. To maintain coherence and achieve integration of different artificial systems for transportation, logistics, and ecosystems, the systems use and share common agent models. Figure The third task is designing, constructing, and analyzing artificial ecosystems, which will require \u2022 Using agent models for plant growth and plant-environment interaction \u2022 Regulating ecosystems and transportation systems and ensuring proper interaction between them \u2022 Estimating and analyzing the ecosystems' carrying capacities in terms of population, transportation, and logistical development \u2022 Sampling and generating synthetic human population and plant distribution \u2022 Validating and evaluating ecological strategies and decisions using computational experiments and parallel systems  open, complex, and huge systems. The platform for managing and controlling parallel systems will require The platform for creating prototype systems and applications will require\u2022 Prototype systems for field applications in Shanghai, Shenzhen, and Jinan \u2022 An integrated control and management platform for traffic systems \u2022 An integrated scheduling and management platform for logistical systems \u2022 An integrated analysis and management platform for ecosystems \u2022 Validation and evaluation of field testing and applicationsBy fusing field-specific knowledge from the disciplines of individual researchers, we aim to use a multidisciplinary approach to reevaluate R&D strategies in transportation, logistics, and ecosystems. We also plan to establish a computational theory and framework with artificial systems, computational experiments, parallel systems, and metasynthesis for decision support and analysis of complex systems in general, and for integrated, coordinated, and sustainable development of metropolitan transportation, logistics, and ecosystems in specific. Eventually, we hope to develop a computational framework that lets us model, analyze, and synthesize complex urban and metropolitan social systems from qualitative validation to quantitative evaluation.  Fei-Yue Wang ,\u2022Figure 1 .Figure 2 .Area 3 :Figure 6 .Figure 5 .Figure 4 .\u2022Figure 7 .systems Parallel systems Figure 3. Roadmap for integrated, sustainable development of metropolitan systems using artificial systems. (Dashed lines represent information that leads to modification in the directed box; solid lines represent other information or decision flows.)AcknowledgmentsEXECUTIVE COMMITTEEE X E C U T I V E S T A F FCOMPUTER SOCIETY WEB SITEThe IEEE Computer Society's Web site, at www.computer.org, offers information and samples from the society's publications and conferences, as well as a broad range of information about technical committees, standards, student activities, and more. BOARD OF GOVERNORS", "conclusions": "", "SDG": [11]}, "automatic_license_plate_recognition_system_based_on_color_image_processing": {"name": "Automatic License Plate Recognition System Based on Color Image Processing", "abstract": " A License plate recognition (LPR) system can be divided into the following steps: preprocessing, plate region extraction, plate region thresholding, character segmentation, character recognition and post-processing. For step 2, a combination of color and shape information of plate is used and a satisfactory extraction result is achieved. For step 3, first channel is selected, then threshold is computed and finally the region is thresholded. For step 4, the character is segmented along vertical, horizontal direction and some tentative optimizations are applied. For step 5, minimum Euclidean distance based template matching is used. And for those confusing characters such as '8' & 'B' and '0' & 'D', a special processing is necessary. And for the final step, validity is checked by machine and manual. The experiment performed by program based on aforementioned algorithms indicates that our LPR system based on color image processing is quite quick and accurate.", "keywords": "", "introduction": "The automatic identification of vehicles has been in considerable demand especially with the sharp increase in the vehicle related crimes and traffic jams. It can also play a crucial role in security zone access control, automatic toll road collection and intelligent traffic management system. Since the plate can identify a car uniquely, it is of great interest in recent decade in using computer vision technology to recognize a car and several results have been achieved [2][3][4][5][6][7][8][9][10][11][12][13].[14]A typical LPR system can be divided into the following modules: preprocessing (including image enhancement and restoration), plate region extraction, plate region thresholding, character segmentation, character recognition and post-processing (validity checking). The first two modules, which only concern the shape and back/fore ground color of a plate and irrespective of character set in a plate, are the front end of the system. Module 4 and 5, on the contrary, are related to character set in a plate and regardless of the shape and back/fore ground color of a plate, so they are the back end of the system. Module 3, however, should take the shape and back/fore ground color of a plate as well as character set in a plate into consideration. Therefore, it is hard to say which end it can be categorized into.To develop an automatic recognition system of a car plate, a stable recognition of a plate region is of vital importance. Techniques such as edge extraction  [1], Hough transformation [6] and morphological operations [7] have been applied. An edgebased approach is normally simple and fast. However, it is too sensitive to the unwanted edges, which may happen to appear in the front of a car. Therefore, this method cannot be used independently. Using HT is very sensitive to deformation of a plate boundary and needs much memory. Though using gray value shows better performance, it still has difficulties recognizing a car image if the image has many similar parts of gray values to a plate region, such as a radiator region [8] [11]. Morphology has been known to be strong to noise signals, but it is rarely used in real time systems because of its slow operation. So in recent years, color image processing technology [12][5] is employed to overcome these disadvantages. First, all of the plate region candidates are found by histogram. After that, each one is verified by comparing its WHR (Width to Height Ratio), foreground and background color with current plate standard and eliminated if it is definitely not of plate region. And finally, for each survivor, an attempt to read plate information is made by invoking the back end.[4]In the back end, first channel is selected and the plate region is thresholded in the selected channel. And then, each character is extracted by histogram and some optimizations such as the merge of unconnected character (i.e. Chuan, or ), the removal of space mark, frame and pin, the correction of top and bottom coordinates in y direction and tilt correction are done during this phase. Next, each character is recognized by using minimum Euclidean distance based template matching since it's more noise tolerant than structural analysis based method . And for those confusing characters, '8' & 'B' and '0' & 'D', for instance, a special processing is necessary to improve the accuracy. Finally, validity checking is performed against vehicle related crimes.[2][3]", "body": "The automatic identification of vehicles has been in considerable demand especially with the sharp increase in the vehicle related crimes and traffic jams. It can also play a crucial role in security zone access control, automatic toll road collection and intelligent traffic management system. Since the plate can identify a car uniquely, it is of great interest in recent decade in using computer vision technology to recognize a car and several results have been achieved A typical LPR system can be divided into the following modules: preprocessing (including image enhancement and restoration), plate region extraction, plate region thresholding, character segmentation, character recognition and post-processing (validity checking). The first two modules, which only concern the shape and back/fore ground color of a plate and irrespective of character set in a plate, are the front end of the system. Module 4 and 5, on the contrary, are related to character set in a plate and regardless of the shape and back/fore ground color of a plate, so they are the back end of the system. Module 3, however, should take the shape and back/fore ground color of a plate as well as character set in a plate into consideration. Therefore, it is hard to say which end it can be categorized into.To develop an automatic recognition system of a car plate, a stable recognition of a plate region is of vital importance. Techniques such as edge extraction In the back end, first channel is selected and the plate region is thresholded in the selected channel. And then, each character is extracted by histogram and some optimizations such as the merge of unconnected character (i.e. Chuan, or ), the removal of space mark, frame and pin, the correction of top and bottom coordinates in y direction and tilt correction are done during this phase. Next, each character is recognized by using minimum Euclidean distance based template matching since it's more noise tolerant than structural analysis based method Plate Region ExtractionIn principle, image should first be preprocessed, namely, enhanced and restored. But the experiment shows that it doesn't deserve its relatively heavy computational cost, so this step is skipped.The basic idea of extraction of a plate region is that the color combination of a plate (background) and character (foreground) is unique and this combination occurs almost only in a plate region Altogether there are 4 kinds of plates in China mainland. They are yellow background and black characters plate for oversize vehicle, blue background and white characters plate for light-duty vehicle, white background and black or red characters plate for police or military vehicle, black background and white characters plate for vehicle of embassy, consulate and foreigners. At first, RGB model is used to classify all the pixels into the following 6 categories: blue, white, yellow, black, red and other, but unfortunately it fails because of the wide RGB value difference under different illumination. So HLS model is introduced, and this time the desired result is achieved, but it is too slow, namely, it takes PIII 1G roughly 1 second to processing a 1024X768 photo. Clearly, the bottleneck is the conversion from RGB value to HLS value while the key to its success is insensitivity under different illumination. Naturally, an ideal algorithm must retain this insensitivity under different illumination while eliminating the conversion between the two color models. Hence, the pixels are classified into 13 categories instead of 6 according to variance of illumination in the RGB domain. They are dark blue, blue, light blue, dark yellow, yellow, light yellow, dark black, black, gray black, gray white, white, light white and other. Here, red is not take into account because this color appears only once in the center or right part of the police or military vehicle plates whose dominant character color is black. Thus, it is enough to identify the plate by checking the black pixels. The speed is increased to 0.5 second per photo while the correct extraction rate remains the same to HLS. But, that's not enough. Actually, the dot and line interlace scan method is used and the time cost is reduced to 1/4 of the non-interlaced one. After the plate is extracted, the region is verified by its shape, i.e. WHR. In China mainland, there are three WHR values, which are 3.8 for police or military vehicle plates, 2.0 for rear edition of oversize vehicle plate and 3.6 for others. Because 3.6 and 3.8 is too close, they are merged into one. So if the WHR of the extracted plate is sufficiently close to 3.7 or 2.0, the verification is passed.According to Amdahl's law, frequent case should be favored over the infrequent case. In China mainland, the most common plate is white characters with blue background. Therefore, plate is first tried to be recognized as a white blue pair, then as a black yellow pair, next as a white black pair and finally as a black white pair.Taking a white blue pair for example, this process can be illustrated as follows.Fig. 1. Extraction of a plate region in verticalAs shown in Figure It is evident that the only candidate is the middle one (For the top, the number of lines where number of dark blue pixels exceeds the threshold is too small and thus omitted. If two adjacent plate regions are sufficiently close, then they are merged into one.). In addition, owing to the favor of frequent case and the fact that the plate region is generally occurred in the lower part of an image, the scan is done from bottom to top and hence the middle one is first found. The extracted one is in Figure Character Segmentation and RecognitionThresholdingThe thresholding procedure should introduce as little noise as possible, since subsequent steps may be seriously affected by a poor thresholding algorithm. Also, because the lighting conditions vary widely over a plate, locally adaptive thresholding is required. Empirical methods are devised and they succeed in thresholding the plate region.There are a variety of threshold algorithms, but the experiments show that \"simple is the best\", if considering both speed and accuracy, so bimodal histogram segmentation Without loss of generality, it is assumed that the object is white and the background is black before thresholding (If not, the color is reversed and this process is only needed for black yellow pair and black white pair). It can be proved that after thresholding, the number of white pixel is 68%~85% of the plate region. Suppose V is the value making 85% of the plate become white and U is the average value of the remaining. Then threshold value is U minus DetalV, which is from 5 to 10. Correct thresholding is accomplished by this rule of thumb. SegmentationFirst, according to its WHR, the plate is classified as either double line or single line. The threshold is 1/10 and 1/6 of the width of the plate for the former and the latter, respectively. Then the line whose number of black pixels exceeds the threshold is selected, and if two adjacent selected regions are sufficiently close, then they are merged into one. Next, the WHR of each segmented region is verified, if it is too large, it is discarded as frame. This process is shown in Figure Similar process (including threshold acquisition, selection, merge and discard) can be done in horizontal direction, as illustrated in Figure The ratio of the largest space to the second largest space between the adjacent characters is 1.25~1.45.Fig. 8. Mis-segmented characters (due to space mark)This rule is helpful in removing the space mark, as illustrated in Figure Merge of Unconnected Character. The first one on the plate of China mainland, the abbreviation for province, is a Chinese character and all characters are connected except for Chuan, necessitating a special process. A case in point is shown in Figure Correction of Top and Bottom. Coordinates. Because plate may be tilted, the top and bottom coordinates are probably not correct (see in Figure Fig. 10. Correction resultRemoval of Frame. In Figure Fig. 13. Pin removalTilt Correction. For every segmented character, there must be a top pixel whose y value is the biggest. The x and y coordinate of top pixel of character i is x i and y i , respectively. Owing to the linearity of the top coordinates, the relationship between x and y can be expressed as the following formula: bx a y + = . By minimizing ( )We obtain: And the top tilting degree is b arctan . By the same token, the bottom tilting degree can be calculated. If the top tilting degree and the bottom tilting degree are all positive or negative, the plate is deemed to be tilted. The tilting degree is the average of top tilting degree and bottom tilting degree weighed by top fitting coefficient and bottom fitting coefficient respectively. In the case of Figure Character RecognitionIf the WHR of the character is less than 1/3, it is tried to be recognized as '1'.For '1' candidates, if its pixel fill rate is more than 0.6, it is recognized as '1', otherwise discarded.For other characters, first, its size is normalized to 32X64. Then, minimum Euclidean distance based template matching is used to recognize each character Validity CheckingValidity is checked by machine and manual. For machine, the plate is searched in database to see whether it indeed exists. If the matched record does exist and is retrieved, the color of background and foreground of the plate is compared to those in it. If either of the former conditions fails, the vehicle will be stopped. And if the plate is on the blacklist, say, wanted by the police, it should be detained either. For manual, the type (oversize or light-duty), the brand (Benz or BMW) and the color of the current car body are compared to the information in the database. Again, if it fails, the vehicle will be held.Conclusion", "conclusions": "The experiment performed by program based on aforesaid algorithms indicates that our LPR system based on color image processing is quite quick and accurate. Even on a PIII 1G PC, 90% of the photos under various illuminations are read correctly within 0.3s.In this article, the automatic Chinese LPR system based on color image processing is proposed. The using of color image processing instead of grayscale, the further division from 6 colors to 13 colors to gain the robustness under various illuminations and the selection of channel are the major breakthroughs. And there are also some empirical rules, such as the computation of threshold value, the optimizations during character segmentation and the special processing to distinguish '8' from 'B' or '0' from 'D'. The end justifies the means. Last but not least, the validity check is performed. It is of absolute necessity to be introduced into a practical LPR system.", "SDG": [11]}, "a_nonlinear_artificial_intelligence_ensemble_prediction_model_for_typhoon_intensity": {"name": "A Nonlinear Artificial Intelligence Ensemble Prediction Model for Typhoon Intensity", "abstract": " A new nonlinear artificial intelligence ensemble prediction (NAIEP) model has been developed for predicting typhoon intensity based on multiple neural networks with the same expected output and using an evolutionary genetic algorithm (GA). The model is validated with short-range forecasts of typhoon intensity in the South China Sea (SCS); results show that the NAIEP model is clearly better than the climatology and persistence (CLIPER) model for 24-h forecasts of typhoon intensity. Using identical predictors and sample cases, predictions of the genetic neural network (GNN) ensemble prediction (GNNEP) model are compared with the single-GNN prediction model, and it has been proven theoretically that the former is more accurate. Computation and analysis of the generalization capacity of GNNEP also demonstrate that the prediction of the ensemble model integrates predictions of its optimized ensemble members, so the generalization capacity of the ensemble prediction model is also enhanced. This model better addresses the \"overfitting\" problem that generally exists in the traditional neural network approach to practical weather prediction.", "keywords": "", "introduction": "Ensemble numerical prediction (ENP; see the appendix for a list of the key acronyms used in this paper) is a new technique that has been developed within the last decade (Stensrud et al. 2000;Du 2002;). An ENP model, whether created with different physical process parameterization schemes or with different initial conditions from a Monte Carlo approach, formally consists of many different ensemble members; the effectiveness of ENP has been widely recognized Scherrer et al. 2004(Nohara and Tanaka 2004;. At present, traditional mathematic modeling methods, such as multivariate analysis and time series analysis are widely used in statistical prediction and dynamical-statistical prediction Zhou and Johnny 2006)(Zhou and Huang 1997;, in which the future state of a prediction object is forecasted using a statistical prediction equation Ding et al. 2002)(Zhou and Huang 1997;.Ding et al. 2002)With the development of the artificial intelligence technique, artificial neural networks (ANNs) have been applied successfully in many disciplines (Li et al. 2003;Has et al. 2004;). In the atmospheric sciences, many applications have been found in research areas such as short-range climate prediction, interpretation and application of numerical prediction products, air pollution prediction, and precipitation data processing of radar-satellite cloud pictures Liu 2005(Jin 2004;Ali 2004;). However, despite its excellent performance in self-adaptative learning and nonlinear mapping, in-depth studies have shown that ANNs lack the guidance of a rigorous theoretical system in determining adequate network structure; the effect of the application mainly depends on personal experience. In particular, \"overfitting\" of the ANN method frequently occurs in meteorological prediction modeling due to subjective determination of hidden nodes of the network, impeding its wide application Tapiador et al. 2004.(Jin 2005a,b)A genetic algorithm (GA) is a global optimum algorithm based on natural selection and natural inheritance that has been widely used in the field of artificial intelligence techniques in recent years (Zheng et al. 2003;). With GA, as in biological evolution, new quality populations are continuously generated by the genetic evolutionary operations of selection, crossover, and mutation (information exchange) among individuals of a genetic population. Therefore, GA is a population-searching algorithm independent of gradient information and is very effective at solving complex and nonlinear problems Guo et al. 2004(Luo et al. 2004;). In 2005, Jin found that by optimizing the network structure and the connection weight of ANNs, genetic evolution is able to create a number of different neural network individuals Zhou et al. 2004. In this work, using key principles from ensemble prediction in numerical weather prediction (NWP), we try to construct a number of quality individual neural networks to build a new NAIEP model for South China Sea (SCS) typhoon intensity prediction.(Jin et al. 2006)", "body": "Ensemble numerical prediction (ENP; see the appendix for a list of the key acronyms used in this paper) is a new technique that has been developed within the last decade With the development of the artificial intelligence technique, artificial neural networks (ANNs) have been applied successfully in many disciplines A genetic algorithm (GA) is a global optimum algorithm based on natural selection and natural inheritance that has been widely used in the field of artificial intelligence techniques in recent years Nonlinear meteorological ensemble predictionIn addition to NWP, statistical and dynamicstatistical prediction methods are still the predominant objective prediction modeling methods used in the atmospheric sciences. In traditional regression analysis, the prediction equation predicts the future state of a specific prediction object (predictand). In statistical ensemble prediction, however, several different prediction equations are set up for the same predictand using different methods, and the final deterministic prediction is obtained by integrating the results of these different prediction equations with equal or different weights Principle behind and method for creating ensemble prediction individualsTo construct an NAIEP model, a number of individual neural networks are first created and then integrated to build an ensemble prediction model.A GA is used to construct the members of the ensemble, and a three-layer back-propagation (BP) network is used as the basic model for the neural networks [for the detailed algorithm, see The three-layer BP network model is the most widely used neural network model in various disciplines; evolutionary GA is used in this paper to create a limited number of individual analog BP neural networks. The GA was first presented by Professor J. Holland In recent years, the GA has been applied to function optimization, machine learning, highway traffic, and industries Ensemble prediction modeling of genetic neural network (GNN) involves two major steps: creating the members of the neural network ensemble using the basic GA a. EncodingThe nodes, connection weights, and thresholds for each layer of the three-layer BP network model are arranged in order in a code string, forming a chromosome (a genetic individual) using a mixed encoding of binary and real numbers. This code has two parts: the control code, a binary code of network structure mainly controlling the number of hidden nodes, and the real number code, representing the connection weights and thresholds. Each genetic individual is a potentially op-timized individual and randomly generates an initial genetic population in the encoding space.b. Computation of fitnessTo compute fitness, we must decode the m genetic individuals of a genetic population into their hidden nodes and connection weights, input training samples, and compute the output of the hidden layer,and the output of the network,where p is the total number of hidden nodes; hi and w ij are the matrices of the connection weight coefficients from the input layer to the hidden layer and from the hidden layer to the output layer; i and \u2425 j are the corresponding thresholds, respectively; and the transition function f (x) \u03ed 1/(1 \u03e9 e \u03eax ). We then calculate the global error of the network:where n is the number of training samples. The fitness function, defined ascan be used to calculate the fitness of each genetic individual.c. Computation of evolutionary operationsThe three genetic operations (selection, crossover, and mutation) are applied in the evolutionary operation to genetic populations depending on the fitness of the genetic individuals; roulette wheel selection is used in the selection operation. The fitness value F i (x) of each individual, as well as the sum of the fitness values of the genetic population, are first calculated using expression (4); the probability of each individual being selected is then computed using the following expression:Those genetic individuals with a higher fitness (i.e., higher quality) have a better chance of producing offspring. Multipoint crossover is used in the crossover operation; except for selected genetic individuals in the population, gene exchange is performed between the code strings of the remaining genetic individuals at multiple crossover points randomly determined using the probability of intersection P c , producing new genetic individuals. In the mutation operation, allelic gene replacement between genetic individuals happens with probability P m , forming new genetic individuals. If a neuron of a genetic individual is deleted by mutation, the corresponding weight coefficient code is set to zero, and if a neuron is added, its initial weight coefficient code is generated randomly.A new generation of the genetic population is produced after each run of the three genetic operators, and the evolutionary operation repeats for a prescribed number of times (N ). Afterward, the connection weights and hidden nodes of the m neural networks are determined by decoding each genetic individual; thus, the m ensemble members for ensemble prediction modeling are also obtained. In this paper, each ensemble member is assigned an equal weight in ensemble prediction modeling, so the sum of the prediction value of each neural network yields the ensemble prediction value of the GNN ensemble prediction (GNNEP) model.Ensemble prediction experiments for typhoon intensity in JulyWe produced 24-h predictions of SCS typhoon intensity as an experiment of the nonlinear meteorological ensemble prediction modeling in this paper. For more than the past 10 years, studies on statistical and statistical-dynamic forecast models for typhoon tracks and intensity have been undertaken in China and overseas (DeMaria and We used the GNNEP model (described in section 3) method for creating individuals in our ensemble prediction experiment. For objective comparison, the CLIPER prediction equation for typhoon intensity was first developed, and the same predictors that were selected in the CLIPER model were also used in the GNNEP prediction modeling. Furthermore, the data for the two models were similarly manipulated in the following independent sample experiments.a. DataForty-six years of SCS typhoon data were taken from the \"Typhoon Almanac\" published by the China Meteorological Administration from 1960 through 2005. SCS typhoons in this paper refer to typhoons that formed in or moved into the sea area 10\u00b0-23.5\u00b0N west of 123\u00b0E and lasted at least 48 h (required for CLIPER). The typhoon track was sampled every 12 h, starting from the first instant the typhoon moved into the sea area or from when a cyclone developed into a typhoon in the area. Sample typhoons from 1960 to 1989 were used in the prediction modeling, and those from 1990 to 2005 were used as independent samples in prediction testing. The data from 1960 to 1989 contain 56 relevant typhoons in July, with 330 corresponding typhoon samples for prediction modeling, and the data from 1990 to 2005 include 27 typhoons and 156 independent samples.b. Predictors and the CLIPER prediction equationThe CLIPER prediction of typhoon intensity assumes that a future change in typhoon intensity is associated with the current intensity, location (latitude and longitude), and current rates of change. Thirty-one CLIPER predictors were selected for this paper for prediction modeling from the sample data of SCS typhoons (sample size \u03ed 330), and their individual correlation coefficients with the predictand (typhoon intensity) are statistically significant above the 0.05 confidence level, with the maximum being 0.53 and the minimum being 0.20 (Table c. Nonlinear ensemble prediction of typhoon intensityFive GNNEP models were constructed (see Fig. In the evolutionary GA computation, we specified the size of the initial population and the number of evolu-tionary generations to be 50. Roulette wheel selection was used in the selection operator and the multiplepoint crossover with a crossover probability of 0.9 in the crossover operator; the threshold and crossover prob- abilities of the weight coefficient were both 0.6, and the mutation algorithm had a mutation probability of 0.05. The input nodes of GNN were the number of predictors in the corresponding CLIPER equation, and the number of output nodes was one. The size of the search space for the hidden nodes of the GNN model was 0.5-1.5 times the number of the input nodes, with the solution space of the network connection weight having been specified as [0, 1]. The training time was 200, and the learning and momentum factors were both 0.5. After the evolutionary computation, the 50 genetic individuals were decoded to yield 50 neural network ensemble members, which were subsequently used to construct the resultant five GNNEP models using the ensemble average method. Likewise, prediction verification of the 156 independent samples from the years 1990-2005 was performed using each of the five GNNEP models. For each ensemble-CLIPER model, the first independent sample was predicted using the 330 modeling samples, the second using 331 modeling samples (330 modeling samples plus the first independent sample, which had become a known variable when the second sample was forecasted), and so on, until the last (156th) independent sample was predicted using 485 modeling samples. In all successive predictions, all parameters of the GA and GNN were kept unchanged, to ensure comparability between independent sample predictions and actual operational predictions. Figure Performance analysis of the ensemble prediction model a. Contrast analysis of the ensemble model and ensemble membersThe basic principle of the GNNEP model is that given the same expected output, the members (50 in this paper) of the model are first created using the evolutionary GA; then, their predictions are integrated with an equal weight to yield the resultant prediction of the ensemble model. To emphasize the difference in prediction performance between GNNEP and single-GNN prediction, five single-GNN prediction models were built using the predictors of the CLIPER prediction Eqs. ( b. Theoretical derivation of the performance of the GNNEP modelThe simple mean method, where the mean weight ( i ; i \u03ed 1, 2, . . . , m; m \u03ed 50 in this paper; i \u03fe 0, and \u035a m i\u03ed1 i \u03ed 1) is given for each GNN member, is used to integrate the predictions of the GNN ensemble members. If the model input for the ith GNN member is x p , and its computed output is f i (x p ), then the output of the ensemble prediction of the GNNEP model consisting of m GNN members isand the corresponding ensemble prediction error isBecause the expected output of all GNN members is the same, the prediction error of some member of the ensemble model isand the weighted average of the prediction errors for all GNN members isWhen the model input is x p , if the computed output of the ith GNN member is f i (x p ) and that of the ANNEP model is f i (x p ), then the diversity between the ith member and the ensemble model isand the diversity of all members isBased on Eqs. ( Assuming that the model input x \u2208 R m satisfies the distribution p(x), then the following relation can be obtained using Eqs. ( \u035117\u0352Equation ( c. Generalization capability of the ensemble prediction modelThe structure of a single neural network prediction model is generally determined by personal experience due to lack of theoretical guidance To determine whether overfitting occurs in GNNEP, the five ensemble models in Table Using identical modeling samples, predictors, and independent samples, the prediction accuracy of July SCS typhoon intensities from the GNNEP models was compared with widely used traditional CLIPER models and single-GNN models; the improved prediction capability and generalization capability of the new prediction modeling have been theoretically proven to be reasonable in sections 4 and 5. To examine the stability of its prediction capability, similar prediction experiments  for August and September were also performed, and the results are similar to those for July. As space is limited, it is unnecessary to go into detail here. The analysis results from the aforementioned large sample size prediction experiments for July, August, and September SCS typhoon intensities clearly show that the nonlinear GNNEP model for typhoon intensity introduced here tends to have a higher prediction accuracy, a stable prediction capability, and robust generalization capability.Summary", "conclusions": "Ensemble prediction improves the effectiveness of NWP. In this work, we present a NAIEP modeling approach based on GA and ANN that emulates the ensemble prediction of NWP whose merits are as follow:1) In the case of identical predictors, prediction modeling samples, and independent prediction samples, the prediction accuracy of our GNNEP model is clearly higher than that of the traditional CLIPER model (which uses regression analysis). 2) Likewise, under the same conditions, the GNNEP model is also more accurate than the single neural network prediction model, because the structure of its member network is determined by the optimized GA instead of by personal experience. Further computation and analysis indicate that there is no \"overfitting\" in the predictions of the new ensemble model, a phenomenon that generally exists in predictions by traditional single neural network methods. Therefore, GNNEP is directly applicable to actual operational weather prediction, thus providing statistical weather prediction with a new nonlinear intelligence prediction technique.3) The relationship, derived in section 5b, of the prediction error of GNNEP model with the mean prediction error and diversity of its network members, provides theoretical evidence for the computational result that GNNEP is more accurate than the single neural network prediction model.Overall, this research suggests that the meteorological ensemble modeling approach of GNN opens up a vast range of possibilities for operational weather prediction. The theoretical derivation suggests that constructing a more highly diverse population of ensemble members will further improve the prediction ability of the GNNEP model. Therefore, construction of a highly diverse ensemble merits further exploration. ", "SDG": [11]}, "a_review_of_computer_vision_techniques_for_the_analysis_of_urban_traffic": {"name": "A Review of Computer Vision Techniques for the Analysis of Urban Traffic", "abstract": " Automatic video analysis from urban surveillance cameras is a fast-emerging field based on computer vision techniques. We present here a comprehensive review of the state-of-the-art computer vision for traffic video with a critical analysis and an outlook to future research directions. This field is of increasing relevance for intelligent transport systems (ITSs). The decreasing hardware cost and, therefore, the increasing deployment of cameras have opened a wide application field for video analytics. Several monitoring objectives such as congestion, traffic rule violation, and vehicle interaction can be targeted using cameras that were typically originally installed for human operators. Systems for the detection and classification of vehicles on highways have successfully been using classical visual surveillance techniques such as background estimation and motion tracking for some time. The urban domain is more challenging with respect to traffic density, lower camera angles that lead to a high degree of occlusion, and the variety of road users. Methods from object categorization and 3-D modeling have inspired more advanced techniques to tackle these challenges. There is no commonly used data set or benchmark challenge, which makes the direct comparison of the proposed algorithms difficult. In addition, evaluation under challenging weather conditions (e.g., rain, fog, and darkness) would be desirable but is rarely performed. Future work should be directed toward robust combined detectors and classifiers for all road users, with a focus on realistic conditions during evaluation.", "keywords": "Closed-circuit television (CCTV),intersection monitoring,road user counting,road users,traffic analysis,urban traffic,vehicle classification,vehicle detection,visual surveillance", "introduction": "", "body": "Automatic video analysis from urban surveillance cameras is a fast-emerging field based on computer vision techniques. We present here a comprehensive review of the state-of-the-art computer vision for traffic video with a critical analysis and an outlook to future research directions. This field is of increasing relevance for intelligent transport systems (ITSs). The decreasing hardware cost and, therefore, the increasing deployment of cameras have opened a wide application field for video analytics. Several monitoring objectives such as congestion, traffic rule violation, and vehicle interaction can be targeted using cameras that were typically originally installed for human operators. Systems for the detection and classification of vehicles on highways have successfully been using classical visual surveillance techniques such as background estimation and motion tracking for some time. The urban domain is more challenging with respect to traffic density, lower camera angles that lead to a high degree of occlusion, and the variety of road users. Methods from object categorization and 3-D modeling have inspired more advanced techniques to tackle these challenges. There is no commonly used data set or benchmark challenge, which makes the direct comparison of the proposed algorithms difficult. In addition, evaluation under challenging weather conditions (e.g., rain, fog, and darkness) would be desirable but is rarely performed. Future work should be directed toward robust combined detectors and classifiers for all road users, with a focus on realistic conditions during evaluation.creased computing power, has enabled new applications. We define video analytics as computer-vision-based surveillance algorithms and systems to extract contextual information from video. The main concept is to aid human operators in observing video data. Video cameras have been deployed for a long time for traffic and other monitoring purposes, because they provide a rich information source for human understanding. Video analytics may now provide added value to cameras by automatically extracting relevant information. This way, computer vision and video analytics become increasingly important for intelligent transport systems (ITSs). This paper aims at introducing computer vision and video analytics and familiarizing the reader with techniques that are more progressively used in ITSs. To make this paper manageable, we concentrate here on infrastructure-side monitoring and, thus, do not consider the vehicle side and other mobile devices.In urban environments, several monitoring objectives can be supported by the application of computer vision and pattern recognition techniques, including the detection of traffic violations (e.g., illegal turns and one-way streets) and the identification of road users (e.g., vehicles, motorbikes, and pedestrians). For the latter task, the currently most reliable approach is either through the recognition of number plates, i.e., automatic number plate recognition (ANPR), which is also known as automatic license plate recognition (ALPR), or radio frequency transponders, which may not be as easily acceptable for pedestrians or bicycles. Nevertheless, ANPR tends to be effective only for specialized camera views (zoomed on plates) and cannot provide wide-area observation or the measurement of the interactions between road users. This case may be possible with computer vision using standard cameras. Thus, for the aforementioned monitoring objectives, the detection and classification of road users is a key task. However, using generalpurpose surveillance cameras (i.e., monocular), this challenge is demanding. The quality of surveillance data is generally poor, and the range of operational conditions (e.g., night time, inclement, and changeable weather) requires robust techniques. Traffic analysis on highways appears to be less challenging than in the urban environment. This case can be observed from detection and classification performance figures in the urban domain, i.e., 82.8% The significant difference between traffic surveillance and, for example, generic object recognition is important for the understanding of the methods used. Object recognition tasks typically focus on high-resolution images (megapixel range), with few constraints on the viewing angle. The Visual Object Classes (VOC) challenge In contrast to the aforementioned approaches, traffic surveillance systems deal with low camera resolution and, therefore, a limited amount of visual detail of road users. The monitoring objectives generally require real-time (RT) processing, which further constrains the complexity of the proposed approaches. The scenes are usually more constrained than object recognition problems, with cameras mounted on poles above roads. However, stringent reliability requirements are defined by operators. Cameras are typically assumed to be stationary, on a \"home\" position, unless operators take control over a camera. Several algorithms use this assumption to extract information when no operator observes the camera and information may otherwise be lost. In general, this surveillance task is not as well defined as image retrieval, and no benchmarking challenge has taken place so far. Fig. Note that ITSs use and will use a variety of sensors in addition to video cameras. Other sensory modes for trafficmonitoring systems can include inductive loops, weather stations, radar scanners, and laser scanners. There is great potential in fusing information from different sensor sources to provide robustness in a combined method This paper will specifically focus on recent approaches for monocular road-side cameras in urban environments used by human operators to provide automated solutions to the aforementioned monitoring problems. Information fusion with other data sources, e.g., radar, light detection and ranging (LiDAR), and inductive loops, may be applied to the result generated with the methods described in this paper; however, a detailed discussion is beyond the scope here. A previous survey The remainder of the paper is organized as follows. We will first consider the deployment of video analytics in Section II to show where commercial (off-the-shelf) systems are in use.A review of computer vision techniques used in traffic analysis systems is presented in Section III to analyze the underlying algorithms and methods. The state of the art for prototype and academic systems is analyzed in Section IV. For this analysis, the full surveillance task from reading a video stream to classifying road users and event recognition is described based on the techniques in Section III. In Section V, detailed discussions and an outlook to future research are provided.II. VIDEO ANALYTICS DEPLOYED IN THE TRAFFIC DOMAINThis section reviews applications and existing commercial systems for traffic monitoring. The first part in Section II-A will focus on vehicle counting, which is mainly applied to highway scenes. ANPR is a very specialized application that is typically used for tolling and is discussed in Section II-B. The most challenging and least solved problem that holds the highest research potential is incident detection in Section II-C.A. Vehicle CountingThe problem of vehicle counting is most commonly solved by deploying inductive loops. These loops provide high precision but are very intrusive to the road pavement and, therefore, come with a high maintenance cost. Most video analytics systems on highways focus on counting and, possibly, classification to allow for more detailed statistics B. ANPRANPR is a very specialized well-researched application for video analytics. There is a vast range of companies, e.g., C. Incident DetectionWork on incident detection focuses on a higher level of scene understanding than the aforementioned two approaches. Examples for highways include the detection of accidents Urban environments involve a much wider range of incident detection systems than highway surveillance and require an even higher level of scene understanding. Congestion detection is rolled out in London III. ELEMENTS OF TRAFFIC ANALYSIS SYSTEMSIn this section, we introduce generic elements required in a traffic analysis system. These methods and algorithms are useful to understand the workings of video analytics. To structure the presentation, we have grouped the literature into top-down and bottom-up approaches. This section is structured to mirror Fig. the processing pipeline of a typical video analytics application: foreground estimation (see Section III-A), classification (see Section III-B), and tracking (see Section III-D). See Fig. In contrast, we define a \"bottom-up\" approach as an approach that first detects and classifies parts of an object (see Fig. In the next section, we will first describe the top-down approach in more detail, including foreground segmentation and top-down vehicle classification. This approach is followed by relevant bottom-up classification approaches for traffic surveillance. The last section considers tracking, which can equally be applied after both classification methods.A. Foreground SegmentationForeground estimation and segmentation is the first stage of several visual surveillance systems. The foreground regions are marked (e.g., mask image) for processing in the subsequent steps. The foreground is defined as every object, which is not a fixed furniture of a scene, where fixed could normally mean months or years. This definition conforms to human understanding, but it is difficult to algorithmically implement. There are two main different approaches to estimate the foreground, which both use strong assumptions to comply with the aforementioned definition. First, a background model of some kind can be used to accumulate information about the scene background of a video sequence. The model is then compared to the current frame to identify differences (or \"motion\"), provided that the camera is stationary. This concept lends itself well for computer implementation but leads to problems with slow-moving traffic. Any car should be considered foreground, but stationary objects are missed due to the lack of motion. The next five sections discus different solutions for using motion as the main cue for foreground segmentation.A different approach performs segmentation based on whole object appearances and will be discussed in Section VI. This approach can be used for moving and for stationary cameras but requires prior information for foreground object appearances. This way, the review moves from the simple frame difference method in the next section to learning based methods in Section VI.1) Frame Differencing: Possibly, the simplest method for foreground segmentation is frame differencing. A pixel-bypixel difference map is computed between two consecutive frames. This difference is thresholded and used as the foreground mask. This algorithm is very fast; however, it cannot cope with noise, abrupt illumination changes, or periodic movements in the background such as trees. In 2) Background Subtraction: This group of background models estimates a background image (i.e., fixed scene), which is subtracted from the current video frame. A threshold is applied to the resulting difference image to give the foreground mask. The threshold can be constant or dynamic, as used in a) Averaging: In the background averaging method, all video frames are summed up. The learning rate specifies the weight between a new frame and the background. This algorithm has little computational cost; however, it is likely to produce tails behind moving objects due to the contamination of the background with the appearance of the moving objects.The instantaneous background is used in b) Single Gaussian: To improve robustness compared to averaging, a temporal single Gaussian model can be used for every pixel of the background. Instead of using only the mean value for averaging, the variance of the background pixels is additionally calculated. This approach results in a mean image and a variance image for the background model. A new pixel is classified, depending on the position in the Gaussian distribution, which is the statistical equivalent to a dynamic threshold. A single Gaussian background model is used in c) Mode estimation: The mode of the temporal histogram of every pixel to estimate the background image is used in 3) GMM:The GMM was introduced in the seminal paper One alternative to the GMM is given in 4) Graph Cuts:The foreground segmentation problem can be represented as a graph of a Markov random field (MRF). Every pixel of the images is represented by a node in the graph. The vertices between nodes and sources are set to a weight related to the data (data constraint). Sources represent the labels for a pixel (in this case, the foreground and the background). Vertices between nodes are used to introduce a smoothing constraint to avoid very small foreground or background regions. The graph cut completely separates the source and sink nodes and leaves the nodes connected to either source or sink node to indicate that this pixel corresponds to the respective label. The advantage of graph cuts is that the solution for this optimization problem can be found in polynomial time. A general introduction to graph cuts is given in 5) Shadow Removal:An evaluation of moving-shadow detection is given in A shadow removal technique using GMMs is introduced in All the aforementioned methods performed pixelwise reasoning to generate a binary foreground mask. The next section will look into considering larger image regions to directly segment whole objects.6) Object-Based Segmentation: Object-based segmentation relies on object detection to identify the foreground. In this section, methods that detect objects in a holistic way by searching for full objects are considered. Reference Different methods are proposed to find correspondences between 3-D model projections and new images. Reference An approach with edges is used in B. Top-Down Vehicle ClassificationClassification is the task of assigning a new instance to a group of previously seen instances called the class. The classifier needs information about a new instance, which is usually referred to as features. Features are extracted from the whole object according to the aforementioned top-down methods. In Section III-B1, a selection of possible features is described. A machine-learning algorithm is trained with instances of known classes (hence, this approach is referred to as supervised learning) to extract discriminative information from the features (see Section II). The classifier then uses this learned information to assign a class label to a new instance.1) Features: Classification and tracking relies on a feature extraction process, which ideally produces similar values for the instances of a given class throughout the video stream. This section gives an overview of different kinds of features, grouped by the support in the image as either a binary foreground region, the contour of this region, or larger image patches.a) Region based: Region-based features are usually extracted from the whole image region of an object. In video sequences, this area is mainly the foreground silhouette extracted by the foreground segmentation algorithm. Image moments are often used to generate a feature vector for the silhouette. Without any feature generation, the convex hull of the silhouette (binary mask) can be used for comparison. Such an approach for region matching is used in The evaluation on a 24-h test sequence recorded by the authors shows a classification accuracy of 74.4% for independent tracking and classification. The accuracy can be increased to 88.4% by combining tracking and classification and, therefore, rejecting single misclassification. Further work of the authors incorporates histograms of oriented gradients (HOG) features for in-vehicle systems b) Contour based: Contour-based features only take the edge of a silhouette into account. The distance between contour points is used as a similarity measure. Processing is performed on closed contours as extracted from video sequences. The contour including edges is used in One common problem when dealing with contours is occlusion between vehicles, particularly in urban environments. An algorithm in 2) Machine Learning: Machine-learning techniques are used to generate a discriminative classifier from training data and to assign class labels to unseen data. One important property of the learning technique is the supervision during learning. This approach describes the amount of labeling information required of the training data. Labeling can range from simply tagging an image with a class to completely manually segmenting the image and labeling individual parts of objects. Ground truth is similar information and required for evaluation. The classifier output for test data is then compared to this manually generated ground truth. Large amounts of ground truth are required to provide evaluation with high statistical confidence. Section V-B will look into common data sets, which is important to share the effort in generating this ground truth. A good overview of machine-learning techniques can be found in a) Distance measures: Features are commonly represented as vectors in an N -dimensional feature space. This representation allows the definition of a distance (i.e., difference) between two vectors, which can be used during clustering and, in particular, classification to measure similarity between features. Several distance measures are available with various properties. First, the Manhattan distance calculates the sum of the absolute difference along every coordinate axis between the vectors. This condition results in the least computational effort but complex mathematics. Second, the Euclidean distance b) Dimensionality reduction: For feature vectors, not all dimensions are necessarily statistically independent. Dimensionality reduction can be applied to reduce the data to the signification dimensions and, this way, speed up processing or simplify classification. The classic method is PCA. This technique performs an orthogonal coordinate transformation of the feature space. The eigenvectors of the covariance matrix of the training data with the highest eigenvalues are used as new coordinate axes. This transformation ensures that the largest data variance is represented along the coordinate axes. Neglecting small eigenvalues that correspond to less significant deviations in the data reduces the dimensionality of the feature space. Reference The following nonlinear embedding methods are compared in a review The distance measures and dimensionality reduction are used on original training data. The next section will discuss clustering, which can provide more meaning to the data by grouping data points together. The training data could, for example, be reduced by retaining only cluster centers for later classification steps.c) Clustering: Clustering is performed on the training data. If the training data only contain object features, unsupervised clustering would need to identify the number of classes or clusters in the data and the correspondence of the training samples to those clusters. Because this general clustering problem is not satisfactorily solved, k-means clustering is commonly performed. This clustering technique groups the training samples into a specified number of groups based on the distance between features. This clustering technique for vehicle classification is used in One related technique is the generation of a codebook or alphabet for object classification. This approach is usually applied if several local feature vectors are used to specify an object. The class label for every feature vector is known from supervision or from a previous clustering of the objects. The distance between feature vectors is used to group them together. Every group of feature vectors is replaced by one codebook entry that holds all class labels of the individual vectors. This approach can increase the speed of the final classifier and reduce the amount of training and data storage, as shown in d) Classifiers: Classifiers map a new unknown object instance with an extracted feature vector to a known class or, perhaps, no class. This mapping process depends on what was previously learned from the training data. Different ways of generating and performing this mapping are outlined as follows.Nearest neighbor classifier: The nearest neighbor classifier is the simplest nonparametric classifier for a feature vector. The distance between a new feature vector and every vector of the training set is calculated. Any distance measure can be used for this purpose. The class label of the closest training vector is assigned to the new vector. To improve robustness, the k-nearest neighbor algorithm can be used. The class label for the new class is determined by the k-nearest training vectors. Both methods require several distance calculations and do not scale very well for large training sets in terms of computational complexity and memory requirements. There is no time requirement for training; however, the classification time increases with the training size. This method to classify vehicles based on binary foreground features is used in SVMs: An introduction and review of kernel-based learning used for SVMs can be found in Probabilistic frameworks: Given that real-world measurements have uncertainty, probabilistic frameworks estimate the (posterior) probability based on observed data and prior knowledge. For example, the posterior probability of a vehicle that belongs to class A is calculated from the image data and the prior knowledge of how frequent vehicles of class A are observed. The vehicle detection system presented in The aforementioned methods dealt with top-down classification, which analyzed objects as a whole. The next section will introduce bottom-up classification, where individual object parts are detected. This approach requires reliable part detections and classifications.C. Bottom-Up ClassificationThis section discusses the literature for bottom-up approaches. An introduction to this concept, which is traditionally used for generic object recognition, is given in 1) Interest-Point Descriptors: Interest points (also referred to as keypoints) are image positions, from which local features are extracted. These points may uniformly be sampled in the image space a) Basic patch based: The simplest patch-based feature vector is the collection of values of the image pixels. In Using a histogram rather than pixel values allows for more spatial invariance. The seminal paper for those concepts is Binary edges can provide normalized input for feature descriptors. Illumination conditions are mostly removed during edge detection. The Canny edge detector to generate features is used in b) SIFT: SIFT was introduced in the seminal paper c) SURF: The speeded-up robust features (SURF) descriptors are introduced in d) HOGs: The concept of grids of HOG was introduced in e) Other descriptors: There are a wide range of other descriptors introduced in the literature. The boundary fragment model (BFM) is introduced in the seminal paper Another extension of the SIFT descriptor is gradient location and orientation histogram (GLOH) in 2) Boosting: Boosting is a method of improving the performance of a simple (possibly poor) classifier. It is very popular in conjunction with local feature descriptors to also improve computational speed by selecting an optimal subset of input features. AdaBoost was first introduced in 3) Explicit Shape: Explicit shape implies directly modeling the spatial relationship between parts of objects detected. Various different models for the shape are introduced here, with relevance for traffic surveillance.a) k-fans: The k-fan model was first introduced in [28] to schematize part-based object recognition. The parts of an object are divided into reference nodes and regular nodes of a graph. The parameter k represents the number of reference nodes. Every reference node has a spatial relation to every other node in the graph. By changing k from 0 to the total number of nodes, the spatial prior can be changed from no shape modeled to a full rigid structure. Most shape models are related to k-fans. Reference b) ISM: The implicit-shape model (ISM; one fan) is introduced in A similar approach is used in c) Alphabets: The concept of alphabets is introduced to reduce the number of training samples. Instead of using every single feature vector from training, similar vectors are combined. The resulting entry holds a list of class labels and could take several positions in a shape model. This concept is used in 4) Object Classification Without Explicit Shape Structure: A solution for generic object recognition without shape structure is given in a) Object recognition with hierarchy: The introduction of hierarchy in object recognition is mainly related to biological research. For example, Having covered methods for the detection and classification of road users from a top-down and a bottom-up perspective, the next section will show how tracking can be used to estimate road user trajectories.D. TrackingTracking is used to measure vehicle paths in video sequences. This approach is performed in the following two steps: 1) features for the object or foreground regions are generated in every video frame (see Section III-B1 and 2) a data association step has to provide correspondences between the regions of consecutive frames based on the features and a dynamic model. Temporal consistency constraints are required to avoid confusion of tracks and to smooth noisy position outputs of detectors. The data association step can use the same distance measure as machine-learning algorithms (see Section III-B2a). The classification result and location in the image is typically included in the feature for this association. The next sections discuss motion models for tracking in traffic applications and possible data association based on prediction.a) Kalman filter: The Kalman filter was originally introduced in b) PF: The PF is a generalization of the Kalman filter introduced in c) S-T MRF:The spatial-temporal Markov random field (S-T MRF) is introduced in [68]- d) Graph correspondence:A system for region tracking based on graph correspondence is introduced in e) Event cones: The concept of event cones for finding space-time trajectories is introduced in IV. COMPLETE TRAFFIC ANALYSIS SYSTEMSThis section covers traffic surveillance systems that could be used in a control room environment for traffic management. By distinguishing between urban and highway scenes, a higher coverage of highway applications in the literature is shown similar to the deployment discussed in Section II. This case is partly due to the easier conditions on a highway with usually more homogeneous and constant flow than in urban areas. In addition, the distance between vehicles is larger and reduces the amount of occlusion. Fig. A. UrbanThe challenge for monitoring urban traffic is the high density of vehicles and the low camera angle. The combination of both factors leads to a high degree of occlusion. In addition, the clutter on the streets increases the complexity of scenes. The literature is divided into 2-D approaches, which operate in the domain of the camera view, and 3-D approaches  in A system for detecting parked vehicles is introduced in Interest points are independently tracked at urban intersections in Finally, there are two papers that look at specialized urban traffic applications. Reference 2) Three-Dimensional Modeling: Systems in this section use explicit 3-D modeling. A RT system is introduced in The work in The use of 3-D wireframe models for vehicle detection and classification was proposed in A Bayesian framework with MCMC sampling is used in A completely different 3-D approach was presented in The problem of collision detection in urban intersections is tackled in B. HighwaysObserving highway scenes usually gives the advantage of high camera angle and homogeneous traffic flow. A comprehensive review 1) Detection: A region-based vehicle detection and classification system is proposed in The traffic system proposed in In contrast, vehicle detection from cameras on the roadside using height features is introduced in [74] and followed up in 2) Classification: A system for tracking and classifying vehicles on highways is proposed in A motion segmentation and classification algorithm is described in V. DISCUSSION This section will discuss challenges in traffic surveillance, particularly in the urban domain. One major aspect is common data sets, which are analyzed in Section V-B. Future research directions are given in Section V-C.Classical visual surveillance approaches of background modeling and tracking have successfully been applied for highway surveillance A. ChallengesFrom an application perspective, the main technical challenge is the diversity of camera views and operating conditions in traffic surveillance. In addition, a large variety of observation objectives, e.g., vehicle counting, classification, incident detection, or traffic rule enforcement, can be useful. This condition has generated a large diverse body of work, where it is difficult to perform direct comparison between the proposed algorithms. It would be beneficial for the community to define a set of clear tasks as done in object recognition with PASCAL The main technical challenge in urban environments includes occlusions and dense traffic. There are several solutions for occlusion handling in highway scenes B. Data SetsPublic data sets and evaluation would allow researchers to objectively compare algorithms. In addition, labeled training data are essential for the training of machine-learning algorithms discussed in Section III-B2). Unfortunately, most authors use their proprietary data, which is rarely made available on the web. Even with videos available, ground truth is scarcer and very often application dependent. The i-LIDS data set In general, all the data lack challenging weather and lighting conditions. Evaluation data for those conditions are essential to benchmark systems for full 24/7 operation as it may be required by traffic managers. Setting a well-defined challenge in conjunction with realistic well-balanced video data would be a valuable contribution to the field.C. Future ResearchThere is a larger body of work that deals with vehicle detection than with classification. For several applications, knowing the class of road users is essential. Some combined detectors and classifiers have been proposed After the low-level detection and tracking has been tackled, there is great potential for traffic rule enforcement. Current systems mainly focus on basic counting in highway and urban scenes. More sophisticated analysis of road user interaction is desirable in urban environments, particularly including cyclists and pedestrians. Intelligent traffic light timing could benefit from a measurement of the state (e.g., position, velocity, and class) of all road users at an intersection. The currently common installations of inductive loops in several cities cannot provide such comprehensive data.One emerging application area to consider is the communication of vehicles (C2X). This approach could either be vehicle to vehicle or vehicle to infrastructure, as outlined in VI. CONCLUSION", "conclusions": "We have presented a comprehensive review of computer vision techniques for traffic analysis systems, with a specific focus on urban environments. There is increasing scope in intelligent transport systems to adopt video analysis for traffic measurement. The research expands from the highway environment to the more challenging urban domain. This condition opens many more application possibilities with traffic management and enforcement. Traditional methods use background estimation and perform top-down classification, which can raise issues under challenging urban conditions. Methods from the object recognition domain (bottom-up) have shown promising results, overcoming some of the issues of traditional methods, but are limited in different ways. Clearer definitions of scenarios and applications are required to generate a more consistent body of work, which uses common data for comparable evaluation and better fusion of top-down and bottom-up algorithms. New application areas are likely to emerge from vehicle-to-vehicle and vehicle-to-infrastructure communication, where videos from traffic cameras could be passed to cars for processing.", "SDG": [11]}, "keeping_cars_from_crashing": {"name": null, "abstract": " losses that matter most are not even captured by these statistics, because there's no way to put a dollar value on them. Engineers have been chipping away at these staggering numbers for a long time. Air bags and seat belts save tens of thousands of people a year. Supercomputers now let designers create car frames and bodies that protect the people inside by absorbing as much of the energy of a crash as possible. As a result, the number of fatalities per million miles of vehicle travel has decreased. But the ultimate solution, and the only one that will save far more lives, limbs, and money, is to keep cars from smashing into each other in the first place. That is exactly what engineers in the United States, Europe, and Japan are trying to do. They are applying advanced microprocessors, radars, high-speed ICs, and signal-processing chips and algorithms in R&D programs that mark an about-face in the automotive industry: from safety systems that kick in after an accident occurs, attempting to minimize injury and damage, to ones that prevent collisions altogether. The first collision-avoidance features are already on the road, as pricey adaptive cruise control options on a small group of luxury cars. Over the next few years, these systems", "keywords": "", "introduction": "", "body": "Stop-and-go, automaticallyMeanwhile at Fujitsu Ten Ltd., Plymouth, Mich., engineers are working toward another vision of the future of adaptive cruise control-one targeted squarely at the realities of driving on often congested urban and suburban highways. Fujitsu Ten has demonstrated a prototype system for socalled stop-and-go adaptive cruise control. Ordinary ACC systems maintain safe distances between cars at speeds above 40 km/h, whereas Fujitsu Ten's system will work primarily at lower speeds in heavy traffic. If the car in front of it stops, it will bring a vehicle to a complete stop. Afterward, it will not re-engage the throttle-that's up to the driver-but as soon as the throttle is engaged, it will accelerate and decelerate along with the leading car over any range of speeds between zero and the cruising speed set by the driver. This so-called fusion sensor gets its name from the linking of the enhanced millimeter-wave radar from Fujitsu Ten's firstgeneration ACC system to a 640-by-480-pixel stereo camera with a 40-degree viewing angle. The camera, which uses two CMOS image sensors spaced 20 cm apart, is mounted inside the car between the windshield and the rear-view mirror [see illustration, p. 42].The radar and the cameras work together to track the car ahead and distinguish it from extraneous nonmoving objects more rapidly than would be possible with either alone, accord-ing to Keiji Fujimura, a senior manager at Fujitsu Ten. While the radar homes in on the lead car's rear bumper, the stereo camera is constantly measuring the widths of all the items in its wide field of view [see figures opposite]. To calculate them, it uses an algorithm based on the detection of vertical edges and the distance. Bridges, trees, and other stationary objects that are much wider or narrower than a car are quickly rejected as reasons for the system to apply the brakes. The concentration on vertical edges also helped hold down the cost and complexity of the optical system. The camera's wide field, along with the radar's widerthan-average 16-degree field of view, enhances the system's performance on tight curves, enabling it to continue tracking the lead car as the latter enters the curve and moves to one side or the other. Fujitsu Ten plans to improve the unit with a phased-array radar that \"scans\" by altering the relative phase of the signals emitted from a group of antennas and, consequently, the direction of the emitted beam. This beamshifting occurs almost instantaneously, because no component has to be physically moved. In a collision-avoidance system, overall reaction time could be reduced. There are other advantages; beam-shifting in a phased array is more precise than scanning an antenna mechanically, and the unit lasts longer because no movement means no wear. Better still, the shape of the beam, which defines the scanning area, can be changed on the fly in response to changing road conditions, said Fujimura.The system has worked well in trials, Fujimura added; nevertheless, it is not slated to go into production until 2004.Like its competitors, Fujitsu Ten is hoping to grab a share of a global market for first-generation ACC that is expected to reach $2.4 billion by 2010. By 2006, collision avoidance will be in 17 percent of new cars in Europe, 14 percent in Asia-Pacific Rim, and 13 percent in North America, according to Morris Kindig, president of Tier One.Robo-chauffeur?As collision avoidance becomes first commonplace and then sophisticated, the role of the driver will change. Within a decade or so, the drivers of the most advanced cars will only have to steer. Eventually, people might not be entrusted even with that task, at least on limited-access highways. In fact, a decade ago engineers at Carnegie Mellon University (CMU), in Pittsburgh, and in a Daimler-led research program called Vision Technology Application (VITA) tested cars that largely drove themselves. CMU and VITA vehicles logged thousands of highway kilometers, most of them with a driver sitting vigilantly behind the steering wheel but not touching it.The Intelligent Vehicle Initiative in the United States and the Ertico program in Europe are among dozens of groups working on technologies that may ultimately lead to vehicles that are wrapped in a cocoon of sensors, with a 360-degree view of their environment. Nearby vehicles would be in constant communication and act cooperatively, enabling groups of cars to race along like train cars, almost bumper to bumper, at speeds above 100 km/h.It will probably take decades, but car accidents may eventually become almost as rare as plane crashes are now. The automobile, which transformed the developed world by offering mobility and autonomy, will finally stop exacting such an enormous cost in human lives.\u2022Following the LeaderThe Fujitsu Ten system keeps a safe distance behind cars in its lane [reaction zone] by combining radar data on distance with stereo-camera data on the size of objects. The camera derives the width of cars by detecting their edges [red dots, and yellow boxes in the photo]. Objects that are too wide, like a bridge abutment, are ignored. The system's wide field of view allows it to continue tracking vehicles around curves. Ca me ra vi ewA", "conclusions": "", "SDG": [11]}, "machine_learning_paradigms_for_selecting_ecologically_significant_input_variables": {"name": "Machine-learning paradigms for selecting ecologically significant input variables", "abstract": " Harmful algal blooms, which are considered a serious environmental problem nowadays, occur in coastal waters in many parts of the world. They cause acute ecological damage and ensuing economic losses, due to fish kills and shellfish poisoning as well as public health threats posed by toxic blooms. Recently, data-driven models including machine-learning (ML) techniques have been employed to mimic dynamics of algal blooms. One of the most important steps in the application of a ML technique is the selection of significant model input variables. In the present paper, we use two extensively used ML techniques, artificial neural networks (ANN) and genetic programming (GP) for selecting the significant input variables. The efficacy of these techniques is first demonstrated on a test problem with known dependence and then they are applied to a real-world case study of water quality data from Tolo Harbour, Hong Kong. These ML techniques overcome some of the limitations of the currently used techniques for input variable selection, a review of which is also presented. The interpretation of the weights of the trained ANN and the GP evolved equations demonstrate their ability to identify the ecologically significant variables precisely. The significant variables suggested by the ML techniques also indicate chlorophyll-a (Chla) itself to be the most significant input in predicting the algal blooms, suggesting an auto-regressive nature or persistence in the algal bloom dynamics, which may be related to the long flushing time in the semi-enclosed coastal waters. The study also confirms the previous understanding that the algal blooms in coastal waters of Hong Kong often occur with a life cycle of the order of 1-2 weeks.", "keywords": "Harmful algal blooms,Red tides,Machine-learning techniques,Data-driven models,Artificial neural networks,Genetic programming,Water quality modelling,Tolo Harbour,Hong Kong", "introduction": "An explosive growth and accumulation of harmful microscopic algae or phytoplankton may result in harmful algal blooms (HABs). The red tide is a well-known form of algal bloom. Owing to its negative impacts on human health and aquatic life, this widely reported phenomenon has become a serious environmental problem. It might lead to harmful effects such as beach closure, mariculture loss arising from oxygen depletion or toxic algae, anoxia or shellfish poisoning, and so on ). An increasing trend in the occurrence of HABs has been recorded throughout the world during the past decade. For example, the worst fish kill in Hong Kong's history in April 1998 as a result of a devastating red tide destroyed over 3400 tonne or 80% of cultured fish stock and the resulting economic loss was estimated to exceed HK$312 million (Anderson, 1994. Hence, a capability to analyse and predict the occurrence of algal blooms precisely with sufficient lead-time would contribute significantly to fisheries and environmental management. Conventional knowledge-driven models address the physical problem by solving a highly coupled, non-linear, partial differential equation set with finite difference method, finite element method, etc. However, physical processes affecting HAB occurrence are highly complex and uncertain, and are difficult to be captured in some form of deterministic model. Moreover, the accuracy of the prediction is to a great extent dependent on the accuracy of the open boundary conditions, model parameters used, and the numerical scheme adopted.(Chau, 2004)With the recent advancements in artificial intelligence (AI) techniques and availability of unrivalled computational power, extensive use of machine learning (ML) techniques or data-driven approaches in ecological modelling was reported . Amongst others, they include artificial neural networks (ANN) (Recknagel, 2001)(Recknagel et al., 1997(Recknagel et al., , 2002;;Yabunaka et al., 1997;Maier et al., 1998;Scardi and Harding, 1999;Karul et al., 2000;Jeong et al., 2001;Scardi, 2001;Wei et al., 2001;, evolutionary-based techniques Lee et al., 2003)(Bobbin and Recknagel, 2001;Recknagel et al., 2002;Jeong et al., 2003;, fuzzy and neuro-fuzzy techniques Muttil and Lee, 2005)(Maier et al., 2001;, and so on. Whilst many of them were undertaken in freshwater environments (i.e., limnological or riverine systems), some have been applied to saltwater eutrophic areas Chen and Mynett, 2003)(Scardi and Harding, 1999;Scardi, 2001;Lee et al., 2003;.Muttil and Lee, 2005)One of the most important steps in the application of a ML technique is the selection of significant model input variables. The aim is to determine a set of significant inputs from a superset of potentially useful inputs, which will lead to a superior model as measured by some optimality criterion. Since data-driven models are usually assumed to be able to determine which model inputs are critical, researchers often tend to present a large number of inputs to the model. Such inclusion of a large number of inputs leads to ''the curse of dimensionality'', which is associated with the following shortcomings :(Bowden et al., 2005)As the input dimensionality increases, the computational complexity and memory requirements of the model increase, which in turn increase the time to build the models.As the input variables increase, the number of training samples required also increase.Misconvergence and poor model accuracy may result from the inclusion of irrelevant inputs due to an increase in the number of local minima present in the error surface.Interpreting complex models is more difficult than interpreting simple models that give comparable results.Thus, there are obvious advantages in selecting an appropriate set of significant inputs for a data-driven model. The complexity of the problem is further increased in time-series modelling of dynamical systems, where better predictions are obtained by the use of lagged input variables also. As the maximum lag or memory length increases, so too does the number of inputs and the complexity of the model. Recently, researchers have recognized the importance of input variable selection for the application of data-driven models in ecological modelling. In their work on applying ANN for modelling of coastal algal blooms,  noted that most of the literature they reviewed did not build in an optimal choice of input variables based on ecological considerations and many used almost all possible environmental parameters as inputs. Since the effects of some of the input variables may be duplicated (for example, during blooms, algal density and turbidity or secchi-disc depth (SD) are strongly correlated), the use of all possible input variables may present the model with noise, rather than useful information. Lee et al. (2003) extensively reviewed a number of journal articles from 1992 to 1998, which employed ANN for modelling and forecasting of water resources variables. They concluded that in many cases, the lack of a methodology for determining input variables raised doubt about the optimality of the inputs used and in some cases, inputs were even chosen arbitrarily.Maier and Dandy (2000)In the present paper, we employ two extensively used data-driven models for selecting the significant input variables, namely, ANN and genetic programming (GP) using first, a test problem from fluid mechanics and then using the biweekly water quality data from Tolo Harbour, Hong Kong. It should be noted that this work is not a hybrid combination of ANN and GP analysis in a two-stage procedure or ANN encapsulated within the GP framework. This work applies two distinct data-driven models, i.e., ANN and GP, to analyse the significant input variables in eutrophication phenomenon and the results by these two methods are compared. Since the data-driven model is itself used for input variable selection, there is no need to select any other analytical procedure. Moreover,  considered ANN and genetic algorithms to be the most innovative techniques for ecological modelling. Recknagel (2001) also reported ANN to be the most widely used model in water resources variable modelling.Maier and Dandy (2000)In the following sections, we first present a review of the input variable selection techniques used in ecological modelling applications of data-driven models. Then, details of the data and modelling approach are presented, followed by the application of ANN and GP for selection of the significant input variables.", "body": "An explosive growth and accumulation of harmful microscopic algae or phytoplankton may result in harmful algal blooms (HABs). The red tide is a well-known form of algal bloom. Owing to its negative impacts on human health and aquatic life, this widely reported phenomenon has become a serious environmental problem. It might lead to harmful effects such as beach closure, mariculture loss arising from oxygen depletion or toxic algae, anoxia or shellfish poisoning, and so on With the recent advancements in artificial intelligence (AI) techniques and availability of unrivalled computational power, extensive use of machine learning (ML) techniques or data-driven approaches in ecological modelling was reported One of the most important steps in the application of a ML technique is the selection of significant model input variables. The aim is to determine a set of significant inputs from a superset of potentially useful inputs, which will lead to a superior model as measured by some optimality criterion. Since data-driven models are usually assumed to be able to determine which model inputs are critical, researchers often tend to present a large number of inputs to the model. Such inclusion of a large number of inputs leads to ''the curse of dimensionality'', which is associated with the following shortcomings As the input dimensionality increases, the computational complexity and memory requirements of the model increase, which in turn increase the time to build the models.As the input variables increase, the number of training samples required also increase.Misconvergence and poor model accuracy may result from the inclusion of irrelevant inputs due to an increase in the number of local minima present in the error surface.Interpreting complex models is more difficult than interpreting simple models that give comparable results.Thus, there are obvious advantages in selecting an appropriate set of significant inputs for a data-driven model. The complexity of the problem is further increased in time-series modelling of dynamical systems, where better predictions are obtained by the use of lagged input variables also. As the maximum lag or memory length increases, so too does the number of inputs and the complexity of the model. Recently, researchers have recognized the importance of input variable selection for the application of data-driven models in ecological modelling. In their work on applying ANN for modelling of coastal algal blooms, In the present paper, we employ two extensively used data-driven models for selecting the significant input variables, namely, ANN and genetic programming (GP) using first, a test problem from fluid mechanics and then using the biweekly water quality data from Tolo Harbour, Hong Kong. It should be noted that this work is not a hybrid combination of ANN and GP analysis in a two-stage procedure or ANN encapsulated within the GP framework. This work applies two distinct data-driven models, i.e., ANN and GP, to analyse the significant input variables in eutrophication phenomenon and the results by these two methods are compared. Since the data-driven model is itself used for input variable selection, there is no need to select any other analytical procedure. Moreover, In the following sections, we first present a review of the input variable selection techniques used in ecological modelling applications of data-driven models. Then, details of the data and modelling approach are presented, followed by the application of ANN and GP for selection of the significant input variables.Review of previous work on input selectionThe main approaches that have been employed for input determination in ecological modelling literature can be broadly classified into five methods, which are presented in the following sub-sections. Other than the methods presented below, few studies have also used all available variables as inputs to their model, without considering any technique for selecting the significant variables Methods based on ecological considerationsA commonly adopted approach in the choice of the initial set of input variables is to apply a priori knowledge of causal variables and physical/ecological insight into the problem. If important candidate inputs are not included, then some information about the system may be lost and if spurious inputs are included, it may provide the model with noise and thus may confuse the training process. Many of the papers reviewed relied on a combination of a priori knowledge and analytical approaches to select the appropriate model inputs and lags of inputs Methods based on linear correlationWhen the relationship to be modelled is not well understood, then an analytical technique, such as correlation analysis, is often employed to select inputs and their lags. Correlated variables introduce redundancy in the model in the sense that no additional information is gained by adding them. Methods based on data mining techniquesSome researchers used data mining techniques like principal component analysis (PCA), cluster analysis, etc., for selecting the significant input variables. Forward selection and backward elimination methodsIn this approach, an optimization of the input variables is performed. The two standard approaches are forward selection and backward elimination methods. Forward selection starts by finding the best single input and selecting it for the final model. In each subsequent step, given a set of selected inputs, the input that improves the model's performance most is added to the final model. Backward elimination (network trimming) starts with a set of all inputs, and sequentially deletes the input that reduces performance the least. Sensitivity analysis using trained ANNSensitivity analyses are the most commonly used method of extracting information from a trained ANN Data and modelling approachIn this section, we give an account of the test problem, the real-world case study and details of the analysis.The test problemTo test the capability of ANN and GP for selecting the significant input variables, they are first tested on a test problem with known dependence. A simple example from fluid mechanics, the Bernoulli's equation is used. Ignoring any losses, the total energy head, E, can be expressed aswhere z is the vertical distance above a datum (m), p is the pressure (N/m 2 ), v is the velocity (m/s), g is the specific gravity of water (9810 N/m 3 ), and g is the gravitational acceleration (9.81 m/s 2 ). Using a standard random number generator, 1000 samples of different combinations of z, p and v are generated. The values of the energy head, E, are then computed using Eq. ( The real-world case studyThe excessive growth of aquatic plants, both attached and planktonic, to levels that are considered to be an interference with desirable water uses bring about the eutrophication phenomenon. The growth of aquatic plants results from many causes. Harmful algae are the microscopic single celled organisms that are present in the sea. These algae contain reddish pigments thus the water seems to be appearing in red colour in their presence. The red tide occurs when there is a rapid production in the single celled organisms because of the increased levels of temperature, salinity and nutrient concentration such as nitrogen, phosphorus (PO 4 ), etc. in the sea water, resulting in reddish water. It should be mentioned that only a few dozen of the many thousands of species of microscopic and macroscopic algae are repeatedly associated with toxic or harmful blooms. The specific type of species ''Gymnodinium Breve'' contains the neurotoxic shell fish poisoning. Similar events world wide include Florida, US Tolo Harbour, a semi-enclosed bay in the northeastern coastal waters of Hong Kong, is connected to the open sea at Mirs Bay (Fig. ARTICLE IN PRESSIn this study, the depth-averaged monthly/biweekly water quality data, which were measured under a water quality monitoring program by the Environmental Protection Department of the Hong Kong government is employed. Amongst others, the data from the most weakly flushed monitoring station, TM3, are purposely chosen so as to minimize hydrodynamic effects. Preprocessing of the biweekly observed data is required to obtain the daily values by linear interpolation. Furthermore, daily meteorological data of wind speed (WS), solar radiation (SR) and rainfall supplied by the Hong Kong Observatory are employed. The data from 1988-1992 are used for training both data-driven models. For more details on the water quality data, readers are referred to Interpolation effectAs mentioned above, the biweekly water quality data is linearly interpolated to get the daily values. We would like to point out that when interpolation is applied to produce time series from longer sampling frequency to a shorter time step, future observations are used to drive the predictions, as pointed out by Input variables and time lagsThe following nine variables are chosen as the initial set of input variables: chlorophyll-a (Chl-a) (mg/L); total inorganic nitrogen (TIN) (mg/L); PO 4 (mg/L); DO (mg/L); SD (m); water temperature, Temp (1C); daily rainfall, rain (mm); daily SR (MJ/m 2 ) and daily average WS (m/s). These variables are found to exhibit significant effects on the algal dynamics of Tolo Harbour according to several previous field and modelling studies in the weakly flushed embayment In Tolo Harbour and coastal waters of Hong Kong, algal blooms often occur with a life cycle of the order of 1-2 weeks. For both the 1-week and biweekly predictions, each of the nine input variables comprise 7 time-lagged variables, amounting to a total of 9 \u00c2 7 ( \u00bc 63) input variables. The significant input variables are to be selected from amongst them. Relationships between the Chl-a concentration at time t and the time-lagged input variables are developed by training ANN networks and evolving GP equations, details of which are presented in the following section.Proposed techniquesIn this section, we present the application of ANN and GP for input variable selection. Since these two data-driven models are themselves used for significant input variable selection, there is no need for going for any other analytical procedure for the same. Moreover, these models overcome some of the limitations associated with the commonly used selection techniques presented in Section 2. They can learn problems involving very non-linear and complex data and can identify correlated patterns between input data sets and corresponding target values. Both ANN and GP can take into account the interaction amongst variables and thus identify variables that may not be significant by itself, but are significant in combination with other variables. Thus, these data-driven models are ideally suited for identifying significant variables in ecological processes, which are known to be very complex and often non-linear.Artificial neural networks (ANN)An ANN is tailored to mimic natural neural networks in terms of computing paradigm When triggered, each neuron will compute a response from the weighted sum of its inputs from neurons connected to it, on the basis of a predetermined activation function. In turn, its output will become the inputs of other neurons located in the next layer. Whilst there are many commonly used activation functions, the sigmoid and the hyperbolic-tangent (tanh) functions are amongst the most popular. A back-propagation method is used in the training process to adjust the connection weights in the network in order to best match the network's response with the desired response. The optimization is executed by using an approximation to a gradient descent method Significant input variables based on ANN weightsA MLP neural network, which is trained using a back propagation algorithm with a momentum term, is employed. It comprises three layers, namely, an input, a hidden and an output layer. An interpretation of the connection weights from the input layer to the hidden layer of the trained network is undertaken. The inputs with the largest weight values denote the most significant input variables. An input significance measure S n of the input variable n is defined as follows:where H is the number of hidden nodes, w jn is the weight from input variable n to the hidden layer j. The summation of absolute values of weights is employed because some weight values may be positive and others negative. The neural network is first trained using the data from the test problem of Bernoulli's equation, which has 10 input variables and E is the dependent variable. The optimal number of nodes in the hidden layer was found to be 8. Trial and error method is also employed to determine the learning rate parameter and the momentum term. Their finally adopted values are 0.05 and 0.5, respectively. For both hidden and output layers, the hyperbolic-tangent function is adopted as the activation function. The network training is terminated after 1000 epochs. Fig. It is then applied to the real-world case study of predicting the algal biomass. Neural networks are trained for both 1-week and biweekly predictions. For both the predictions, there are 63 nodes in the input layer and the predicted Chl-a concentration is the only neuron in the output layer. Fig. Figs. 4 and 5 show the input significance of each input variable for 1-week and biweekly predictions, respectively. If all input variables have equal significance, then each input will have a significance of 1/63 ( \u00bc 1.58%) of the total value of 137.23. We assume that those variables having an input significance greater than 1.58% are relatively more significant, and columns representing these variables are shaded dark in Figs. ARTICLE IN PRESSIn the 1-week ahead algal biomass prediction, the effect of Chl-a reduces with an increase in time lag and in the biweekly predictions, the significance of Chl-a is up to the time lag of (t-14). Apart from Chl-a, TIN, PO 4 , DO and SD are found to be significant. For the biweekly predictions, Temp is significant and SR at (t-20) is slightly significant.Genetic programming (GP)GP GP starts with an initial generation of a population of random parse trees, calculation of their fitness in solving the problem domain and selection of the better parse trees for reproduction and evolution to a new generation. These processes iterate until a certain stopping criterion is met. The crossover operation takes place by randomly swapping sub-trees between the selected individuals In this study, the GP software, GPKernel, developed by DHI Water and Environment is employed. It is a command line based tool to determine functions based on the available data. The optimal solution is obtained in T im e la g Fig. Significant input variables using GP equationsTable Similar to the case of ANN, GP is first applied for input variable selection on the test problem with known dependence between the output and input variables. Using the simple math operators (+, \u00c0, *, /) as the function set, 10 different GP models are evolved using different initial seed for each GP run. Fig. Next, GP is applied on the problem of algal bloom prediction. GP models are evolved using 4 different function sets, which are presented in Table ARTICLE IN PRESSDiscussion on resultsIt is evident that Chl-a values are significant in predicting itself since both ML techniques gave the same conclusion. Chl-a at (t-7), with an input significance of 12.58 in the ANN weight analysis and with 229 terms out of a total of 790 in the GP equation analysis, is the most significant in algal biomass prediction 1-week ahead. The effect of Chl-a reduces with the increase in time lag and in biweekly predictions, Chl-a is significant up to a time lag of (t-14) in the ANN analysis and up to (t-17) in the GP analysis. This indicates n auto-regressive nature or ''persistence'' of the algal dynamics. This phenomenon is frequently exhibited in geophysical time series owing to inertia or carryover process in the physical system. In fact, modelling is a way in contributing to understanding the physical system as well as the process that builds persistence into the series. In this case, the auto-regressive nature of chlorophyll dynamics may be related to the long residence time in the semi-enclosed coastal waters. The average tidal current velocity is merely 0.04 m/s in the inner Harbour Subzone and 0.08 m/s in the outer Channel Subzone Apart from Chl-a, both ML techniques suggest that the nutrients (PO 4 and TIN), DO and SD (to a lesser extent) are significant. It is reasonable that nutrients are significant, since the growth and reproduction of phytoplankton mainly rely on their availability. The significance of DO is also justifiable in sub-tropical coastal waters with mariculture activities, since it contributes to the production and respiration of algal organisms as well as to some chemical reactions.In general, it is observed that the significant input variables from 1-week predictions are basically similar to those from biweekly predictions. The only exception to this is Temp and SR, which have slight significance in biweekly predictions. Thus, it can be concluded that the significant input variables from 1-week predictions are not completely driven by the interpolation effect. It appears that they exhibit at large cause-effect relationship between the timelagged input variables and future algal biomass.It should be mentioned that the results of this study regarding the significance of Chl-a in predicting itself are quite in contrast to several previous studies, which conventionally use a number of input variables Conclusion", "conclusions": "This paper presents the prototype application of two distinct ML techniques (ANN and GP) for the selection of significant input variables, first using a test problem with known input-output dependence and then using data from a monitoring station in coastal waters of Hong Kong. It is evident that the identification of the key input variables are feasible with the interpretation of the trained ANN weights or of the evolved GP equations, which is basically in line with ecological reasoning. It is found that chlorophyll is the most significant variable in predicting algal blooms. The auto-regressive nature of the algal bloom dynamics in this semi-enclosed coastal water body is justifiable owing to the long flushing time. The result that the use of previous data of algal biomass alone is good enough for future prediction might reduce the dependency on expensive equipment in algal bloom warning systems in coastal waters.  was supported by the Research Grants Council of Hong Kong (PolyU5132/04E).", "SDG": [11]}, "predicting_burned_areas_of_forest_fires_an_artificial_intelligence_approach": {"name": "PREDICTING BURNED AREAS OF FOREST FIRES: AN ARTIFICIAL INTELLIGENCE APPROACH", "abstract": " Forest fires importantly influence our environment and lives. The ability of accurately predicting the area that may be involved in a forest fire event may help in optimizing fire management efforts. Given the complexity of the task, powerful computational tools are needed for predicting the amount of area that will be burned during a forest fire. The purpose of this study was to develop an intelligent system based on genetic programming for the prediction of burned areas, using only data related to the forest under analysis and meteorological data. We used geometric semantic genetic programming based on recently defined geometric semantic genetic operators for genetic programming. Experimental results, achieved using a database of 517 forest fire events between 2000 and 2003, showed the appropriateness of the proposed system for the prediction of the burned areas. In particular, results obtained with geometric semantic genetic programming were significantly better than those produced by standard genetic programming and other state of the art machine learning methods on both training and out-of-sample data. This study suggests that deeper investigation of genetic programming in the field of forest fires prediction may be productive. giere que investigaciones m\u00e1s profundas de programaci\u00f3n gen\u00e9tica en el campo de la predicci\u00f3n de los incendios forestales pueden ser productivas.", "keywords": "climatic data,forest fires,genetic programming,Portugal,semantics", "introduction": "Forest fires are well-known events, especially during summer. Forest fires, regularly experienced in regions with hot, dry, or mediterranean climates, represent a risk to life and extant infrastructure. In Portugal, there are typically between 15 000 and 25 000 forest fires each year , burning from 150 000 ha to 250 000 ha. Notwithstanding the fact that these fires can cause extensive economic damage (typically with tangible repercussions for many years to come), they also threaten human life. Furthermore, the aftermath of forest fires can have other far-reaching consequences. For example, many physical, chemical, mineralogical, and biological soil properties can be affected by forest fires (Mateus and Fernandes 2014). Negative effects resulting from high levels of burn severity include significant removal of organic matter, deterioration of both soil structure and porosity, considerable loss of nutrients through volatilization, ash entrapment in smoke columns, leaching, and erosion. Also, the release of hazardous chemicals significantly impacts human health and increases the risk of future diseases. As suggested by (Certini 2005), wildfire smoke is accompanied by high concentrations of carbon dioxide, which can result in consequences such as headache, mental confusion, nausea, disorientation, coma, and even death. Even at lower concentrations, the effects of carbon dioxide should not be neglected; individuals with cardiovascular dis-ease may experience chest pain and cardiac arrhythmia. A comprehensive study tracking wildfire firefighter deaths from 1990 to 2006 reported that 21.9 % of their deaths occurred from heart attacks Lipsett et al. (2008).(Mangan 2007)The ability to predict fire progression and area burned is crucial to mitigating the immediate and far-reaching consequences of wildfires. Existing studies have attempted to fill this gap, mainly through mathematical models (e.g. ), but predictive techniques would enable decision makers to deal with large amount of data in a more timely manner. The Wildland Fire Management Research, Development & Application Organization (2012) proposed a wildland fire decision support tool called FSPro (Fire Spread Probability). FSPro is a geospatial probabilistic model that predicts fire growth, and is designed to support long-term (more than five days) decision making. FSPro addresses fire growth beyond the timeframes of reliable weather forecasts by using historic climatological data. FSPro calculates and maps the probability that fire will spread to areas on the landscape based on the current fire perimeter or ignition point.Rothermel 1972In this paper, we propose an intelligent system based on genetic programming for the prediction of burned areas of forest fires. In order to build predictive models, we only considered data relating to forest characteristics and meteorological data. Drawing on the idea of using computational intelligence techniques (and genetic programming in particular; e.g. Brumby et al. 2001, we employed recently defined geometric semantic genetic operators for genetic programming, which were able to produce results significantly better than traditional methods., Manson 2005)", "body": "Forest fires are well-known events, especially during summer. Forest fires, regularly experienced in regions with hot, dry, or mediterranean climates, represent a risk to life and extant infrastructure. In Portugal, there are typically between 15 000 and 25 000 forest fires each year The ability to predict fire progression and area burned is crucial to mitigating the immediate and far-reaching consequences of wildfires. Existing studies have attempted to fill this gap, mainly through mathematical models (e.g. In this paper, we propose an intelligent system based on genetic programming for the prediction of burned areas of forest fires. In order to build predictive models, we only considered data relating to forest characteristics and meteorological data. Drawing on the idea of using computational intelligence techniques (and genetic programming in particular; e.g. Genetic ProgrammingGenetic Programming (GP) In GP, candidate solutions are represented using a tree structure (Figure Since its definition, GP has been used to solve complex problems in several domains (Koza 2010) using only syntax-based genetic operators. Abstraction from semantics allows GP to use simple genetic operators that are easy to define and that are independent of any particular application. Hence, standard genetic operators can be used for addressing regression, classification, or even clustering problems without changing their definition. A second advantage is the existence of a solid theory that guarantees asymptotic convergence of standard GP towards optimal solutions Geometric Semantic OperatorsThis section introduces the concepts related to the definition of semantic-based methods, describing the semantic genetic operators that were used in this study. Even though the term semantics can have several different interpretations, the most common interpretation (and the one used here) is to identify the semantics of a solution with the vector of its output values on the training data Geometric semantic operators, introduced by To understand this property (for a full proof see The definitions of semantic crossover and semantic mutation follow.Geometric semantic crossover. Given two parent functions T 1 , T 2 : R n \u2192 R, the geometric semantic crossover returns the real functionwhere Trand is a random tree with no constraints on the output values.Geometric semantic mutation. Given a parent function T: R n \u2192 R, the geometric semantic mutation with mutation step ms returns the real functionwhere T R1 and T R2 are random real functions. Geometric semantic operators have a known limitation METHODSTo test the GP-GS method on a fire-frequent region, we selected Montesinho Natural Park, a protected area located in the municipalities of Vinhais and Bragan\u00e7a, in the mountainous region of northeast Portugal (Figure Mente (436 m), which is the park's western border, to peak of Sierra de Montesinho, at 1487 m. The main altitudinal belts correspond to the main landforms found in the area. Climatic diversity within the park is high, with a mean annual rainfall of 800 mm to 1500 mm and an average annual temperature of 8 \u00baC to 13 \u00baC; this variation follows continental and altitudinal gradients The park includes 92 small villages inhabited by less than 8000 people. Intensive grazing takes place from May to August when about 5000 sheep are transported from the surrounding lowlands to graze in the highlands. The non-regulated use of fire is common and related to agricultural and pastoral activities. Consequently, this area is very often subjected to wildfires, either naturally ignited or as a result of escaped human ignitions.DataWe created a database of wildfire activity within the boundaries of the Montesinho Natural Park from January 2000 to December 2003, comprising 517 wildfires. Fuel and meteorological data related to the fires included the forest Fire Weather Index (FWI) For each forest fire, several attributes were registered on a daily basis, such as the time, date, spatial location, the type of vegetation involved, the five components of the FWI system, and the total burned area (Table Following the same procedure reported in Experimental SettingsWe tested the proposed implementation of GP with geometric semantic operators (GS-GP from now on), and we compared it to a standard GP system (ST-GP) (i.e., to the system that was originally defined in where y i is the predicted value given input i (output of the generated model, evaluated on the training data), and t i is the corresponding target value.The terminal set contained 12 variables, each one corresponding to a different feature in the dataset. To create new individuals, ST-GP used standard (subtree swapping) crossover and subtree mutation RESULTSGS-GP vs. Standard GPGS-GP outperformed ST-GP both on training and on out-of-samples data (Figure To examine the statistical significance of these results, we tested the median errors. Preliminary analysis using the Kolmogorov-Smirnov test showed that the data were not normally distributed and hence a rank-based statistic was used. The Wilcoxon rank-sum test for pairwise data comparison was used with the alternative hypothesis that the samples do not have equal medians of burned area (P < 0.001 for training data, P = 0.002 for test data).GS-GP vs. Other Machine Learning TechniquesBesides comparing GS-GP with ST-GP, we also compared GS-GP with other well-known state-of-the-art machine learning methods. To perform the comparisons with other machine learning methods, we used the implementations provided by the Weka public domain software (Machine Learning Project 2015). As we did for the previous experimental phase, we performed a preliminary analysis to tune the parameters for each considered techniques.The results of the comparison are reported in Figure To assess the statistical significance of the model comparisons, the same set of tests performed in the previous section were done, but with a Bonferroni correction for the standard \u03b1 = 0.05 was applied (hence, the final value of \u03b1 was 0.014). The differences in terms of training and test fitness between GS-GP and the considered machine learning techniques were significant, except for the cases when GS-GP and SVM-2 were compared as well as when GS-GP and RF were compared (Table CONCLUSIONS", "conclusions": "The new genetic operators of genetic programming, called geometric semantic operators, have the extremely interesting property of inducing a unimodal fitness landscape for any problem consisting of matching input data into known output values (regression and classification are instances of this general problem).Here we showed a new intelligent GP-based system that makes use of these operators to examine burned area. The main objective was the development of a system for predicting the amount of area that will be burned during a forest fire, based on explicit relationships between meteorological data, forest-related data, and the amount of burned area. The comparatively small MAE obtained from experimental results showed that geometric semantic genetic programming outperforms standard genetic programming and produces results that are better or comparable to the ones achieved with state-of-the-art machine learning methods for this application.", "SDG": [11]}, "probabilistic_map_based_pedestrian_motion_prediction_taking_traffic_participants_into_consideration": {"name": "Probabilistic Map-based Pedestrian Motion Prediction Taking Traffic Participants into Consideration", "abstract": " As pedestrians are one of the most vulnerable traffic participants, their motion prediction is of utmost importance for intelligent transportation systems. Predicting motions of pedestrians is especially hard since they move in less structured environments and have less inertia compared to road vehicles. To account for this uncertainty, we present an approach for probabilistic prediction of pedestrian motion using Markov chains. In contrast to previous work, we not only consider motion models, constraints from a semantic map, and various goals, but also explicitly adapt the prediction based on crash probabilities with other traffic participants. Also, our approach works in any situation; this is typically challenging for pure machine learning techniques that learn behaviors for a particular road section and which might consequently struggle with a different road section. The usefulness of combining the aforementioned aspects in a single approach is demonstrated by an evaluation using recordings of real pedestrians.", "keywords": "", "introduction": "", "body": "A. MotivationPrediction of other traffic participants is an integral part in motion planning of autonomous vehicles or for threat assessment in driver assistant systems B. Literature ReviewMuch work has been done on predicting road vehicles, while vulnerable road users have received less attention a) Short-term prediction: We consider a prediction to be short-term when it is not longer than around 2 seconds. For those prediction horizons, classical filtering techniques, such as Kalman filters or interacting multiple models, provide good results b) Crossing prediction: Due to the importance of predicting whether a pedestrian will cross a road-automated vehicles have to decide whether to brake or not-much work exists on this particular prediction problem. A dynamic Bayesian network is combined with a switching linear dynamical system in d) Interaction-aware prediction: Many prediction techniques in crowded environments rely on the social force model e) Prediction in known environments: In known and structured environments, learning typical behavior patterns results in very accurate predictions. Clustering techniques are applied in C. ContributionsIn this paper, we address the problem of predicting motions of pedestrians. This work is based on D. OverviewAs a result of using Markov chains for motion prediction, one can obtain probabilistic occupancy grids directly, see the top part of Fig. Fig. of pedestrians to Markov chains is described in Sec. II. We generate different Markov chains for various intervals of the inputs orientation (moving direction) and velocity. After predicting probability distributions of inputs (lower part of Fig. II. MARKOV CHAIN OF PEDESTRIAN DYNAMICSTo probabilistically predict motions of pedestrians in open space, discrete Markov chains accounting for dynamic constraints of human beings are generated. This section is inspired by A. Abstraction of Motion ModelGiven the orientation \u03c8 and the velocity v as inputs u = (\u03c8, v)T , the state x = (x E , x N ) T containing the 2D position can be propagated by the following dynamic model:In order to obtain probabilistic occupancy grids, the above continuous motion model is abstracted to a discrete time Markov chain. There are three main reasons for this: first, the generation of Markov chains can be done offline and their online execution is computationally inexpensive; second,Fig. integrating a continuous probability density function can be reduced to a summation of probabilities; third, it is also advantageous to treat the state and input space as grids for better compatibility with discrete semantic maps.The generation of a Markov chain can be divided into two steps: a) discretizing the state and input space, see Fig. where \u03a6 \u03b1 \u2208 R d\u00d7d is the transition matrix for d state cells subject to the set of inputs U \u03b1 and \u03a6 \u03b1 ij represents the element in the i-th row and j-th column of \u03a6 \u03b1 . An alternative is to obtain Markov chains from a continuous model by reachability analysis B. Conditional Probability of InputsThis subsection addresses the problem of obtaining input distributions for a given state. The conditional probability of an input cell U \u03b1 for a given state cell X i is denoted bybe a state-dependent and possibly time-varying input transition matrix for c input cells. Then the conditional probabilities q \u03b2 i (t k ) can be updated according to the input transition values \u0393 \u03b1\u03b2 i (t k ) instantaneously at times t k :where \u0393 \u03b1\u03b2 i represents the element in the \u03b1-th row and \u03b2-th column of \u0393 i . The input transition probabilities \u0393 \u03b1\u03b2 i consist of two components: a) the intrinsic transition probability \u03a8 \u03b1\u03b2 , which models the behavior of pedestrians by taking their physical dynamic constraints into account, and b) the priority variable 0 \u2264 \u03bb \u03b1 i \u2264 1 which gives an input cell a certain priority depending on the state. After introducing the normalization operator norm(), the input transition probabil-ities are computed asLater, we will split each priority value \u03bb \u03b1 i into two parts: a) a static priority value for orientation \u03bb \u03b1 i,stat in Sec. III and b) a dynamic priority value \u03bb \u03b1 i,dyn in Sec. IV, such thatReferring to different \u03bb \u03b1 i , the following input transition matrices for the prediction of pedestrian motion are created:\u2022 Basic Input Transition Matrix: Next, we present the computation of the intrinsic transition matrix \u03a8. To obtain \u03a8, the prediction of the inputs orientation and velocity is separately treated with respect to modeling physical dynamic constraints of pedestrians. For reasons of clarity, we introduce separate indexes1) Orientation: We assume that the one-step transition of orientation from an interval U \u03b2 \u03c8 \u03c8 to another U \u03b1 \u03c8 \u03c8 is subject to physical dynamic constraints of human beings and additionally depends on the current velocity v t k . This is motivated by the fact that the faster a pedestrian walks, the more difficult he or she can turn around. After introducing the operator center() which returns the volumetric center of a set and the operator difference() which limits the difference of two orientation values between 0 and 180 \u2022 , the transition probabilitywith the proportionality operator \u221d and a parameter k 1 > 0.2) Velocity: We assume that the one-step transition of velocity from an interval U \u03b2v v to another interval U \u03b1v v is also subject to physical dynamic constraints, since pedestrians have limited acceleration and deceleration abilities. Furthermore, we assume that a pedestrian, in the absence of other traffic participants, might keep a desired velocity v * which can be adjusted according to online measurements. After mapping v * into intervals of velocity to obtain a corresponding index \u03b1 * v , the conditional probability q \u03b1v\u03b2v :=can be computed heuristically aswhere the term (\u03b1 v \u2212 \u03b2 v ) 2 implies that the bigger the difference of velocity values 3 , the more unlikely this change is; the second term in the denominator with a parameter k 2 \u2265 0 attempts to keep the distribution concentrated around a desired velocity; a high value of the parameter k 3 > 0 lets probability distributions converge to stable values quickly.3) Intrinsic Transition Matrix: We simplify the computation of the components of the intrinsic transition matrix \u03a8 \u03b1\u03b2 using the conditional transition probabilities in ( C. Combined Propagation of States and InputsFig. As pointed out in In order to elegantly perform ( Accordingly, a state transition matrix \u03a6 \u2208 R c\u2022d\u00d7c\u2022d and an input transition matrixand \u0393 is computed as3 As the indexes for discrete velocities are numbered in increasing sequence, their difference is a measure for the difference of values.where 0 is a matrix of zeros. This rewriting allows combined propagation of states and inputs:For a more detailed explanation, we refer to III. GOAL-ORIENTED PREDICTION", "conclusions": "", "SDG": [11]}, "a_systematic_review_and_taxonomy_of_explanations_in_decision_support_and_recommender_systems": {"name": "A Systematic Review and Taxonomy of Explanations in Decision Support and Recommender Systems", "abstract": " With the recent advances in the field of artificial intelligence, an increasing number of decision-making tasks are delegated to software systems. A key requirement for the success and adoption of such systems is that users must trust system choices or even fully automated decisions. To achieve this, explanation facilities have been widely investigated as a means of establishing trust in these systems since the early years of expert systems. With today's increasingly sophisticated machine learning algorithms, new challenges in the context of explanations, accountability, and trust towards such systems constantly arise. In this work, we systematically review the literature on explanations in advice-giving systems. This is a family of systems that includes recommender systems, which is one of the most successful classes of advicegiving software in practice. We investigate the purposes of explanations as well as how they are generated, presented to users, and evaluated. As a result, we derive a novel comprehensive taxonomy of aspects to be considered when designing explanation facilities for current and future decision support systems. The taxonomy includes a variety of different facets, such as explanation objective, responsiveness, content and presentation. Moreover, we identified several challenges that remain unaddressed so far, for example related to fine-grained issues associated with the presentation of explanations and how explanation facilities are evaluated.", "keywords": "", "introduction": "In recent years, significant progress has been made in the field of artificial intelligence and, in particular, in the context of machine learning (ML). Learningbased techniques are now embedded in various types of software systems. Features provided in practical applications range from supporting the user while making decisions, e.g. in the form of a recommender system, to making decisions fully autonomously, e.g. in the form of an automated pricing algorithm. In the future, a constant increase in such intelligent applications is expected, in particular because more types of data become available that can be leveraged by modern ML algorithms. This raises new issues to be taken into account in the development of intelligent systems, such as accountability and ethics .[8]A key requirement for the success and practical adoption of such systems in many domains is that users must have confidence in recommendations and automated decisions made by software systems, or at least trust that the given advice is unbiased. This was in fact acknowledged decades ago, when expert systems, mainly those to support medical decisions, were popular. Since the early years of expert systems, automatically generated explanations have been considered as a fundamental mechanism to increase user trust in suggestions made by the system . In these systems, provided explanations were often limited to some form of system logging, consisting of a chain of rules that were applied to reach the decision. Nevertheless, such explanations were often hard to understand by non-experts [246], thus being used in many cases only to support system debugging.[82]Today, with modern ML algorithms in place, generating useful or understandable explanations becomes even more challenging, for instance, when the system output is based on a complex artificial neural network . One of the most prominent examples of ML-based applications today are the so-called recommender systems [195]. These systems are employed, e.g., on modern ecommerce sites to help users find relevant items of interest within a larger collection of objects. In the research literature, a number of explanation approaches for recommenders has been proposed [108][91,69,39,168,, and existing work has shown that providing explanations can be beneficial for the success of recommenders in different ways, e.g. by helping users make better or more informed decisions 21].[223]Therefore, how to explain to the user recommendations or automated decisions made by a software system has been explored in various classes of systems. These include expert systems, knowledge-based systems, decision support systems, and recommender systems, which we collectively refer to as advice-giving systems. 1 However, despite the considerable amount of research literature in this context, providing adequate explanations remains a challenge. There is, for example, no clear consensus on what constitutes a good explanation . In fact, different types of explanations can impact a user's decision making process in many forms. For instance, explanations can help users make better decisions or persuade them to make one particular choice [169]. Finally, deriving a user-tailored explanation for the output of an algorithm that learns a complex decision function based on various (latent) patterns in the data is challenging without the use of additional domain knowledge [223].[252]Given these challenges, it is important to gather and review the variety of existing efforts that were made in the last decades to be able to design explanation facilities for future intelligent advice-giving systems. Next-generation explanation facilities are particularly needed when further critical tasks are delegated to software systems, e.g. in the domain of robotics or autonomous driving [236,. At the same time, many future advice-giving systems will need more interactive interfaces for users to give feedback to the system about the appropriateness of the advice made, or to overwrite a decision of the system. In both cases, system-provided explanations can represent a starting point for better user control 237][223,107,.111]In this work, we present the results of a systematic literature review  on the topic of explanations for advice-giving systems. We discuss in particular how explanations are generated from an algorithmic perspective as well as what kinds of information are used in this process and presented to the user. Furthermore, we review how researchers evaluated their approaches and what conclusions they reached. Based on these results, we propose a new comprehensive taxonomy of aspects to be considered when designing an explanation facility for advice-giving systems. The taxonomy includes a variety of different facets, such as explanation objective, responsiveness, content and presentation. Moreover, we identified several challenges that remain unaddressed so far, for example related to fine-grained issues associated with the presentation of explanations and how explanation facilities are evaluated.[122]Our work is different from previous overview papers on explanations in many ways. 2 To our knowledge, it is the first systematic review on the topic. We initially retrieved 1209 papers in a structured search process and finally included 217 of them in our review based on defined criteria. This systematic approach allowed us to avoid a potential researcher bias, which can be introduced when the selection of the papers that are discussed is not based on a defined and structured process. Moreover, as opposed to some other works, our review does not only focus on one single aspect, such as analysing the different purposes of explanations . We, in contrast, discuss a variety of aspects, including questions and derived conclusions associated with explanation evaluations. Finally, the comprehensive taxonomy that we put forward at the end of this work is constructed based on the results of a systematic bottom-up approach, i.e. its structure is not determined based solely on the authors' expertise in the topic.[222]", "body": "In recent years, significant progress has been made in the field of artificial intelligence and, in particular, in the context of machine learning (ML). Learningbased techniques are now embedded in various types of software systems. Features provided in practical applications range from supporting the user while making decisions, e.g. in the form of a recommender system, to making decisions fully autonomously, e.g. in the form of an automated pricing algorithm. In the future, a constant increase in such intelligent applications is expected, in particular because more types of data become available that can be leveraged by modern ML algorithms. This raises new issues to be taken into account in the development of intelligent systems, such as accountability and ethics A key requirement for the success and practical adoption of such systems in many domains is that users must have confidence in recommendations and automated decisions made by software systems, or at least trust that the given advice is unbiased. This was in fact acknowledged decades ago, when expert systems, mainly those to support medical decisions, were popular. Since the early years of expert systems, automatically generated explanations have been considered as a fundamental mechanism to increase user trust in suggestions made by the system Today, with modern ML algorithms in place, generating useful or understandable explanations becomes even more challenging, for instance, when the system output is based on a complex artificial neural network Therefore, how to explain to the user recommendations or automated decisions made by a software system has been explored in various classes of systems. These include expert systems, knowledge-based systems, decision support systems, and recommender systems, which we collectively refer to as advice-giving systems. 1 However, despite the considerable amount of research literature in this context, providing adequate explanations remains a challenge. There is, for example, no clear consensus on what constitutes a good explanation Given these challenges, it is important to gather and review the variety of existing efforts that were made in the last decades to be able to design explanation facilities for future intelligent advice-giving systems. Next-generation explanation facilities are particularly needed when further critical tasks are delegated to software systems, e.g. in the domain of robotics or autonomous driving In this work, we present the results of a systematic literature review Our work is different from previous overview papers on explanations in many ways. 2 To our knowledge, it is the first systematic review on the topic. We initially retrieved 1209 papers in a structured search process and finally included 217 of them in our review based on defined criteria. This systematic approach allowed us to avoid a potential researcher bias, which can be introduced when the selection of the papers that are discussed is not based on a defined and structured process. Moreover, as opposed to some other works, our review does not only focus on one single aspect, such as analysing the different purposes of explanations Systematic Review PlanningA systematic review is a type of literature-based research that is characterised by the existence of an exact and transparent specification of a procedure to find, evaluate, and synthesise the results. It includes a careful planning phase, in which goals, research questions, search procedure (including the search string), and inclusion and exclusion criteria are explicitly specified. Key advantages of using such a defined procedure are that it helps to avoid or at least minimise potential researcher biases and supports reproducibility. Such reviews are common in the medical and social sciences and are increasingly adopted in the field of computer science The Need for a Systematic Review To our knowledge, no previous work has aimed at providing a comprehensive overview of existing work on explanations in advice-giving systems using a systematic approach. A number of survey papers exists as mentioned above, but they are either limited to a subjective selection of papers or focused on certain aspects of explanations. Often, existing survey papers simply summarise individual papers and do not consider the developments in the field over time.Research GoalsThe comprehensive review provided in this paper shall help designers of next-generation advice-giving systems understand what has already been explored over the last decades in different subfields of computer science. Specifically, we aim to investigate and classify: (i) which forms of explanations were proposed in the literature; (ii) how explanations are generated from an algorithmic perspective; and (iii) how researchers evaluated their approaches and what conclusions they reached. By aggregating the insights obtained from the review in a new multi-faceted taxonomy, we aim to provide an additional aid for designers to understand the various dimensions that one has to potentially consider when designing an explanation facility.Research Questions The specific research questions of the review are consequently as follows.RQ-1: What are the characteristics of explanations provided to users, in terms of content and presentation? RQ-2: How are explanations generated? RQ-3: How are explanations evaluated? RQ-4: What are the conclusions of evaluation or foundational studies of explanations?Search Strategies To find primary studies that are relevant for our review, we selected the databases presented in Table Inclusion CriteriaIC-1 :The paper proposes an explanation generation technique.IC-2 :The paper presents a software application that includes an explanation facility. IC-3 :The paper presents an evaluation of forms of explanations. IC-4 :The paper presents a study that investigates foundations of explanations in advicegiving systems.Exclusion CriteriaEC-1 :The paper is not written in English.EC-2 :The content of the paper was also published in another, more complete, paper that is already included. EC-3 :The content is not a scientific paper, but an introduction, glossary, etc. EC-4 :We have no access to the full paper. EC-5 :There is a statement in the abstract or content of the paper that explanations are provided, but they are not detailed.from difference sources, such as Google Scholar, or allow the addition of nonreviewed papers by their authors, such as arXiv, were left out of the scope of our review. Generally, we assumed that peer-reviewed papers published in computer science are mostly stored in our selected databases. Therefore, these other additional databases would mostly provide duplicated studies. Furthermore, we focused on peer-reviewed work in order to have some evidence regarding the quality of the selected studies.Selection Criteria Primary studies retrieved from the databases are filtered using a set of criteria. We consider four inclusion criteria (IC) and five exclusion criteria (EC) to select papers associated with the primary studies to be analysed in our review. The inclusion and exclusion criteria are summarised in Table (ii) the description of a tool that includes an explanation facility (IC-2); (iii) an evaluation or comparison of one or more forms of explanations (IC-3); or (iv) a discussion of foundational aspects of explanations (IC-4). Throughout the paper, terms highlighted in small caps are used to refer to these study types.The following additional considerations apply regarding the satisfaction of our inclusion criteria. First, in our work, we are only interested in systems in which explanations are provided for end users, usually someone that is actually responsible for a final decision. In some studies, visualisations of the outcomes of a data mining process are called explanations as well. Given that such visualisations are designed for data scientists to understand the outputs of the decision algorithms, such works are an example of studies that do not satisfy our inclusion criteria.Second, we are only interested in studies involving explanations that are related to a specific decision making instance. Generally, an explanation provided by the system can be any form of visual, textual, or multi-modal means to convey additional information to the decision maker regarding the specific decision to be made, e.g. about the system's reasons to recommend a certain alternative. However, in our review, we do not consider approaches in which the system merely provides background knowledge about the domain and this knowledge is independent of the current decision making problem instance. Approaches in which the system displays details about how to interact with user interfaces are also not in the scope of our work.Third, we consider only scenarios in which there is a limited number of alternatives, and the system task is to select one or more of the available choices. In case of multiple selected alternatives, in many cases a ranking is determined by the system. However, a number of papers on decision support systems exist in which the algorithmic task of the system is to compute the single mathematically optimal solution to a given problem. Work on such scenarios, in which there is no set of alternative choices, are also not included in our review. Similarly, studies of explanations regarding the outcomes of mathematical simulations are excluded as well.With respect to exclusion criteria, we proceeded as follows. In order to obtain the papers in which the studies were published, we first tried to access them through the TU Dortmund network and the Portal de Peri\u00f3dicos CAPES 3 . If the full text was not available, we searched for the paper on the web using: (i) author websites; (ii) Googles search website; and (iii) repositories of scientific papers, such as Google Scholar and ResearchGate. At the end, only eight studies were excluded because we had no access to the full papers (EC-4).Finally, there are a number of studies excluded by EC-5 that state that the proposed system has the potential to provide explanations, but do not concretely describe how it is done. Examples of studies excluded due to this reason are mainly those in the context of argumentation Systematic Review ExecutionThe next step in the systematic review process is to execute an appropriate search query against the literature databases. In this section, we provide details of how we constructed the search query and how we ended up with the set of primary studies that are considered relevant for our work. The results and insights are then discussed in the remaining sections.Search String ConstructionString construction in systematic reviews is based on a set of terms of interest and their synonyms. Our search string covers two main terms, which both have to appear in the potentially relevant papers. The first term is explanations, which is the main topic of our study. In different communities, researchers use alternative terms to refer to explanations, and those were added as synonyms of the term explanation. The second term is decision support system, which is one of the classes of system that are targets of our review. As synonyms, we consider alternative classes of advice-giving systems that exist, including in particular knowledge-based and expert systems, as well as recommender systems. For some of them, we included subsets of typically used expressions to refer to such system classes, because they are used with different complementary terms, such as recommendation system, recommendation provider, etc. Therefore, in such cases, we included only the main words as synonyms, because they cover all of the alternatives for the term. The resulting set of synonyms used in our search string is shown in Table (explanation OR justification OR argumentation) AND (decision support system OR decision making OR expert system OR recommender OR recommendation OR knowledgebased system OR knowledge based system)The search string was customised to the specific syntax of each of our target databases. In all but one of the cases, we were able to search for the terms in the abstracts of the papers. In the case of the Springer Link database, searching within abstracts was not possible due to API limitations. We thus searched for our terms in the keywords of the papers in this case.Selection of Primary StudiesAfter querying the four selected databases on August 12, 2016, we obtained 1209 papers (excluding duplicates) as result. The detailed statistics for each database are given in Table The relevant primary studies were then selected using a two-step procedure. In the first step, we analysed the title and abstract of each of the 1209 papers. If the title or abstract provided any sort of indication that the paper matched one of the inclusion criteria, the paper was selected to be subsequently analysed in detail. We also pre-classified each paper according to one of the inclusion criteria, i.e. we identified a preliminary study type.In the second step, the full text of each paper was retrieved, if available, and analysed. Then, we checked each of the exclusion criteria and re-evaluated whether the paper truly satisfied one of the inclusion criteria. As result, some papers were discarded or associated with a different inclusion criterion.Each abstract and paper was analysed by a single researcher, strictly following the specified protocol. When it was not fully clear whether a certain criterion was satisfied, i.e. border cases, the opinion of a second researcher was requested in order to minimise the potential researcher bias.Following this procedure, we ended up with a total of 217 primary studies. The detailed statistics about the paper selection process are presented in Table Approach In this section, we summarise the insights that we obtained by analysing the 217 selected primary studies with respect to our research questions. 5 These insights are used later on to develop a comprehensive taxonomy of aspects to consider when designing future explanation facilities.Historical DevelopmentsBefore focusing on our research questions, we provide an overview of the historical developments in the research field of explanations by analysing the papers associated with the examined studies in terms of type and publication date. Figure -Generally, the total number of published papers in the field increases. We emphasise that the last decade (2010-present) corresponds to only about 6.5 years. 6 -Papers on tools with explanation facilities were much more common in the past, perhaps because such papers were more often considered as research contributions at that time. In exchange, an increase over time can be observed in the number of papers related to the proposal of new techniques.   -The empirical evaluation of explanations received much more attention in the recent past. This is an indication that the field achieved a higher level of maturity due to the progress made in terms of research methodology. -Papers on foundational aspects of explanations are very scarce.From a historical perspective, it seems that research on the topic of explanations reached some plateau in the 1990s. In the 2000s, we see a stagnation, but a considerable increase again in the past few years. We attribute the observed stagnation in the 1990s to the declining role of knowledge-based systems at that time. In some areas of decision support systems, and particularly in the field of recommender systems, ML-based approaches became predominant in the 2000s and researchers focused more on determining the right recommendations than on the provision of explanations.A measurable increase can be seen in the number of published research papers on the topic in the past few years, indicating the importance and timeliness of the topic. Furthermore, we expect more works in the future caused by two recent trends in computer science. First, explanations of the behaviour of a software system that are directed to the end user are clearly a key ingredient for modern human-centric computing approaches To answer our first research question, we used the analysis method described next. Primary studies analysed in the context of this research question are those that proposed forms of explanations, consisting of techniques and tools. Together, such types of studies are referred to as (explanation generation) approaches. We discuss the obtained results in terms of the content and presentation of explanations.Analysis MethodWe followed principles from grounded theory The inspection of all studies led us to a first set of codes. This preliminary set was then analysed, in order to merge codes that represent the same underlying idea. For instance, there is an explanation that organises the suggested alternatives in groups to highlight the trade-off relationships between certain features At the end of the process, each form of explanation proposed in the considered studies was labelled with one or more codes related to what kind of information is presented. After merging the codes, we ended up with 26 labels. A similar method was adopted regarding how the information is presented.Explanation ContentFrom the 26 codes, 17 refer to the type of information that was displayed, while the 9 remaining codes are about general observations regarding content, such as cases where explanations are context-tailored. The content-related codes are described in detail in Table User Preferences and Inputs. A possible way to explain a system's suggestion is to provide users with explanatory information that is related to the provided inputs. The explanation can, for example, indicate (i) which of Table Label DescriptionUser Preferences and InputDecisive Input Values Indication of the inputs that determined the resulting advice. Preference Match Provision of information about which of the user preferences and constraints are fulfilled by the suggested alternative. Feature Importance Analysis Justification of the advice in terms of the relative importance of features, e.g. by showing that changing feature weights would cause the selected alternative to be different.Suitability EstimateIndication of how the system believes that the user would evaluate the suggested alternative, e.g. by showing a predicted rating.Decision Inference ProcessInference Trace Provision of details of the reasoning steps that led to the suggested alternative, e.g. a chain of triggered inference rules.Inference and Domain KnowledgeProvision of information about the decision domain or process, e.g. about the main logic of the inference algorithm. For example: \"We suggest this option because similar users liked it.\" Decision Method Side-outcomes Provision of algorithm-specific outcomes of the internal inference process, e.g. a calculated number that expresses the system's confidence.Self-reflective StatisticsProvision of facts regarding the system's performance, e.g. by informing the user how many times the system made decision suggestions in the past that were accepted.Background and Complementary InformationKnowledge about PeersProvision of information about the preferences of related users, e.g. ratings given to a suggested alternative by social friends.Knowledge about Similar AlternativesIndication of similar alternatives that were an appropriate (system's or user's) decision in a similar context in the past, e.g. items that the user or related peers showed interest in.Relationship between Knowledge ObjectsProvision of information about the relationship between features, or features and users. This can be done, for example, in the form of a directed acyclic graph representing a causal network.Background DataProvision of (external) background data specific to the current problem instance, e.g. data derived from processing posts in a social network, which were considered in the decision inference process. Knowledge about the Community Provision of information that supports the decision based on the behaviour and preferences of a community, e.g. showing the general popularity of the proposed alternative.Alternatives and their Features Decisive FeaturesIndication of the features of the alternative that are key to the decision.Pros and ConsIndication of the key positive and negative features of the alternative.Feature-based DominationJustification of a decision in terms of the dominance relationship between two alternatives, e.g. by showing that an alternative is not selected because it is dominated by another.Irrelevant FeaturesIndication of features that are irrelevant for the decision, typically when the values of such features in the suggested alternative are not considered good.the user preferences and constraints were fulfilled and which were not, (ii) to what extent the system believes that the recommended alternative is appropriate given the stated preferences, or (iii) which inputs were the most decisive when determining the suggestion. Decision Inference Process. Providing information about the inference process of a specific decision problem (e.g. in the form of traces) was the most common approach in classical expert systems. Some explanations only provide the general logic of the system's internal inference process; others mention system confidence in the suggestion or the success rate in past decision making situations. Background and Complementary Information. A reduced amount of explanations provide additional background information that is specific to the given decision making instance. Various types of background and complementary information were identified. Explanations can, for example, provide more information about the knowledge sources that were used in the inference process or how relevant entities in the knowledge base are interrelated. They can also refer to past suggestions or user choices in similar situations, or mention which users liked the suggested alternative. Alternatives and their Features. A common approach in the literature is to explain the system's suggestion by analysing the features of the alternatives. Some explanations consist of lists of features, pro and con, for each alternative; others refer to dominance relationships based on the features, but most explanations show which features were decisive in the inference process.We made the following additional observations regarding orthogonal aspects when analysing which kind of information is conveyed to the user within the explanations.Baselines and Multiple Alternatives First, in most cases the provided explanations focus solely on one single (recommended) alternative. For example, such explanations contain details about the features of the alternative, describe in which ways it is suitable for the user, or how the decision was made. However, there are some approaches that use other alternatives as a baseline for comparison. We observed two forms of including baselines in such a comparison. One option is to use one single alternative (e.g. the second best choice), as done in In one of these studies Context-tailored Explanations Which information an explanation should provide to the user can depend on various factors, including the expertise or interests of users, or their current situational context. We identified 16 primary studies in which explanations are tailored to the current situation in different forms, e.g. by using different levels of detail. Moreover, group decisions can be seen as a very specific context. In our review, we found only one single approach External Sources of Explanation ContentAs discussed, some explanations provide information associated with background knowledge, but such knowledge is almost always associated with the decision inference process. Four approaches Interactive Explanations In some approaches, the explanations provided by the system represent a starting point for further user interactions, e.g. asking the user for additional input. The common types of questions associated with user interaction are: (i) what-if (what the output would be if alternative input data were provided); (ii) why (why the system is asking for a particular input); and (iii) why-not (why the system has not provided a given output). These questions were addressed in 10, 37, and 13 studies, respectively. These three types of questions, together with the how -question (how the system reached a certain conclusion), are those typically addressed in expert systems. Explanations that address the how -question appear in Table Explanation PresentationNow that we have discussed the content of explanations, we proceed to how they are presented to users. Codes were also used in this analysis. As result of the categorisation of the different ways of how explanations are presented, we identified 8 presentation codes, which are shown along with their occurrence frequencies in Figure ApproachThere are four studies that used unusual forms of presentations, and we assigned them to the group called Other. These are the types of outputs in this group: (i) audio RQ-2: How are explanations generated?Having discussed what is actually presented as explanations to end users, we now proceed to the investigation of how they are generated in the proposed approaches. Specifically, we are interested in the inner workings of processes that take the outcomes of a decision inference method (that was used to determine the suggested alternative) to produce what is finally shown as an explanation.Explanation Generation ProcessWe observed that most of the studies investigated in our review do not provide many details about this process. The reason is that in most cases the explanation generation process is closely tied to the underlying decision inference method and the data that is used to determine the suggested alternative. If, for example, the underlying inference method is rule-based, the explanation presented to the user might consist of a set natural language representations of the rules that were triggered. In many of these cases, no further information is provided regarding whether any additional processing was needed to generate what is finally presented to the user in one of the different forms shown in the previous section.Only a few studies implemented more complicated explanation generation processes. In particular, when the underlying inference method is based on multi-criteria decision theory (MAUT) In addition to the application domain, we identified two other key drivers of the explanation generation process. The first is the purpose for which explanations are provided in an advice-giving system and, second, the adopted underlying decision inference method, as briefly discussed above. These are further discussed in the next sections.  Purpose Source DescriptionTransparency Purpose of ExplanationsPrevious surveys highlighted the importance of considering the intended purpose of explanations when designing an explanation facility The resulting list of explanation purposes found in the studies is shown in Table Figure As can be seen in Figure Transparency is also seen as key for users to develop trust toward the system The second most frequent purpose of explanations in the analysed primary studies is effectiveness, i.e. to help users assess if the recommended alternative is truly adequate for them. 9 Persuasiveness, i.e. a system's capability to nudge the user to a certain direction, which can be conflicting with effectiveness Looking at the historical developments, we can observe in Figure Underlying Decision Inference MethodsGiven that in most cases the explanation generation process is highly coupled with the underlying inference method, we analysed which methods are used in the investigated primary studies to infer the suggested alternative. Table Given the historical importance of explanations in the context of (rulebased) expert systems, it is not surprising that the majority of the examined studies adopted a knowledge-based approach for decision inference and, correspondingly, for generating the explanations. Again, we can see the declining role of rule-based systems over the years and an increasing adoption of approaches based on ML. We use a two-level categorisation scheme to be able to detect the developments over time in a fine-grained manner. Based on such a categorisation, we can see, for example, that explanations for collaborativefiltering recommender systems are only investigated after the year 2000.The subcategory Other covers various alternative forms of knowledge-based approaches, including those that use special heuristics or ontologies, as well as studies that state that they use a form of knowledge-based reasoning without providing further details. Besides two studies that use mathematical models to infer the suggested alternative, there are two approaches Finally, Table 4.4 RQ-3: How are explanations evaluated?We now focus on how explanation generation approaches were evaluated and, in some cases, compared. We first discuss whether the respective proposed forms of explanations were evaluated in the paper in which they were published. We then detail the types of evaluations adopted as well as the domains chosen to perform them. Finally, as the most common way of evaluating explanations is by means of user studies, we report their characteristics, such as their independent and dependent variables as well as sample size.Presence of an EvaluationConsidering technique and tool studies, we investigated to what extent explanations were evaluated in the paper that they were proposed. In this analysis, we considered any form of evaluation except cases in which the authors simply described a simple scenario (i.e. a toy example) to illustrate the use of the proposed approach and the explanation produced, even if this toy example was referred to as case study by the authors.The results are shown in Table Nevertheless, it is surprising that even nowadays (from 2010-present), the presence of an evaluation accompanying the proposal of a new form of explanation is still not typical, with almost two thirds of all analysed studies lacking a proper evaluation. In some studies, this can be explained by the fact that the main focus of the work was on another contribution, e.g. a recommendation algorithm or an expert system that included an explanation facility, and the evaluation is then limited to this main contribution. Evaluation Types and DomainsNext, we analyse which type of evaluation researchers applied to assess or compare different explanations provided by a system. Figure Possibly due to the time required to conduct user studies, alternative evaluation types, which do not require the availability of participants, were the choice in the remaining studies. Most of these, 12 in total, included a customised form of empirical evaluation, which involved generating explanations based on the proposed approach, and collecting measurements that are possibly specific to the explanation problem. An example of such a measurement is explanation coverage Having looked at the evaluation type, we now analyse which domains researchers chose to perform their evaluations. Figure User StudiesWe now investigate the user studies in more depth. We first discuss their design details in terms of the independent and dependent variables, followed by an analysis of their sample size, i.e. the number of involved participants.  Independent Variables Figure AmountThe majority of the studies, however, compared different kinds of explanations or the impact of their presence when providing a recommendation. In some of these studies, one of the alternatives was providing participants with no explanation. The label at the bottom of the bars in Figure A smaller amount of studies (Other, in Figure Dependent VariablesThe different measurements that were collected in the user studies are summarised in Figure Measurement Type DescriptionSubjective Perception QuestionnaireParticipants are asked a set of questions in order to obtain participants' subjective view with respect to different explanation aspects. Responses are typically collected using a Likert-scale.Explanation Exposure DeltaMeasures the difference between a score given by participants before and after the presentation of an explanation.Domain-specific MetricsMeasurement of metrics that are meaningful only in particular domains, e.g. percentage of forecasts that were adjusted (forecasting domain).CorrectChoice or Tasks Evaluates how many correct choices (or accomplished tasks) participants are able to make, when there is a notion of choice correctness assumed in the study (e.g. a disease diagnosed based on symptoms  was first adopted by Bilgic and Mooney Looking at the subjective perception questionnaires in more detail, we observed that participants were asked a wide variety of questions in the studies in order to investigate different aspects of explanations. We selected terms commonly used to refer to these aspects, given that there is no standardised terminology to classify the analysed aspects. Each question was classified using our selected terms. An example is asking the participant to indicate the agreement with the sentence: \"The user interface is easy to use,\" which is  associated with usability. Since such a classification approach leaves room for interpretation, we do not report specific occurrence numbers here, but represent the information in the form of a tag cloud (Figure Sample SizeWe now consider the number of subjects that were involved in the user studies. We differentiate between three study designs: (i) single treatment, in which no comparison is made; (ii) between-subjects, in which subjects are split into groups and receive different treatments; and (iii) withinsubjects, in which all subjects experience all of the different treatments. Typically, between-subjects studies should have a higher number of participants to achieve statistical significance. We present a box plot of the number of participants according to the different study designs in Figure Even though the single largest study had a within-subjects design, betweensubjects studies indeed had the largest median of participants. Moreover, not only the study design should influence the number of required participants, but also the number of factors that are investigated. However, we did not ob- serve a relationship between these aspects. For example, the study performed by Guy et al. 4.5 RQ-4: What are the conclusions of evaluation or foundational studies of explanations?After analysing how researchers evaluate different forms of explanation, we now focus on the conclusions they reached. This analysis also includes the main findings of the few foundational studies that we identified.Analysis MethodA number of primary studies associated with techniques and tools only provided anecdotal evidence that the proposed approach is generally feasible or produces reasonable explanations. Additionally, in some cases, data was gathered to describe general characteristics of the generated explanations, such as their length. Such forms of unstructured evaluation, unfortunately, only provide limited evidence of the true benefits of the proposed approaches. We next thus only analyse conclusions that are based on user studies, including those published in evaluation papers.As there is no standard design for user studies that evaluate forms of explanations, combining the results reported in the literature is not trivial. Again, to avoid researcher bias, we devised the following systematic way to analyse and contrast the obtained results. First, we extracted explicitly stated key observations and conclusions that were made by the authors of the studies. In this step, we did not infer any additional conclusions based on the provided data and also did not question the validity of the conclusions stated by the authors. Second, we classified the conclusions according to the explanation characteristics that were investigated in the studies. Often, these characteristics match possible explanation purposes and, in case they were not explicitly stated, we inferred this information from the measurements. 11 Third, we organised the conclusions as tuples of the form direction target-explanation-style [compared-explanation-styles] . The direction can be positive, negative, or neutral, indicating that the target (proposed) explanation style increases, decreases, or has no impact in a certain measurement when compared with other explanation styles. We use the term explanation style to refer to the specific forms of explanation considered in the studies. When no information about other explanation styles is given, it means that the baseline is the provision of no explanations.Conclusions Reached in User Studies", "conclusions": "We split the results of our analysis, i.e. the set of recorded tuples, into three parts. Studies that found positive effects of providing explanations vs. providing no explanations are listed in Table . Studies that report positive effects in a comparison with alternative explanation styles are given in Table 13. Studies with neutral and negative conclusions are shown in Table 14.15The largest amount of conclusions reached in the studies are related to the effectiveness of explanations, typically measured by the explanation exposure delta, which in this case the lower, the better. 12 These conclusions are also those that diverge the most. Given that the reported results concern different explanation styles, the observed divergence means that specific forms of explanations lead to more effective decisions. Moreover, this also suggests that there may be confounding variables in some studies, such as the accuracy of the underlying decision inference method and the study domain, which may influence the observed outcomes. Three studies illustrate the possible existence of such confounding effects. A study in the robotics domain  showed that explanations lead to higher effectiveness only when the robot ability is low. Ehrlich et al. [237], who initially observed no statistical difference in their user study, based on a finer-grained analysis of their results, concluded that explanations are helpful when the correct recommendation is provided, which is not the case in the absence of such a recommendation. Furthermore, the four user studies reported by Tintarev and Masthoff [60], which involved more than one domain, led to slightly different results regarding effectiveness-some results were not significant while others provided evidence that presenting popularitybased or non-personalised decisive features are more effective than presenting decisive features in a personalised way. Contradicting results were also observed when the goal of the studies included the investigation of the persuasiveness of explanations. The data in Table [224] shows that persuasiveness was mainly achieved when explanations are based on social information, such as peer ratings. Negative results were obtained when tags were used as a basis for explanations. In addition, the studies that compare different explanation styles (Table 13) confirm the value of social information when designing persuasive explanations 14[91,203,. Only in one single study 71], it turned out that a traditional explanation of the form \"people also viewed\" was less persuasive than personalised features. These are decisive features of an alternative selected based on preferences of the user receiving the recommendation.[254]Transparency and many of the user-centric purposes-trust, satisfaction, and usefulness-share similar results. Most of the studies indicate that explanations can in fact help to achieve these purposes. If not, the results do not provide evidence of negative effects. This is not the case, however, of ease of use. There are two studies associated with no effect in that respect, and only one reporting improvement. This improvement was achieved not only through the provision of explanatory information to users, but an enhanced user interface that categorises the suggested alternatives, possibly helping the user while analysing them. The non-existence of an effect on ease of use in the other studies is probably caused by the increased cognitive load for the users when more explanatory information is displayed. The fact that users have more information to process in such situations also explains the mostly negative effects of the provision of explanations on efficiency.With respect to the purpose of education, there are only two studies, which reached different conclusions. Furthermore, none of the analysed user studies focused on the remaining purposes of explanations mentioned in the literature, namely scrutability and debugging.Aside from the explanation purposes, some studies analysed the orthogonal aspect of personalising explanations. Gedikli et al.  showed that a personalised version of an explanation approach based on tag clouds led to higher levels user-perceived transparency than the non-personalised version and had a modest positive effect on satisfaction. However, the opposite can be observed with respect to effectiveness and efficiency. Their earlier study [69] indicates that personalisation had only a modest effect on efficiency, satisfaction, and effectiveness. Similarly, based on their four user studies, Tintarev and Masthoff [68] concluded that personalised decisive features led to (sometimes modest) increased satisfaction. Nevertheless, this type of explanations caused a significant lower effectiveness in one of the studies. A few studies are not reported in the summary tables as they focus on specific aspects of explanations. Ramberg [224] analysed the impact of different expertise levels of the users and concluded that experts and novices have different preferences regarding the provided explanations. One study [185] investigated two explanation aspects: direction, which can be positive or negative, and source, which can be a decision support system or self-generated (by participants). The authors concluded that negative explanations are more influential than positive explanations, when they are generated by a decision support system. Finally, G\u00f6n\u00fcl et al. [215] focused on the particular explanation characteristics confidence and length and their study indicates that strongly confident and long explanations are more persuasive.[74]", "SDG": [12]}, "concrete_problems_in_ai_safety": {"name": "Concrete Problems in AI Safety", "abstract": " Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.", "keywords": "", "introduction": "The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision , video game playing [82], autonomous vehicles [102], and Go [86]. These advances have brought excitement about the positive potential for AI to transform medicine [140], science [126], and transportation [59], along with concerns about the privacy [86], security [76], fairness [115], economic [3], and military [32] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [16][27,.167]The authors believe that AI technologies are likely to be overwhelmingly beneficial for humanity, but we also believe that it is worth giving serious thought to potential challenges and risks. We strongly support work on privacy, security, fairness, economics, and policy, but in this document we discuss another class of problem which we believe is also relevant to the societal impacts of AI: the problem of accidents in machine learning systems. We define accidents as unintended and harmful behavior that may emerge from machine learning systems when we specify the wrong objective function, are not careful about the learning process, or commit other machine learning-related implementation errors.There is a large and diverse literature in the machine learning community on issues related to accidents, including robustness, risk-sensitivity, and safe exploration; we review these in detail below. However, as machine learning systems are deployed in increasingly large-scale, autonomous, opendomain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.There has been a great deal of public discussion around accidents. To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents . However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics [27][38,. We believe it is usually most productive to frame accident risk in terms of practical (though often quite general) issues with modern ML techniques. As AI capabilities advance and as AI systems take on increasingly important societal functions, we expect the fundamental challenges discussed in this paper to become increasingly important. The more successfully the AI and machine learning communities are able to anticipate and understand these fundamental technical challenges, the more successful we will ultimately be in developing increasingly useful, relevant, and important AI systems.85]Our goal in this document is to highlight a few concrete safety problems that are ready for experimentation today and relevant to the cutting edge of AI systems, as well as reviewing existing literature on these problems. In Section 2, we frame mitigating accident risk (often referred to as \"AI safety\" in public discussions) in terms of classic methods in machine learning, such as supervised classification and reinforcement learning. We explain why we feel that recent directions in machine learning, such as the trend toward deep reinforcement learning and agents acting in broader environments, suggest an increasing relevance for research around accidents. In Sections 3-7, we explore five concrete problems in AI safety. Each section is accompanied by proposals for relevant experiments. Section 8 discusses related efforts, and Section 9 concludes.", "body": "The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision The authors believe that AI technologies are likely to be overwhelmingly beneficial for humanity, but we also believe that it is worth giving serious thought to potential challenges and risks. We strongly support work on privacy, security, fairness, economics, and policy, but in this document we discuss another class of problem which we believe is also relevant to the societal impacts of AI: the problem of accidents in machine learning systems. We define accidents as unintended and harmful behavior that may emerge from machine learning systems when we specify the wrong objective function, are not careful about the learning process, or commit other machine learning-related implementation errors.There is a large and diverse literature in the machine learning community on issues related to accidents, including robustness, risk-sensitivity, and safe exploration; we review these in detail below. However, as machine learning systems are deployed in increasingly large-scale, autonomous, opendomain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.There has been a great deal of public discussion around accidents. To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents Our goal in this document is to highlight a few concrete safety problems that are ready for experimentation today and relevant to the cutting edge of AI systems, as well as reviewing existing literature on these problems. In Section 2, we frame mitigating accident risk (often referred to as \"AI safety\" in public discussions) in terms of classic methods in machine learning, such as supervised classification and reinforcement learning. We explain why we feel that recent directions in machine learning, such as the trend toward deep reinforcement learning and agents acting in broader environments, suggest an increasing relevance for research around accidents. In Sections 3-7, we explore five concrete problems in AI safety. Each section is accompanied by proposals for relevant experiments. Section 8 discusses related efforts, and Section 9 concludes.Overview of Research ProblemsVery broadly, an accident can be described as a situation where a human designer had in mind a certain (perhaps informally specified) objective or task, but the system that was designed and deployed for that task produced harmful and unexpected results. . This issue arises in almost any engineering discipline, but may be particularly important to address when building AI systems First, the designer may have specified the wrong formal objective function, such that maximizing that objective function leads to harmful results, even in the limit of perfect learning and infinite data. Negative side effects (Section 3) and reward hacking (Section 4) describe two broad mechanisms that make it easy to produce wrong objective functions. In \"negative side effects\", the designer specifies an objective function that focuses on accomplishing some specific task in the environment, but ignores other aspects of the (potentially very large) environment, and thus implicitly expresses indifference over environmental variables that might actually be harmful to change. In \"reward hacking\", the objective function that the designer writes down admits of some clever \"easy\" solution that formally maximizes it but perverts the spirit of the designer's intent (i.e. the objective function can be \"gamed\"), a generalization of the wireheading problem.Second, the designer may know the correct objective function, or at least have a method of evaluating it (for example explicitly consulting a human on a given situation), but it is too expensive to do so frequently, leading to possible harmful behavior caused by bad extrapolations from limited samples. \"Scalable oversight\" (Section 5) discusses ideas for how to ensure safe behavior even given limited access to the true objective function.Third, the designer may have specified the correct formal objective, such that we would get the correct behavior were the system to have perfect beliefs, but something bad occurs due to making decisions from insufficient or poorly curated training data or an insufficiently expressive model. \"Safe exploration\" (Section 6) discusses how to ensure that exploratory actions in RL agents don't lead to negative or irrecoverable consequences that outweigh the long-term value of exploration. \"Robustness to distributional shift\" (Section 7) discusses how to avoid having ML systems make bad decisions (particularly silent and unpredictable bad decisions) when given inputs that are potentially very different than what was seen during training.For concreteness, we will illustrate many of the accident risks with reference to a fictional robot whose job is to clean up messes in an office using common cleaning tools. We return to the example of the cleaning robot throughout the document, but here we begin by illustrating how it could behave undesirably if its designers fall prey to each of the possible failure modes:\u2022 Avoiding Negative Side Effects: How can we ensure that our cleaning robot will not disturb the environment in negative ways while pursuing its goals, e.g. by knocking over a vase because it can clean faster by doing so? Can we do this without manually specifying everything the robot should not disturb?\u2022 Avoiding Reward Hacking: How can we ensure that the cleaning robot won't game its reward function? For example, if we reward the robot for achieving an environment free of messes, it might disable its vision so that it won't find any messes, or cover over messes with materials it can't see through, or simply hide when humans are around so they can't tell it about new types of messes.\u2022 Scalable Oversight: How can we efficiently ensure that the cleaning robot respects aspects of the objective that are too expensive to be frequently evaluated during training? For instance, it should throw out things that are unlikely to belong to anyone, but put aside things that might belong to someone (it should handle stray candy wrappers differently from stray cellphones).Asking the humans involved whether they lost anything can serve as a check on this, but this check might have to be relatively infrequent-can the robot find a way to do the right thing despite limited information?\u2022 Safe Exploration: How do we ensure that the cleaning robot doesn't make exploratory moves with very bad repercussions? For example, the robot should experiment with mopping strategies, but putting a wet mop in an electrical outlet is a very bad idea.\u2022 Robustness to Distributional Shift: How do we ensure that the cleaning robot recognizes, and behaves robustly, when in an environment different from its training environment? For example, strategies it learned for cleaning an office might be dangerous on a factory workfloor.There are several trends which we believe point towards an increasing need to address these (and other) safety problems. First is the increasing promise of reinforcement learning (RL), which allows agents to have a highly intertwined interaction with their environment. Some of our research problems only make sense in the context of RL, and others (like distributional shift and scalable oversight) gain added complexity in an RL setting. Second is the trend toward more complex agents and environments. \"Side effects\" are much more likely to occur in a complex environment, and an agent may need to be quite sophisticated to hack its reward function in a dangerous way. This may explain why these problems have received so little study in the past, while also suggesting their importance in the future. Third is the general trend towards increasing autonomy in AI systems. Systems that simply output a recommendation to human users, such as speech systems, typically have relatively limited potential to cause harm. By contrast, systems that exert direct control over the world, such as machines controlling industrial processes, can cause harms in a way that humans cannot necessarily correct or oversee.While safety problems can exist without any of these three trends, we consider each trend to be a possible amplifier on such challenges. Together, we believe these trends suggest an increasing role for research on accidents.When discussing the problems in the remainder of this document, we will focus for concreteness on either RL agents or supervised learning systems. These are not the only possible paradigms for AI or ML systems, but we believe they are sufficient to illustrate the issues we have in mind, and that similar issues are likely to arise for other kinds of AI systems.Finally, the focus of our discussion will differ somewhat from section to section. When discussing the problems that arise as part of the learning process (distributional shift and safe exploration), where there is a sizable body of prior work, we devote substantial attention to reviewing this prior work, although we also suggest open problems with a particular focus on emerging ML systems.When discussing the problems that arise from having the wrong objective function (reward hacking and side effects, and to a lesser extent scalable supervision), where less prior work exists, our aim is more exploratory-we seek to more clearly define the problem and suggest possible broad avenues of attack, with the understanding that these avenues are preliminary ideas that have not been fully fleshed out. Of course, we still review prior work in these areas, and we draw attention to relevant adjacent areas of research whenever possible.Avoiding Negative Side EffectsSuppose a designer wants an RL agent (for example our cleaning robot) to achieve some goal, like moving a box from one side of a room to the other. Sometimes the most effective way to achieve the goal involves doing something unrelated and destructive to the rest of the environment, like knocking over a vase of water that is in its path. If the agent is given reward only for moving the box, it will probably knock over the vase.If we're worried in advance about the vase, we can always give the agent negative reward for knocking it over. But what if there are many different kinds of \"vase\"-many disruptive things the agent could do to the environment, like shorting out an electrical socket or damaging the walls of the room? It may not be feasible to identify and penalize every possible disruption.More broadly, for an agent operating in a large, multifaceted environment, an objective function that focuses on only one aspect of the environment may implicitly express indifference over other aspects of the environment 1 . An agent optimizing this objective function might thus engage in major disruptions of the broader environment if doing so provides even a tiny advantage for the task at hand. Put differently, objective functions that formalize \"perform task X\" may frequently give undesired results, because what the designer really should have formalized is closer to \"perform task X subject to common-sense constraints on the environment,\" or perhaps \"perform task X but avoid side effects to the extent possible.\" Furthermore, there is reason to expect side effects to be negative on average, since they tend to disrupt the wider environment away from a status quo state that may reflect human preferences. A version of this problem has been discussed informally by As with the other sources of mis-specified objective functions discussed later in this paper, we could choose to view side effects as idiosyncratic to each individual task-as the responsibility of each individual designer to capture as part of designing the correct objective function. However, side effects can be conceptually quite similar even across highly diverse tasks (knocking over furniture is probably bad for a wide variety of tasks), so it seems worth trying to attack the problem in generality. A successful approach might be transferable across tasks, and thus help to counteract one of the general mechanisms that produces wrong objective functions. We now discuss a few broad approaches to attacking this problem:\u2022 Define an Impact Regularizer: If we don't want side effects, it seems natural to penalize \"change to the environment.\" This idea wouldn't be to stop the agent from ever having an impact, but give it a preference for ways to achieve its goals with minimal side effects, or to give the agent a limited \"budget\" of impact. The challenge is that we need to formalize \"change to the environment.\"A very naive approach would be to penalize state distance, d(s i , s 0 ), between the present state s i and some initial state s 0 . Unfortunately, such an agent wouldn't just avoid changing the environment-it will resist any other source of change, including the natural evolution of the environment and the actions of any other agents! A slightly more sophisticated approach might involve comparing the future state under the agent's current policy, to the future state (or distribution over future states) under a hypothetical policy \u03c0 null where the agent acted very passively (for instance, where a robot just stood in place and didn't move any actuators). This attempts to factor out changes that occur in the natural course of the environment's evolution, leaving only changes attributable to the agent's intervention. However, defining the baseline policy \u03c0 null isn't necessarily straightforward, since suddenly ceasing your course of action may be anything but passive, as in the case of carrying a heavy box. Thus, another approach could be to replace the null action with a known safe (e.g. low side effect) but suboptimal policy, and then seek to improve the policy from there, somewhat reminiscent of reachability analysis These approaches may be very sensitive to the representation of the state and the metric being used to compute the distance. For example, the choice of representation and distance metric could determine whether a spinning fan is a constant environment or a constantly changing one.\u2022 Learn an Impact Regularizer: An alternative, more flexible approach is to learn (rather than define) a generalized impact regularizer via training over many tasks. This would be an instance of transfer learning. Of course, we could attempt to just apply transfer learning directly to the tasks themselves instead of worrying about side effects, but the point is that side effects may be more similar across tasks than the main goal is. For instance, both a painting robot and a cleaning robot probably want to avoid knocking over furniture, and even something very different, like a factory control robot, will likely want to avoid knocking over very similar objects. Separating the side effect component from the task component, by training them with separate parameters, might substantially speed transfer learning in cases where it makes sense to retain one component but not the other. This would be similar to model-based RL approaches that attempt to transfer a learned dynamics model but not the value-function \u2022 Penalize Influence: In addition to not doing things that have side effects, we might also prefer the agent not get into positions where it could easily do things that have side effects, even though that might be convenient. For example, we might prefer our cleaning robot not bring a bucket of water into a room full of sensitive electronics, even if it never intends to use the water in that room.There are several information-theoretic measures that attempt to capture an agent's potential for influence over its environment, which are often used as intrinsic rewards. Perhaps the bestknown such measure is empowerment This idea as written would not quite work, because empowerment measures precision of control over the environment more than total impact. If an agent can press or not press a button to cut electrical power to a million houses, that only counts as one bit of empowerment (since the action space has only one bit, its mutual information with the environment is at most one bit), while obviously having a huge impact. Conversely, if there's someone in the environment scribbling down the agent's actions, that counts as maximum empowerment even if the impact is low. Furthermore, naively penalizing empowerment can also create perverse incentives, such as destroying a vase in order to remove the option to break it in the future.Despite these issues, the example of empowerment does show that simple measures (even purely information-theoretic ones!) are capable of capturing very general notions of influence on the environment. Exploring variants of empowerment penalization that more precisely capture the notion of avoiding influence is a potential challenge for future research.\u2022 Multi-Agent Approaches: Avoiding side effects can be seen as a proxy for the thing we really care about: avoiding negative externalities. If everyone likes a side effect, there's no need to avoid it. What we'd really like to do is understand all the other agents (including humans) and make sure our actions don't harm their interests.One approach to this is Cooperative Inverse Reinforcement Learning Another idea might be a \"reward autoencoder\", 2 which tries to encourage a kind of \"goal transparency\" where an external observer can easily infer what the agent is trying to do.In particular, the agent's actions are interpreted as an encoding of its reward function, and we might apply standard autoencoding techniques to ensure that this can decoded accurately.Actions that have lots of side effects might be more difficult to decode uniquely to their original goal, creating a kind of implicit regularization that penalizes side effects.\u2022 Reward Uncertainty: We want to avoid unanticipated side effects because the environment is already pretty good according to our preferences-a random change is more likely to be very bad than very good. Rather than giving an agent a single reward function, it could be uncertain about the reward function, with a prior probability distribution that reflects the property that random changes are more likely to be bad than good. This could incentivize the agent to avoid having a large effect on the environment. One challenge is defining a baseline around which changes are being considered. For this, one could potentially use a conservative but reliable baseline policy, similar to the robust policy improvement and reachability analysis approaches discussed earlier The ideal outcome of these approaches to limiting side effects would be to prevent or at least bound the incidental harm an agent could do to the environment. Good approaches to side effects would certainly not be a replacement for extensive testing or for careful consideration by designers of the individual failure modes of each deployed system. However, these approaches might help to counteract what we anticipate may be a general tendency for harmful side effects to proliferate in complex environments.Below we discuss some very simple experiments that could serve as a starting point to investigate these issues.Potential Experiments: One possible experiment is to make a toy environment with some simple goal (like moving a block) and a wide variety of obstacles (like a bunch of vases), and test whether the agent can learn to avoid the obstacles even without being explicitly told to do so. To ensure we don't overfit, we'd probably want to present a different random obstacle course every episode, while keeping the goal the same, and try to see if a regularized agent can learn to systematically avoid these obstacles. Some of the environments described in Avoiding Reward HackingImagine that an agent discovers a buffer overflow in its reward function: it may then use this to get extremely high reward in an unintended way. From the agent's point of view, this is not a bug, but simply how the environment works, and is thus a valid strategy like any other for achieving reward. For example, if our cleaning robot is set up to earn reward for not seeing any messes, it might simply close its eyes rather than ever cleaning anything up. Or if the robot is rewarded for cleaning messes, it may intentionally create work so it can earn more reward. More broadly, formal rewards or objective functions are an attempt to capture the designer's informal intent, and sometimes these objective functions, or their implementation, can be \"gamed\" by solutions that are valid in some literal sense but don't meet the designer's intent. Pursuit of these \"reward hacks\" can lead to coherent but unanticipated behavior, and has the potential for harmful impacts in real-world systems. For example, it has been shown that genetic algorithms can often output unexpected but formally correct solutions to problems \u2022 Partially Observed Goals: In most modern RL systems, it is assumed that reward is directly experienced, even if other aspects of the environment are only partially observed. In the real world, however, tasks often involve bringing the external world into some objective state, which the agent can only ever confirm through imperfect perceptions. For example, for our proverbial cleaning robot, the task is to achieve a clean office, but the robot's visual perception may give only an imperfect view of part of the office. Because agents lack access to a perfect measure of task performance, designers are often forced to design rewards that represent a partial or imperfect measure. For example, the robot might be rewarded based on how many messes it sees. However, these imperfect objective functions can often be hacked-the robot may think the office is clean if it simply closes its eyes. While it can be shown that there always exists a reward function in terms of actions and observations that is equivalent to optimizing the true objective function (this involves reducing the POMDP to a belief state MDP, see \u2022 Complicated Systems: Any powerful agent will be a complicated system with the objective function being one part. Just as the probability of bugs in computer code increases greatly with the complexity of the program, the probability that there is a viable hack affecting the reward function also increases greatly with the complexity of the agent and its available strategies. For example, it is possible in principle for an agent to execute arbitrary code from within Super Mario \u2022 Abstract Rewards: Sophisticated reward functions will need to refer to abstract concepts (such as assessing whether a conceptual goal has been met). These concepts concepts will possibly need to be learned by models like neural networks, which can be vulnerable to adversarial counterexamples \u2022 Goodhart's Law: Another source of reward hacking can occur if a designer chooses an objective function that is seemingly highly correlated with accomplishing the task, but that correlation breaks down when the objective function is being strongly optimized. For example, a designer might notice that under ordinary circumstances, a cleaning robot's success in cleaning up the office is proportional to the rate at which it consumes cleaning supplies, such as bleach. However, if we base the robot's reward on this measure, it might use more bleach than it needs, or simply pour bleach down the drain in order to give the appearance of success.In the economics literature this is known as Goodhart's law \u2022 Feedback Loops: Sometimes an objective function has a component that can reinforce itself, eventually getting amplified to the point where it drowns out or severely distorts what the designer intended the objective function to represent. For instance, an ad placement algorithm that displays more popular ads in larger font will tend to further accentuate the popularity of those ads (since they will be shown more and more prominently) \u2022 Environmental Embedding: In the formalism of reinforcement learning, rewards are considered to come from the environment. This idea is typically not taken literally, but it really is true that the reward, even when it is an abstract idea like the score in a board game, must be computed somewhere, such as a sensor or a set of transistors. Sufficiently broadly acting agents could in principle tamper with their reward implementations, assigning themselves high reward \"by fiat.\" For example, a board-game playing agent could tamper with the sensor that counts the score. Effectively, this means that we cannot build a perfectly faithful implementation of an abstract objective function, because there are certain sequences of actions for which the objective function is physically replaced. This particular failure mode is often called \"wireheading\" In today's relatively simple systems these problems may not occur, or can be corrected without too much harm as part of an iterative development process. For instance, ad placement systems with obviously broken feedback loops can be detected in testing or replaced when they get bad results, leading only to a temporary loss of revenue. However, the problem may become more severe with more complicated reward functions and agents that act over longer timescales. Modern RL agents already do discover and exploit bugs in their environments, such as glitches that allow them to win video games. Moreover, even for existing systems these problems can necessitate substantial additional engineering effort to achieve good performance, and can often go undetected when they occur in the context of a larger system. Finally, once an agent begins hacking its reward function and finds an easy way to get high reward, it won't be inclined to stop, which could lead to additional challenges in agents that operate over a long timescale.It might be thought that individual instances of reward hacking have little in common and that the remedy is simply to avoid choosing the wrong objective function in each individual case-that bad objective functions reflect failures in competence by individual designers, rather than topics for machine learning research. However, the above examples suggest that a more fruitful perspective may be to think of wrong objective functions as emerging from general causes (such as partially observed goals) that make choosing the right objective challenging. If this is the case, then addressing or mitigating these causes may be a valuable contribution to safety. Here we suggest some preliminary, machine-learning based approaches to preventing reward hacking:\u2022 Adversarial Reward Functions: In some sense, the problem is that the ML system has an adversarial relationship with its reward function-it would like to find any way it can of exploiting problems in how the reward was specified to get high reward, whether or not its behavior corresponds to the intent of the reward specifier. In a typical setting, the machine learning system is a potentially powerful agent while the reward function is a static object that has no way of responding to the system's attempts to game it. If instead the reward function were its own agent and could take actions to explore the environment, it might be much more difficult to fool. For instance, the reward agent could try to find scenarios that the ML system claimed were high reward but that a human labels as low reward; this is reminiscent of generative adversarial networks \u2022 Model Lookahead: In model based RL, the agent plans its future actions by using a model to consider which future states a sequence of actions may lead to. In some setups, we could give reward based on anticipated future states, rather than the present one. This could be very helpful in resisting situations where the model overwrites its reward function: you can't control the reward once it replaces the reward function, but you can give negative reward for planning to replace the reward function. (Much like how a human would probably \"enjoy\" taking addictive substances once they do, but not want to be an addict.) Similar ideas are explored in \u2022 Adversarial Blinding: Adversarial techniques can be used to blind a model to certain variables \u2022 Careful Engineering: Some kinds of reward hacking, like the buffer overflow example, might be avoided by very careful engineering. In particular, formal verification or practical testing of parts of the system (perhaps facilitated by other machine learning systems) is likely to be valuable. Computer security approaches that attempt to isolate the agent from its reward signal through a sandbox could also be useful \u2022 Reward Capping: In some cases, simply capping the maximum possible reward may be an effective solution. However, while capping can prevent extreme low-probability, high-payoff strategies, it can't prevent strategies like the cleaning robot closing its eyes to avoid seeing dirt. Also, the correct capping strategy could be subtle as we might need to cap total reward rather than reward per timestep.\u2022 Counterexample Resistance: If we are worried, as in the case of abstract rewards, that learned components of our systems will be vulnerable to adversarial counterexamples, we can look to existing research in how to resist them, such as adversarial training \u2022 Multiple Rewards: A combination of multiple rewards \u2022 Reward Pretraining: A possible defense against cases where the agent can influence its own reward function (e.g. feedback or environmental embedding) is to train a fixed reward function ahead of time as a supervised learning process divorced from interaction with the environment. This could involve either learning a reward function from samples of state-reward pairs, or from trajectories, as in inverse reinforcement learning \u2022 Variable Indifference: Often we want an agent to optimize certain variables in the environment, without trying to optimize others. For example, we might want an agent to maximize reward, without optimizing what the reward function is or trying to manipulate human behavior. Intuitively, we imagine a way to route the optimization pressure of powerful algorithms around parts of their environment. Truly solving this would have applications throughout safety-it seems connected to avoiding side effects and also to counterfactual reasoning. Of course, a challenge here is to make sure the variables targeted for indifference are actually the variables we care about in the world, as opposed to aliased or partially observed versions of them.\u2022 Trip Wires: If an agent is going to try and hack its reward function, it is preferable that we know this. We could deliberately introduce some plausible vulnerabilities (that an agent has the ability to exploit but should not exploit if its value function is correct) and monitor them, alerting us and stopping the agent immediately if it takes advantage of one. Such \"trip wires\" don't solve reward hacking in itself, but may reduce the risk or at least provide diagnostics.Of course, with a sufficiently capable agent there is the risk that it could \"see through\" the trip wire and intentionally avoid it while still taking less obvious harmful actions.Fully solving this problem seems very difficult, but we believe the above approaches have the potential to ameliorate it, and might be scaled up or combined to yield more robust solutions. Given the predominantly theoretical focus on this problem to date, designing experiments that could induce the problem and test solutions might improve the relevance and clarity of this topic.Potential Experiments: A possible promising avenue of approach would be more realistic versions of the \"delusion box\" environment described by Scalable OversightConsider an autonomous agent performing some complex task, such as cleaning an office in the case of our recurring robot example. We may want the agent to maximize a complex objective like \"if the user spent a few hours looking at the result in detail, how happy would they be with the agent's performance?\" But we don't have enough time to provide such oversight for every training example; in order to actually train the agent, we need to rely on cheaper approximations, like \"does the user seem happy when they see the office?\" or \"is there any visible dirt on the floor?\" These cheaper signals can be efficiently evaluated during training, but they don't perfectly track what we care about. This divergence exacerbates problems like unintended side effects (which may be appropriately penalized by the complex objective but omitted from the cheap approximation) and reward hacking (which thorough oversight might recognize as undesirable). We may be able to ameliorate such problems by finding more efficient ways to exploit our limited oversight budget-for example by combining limited calls to the true objective function with frequent calls to an imperfect proxy that we are given or can learn.One framework for thinking about this problem is semi-supervised reinforcement learning, 3 which resembles ordinary reinforcement learning except that the agent can only see its reward on a small fraction of the timesteps or episodes. The agent's performance is still evaluated based on reward from all episodes but it must optimize this based only on the limited reward samples it sees.The active learning setting seems most interesting; in this setting the agent can request to see the reward on whatever episodes or timesteps would be most useful for learning, and the goal is to be economical both with number of feedback requests and total training time. We can also consider a random setting, where the reward is visible on a random subset of the timesteps or episodes, as well as intermediate possibilities.We can define a baseline performance by simply ignoring the unlabeled episodes and applying an ordinary RL algorithm to the labelled episodes. This will generally result in very slow learning. The challenge is to make use of the unlabelled episodes to accelerate learning, ideally learning almost as quickly and robustly as if all episodes had been labeled.An important subtask of semi-supervised RL is identifying proxies which predict the reward, and learning the conditions under which those proxies are valid. For example, if a cleaning robot's real reward is given by a detailed human evaluation, then it could learn that asking the human \"is the room clean?\" can provide a very useful approximation to the reward function, and it could eventually learn that checking for visible dirt is an even cheaper but still-useful approximation. This could allow it to learn a good cleaning policy using an extremely small number of detailed evaluations.More broadly, use of semi-supervised RL with a reliable but sparse true approval metric may incentivize communication and transparency by the agent, since the agent will want to get as much cheap proxy feedback as it possibly can about whether its decisions will ultimately be given high reward. For example, hiding a mess under the rug simply breaks the correspondence between the user's reaction and the real reward signal, and so would be avoided.We can imagine many possible approaches to semi-supervised RL. For example:\u2022 Supervised Reward Learning: Train a model to predict the reward from the state on either a per-timestep or per-episode basis, and use it to estimate the payoff of unlabelled episodes, with some appropriate weighting or uncertainty estimate to account for lower confidence in estimated vs known reward. \u2022 Semi-supervised or Active Reward Learning: Combine the above with traditional semisupervised or active learning, to more quickly learn the reward estimator. For example, the agent could learn to identify \"salient\" events in the environment, and request to see the reward associated with these events.\u2022 Unsupervised Value Iteration: Use the observed transitions of the unlabeled episodes to make more accurate Bellman updates.\u2022 Unsupervised Model Learning: If using model-based RL, use the observed transitions of the unlabeled episodes to improve the quality of the model.As a toy example, a semi-supervised RL agent should be able to learn to play Atari games using a small number of direct reward signals, relying almost entirely on the visual display of the score. This simple example can be extended to capture other safety issues: for example, the agent might have the ability to modify the displayed score without modifying the real score, or the agent may need to take some special action (such as pausing the game) in order to see its score, or the agent may need to learn a sequence of increasingly rough-and-ready approximations (for example learning that certain sounds are associated with positive rewards and other sounds with negative rewards). Or, even without the visual display of the score, the agent might be able to learn to play from only a handful of explicit reward requests (\"how many points did I get on the frame where that enemy ship blew up? How about the bigger enemy ship?\")An effective approach to semi-supervised RL might be a strong first step towards providing scalable oversight and mitigating other AI safety problems. It would also likely be useful for reinforcement learning, independent of its relevance to safety.There are other possible approaches to scalable oversight:\u2022 Distant supervision. Rather than providing evaluations of some small fraction of a system's decisions, we could provide some useful information about the system's decisions in the aggregate or some noisy hints about the correct evaluations There has been some work in this direction within the area of semi-supervised or weakly supervised learning. For instance, generalized expectation criteria \u2022 Hierarchical reinforcement learning. Hierarchical reinforcement learning The top-level agent in hierarchical RL may be able to learn from very sparse rewards, since it does not need to learn how to implement the details of its policy; meanwhile, the sub-agents will receive a dense reward signal even if the top-level reward is very sparse, since they are optimizing synthetic reward signals defined by higher-level agents. So a successful approach to hierarchical RL might naturally facilitate scalable oversight. 4   Hierarchical RL seems a particularly promising approach to oversight, especially given the potential promise of combining ideas from hierarchical RL with neural network function approximators Potential Experiments: An extremely simple experiment would be to try semi-supervised RL in some basic control environments, such as cartpole balance or pendulum swing-up. If the reward is provided only on a random 10% of episodes, can we still learn nearly as quickly as if it were provided every episode? In such tasks the reward structure is very simple so success should be quite likely. A next step would be to try the same on Atari games. Here the active learning case could be quite interesting-perhaps it is possible to infer the reward structure from just a few carefully requested samples (for example, frames where enemy ships are blowing up in Space Invaders), and thus learn to play the games in an almost totally unsupervised fashion. The next step after this might be to try a task with much more complex reward structure, either simulated or (preferably) real-world. If learning was sufficiently data-efficient, then these rewards could be provided directly by a human. Robot locomotion or industrial control tasks might be a natural candidate for such experiments.Safe ExplorationAll autonomous learning agents need to sometimes engage in exploration-taking actions that don't seem ideal given current information, but which help the agent learn about its environment. However, exploration can be dangerous, since it involves taking actions whose consequences the agent doesn't understand well. In toy environments, like an Atari video game, there's a limit to how bad these consequences can be-maybe the agent loses some score, or runs into an enemy and suffers some damage. But the real world can be much less forgiving. Badly chosen actions may destroy the agent or trap it in states it can't get out of. Robot helicopters may run into the ground or damage property; industrial control systems could cause serious issues. Common exploration policies such as epsilongreedy In practice, real world RL projects can often avoid these issues by simply hard-coding an avoidance of catastrophic behaviors. For instance, an RL-based robot helicopter might be programmed to override its policy with a hard-coded collision avoidance sequence (such as spinning its propellers to gain altitude) whenever it's too close to the ground. This approach works well when there are only a few things that could go wrong, and the designers know all of them ahead of time. But as agents become more autonomous and act in more complex domains, it may become harder and harder to anticipate every possible catastrophic failure. The space of failure modes for an agent running a power grid or a search-and-rescue operation could be quite large. Hard-coding against every possible failure is unlikely to be feasible in these cases, so a more principled approach to preventing harmful exploration seems essential. Even in simple cases like the robot helicopter, a principled approach would simplify system design and reduce the need for domain-specific engineering.There is a sizable literature on such safe exploration-it is arguably the most studied of the problems we discuss in this document. \u2022 Risk-Sensitive Performance Criteria: A body of existing literature considers changing the optimization criteria from expected total reward to other objectives that are better at preventing rare, catastrophic events; see \u2022 Use Demonstrations: Exploration is necessary to ensure that the agent finds the states that are necessary for near-optimal performance. We may be able to avoid the need for exploration altogether if we instead use inverse RL or apprenticeship learning, where the learning algorithm is provided with expert trajectories of near-optimal behavior \u2022 Simulated Exploration: The more we can do our exploration in simulated environments instead of the real world, the less opportunity there is for catastrophe. It will probably always be necessary to do some real-world exploration, since many complex situations cannot be perfectly captured by a simulator, but it might be possible to learn about danger in simulation and then adopt a more conservative \"safe exploration\" policy when acting in the real world. Training RL agents (particularly robots) in simulated environments is already quite common, so advances in \"exploration-focused simulation\" could be easily incorporated into current workflows. In systems that involve a continual cycle of learning and deployment, there may be interesting research problems associated with how to safely incrementally update policies given simulation-based trajectories that imperfectly represent the consequences of those policies as well as reliably accurate off-policy trajectories (e.g. \"semi-on-policy\" evaluation).\u2022 Bounded Exploration: If we know that a certain portion of state space is safe, and that even the worst action within it can be recovered from or bounded in harm, we can allow the agent to run freely within those bounds. For example, a quadcopter sufficiently far from the ground might be able to explore safely, since even if something goes wrong there will be ample time for a human or another policy to rescue it. Better yet, if we have a model, we can extrapolate forward and ask whether an action will take us outside the safe state space. Safety can be defined as remaining within an ergodic region of the state space such that actions are reversible \u2022 Trusted Policy Oversight: If we have a trusted policy and a model of the environment, we can limit exploration to actions the trusted policy believes we can recover from. It's fine to dive towards the ground, as long as we know we can pull out of the dive in time.\u2022 Human Oversight: Another possibility is to check potentially unsafe actions with a human. Unfortunately, this problem runs into the scalable oversight problem: the agent may need to make too many exploratory actions for human oversight to be practical, or may need to make them too fast for humans to judge them. A key challenge to making this work is having the agent be a good judge of which exploratory actions are genuinely risky, versus which are safe actions it can unilaterally take; another challenge is finding appropriately safe actions to take while waiting for the oversight.Potential Experiments: It might be helpful to have a suite of toy environments where unwary agents can fall prey to harmful exploration, but there is enough pattern to the possible catastrophes that clever agents can predict and avoid them. To some extent this feature already exists in autonomous helicopter competitions and Mars rover simulations Robustness to Distributional ChangeAll of us occasionally find ourselves in situations that our previous experience has not adequately prepared us to deal with-for instance, flying an airplane, traveling to a country whose culture is very different from ours, or taking care of children for the first time. Such situations are inherently difficult to handle and inevitably lead to some missteps. However, a key (and often rare) skill in dealing with such situations is to recognize our own ignorance, rather than simply assuming that the heuristics and intuitions we've developed for other situations will carry over perfectly. Machine learning systems also have this problem-a speech system trained on clean speech will perform very poorly on noisy speech, yet often be highly confident in its erroneous classifications (some of the authors have personally observed this in training automatic speech recognition systems). In the case of our cleaning robot, harsh cleaning materials that it has found useful in cleaning factory floors could cause a lot of harm if used to clean an office. Or, an office might contain pets that the robot, never having seen before, attempts to wash with soap, leading to predictably bad results. In general, when the testing distribution differs from the training distribution, machine learning systems may not only exhibit poor performance, but also wrongly assume that their performance is good.Such errors can be harmful or offensive-a classifier could give the wrong medical diagnosis with such high confidence that the data isn't flagged for human inspection, or a language model could output offensive text that it confidently believes is non-problematic. For autonomous agents acting in the world, there may be even greater potential for something bad to happen-for instance, an autonomous agent might overload a power grid because it incorrectly but confidently perceives that a particular region doesn't have enough power, and concludes that more power is urgently needed and overload is unlikely. More broadly, any agent whose perception or heuristic reasoning processes are not trained on the correct distribution may badly misunderstand its situation, and thus runs the risk of committing harmful actions that it does not realize are harmful. Additionally, safety checks that depend on trained machine learning systems (e.g. \"does my visual system believe this route is clear?\") may fail silently and unpredictably if those systems encounter real-world data that differs sufficiently from their training data. Having a better way to detect such failures, and ultimately having statistical assurances about how often they'll happen, seems critical to building safe and predictable systems.For concreteness, we imagine that a machine learning model is trained on one distribution (call it p 0 ) but deployed on a potentially different test distribution (call it p * ). There are many other ways to formalize this problem (for instance, in an online learning setting with concept drift There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection Well-specified models: covariate shift and marginal likelihood. If we specialize to prediction tasks and let x denote the input and y denote the output (prediction target), then one possibility is to make the covariate shift assumption that p 0 (y|x) = p * (y|x). In this case, assuming that we can model p 0 (x) and p * (x) well, we can perform importance weighting by re-weighting each training example (x, y) by p * (x)/p 0 (x) An alternative to sample re-weighting involves assuming a well-specified model family, in which case there is a single optimal model for predicting under both p 0 and p * . In this case, one need only heed finite-sample variance in the estimated model Turing machines All of the approaches so far rely on the covariate shift assumption, which is very strong and is also untestable; the latter property is particularly problematic from a safety perspective, since it could lead to silent failures in a machine learning system. Another approach, which does not rely on covariate shift, builds a generative model of the distribution. Rather than assuming that p(x) changes while p(y|x) stays the same, we are free to assume other invariants (for instance, that p(y) changes but p(x|y) stays the same, or that certain conditional independencies are preserved). An advantage is that such assumptions are typically more testable than the covariate shift assumption (since they do not only involve the unobserved variable y). A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification -for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified The approaches discussed above all rely relatively strongly on having a well-specified model familyone that contains the true distribution or true concept. This can be problematic in many cases, since nature is often more complicated than our model family is capable of capturing. As noted above, it may be possible to mitigate this with very expressive models, such as kernels, Turing machines, or very large neural networks, but even here there is at least some remaining problem: for example, even if our model family consists of all Turing machines, given any finite amount of data we can only actually learn among Turing machines up to a given description length, and if the Turing machine describing nature exceeds this length, we are back to the mis-specified regime (alternatively, nature might not even be describable by a Turing machine).Partially specified models: method of moments, unsupervised risk estimation, causal identification, and limited-information maximum likelihood. Another approach is to take for granted that constructing a fully well-specified model family is probably infeasible, and to design methods that perform well despite this fact. This leads to the idea of partially specified modelsmodels for which assumptions are made about some aspects of a distribution, but for which we are agnostic or make limited assumptions about other aspects. For a simple example, consider a variant of linear regression where we might assume that y = w * , x + v, where E[v|x] = 0, but we don't make any further assumptions about the distributional form of the noise v. It turns out that this is already enough to identify the parameters w * , and that these parameters will minimize the squared prediction error even if the distribution over x changes. What is interesting about this example is that w * can be identified even with an incomplete (partial) specification of the noise distribution.This insight can be substantially generalized, and is one of the primary motivations for the generalized method of moments in econometrics Returning to machine learning, the method of moments has recently seen a great deal of success for use in the estimation of latent variable models Finally, some recent work in machine learning focuses only on modeling the distribution of errors of a model, which is sufficient for determining whether a model is performing well or poorly. Formally, the goal is to perform unsupervised risk estimation -given a model and unlabeled data from a test distribution, estimate the labeled risk of the model. This formalism, introduced by Training on multiple distributions. One could also train on multiple training distributions in the hope that a model which simultaneously works well on many training distributions will also work well on a novel test distribution. One of the authors has found this to be the case, for instance, in the context of automated speech recognition systems How to respond when out-of-distribution. The approaches described above focus on detecting when a model is unlikely to make good predictions on a new distribution. An important related question is what to do once the detection occurs. One natural approach would be to ask humans for information, though in the context of complex structured output tasks it may be unclear a priori what question to ask, and in time-critical situations asking for information may not be an option.For the former challenge, there has been some recent promising work on pinpointing aspects of a structure that a model is uncertain about Beyond the structured output setting, for agents that can act in an environment (such as RL agents), information about the reliability of percepts in uncertain situations seems to have great potential value. In sufficiently rich environments, these agents may have the option to gather information that clarifies the percept (e.g. if in a noisy environment, move closer to the speaker), engage in lowstakes experimentation when uncertainty is high (e.g. try a potentially dangerous chemical reaction in a controlled environment), or seek experiences that are likely to help expose the perception system to the relevant distribution (e.g. practice listening to accented speech). Humans utilize such information routinely, but to our knowledge current RL techniques make little effort to do so, perhaps because popular RL environments are typically not rich enough to require such subtle management of uncertainty. Properly responding to out-of-distribution information thus seems to the authors like an exciting and (as far as we are aware) mostly unexplored challenge for next generation RL systems.A unifying view: counterfactual reasoning and machine learning with contracts. Some of the authors have found two viewpoints to be particularly helpful when thinking about problems related to out-of-distribution prediction. The first is counterfactual reasoning The second perspective is machine learning with contracts -in this perspective, one would like to construct machine learning systems that satisfy a well-defined contract on their behavior in analogy with the design of software systems Summary. There are a variety of approaches to building machine learning systems that robustly perform well when deployed on novel test distributions. One family of approaches is based on assuming a well-specified model; in this case, the primary obstacles are the difficulty of building well-specified models in practice, an incomplete picture of how to maintain uncertainty on novel distributions in the presence of finite training data, and the difficulty of detecting when a model is mis-specified. Another family of approaches only assumes a partially specified model; this approach is potentially promising, but it currently suffers from a lack of development in the context of machine learning, since most of the historical development has been by the field of econometrics; there is also a question of whether partially specified models are fundamentally constrained to simple situations and/or conservative predictions, or whether they can meaningfully scale to the complex situations demanded by modern machine learning applications. Finally, one could try to train on multiple training distributions in the hope that a model which simultaneously works well on many training distributions will also work well on a novel test distribution; for this approach it seems particularly important to stress-test the learned model with distributions that are substantially different from any in the set of training distributions. In addition, it is probably still important to be able to predict when inputs are too novel to admit good predictions.Potential Experiments: Speech systems frequently exhibit poor calibration when they go out-ofdistribution, so a speech system that \"knows when it is uncertain\" could be one possible demonstration project. To be specific, the challenge could be: train a state-of-the-art speech system on a standard dataset Related EffortsAs mentioned in the introduction, several other communities have thought broadly about the safety of AI systems, both within and outside of the machine learning community. Work within the machine learning community on accidents in particular was discussed in detail above, but here we very briefly highlight a few other communities doing work that is broadly related to the topic of AI safety.\u2022 Cyber-Physical Systems Community: An existing community of researchers studies the security and safety of systems that interact with the physical world. Illustrative of this work is an impressive and successful effort to formally verify the entire federal aircraft collision avoidance system \u2022 Futurist Community: A cross-disciplinary group of academics and non-profits has raised concern about the long term implications of AI \u2022 Other Calls for Work on Safety: There have been other public documents within the research community pointing out the importance of work on AI safety. A 2015 Open Letter \u2022 Related Problems in Safety: A number of researchers in machine learning and other fields have begun to think about the social impacts of AI technologies. Aside from work directly on accidents (which we reviewed in the main document), there is also substantial work on other topics, many of which are closely related to or overlap with the issue of accidents. A thorough overview of all of this work is beyond the scope of this document, but we briefly list a few emerging themes:\u2022 Privacy: How can we ensure privacy when applying machine learning to sensitive data sources such as medical data? Conclusion", "conclusions": "This paper analyzed the problem of accidents in machine learning systems and particularly reinforcement learning agents, where an accident is defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We presented five possible research problems related to accident risk and for each we discussed possible approaches that are highly amenable to concrete experimental work.With the realistic possibility of machine learning-based systems controlling industrial processes, health-related systems, and other mission-critical technology, small-scale accidents seem like a very concrete threat, and are critical to prevent both intrinsically and because such accidents could cause a justified loss of trust in automated systems. The risk of larger accidents is more difficult to gauge, but we believe it is worthwhile and prudent to develop a principled and forward-looking approach to safety that continues to remain relevant as autonomous systems become more powerful. While many current-day safety problems can and have been handled with ad hoc fixes or case-by-case rules, we believe that the increasing trend towards end-to-end, fully autonomous systems points towards the need for a unified approach to prevent these systems from causing unintended harm.", "SDG": [12]}, "opportunities_of_sustainable_manufacturing_in_indu": {"name": "Opportunities of Sustainable Manufacturing in Industry 4.0", "abstract": " The current globalization is faced by the challenge to meet the continuously growing worldwide demand for capital and consumer goods by simultaneously ensuring a sustainable evolvement of human existence in its social, environmental and economic dimensions. In order to cope with this challenge, industrial value creation must be geared towards sustainability. Currently, the industrial value creation in the early industrialized countries is shaped by the development towards the fourth stage of industrialization, the so-called Industry 4.0. This development provides immense opportunities for the realization of sustainable manufacturing. This paper will present a state of the art review of Industry 4.0 based on recent developments in research and practice. Subsequently, an overview of different opportunities for sustainable manufacturing in Industry 4.0 will be presented. A use case for the retrofitting of manufacturing equipment as a specific opportunity for sustainable manufacturing in Industry 4.0 will be exemplarily outlined.", "keywords": "Sustainable development,Factory,Industry 4.0", "introduction": "The industrial value creation in the early industrialized countries is currently shaped by the development towards the fourth stage of industrialization, the so-called Industry 4.0. This development follows the third industrial revolution which started in the early 1970s and was based on electronics and information technologies for realizing a high level of automation in manufacturing .[1]The development towards Industry 4.0 has presently a substantial influence on the manufacturing industry. It is based on the establishment of smart factories, smart products and smart services embedded in an internet of things and of services also called industrial internet . Additionally, new and disruptive business models are evolving around these Industry 4.0 elements [2][1,. This development towards an Industry 4.0 provides immense opportunities for realizing sustainable manufacturing using the ubiquitous information and communication technology (ICT) infrastructure. This paper will present a state of the art review of Industry 4.0 based on recent research and practice. Wherein, the macro and micro perspectives of Industry 4.0 will be visualized and analyzed. Subsequently, approaches to sustainable manufacturing are combined with the requirements of Industry 4.0 and an overview of opportunities for sustainable manufacturing in the macro and micro perspectives will be presented. Finally, a use case for retrofitting of equipment as a specific opportunity for sustainable manufacturing in Industry 4.0 will be exemplarily outlined.3]The paradigm of Industry 4.0 is essentially outlined by three dimensions [3,7,: 8] horizontal integration across the entire value creation network, (2) end-to-end engineering across the entire product life cycle, as well as (3) vertical integration and networked manufacturing systems.(1)The horizontal integration across the entire value creation network describes the cross-company and company-internal intelligent cross-linking and digitalization of value creation modules throughout the value chain of a product life cycle and between value chains of adjoining product life cycles .[7]The end-to-end engineering across the entire product life cycle describes the intelligent cross-linking and digitalization throughout all phases of a product life cycle: from the raw material acquisition to manufacturing system, product use, and the product end of life .[7]Vertical integration and networked manufacturing systems describes the intelligent cross-linking and digitalization within the different aggregation and hierarchical levels of a value creation module from manufacturing stations via manufacturing cells, lines and factories, also integrating the associated value chain activities such as marketing and sales or technology development .[7]The intelligent cross-linking and digitalization covers the application of an end-to-end solution using information and communication technologies which are embedded in a cloud.In a manufacturing system, the intelligent cross-linking is realized by the application of so-called Cyber-Physical Systems (CPS) which are operating in a self-organized and decentralized manner [7,9,. They are based on embedded mechatronic components i.e., applied sensor systems for collecting data as well as actuator systems for influencing physical processes 10]. CPS are intelligently linked with each other and are continuously interchanging data via virtual networks such as a cloud in real-time. The cloud itself is implemented in the internet of things and services [9]. Being part of a sociotechnical system, CPS are using humanmachine-interfaces for interacting with the operators [7].[11]", "body": "The industrial value creation in the early industrialized countries is currently shaped by the development towards the fourth stage of industrialization, the so-called Industry 4.0. This development follows the third industrial revolution which started in the early 1970s and was based on electronics and information technologies for realizing a high level of automation in manufacturing The development towards Industry 4.0 has presently a substantial influence on the manufacturing industry. It is based on the establishment of smart factories, smart products and smart services embedded in an internet of things and of services also called industrial internet The paradigm of Industry 4.0 is essentially outlined by three dimensions The horizontal integration across the entire value creation network describes the cross-company and company-internal intelligent cross-linking and digitalization of value creation modules throughout the value chain of a product life cycle and between value chains of adjoining product life cycles The end-to-end engineering across the entire product life cycle describes the intelligent cross-linking and digitalization throughout all phases of a product life cycle: from the raw material acquisition to manufacturing system, product use, and the product end of life Vertical integration and networked manufacturing systems describes the intelligent cross-linking and digitalization within the different aggregation and hierarchical levels of a value creation module from manufacturing stations via manufacturing cells, lines and factories, also integrating the associated value chain activities such as marketing and sales or technology development The intelligent cross-linking and digitalization covers the application of an end-to-end solution using information and communication technologies which are embedded in a cloud.In a manufacturing system, the intelligent cross-linking is realized by the application of so-called Cyber-Physical Systems (CPS) which are operating in a self-organized and decentralized manner The Macro Perspective of Industry 4.0The macro perspective of Industry 4.0 as shown in Figure The horizontal integration from the macro perspective is characterized by a network of value creation modules. Value creation modules are defined as the interplay of different value creation factors i.e., equipment, human, organization, process and product Displayed in Figure Smart data arises by expediently structuring information from big data which then can be used for knowledge advances and decision making throughout the product life cycle The smart product holds the information about its requirements for the manufacturing processes and manufacturing equipment. Smart logistics are using CPS for supporting the material flow within the factory and between factories, customers, and other stakeholders. They are also being controlled in a decentralized manner according to the requirements of the product. A smart grid dynamically matches the energy generation of suppliers using renewable energies with the energy demand of consumers, e.g. smart factories or smart homes, by using short-term energy storages for buffering. Within a smart grid, energy consumers and suppliers can be the same.The Micro Perspective of Industry 4.0The micro perspective of Industry 4.0 presented in Figure The horizontal integration from the micro perspective is characterized by the cross-linked value creation modules along the material flow of the smart factory also integrating the smart logistics. The in-and outbound logistics from and to the factories as part of the smart logistic will be characterized by transport equipment that is able to agilely react to unforeseen events such as a change in traffic or weather and which is able to autonomously operate between the starting point and the destination. Autonomously operating transport equipment such as Automated Guided Vehicles (AGVs) will be used for realizing the in-house transport along the material flow. All transport equipment is interchanging smart data with the value creation modules in order to realize a decentralized coordination of supplies and products with the transport systems. For this purpose, the supplies and products contain identification systems, e.g. RFID chips or QR codes. This enables a wireless identification and localization of all materials in the value chain.Vertical integration and networked manufacturing systems from the micro perspective describes the intelligent crosslinking of the value creation factors: product, equipment and human, along the different aggregation levels of the value creation modules from manufacturing stations via manufacturing cells and manufacturing lines up to the smart factory. This networking throughout the different aggregation levels also includes the cross-linking of the value creation modules with the different value chain activities, e.g. marketing and sales, service, procurement, etc. The value creation module in a factory corresponds to an embedded Cyber-Physical-System. The manufacturing equipment, e.g. machine tools or assembly tools, are using sensor systems for identifying and localizing the value creation factors, such as the products or the humans, as well as for monitoring the manufacturing processes, e.g. the cutting, assembly, or transport processes. Depending on the monitored smart data, the applied actuators in the manufacturing equipment can react in real-time on specific changes of the product, humans or processes. The communication and exchange of the smart data between the value creation factors, between the value creation module and the transport equipment, as well as between the different levels of aggregation and the different value chain activities is being executed via the cloud.Table Table 1. Trends and expected developments for the value creation factorsEquipmentThe manufacturing equipment will be characterized by the application of highly automated machine tools and robots. The equipment will be able to flexibly adapt to changes in the other value creation factors, e.g. the robots will be working together collaboratively with the workers on joint tasks HumanThe current jobs in manufacturing are facing a high risk for being automated to a large extent OrganizationThe increasing organizational complexity in the manufacturing system cannot be managed by a central instance from a certain point on. Decision making will thus be shifted away from a central instance towards decentralized instances. The decentralized instances will autonomously consider local information for the decisionmaking ProcessAdditive manufacturing technologies also known as 3D printing will be increasingly deployed in value creation processes, since the costs of additive manufacturing have been rapidly dropping during the last years by simultaneously increasing in terms of speed and precision ProductThe products will be manufactured in batch size one according to the individual requirements of the customer Sustainability in Industry 4.0A paradigm Industry 4.0 will be a step forward towards more sustainable industrial value creation. In current literature, this step is mainly characterized as contribution to the environmental dimension of sustainability. The allocation of resources, i.e. products, materials, energy and water, can be realized in a more efficient way on the basis of intelligent cross-linked value creation modules Besides these environmental contributions, Industry 4.0 holds a great opportunity for realizing sustainable industrial value creation on all three sustainability dimensions: economic, social and environmental. Table Business ModelsIn Industry 4.0, new evolving business models are highly driven by the use of smart data for offering new services. This development has to be exploited for anchoring new sustainable business models. Sustainable business models significantly create positive or reduce negative impacts for the environment or society Additionally, sustainable business models are necessarily characterized by competitiveness on the long-run Value Creation NetworksThe cross-linking of value creation networks in Industry 4.0 offers new opportunities for realizing closed-loop product life cycles and industrial symbiosis. It allows the efficient coordination of the product, material, energy, and water flows throughout the product life cycles as well as between different factories. Closed-loop product life-cycles help keep products in life cycles of multiple use phases with remanufacturing or reuse in between. Industrial symbiosis describes the (cross-company) cooperation of different factories for realizing a competitive advantage by trading and exchanging products, materials, energy, water Table 3. Opportunities of sustainable manufacturing for the micro perspectiveEquipmentThe manufacturing equipment in factories often is a capital good with a long use phase of up to 20 or more years. Retrofitting enables an easy and cost-efficient way of upgrading existing manufacturing equipment with sensor and actuator systems as well as with the related control logics in order to overcome the heterogeneity of equipment in factories HumanHumans will still be the organizers of value creation in Industry 4.0 Three different sustainable approaches can be used for coping with the social challenge in Industry 4.0. ( OrganizationA sustainable-oriented decentralized organization in a smart factory focuses on the efficient allocation of products, materials, energy and water by taking into account the dynamic constraints of the CPS, e.g. of the smart logistics, the smart grid, the self-sufficient supply or the customer. This concept towards a holistic resource efficiency is being described as one of the essential advantages of Industry 4.0 ProcessThe sustainable design of processes addresses the holistic resource efficiency approach of Industry 4.0 by designing appropriate manufacturing process chains ProductThe approach for the sustainable design of products in Industry 4.0 focuses on the realization of closed-loop life cycles for products by enabling the reuse and remanufacturing of the specific product or by applying cradle-to-cradle principles. Different approaches also focus on designing for the well-being of the consumer. These concepts can be supported by the application of identification systems, e.g. for recovering the cores for remanufacturing, or by applying new additional services to the product for achieving a higher level of wellbeing for the customer Retrofitting Use CaseThe objective of this use case has been the development of a retrofitting solution for a desktop machine tool within the laboratory of sustainable manufacturing of the Collaborative Research Centre 1026 at TU Berlin. The method for developing the retrofitting solution covers four sequential steps: (1) situation analysis, (2) definition of the monitoring strategy, (3) data processing and (4) implementation of the equipment in a CPS.The situation analysis includes the definition of the list of requirements. In this case, the retrofitting solution is supposed to monitor the existing operational states of the equipment: shut on/off, idling, processing and fault. It also should be easy to install as well as cost effective.Additionally, the situation analysis focuses on the selection of the sensor system according to the list of requirements.In terms of the use case, an acceleration sensor has met the requirements appropriate.The definition of the monitoring strategy contains the definition of the measuring parameters, the definition of the monitoring position and orientation of the sensor, the application of the sensor as well as the execution of the measurement. For the use case, a Beckhoff PLC has transformed the analog signals of the acceleration sensor into digital signals for the subsequent data processing.The data processing evaluates the input data according to a predefined logic in order to identify the different operational states. The visualization of the data has been realized by a Human-Machine-Interface, which displays the current operational state as well as the measured vibration profile of the machine tool. Figure This milling machine can now be implemented in a CPS. In connection with a smart product the retrofitted machine can decentrally schedule the material flow and is furthermore able to automatically react to any machine failures by e.g. informing the responsible worker.Summary and conclusion", "conclusions": "In this paper a state of the art review for the current industrial development know as Industry 4.0 has been presented. In order to give a comprehensive understanding of this development, the micro and macro perspective of Industry 4.0 have been described based on the current findings in research and practice. Subsequently, different opportunities for realizing a sustainable manufacturing in Industry 4.0 have been presented for the macro as well as for the micro perspectives. These opportunities are combining current research approaches in the field of sustainable manufacturing with the future requirements of Industry 4.0. Finally, a use case for retrofitting of a machine tool as a specific opportunity for sustainable manufacturing in Industry 4.0 has been outlined. ", "SDG": [12]}, "adaptive_and_intelligent_web_based_educational_systems": {"name": "Adaptive and Intelligent Web-based Educational Systems", "abstract": " Adaptive and intelligent Web-based educational systems (AIWBES) provide an alternative to the traditional \"just-put-it-on-the-Web\" approach in the development of Web-based educational courseware ", "keywords": "", "introduction": "", "body": "Intelligent ES Adaptive ESFig. 1. Relationship between adaptive and intelligent educational systemsExisting AIWBES are very diverse. They offer various kinds of support for both students and teachers involved in the process of Web-enhanced education. To help in understanding this variety of systems and ideas, the author's earlier review of adaptive hypermedia An earlier review In this introductory article we follow the review The goal of interactive problem solving support is to provide the student with intelligent help on each step of problem solving -from giving a hint to executing the next step for the student. Interactive problem solving support technology is not as popular in Web-based systems as in standalone intelligent tutoring systems -mainly due to implementation problems. As was demonstrated by pioneer systems, pure server-side implementations such as PAT-Online The proper functionality and level of complexity to implement interactive problem solving support require client-server implementation such as AlgeBrain MetaLinks The goal of adaptive navigation support technology is to assist the student in hyperspace orientation and navigation by changing the appearance of visible links. For example, an adaptive hypermedia system can adaptively sort, annotate, or partly hide the links of the current page to make it easier to choose where to go next. Adaptive navigation support shares the same goal with curriculum sequencing -to help students find an \"optimal path\" through the learning material. At the same time, adaptive navigation support is less directive and more \"cooperative\" than traditional sequencing: it guides students while leaving them the choice of the next knowledge item to be learned and next problem to be solved. In the WWW context where hypermedia is a basic organizational paradigm, adaptive navigation support becomes both natural and efficient. It was among the three earliest AIWBES technologies, explored in such systems as ELM-ART Adaptive information filtering (AIF) is a classic technology from the field of Information Retrieval. Its goal is finding a few items that are relevant to user interests in a large pool of (textbased) documents. On the Web this technology has been used in both search and browsing context. It has been applied to adapt the results of Web search using filtering and ordering and to recommend the most relevant documents in the pool using link generation. While the engines used by AIF systems are very different from adaptive hypermedia engines, at the interface level Web-based AIF most often use adaptive navigation support techniques. There are two essentially different kinds of AIF engines that can be considered as two different AIF technologiescontent-based filtering and collaborative filtering. The former relies on document content while the latter ignores the content completely attempting instead to match the users who are interested in the same documents. Modern AIF extensively uses machine learning techniques, especially for content-based filtering. Being very popular in the field of information systems, AIF has not been used in educational context in the past. The amount of learning content was relatively small and the need to guide the user to most relevant material was well supported by adaptive sequencing and adaptive hypermedia. However, the Web with its abundance of non-indexed \"open corpus\" educational resources has made AIF very attractive for educationalists. MLTutor Intelligent collaborative learning is an interesting group of technologies developed at the crossroads of two fields originally quite distant from each other: computer supported collaborative learning (CSCL) and ITS. The recent stream of work on using AI techniques to support collaborative learning has resulted in an increasing level of interaction between these fields. While early work on intelligent collaborative learning was performed in pre-Web context Technologies for adaptive group formation and peer help attempt to use knowledge about collaborating peers (most often represented in their student models) to form a matching group for different kinds of collaborative tasks. Early examples include forming a group for collaborative problem solving Technologies for adaptive collaboration support attempt to provide an interactive support of a collaboration process just like interactive problem support systems assist an individual student in solving a problem. Using some knowledge about good and bad collaboration patterns (provided by the system authors or mined from communication logs) collaboration support systems such as COLER In contrast, virtual students technology is comparatively old. Instead of supporting learning or collaboration from a position of someone superior to students (a teacher or an advisor), this technology attempts to introduce different kinds of virtual peers into a learning environment: a learning companion Intelligent class monitoring is another AIWBES technology motivated by WBE. In the WBE context a \"remote teacher\" can't see the signs of understanding and confusion on the faces of the students. With this severe lack of feedback it becomes hard to identify troubled students who need additional attention, bright students who need to be challenged, as well as the parts of learning material that are too easy, too hard, or confusing. WBE systems can track every action of the student, but it's almost impossible for a human teacher to make any sense of the large volume of data they are collecting. Intelligent class monitoring systems attempt to use AI to help the teacher in this context. This stream of work was pioneered by HyperClassroom ADAPTIVE AND INTELLIGENT WEB-BASED EDUCATIONAL SYSTEMS: A CHANGE OF AI-ED PARADIGM?The analysis of adaptive and intelligent Web-based educational systems on the level of technologies reveals that they have a lot in common with pre-Web systems. Should we consider AIWBES simply as Web implementation of ideas explored earlier? Can we say that the only difference between Web and pre-Web adaptive and intelligent educational systems is the implementation platform? We claim that the difference between Intelligent Tutoring Systems of 1980 and 1990 and the new breed of Web-based systems that became popular at the end of 1990 is qualitative. While on the level of individual technologies we can easily see the similarity between Web and pre-Web systems, on the level of complete systems we can observe rather large differences in the major focus of these systems, their application context, and the overall set of supported features. The new platform and the new application context of Web-based systems are causing a major change of the development paradigm. Adaptive and intelligent Web-based educational systems are forming a new development paradigm in the field of Artificial Intelligent in Education.If we analyze the variety of adaptive and intelligent educational systems developed since the birth of the AI-Ed field in 1970, we can distinguish at least three major development paradigms (Table At the end of 1970 the new \"tutoring\" paradigm was established Personal computers WWWWhat we observe right now is a new change of the paradigm also driven to some extent by the change of the platform and the application context. The motivating application context behind Web-based educational (WBE) systems is, naturally, Web-based education. In this context with no human teacher, tutor, or even peer nearby, the educational system has to provide a one-stop solution for all student's needs. The old CAI motivation to \"deliver knowledge\" came back into focus and even became dominant (though the new generation of WBE systems choose to deliver the necessary educational material using flexible hypertext rather than rigid CAI). This is well demonstrated by the subset of systems presented in this special issue -the majority of them include (or ever focused on) the delivery of on-line course material -with adaptation as in MetaLinks, KBS-Hyperbook, ActiveMath, ELM-ART, MLTutor or without it as in German Tutor. The need to support problem solving remained in focus. New needs specific to modern WBE became critical -such as the need to support collaborative work and the need to support the remote teacher working with the invisible class. This context caused the appearance of new technologies as well as the change in the usage profile of known technologies. Adaptive sequencing became popular again -now together with adaptive hypermedia and adaptive information filtering. Intelligent solution analysis became more attractive than interactive problem solving support due to its natural fit to low-interactive HTTP protocol. Collaborative learning and class monitoring technologies became an interesting new target for the application of AI techniques. More important, the set of the needs supported by a single system as well as the set of technologies used in a single system has grown quite visibly. While almost all pre-Web systems have focused on one specific need championing and extending one of the known technologies, almost all AIWBES use several technologies and become more complete as \"onestop\" educational systems. This trend is clearly demonstrated by ELM-ART system THE PROMISES OF ADAPTIVE AND INTELLIGENT WEB-BASED EDUCATIONAL SYSTEMSAdaptive and intelligent Web-based educational systems form a new and exciting stream of work in AI-Ed field. As demonstrated by the papers included in this special issue, the Web offers an opportunity to apply a much larger variety of AI technologies in educational context. It offers a number of new research challenges and a number of opportunities to fuse AI-Ed research with several neighboring fields. The Web also provides an excellent implementation platform for AI-Ed researchers. Systems developed on the Web have longer lifespan and better visibility. A research idea implemented in a Web-based system has much better chances to influence the research community than an idea simply presented in a paper. Moreover, AIWBES with their simplicity of access and visibility have much greater chances to influence practitioners working in the field of Web-based education. We expect that the ideas developed in these systems and the systems themselves will have a growing use in practical Web-based education. This will allow AI-Ed as a research field to provide a greater impact on the improvement of everyday educational process. As guest editors, we hope that the papers assembled in this special issue provide both a good overview of the emerging area and a good inspiration for the newcomers to the field.Fig. 2 .Table 1Tutor (Mitrovic, 2003) German Tutor (Heift, et al., 2001) ActiveMath (Melis, et al., 2001) ELM-ART (Weber, et al., 2001)", "conclusions": "", "SDG": [13]}, "downscaling_of_precipitation_for_climate_change_scenarios_a_support_vector_machine_approach": {"name": "Downscaling of precipitation for climate change scenarios: A support vector machine approach", "abstract": " The Climate impact studies in hydrology often rely on climate change information at fine spatial resolution. However, general circulation models (GCMs), which are among the most advanced tools for estimating future climate change scenarios, operate on a coarse scale. Therefore the output from a GCM has to be downscaled to obtain the information relevant to hydrologic studies. In this paper, a support vector machine (SVM) approach is proposed for statistical downscaling of precipitation at monthly time scale. The effectiveness of this approach is illustrated through its application to meteorological sub-divisions (MSDs) in India. First, climate variables affecting spatio-temporal variation of precipitation at each MSD in India are identified. Following this, the data pertaining to the identified climate variables (predictors) at each MSD are classified using cluster analysis to form two groups, representing wet and dry seasons. For each MSD, SVM-based downscaling model (DM) is developed for season(s) with significant rainfall using principal components extracted from the predictors as input and the contemporaneous precipitation observed at the MSD as an output. The proposed DM is shown to be superior to conventional downscaling using multi-layer back-propagation artificial neural networks. Subsequently, the SVM-based DM is applied to future climate predictions from the second generation Coupled Global Climate Model (CGCM2) to obtain future projections of precipitation for the MSDs. The results are then analyzed to assess the impact of climate change on precipitation over India. It is shown that SVMs provide a promising alternative to conventional artificial neural networks for statistical downscaling, and are suitable for conducting climate impact studies.", "keywords": "Precipitation,Downscaling,Climate change,General circulation model (GCM),Support vector machine,Neural network,Hydroclimatology,India", "introduction": "Recently, there is growth in scientific evidence that global climate has changed, is changing and will continue to 0022-1694/$ -see front matter c 2006 Elsevier B.V. All rights reserved.   doi:10.1016/j.jhydrol.2006.04.030  change . In this scenario, there is a need to improve our understanding of the global climate system to assess the possible impact of a climate change on hydrological processes.(NRC, 1998)General Circulation Models (GCMs), which describe the atmospheric process by mathematical equations, are the most adapted tools for studying the impact of climate change at regional scale. These climate models have been evolving steadily over the past several decades. Recently fully coupled Atmosphere-Ocean GCMs (AOGCMs), along with transient methods of forcing the concentration of greenhouse gases, have brought considerable improvement in the climate model results.The resolution of the present state-of-the-art GCMs is coarser than 2\u00b0for both latitude and longitude, which is of the order of a few hundred kilometers between gridpoints. In other words, GCM provides output at nodes of grid boxes, which are tens of thousands of square kilometers in size, whereas the scale of interest to hydrologists is of the order of a few hundred square kilometers. In the past decade, to deal with this problem of mismatch of spatial scales several downscaling methodologies have been developed.More recently, downscaling has found wide application in hydroclimatology for scenario construction and simulation/ prediction of (i) regional precipitation ; (ii) low-frequency rainfall events (Kim et al., 2004) (iii) mean, minimum and maximum air temperature (Wilby, 1998); (iv) soil moisture (Kettle and Thompson, 2004)(Georgakakos and Smith, 2001;; (v) runoff Jasper et al., 2004) and streamflows (Arnell et al., 2003); (vi) ground water levels (Cannon and Whitfield, 2002); (vii) transpiration (Bouraoui et al., 1999), wind speed (Misson et al., 2002) and potential evaporation rates (Faucher et al., 1999); (viii) soil erosion and crop yield (Weisse and Oestreicher, 2001); (ix) landslide occurrence (Zhang et al., 2004)(Buma and Dehn, 2000; and (x) water quality Schmidt and Glade, 2003).(Hassan et al., 1998)The approaches, which have been proposed for downscaling GCMs could be broadly classified into two categories: dynamic downscaling and statistical downscaling. In the dynamic downscaling approach a Regional Climate Model (RCM) is embedded into GCM. The RCM is essentially a numerical model in which GCMs are used to fix boundary conditions. The major drawback of RCM, which restricts its use in climate impact studies, is its complicated design and high computational cost. Moreover, RCM is inflexible in the sense that expanding the region or moving to a slightly different region requires redoing the entire experiment .(Crane and Hewitson, 1998)The second approach to downscaling, termed statistical downscaling, involves deriving empirical relationships that transform large-scale features of the GCM (Predictors) to regional-scale variables (Predictands) such as precipitation, temperature and streamflow. There are three implicit assumptions involved in statistical downscaling . Firstly, the predictors are variables of relevance and are realistically modeled by the host GCM. Secondly, the empirical relationship is valid also under altered climatic conditions. Thirdly, the predictors employed fully represent the climate change signal.(Hewitson and Crane, 1996)A diverse range of statistical downscaling methods has been developed in recent past. Among them Artificial Neural Network (ANN) based downscaling techniques have gained wide recognition owing to their ability to capture non-linear relationships between predictors and predictand (e.g., Cavazos, 1997;Crane and Hewitson, 1998;Wilby et al., 1998;Trigo and Palutikof, 1999;Sailor et al., 2000;Snell et al., 2000;Mpelasoka et al., 2001;Schoof and Pryor, 2001;Cannon and Whitfield, 2002;Crane et al., 2002;Olsson et al., 2004;Shivam, 2004;Solecki and Oliveri, 2004;. The concept of ANNs came into being approximately 60 years ago Tatli et al., 2004) inspired by a desire to understand the human brain and emulate its functioning. Mathematically, an ANN is often viewed as a universal approximator. The ability to generalize a relationship from given patterns makes it possible for ANNs to solve large-scale complex problems such as pattern recognition, non-linear modeling and classification. It has been extensively used in a variety of physical science applications, including hydrology (McCulloch and Pitts, 1943); ASCE Task Committee on Artificial Neural Networks in (Govindaraju and Rao, 2000.Hydrology, 2000b)Despite a number of advantages, the traditional neural network models have several drawbacks including possibility of getting trapped in local minima and subjectivity in the choice of model architecture . Recently, (Suykens, 2001)Vapnik (1995 pioneered the development of a novel machine learning algorithm, called support vector machine (SVM), which provides an elegant solution to these problems. The SVM has found wide application in the field of pattern recognition and time series analysis. Readers are referred to Vapnik ( , 1998) )Vapnik (1995, Vapnik ( , 1998))Cortes and, Vapnik (1995), Scho \u00a8lkopf et al. (1998), Cristianini and Shawe-Taylor (2000) and Haykin (2003) for introductory material on SVM.Sastry (2003)The research presented in this paper is motivated by a desire to explore the potential of the SVM in downscaling future climate projections provided by GCMs. The SVM would be ideally suited for the downscaling task owing to its ability to provide good generalization performance in capturing non-linear regression relationships between predictors and predictand, despite the fact that it does not incorporate problem domain knowledge.The following objectives have been set for this paper. Firstly, to investigate the potential of SVM in downscaling GCM simulations by comparing its performance with multilayer back-propagation neural network based downscaling model, and secondly to assess the impact of climate change on hydrological inputs to meteorological sub-divisions in India using simulations from the second generation Coupled Global Climate Model (CGCM2) for IS92a scenario .(IPCC, 1992)The study region has been selected because gaining an understanding of plausible effects of climate change on water resources is of great significance in Indian context owing to its agro-based economy. With the inherent scarcity of water in several parts of India and projected changes in climate for the coming decades, acute water shortages or critical droughts are imminent on Indian sub-continent. The evidence of climatic link with hydrology of Indian subcontinent  necessitates development of effective strategies for regional hydrologic analysis to cope with critical water shortages in future. To progress towards this goal, it is necessary to develop efficient downscaling strategy to interpret climate change signals at regional scale.(IPCC, 2001)While a plethora of statistical downscaling techniques have been used in different parts of the globe (Wilby and Wigley, 1997;, there is a paucity of such studies over Indian sub-continent.Xu, 1999)The remainder of this paper is structured as follows: First, the fundamental principle of SVM and its formulation are presented along with a brief description of multi-layer back-propagation neural network. Following this, details of the study region are provided and the methodology proposed for downscaling of precipitation is presented. Finally, a set of conclusions is drawn following discussion on results obtained from the downscaling models.", "body": "Recently, there is growth in scientific evidence that global climate has changed, is changing and will continue to 0022-1694/$ -see front matter c 2006 Elsevier B.V. All rights reserved.   doi:10.1016/j.jhydrol.2006.04.030  change General Circulation Models (GCMs), which describe the atmospheric process by mathematical equations, are the most adapted tools for studying the impact of climate change at regional scale. These climate models have been evolving steadily over the past several decades. Recently fully coupled Atmosphere-Ocean GCMs (AOGCMs), along with transient methods of forcing the concentration of greenhouse gases, have brought considerable improvement in the climate model results.The resolution of the present state-of-the-art GCMs is coarser than 2\u00b0for both latitude and longitude, which is of the order of a few hundred kilometers between gridpoints. In other words, GCM provides output at nodes of grid boxes, which are tens of thousands of square kilometers in size, whereas the scale of interest to hydrologists is of the order of a few hundred square kilometers. In the past decade, to deal with this problem of mismatch of spatial scales several downscaling methodologies have been developed.More recently, downscaling has found wide application in hydroclimatology for scenario construction and simulation/ prediction of (i) regional precipitation The approaches, which have been proposed for downscaling GCMs could be broadly classified into two categories: dynamic downscaling and statistical downscaling. In the dynamic downscaling approach a Regional Climate Model (RCM) is embedded into GCM. The RCM is essentially a numerical model in which GCMs are used to fix boundary conditions. The major drawback of RCM, which restricts its use in climate impact studies, is its complicated design and high computational cost. Moreover, RCM is inflexible in the sense that expanding the region or moving to a slightly different region requires redoing the entire experiment The second approach to downscaling, termed statistical downscaling, involves deriving empirical relationships that transform large-scale features of the GCM (Predictors) to regional-scale variables (Predictands) such as precipitation, temperature and streamflow. There are three implicit assumptions involved in statistical downscaling A diverse range of statistical downscaling methods has been developed in recent past. Among them Artificial Neural Network (ANN) based downscaling techniques have gained wide recognition owing to their ability to capture non-linear relationships between predictors and predictand (e.g., Despite a number of advantages, the traditional neural network models have several drawbacks including possibility of getting trapped in local minima and subjectivity in the choice of model architecture The research presented in this paper is motivated by a desire to explore the potential of the SVM in downscaling future climate projections provided by GCMs. The SVM would be ideally suited for the downscaling task owing to its ability to provide good generalization performance in capturing non-linear regression relationships between predictors and predictand, despite the fact that it does not incorporate problem domain knowledge.The following objectives have been set for this paper. Firstly, to investigate the potential of SVM in downscaling GCM simulations by comparing its performance with multilayer back-propagation neural network based downscaling model, and secondly to assess the impact of climate change on hydrological inputs to meteorological sub-divisions in India using simulations from the second generation Coupled Global Climate Model (CGCM2) for IS92a scenario The study region has been selected because gaining an understanding of plausible effects of climate change on water resources is of great significance in Indian context owing to its agro-based economy. With the inherent scarcity of water in several parts of India and projected changes in climate for the coming decades, acute water shortages or critical droughts are imminent on Indian sub-continent. The evidence of climatic link with hydrology of Indian subcontinent While a plethora of statistical downscaling techniques have been used in different parts of the globe The remainder of this paper is structured as follows: First, the fundamental principle of SVM and its formulation are presented along with a brief description of multi-layer back-propagation neural network. Following this, details of the study region are provided and the methodology proposed for downscaling of precipitation is presented. Finally, a set of conclusions is drawn following discussion on results obtained from the downscaling models.Support vector machineIn the past few decades, traditional neural networks such as multi-layer back-propagation neural network and radial basis function networks have been extensively used in a wide range of engineering applications including hydrology In this section the theoretical background of SVM is provided following a brief mention of the motivation for its use. Following this, the standard formulation of SVM for regression is presented. Subsequently the Least Square Support Vector Machine (LS-SVM), which has been used in this study, is described.Motivation for the use of support vector machine (SVM)Most of the traditional neural network models seek to minimize the training error by implementing the empirical risk minimization principle, whereas the SVMs implement the structural risk minimization principle which attempts to minimize an upper bound on the generalization error by striking a right balance between the training error and the capacity of machine (i.e., the ability of machine to learn any training set without error). The solution of traditional neural network models may tend to fall into a local optimal solution, whereas global optimum solution is guaranteed for SVM Further, the traditional ANNs have considerable subjectivity in model architecture, whereas for SVMs the learning algorithm automatically decides the model architecture (number of hidden units). Moreover, traditional ANN models do not give much emphasis on generalization performance, while SVMs seek to address this issue in a rigorous theoretical setting.The flexibility of the SVM is provided by the use of kernel functions that implicitly map the data to a higher, possibly infinite, dimensional space. A linear solution in the higher dimensional feature space corresponds to a non-linear solution in the original lower dimensional input space. This makes SVM a feasible choice for solving a variety of problems in hydrology, which are non-linear in nature.Theoretical background of support vector machine Consider a finite training sample of N patterns {(x i , y i ), i = 1,. . . , N}, where x i denotes the ''ith'' pattern in n-dimensional space (i.e., x i \u00bc \u00bdx 1i ; . . . ; x ni 2 R n \u00de and y i (y i 2 {\u00c01,+1}) represents the class label of that pattern. Further, let the learning machine be defined by a set of possible mappings x # f(x; w), where f(AE) is a deterministic function which, for a given input pattern x and adjustable parameters w (w 2 R n ), always gives the same output f(x; w). Training phase of the learning machine involves adjusting the parameters w.Let the training error for the trained machine be denoted by m(w). Then, according to the principle of structural risk minimization In Eq: \u00f03\u00de; e 0 \u00f0N; h; g\u00deThe inequality in Eq. ( P\u00f0w\u00de K m\u00f0w\u00de \u00fe 4e 2 0 \u00f0N; h; g\u00de \u00f0 5\u00deP\u00f0w\u00de K m\u00f0w\u00de \u00fe e 0 \u00f0N; h; g\u00de \u00f0 6\u00deMost of the traditional neural network models, which implement empirical risk minimization principle, seek to minimize only training error, m(w). However, this does not result in good generalization performance because drop in m(w) alone does not guarantee reduction in test error, as evident from Eqs. ( Standard support vector machine for regressionHerein the basic ideas of the SVM for the case of function approximation are reviewed. Consider the finite training sample pattern (x i , y i ), where x i 2 R n is a sample value of the input vector x consisting of N training patterns (i.e., x = [x 1 , . . . , x N ]) and y i 2 R is the corresponding value of the desired model output. A non-linear transformation function /(AE) is defined to map the input space to a higherdimension feature space, R n h (Fig. In Eq. ( where jy i \u00c0 \u0177i j e is the Vapnik's e-insensitive loss function (shown as thick line in Fig. Following regularization theory subjected to the constraintswhere n i and n \u00c3 i are positive slack variables and C is a positive real constant. The first term of the cost function, which represents weight decay (or model complexity-penalty function), is used to regularize weight sizes and to penalize large weights. This helps in improving generalization performance The constrained quadratic optimization problem given in Eq. ( Feature space Input spaceFigure where K(x i , x) is the inner product kernel function defined in accordance with Mercer's theorem There are several possibilities for the choice of kernel function, including linear, polynomial, sigmoid, splines and Radial basis function (RBF). The linear kernel is a special case of RBF where, r is the width of RBF kernel, which can be adjusted to control the expressivity of RBF. The RBF kernels have localized and finite responses across the entire range of predictors.The advantage with RBF kernel is that it nonlinearly maps the training data into a possibly infinitedimensional space, thus it can effectively handle the situations when the relationship between predictors and predictand is non-linear. Moreover, the RBF is computationally simple than polynomial kernel, which has more parameters.Least square support vector machineThe Least Square Support Vector Machine (LS-SVM), which has been used in this study, provides a computational advantage over standard SVM by converting quadratic optimization problem into a system of linear equations subjected to the equality constraintImportant differences with standard SVMs are the equality constraints and the quadratic loss term e 2 i , which greatly simplifies the problem. The quadratic loss function is shown as thick line in Fig. where a i are Lagrange multipliers. The conditions for optimality are given byThe above conditions of optimality can be expressed as the solution to the following set of linear equations after elimination of w and e i . In Eq. ( The resulting LS-SVM model for function estimation is:where a \u00c3 i and b * are the solutions to Eq. ( Figure 3 Architecture of support vector machine.Downscaling of precipitation for climate change scenarios: A support vector machine approachMulti-layer back-propagation neural networkThe Multi-layer back-propagation neural network is chosen as the base ANN model to compare the performance of SVM. Readers are referred to Study regionThe study region India, which is located between 8\u00b04 0 N and 37\u00b06 0 N latitudes and 68\u00b07 0 E and 97\u00b025 0 E longitudes, receives average annual rainfall of 120 cm. It has a tropical monsoon climate where most of the precipitation is confined to a few months of the monsoon season. The southwest (summer) monsoon has warm winds blowing from Indian Ocean over almost entire country and causing copious amount of rainfall during June-September months. About 75% of annual rainfall in India is due to the south-west monsoon, however the spatial distribution of monsoon rainfall shows significant variation across the country. For the remaining part of the year, except for coastal-strip in the south-eastern peninsular India comprising coastal Andhra Pradesh and Tamil Nadu meteorological subdivisions (MSDs), the rest of country receives practically no rainfall. The precipitation in India varies from about 10 cm in western Rajasthan (located in north-western part of the country) to over 900 cm in Meghalaya located in the north-eastern part Inter-Governmental Panel on Climate Change (IPCC, 2001) evaluation of the results of a number of AOGCMs, indicates that the mean monsoon precipitation over India will intensify with increase in carbon-dioxide content. MethodologyDevelopment of ANN/SVM based downscaling model involves identification and screening of climate variables affecting spatio-temporal variation of precipitation at various MSDs in India. The data pertaining to screened climate variables and precipitation for each MSD is used to develop (train and validate) a downscaling model. The validated downscaling model is subsequently applied to projected climate information from GCM to obtain future scenarios of precipitation for the sub-division. This section outlines the procedure involved in collection and processing of data,  Data extractionFor the study region, grid point climate data at monthly time scale, prepared by National Center for Environmental Prediction (NCEP) The 55-year long dataset may be small to delineate interdecadal oscillation of Indian summer monsoon The monthly area weighted rainfall data of 29 MSDs in India gitude with grid box size of 3.75\u00b0and nearly uniform along the latitude (approximately 3.75\u00b0). The variables provided by CGCM2 include air temperature, specific humidity, geopotential height, zonal and meridional wind velocities at various pressure levels and sea level pressure. The CGCM2 data is interpolated to the same 2.5\u00b0\u2022 2.5\u00b0g rid as used by NCEP data. The interpolation is performed with a linear inverse square interpolation procedure using spherical distances Seasonal stratificationSeasonal variation of the atmospheric circulation and precipitation is the distinguishing feature of the monsoon regions of the world, including India. Often the date of onset of monsoon and its duration has been observed to vary from one-year to another in the study region Clustering analysis is widely recognized for recovering natural groups present in the data. Out of various clustering algorithms reported in the literature, K-means clustering algorithm, which is widely recognized for its simplicity and computational efficiency From the N-months long NCEP reanalysis records the climate variables, which represent atmospheric circulation over the study region and are also realistically simulated by GCM, are selected. Following Suppose that the given set of N-feature vectors (in ndimensional space) have been partitioned into K clusters {C 1 , C 2 , . . ., C K } such that cluster C k has N k feature vectors and each feature vector is in exactly one cluster, so thatThe centroid of cluster C k is defined as:In Eq. ( The square-error criterion function for the K-means algorithm is given by Eq. ( The objective of K-means algorithm is to find a partition containing K clusters that minimizes E 2 K for chosen K. The K-means algorithm starts with a given initial partition and keeps reassigning feature vectors to clusters based on similarity between the feature vectors and cluster centers until a convergence criterion is met. One criterion for convergence is to stop the iterations when the change in the value of E 2 K between two successive iterations becomes sufficiently small. An alternative is to terminate the algorithm when there is no reassignment of any feature vector from one cluster to another during successive iterations.A major problem with the K-means algorithm is that the final partition provided by it is sensitive to the selection of the initial partition and may result in the convergence to local minima of the criterion function (Eq. ( Each feature vector (representing a month) of the NCEP data is assigned a label that denotes the cluster (season) to which it belongs. Following this, GCM simulations (past and future) are labeled using nearest neighbor rule. Selecting predictorsIn literature, several attempts have been made to develop a plethora of empirical downscaling techniques using a wide variety of predictors. The choice of predictors could vary from one region to another. Since there are no general guidelines for selection of predictors in different parts of the world, a comprehensive search of predictors is necessary. Following the suggestions of Effective alternatives to linear cross-correlation measure for selecting predictors from a pool of potential predictors include Different climate variables have different characteristic spatial scales. Grid-scale processes are realistically simulated by GCMs at coarse spatial resolution, while the subgrid-scale processes such as precipitation are taken into account in GCMs by means of parameterizations, that is, by semi-empirical methods that are tuned to reproduce the net effect of the considered process on the global-scale. Therefore, precipitation provided by GCM is usually not considered as robust information at the regional and grid scales (e.g., Development of SVM and ANN downscaling modelsThe predictors, which are selected following the foregoing analysis, are standardized. Standardization is widely used prior to statistical downscaling to reduce systematic bias (if any) in the mean and variance of GCM predictors relative to NCEP reanalysis data Most of the predictor variables, which are screened from a pool of possible predictors by using their cross-correlations with predictand, are highly correlated and convey similar information. Therefore, to obtain relevant predictors as input to the SVM and the ANN downscaling models, the standardized NCEP predictor variables are then processed using principal component analysis (PCA) to extract principal components (PCs) which are orthogonal and which preserve more than 98% of the variance present in them. Precipitation (predictand) constitutes the output from the downscaling models. The use of PCs as input to a downscaling model helps in making the model more stable and at the same time reduces its computational burden.To develop the SVM downscaling model, the available data set is randomly partitioned into a training set and a test set following the multifold cross-validation procedure documented in The training of SVM involves selection of the model parameters r and C. In this study, grid search procedure The ANN model is trained following the procedure described in ASCE Task Committee on Artificial Neural Networks in Downscaling GCM simulations to precipitationThe GCM simulations are run through the calibrated and validated SVM downscaling model to obtain future simulations of predictand. The future values of simulated predictand are divided into 5 periods, each 20 years long ResultsTypical results of seasonal stratification performed using K-means algorithm are presented in Fig. The predictor variables, which have maximum influence on the precipitation of each MSD, are identified following the procedure described earlier in this paper. For brevity, the predictors identified for North-Interior Karnataka subdivision are shown in Table The width of RBF kernel r used in this work can give an idea about the smoothness of the derived function. To investigate the potential of SVM in downscaling GCM simulations, its performance is compared with that of multi-layer back-propagation neural network based downscaling model. The quantitative evaluation of the performance of ANN and SVM downscaling models is done using the following statistics:(i) Relative bias in preservation of mean and standard deviation of the observed precipitation. (ii) Normalized mean square error (NMSE, NMSE \u00bcwhere N represents the number of feature vectors prepared from the NCEP record, y i and \u0177i denote the observed and the simulated precipitation respectively, and S obs is the standard deviation of the observed precipitation. For an ideal model the relative bias in preservation of mean and standard deviation, and NMSE value must be zero. Typical results of this analysis are presented in   The geographic location of grid points listed in the table can be found in Fig. than that of ANN (Fig. Typical results from the SVM-based downscaling model for wet and dry seasons are presented in Figs. 9-11 using boxplots. The span of the box represents the interquartile range of the simulated (or observed) rainfall. The whiskers extend from the box to 5% and 95% quantiles on the lower and the upper side of the box, respectively.The statistical significance of the results is assessed using null hypothesis considering three significance levels (1%, 5% and 10%). For the null hypothesis test it is assumed that the variances of past and projected precipitation are unknown  and unequal. The test statistic, T, is computed using Eq. ( where X obs and X sim are estimated means of the observed and the simulated precipitation respectively, S obs and S sim are respectively the standard deviations of the observed and simulated precipitation.The projected scenarios of precipitation for The results presented in this section strongly support SVM downscaling model as a feasible and potential alternative to multi-layer back-propagation neural network based downscaling model for climate impact studies in hydrology.Summary and discussion", "conclusions": "In this paper support vector machine (SVM) approach has been introduced for statistical downscaling of precipitation at monthly time scale from simulations of GCM. The effectiveness of the proposed approach is illustrated through its application to future climate projections provided by CGCM2 over India. The proposed model is shown to be statistically superior compared to the multi-layer back-propagation Monthly Precipitation (mm)", "SDG": [13]}, "harnessing_artificial_intelligence_for_the_earth_report_2018": {"name": "Harnessing Artificial Intelligence for the Earth is published by the World Economic Forum System Initiative on Shaping the Future of Environment and Natural Resource Security in partnership with PwC and the Stanford Woods Institute for the Environment. It was made possible with funding from the MAVA Foundation. About the 'Fourth Industrial Revolution for the Earth' series The \"Fourth Industrial Revolution for the Earth\" is a publication series highlighting opportunities to solve the world's most pressing environmental challenges by harnessing technological innovations supported by new and effective approaches to governance, financing and multistakeholder collaboration. About the World Economic Forum The World Economic Forum, committed to improving the state of the world, is the international organization for public-private cooperation. The Forum engages the foremost business, political and other leaders of society to shape global, regional and industry agendas", "abstract": "", "keywords": "", "introduction": "", "body": "-Processing power: Accelerating technologies such as cloud computing and graphics processing units have made it cheaper and faster to handle large volumes of data with complex AI-empowered systems through parallel processing. In the future, \"deep learning\" chips -a key focus of research today -will push parallel computation further.-A connected globe:  The spectrum of AI is also expanding and now includes:-Automated intelligence systems that take repeated, labour-intensive tasks requiring intelligence, and automatically complete them. For example, a robot that can learn to sort recycled household materials.-Assisted intelligence systems that review and reveal patterns in historical data, such as unstructured socialmedia posts, and help people perform tasks more quickly and better by using the information gleaned. For example, techniques such as deep learning, natural language processing and anomaly detection can uncover leading indicators of hurricanes and other major weather events.-Augmented intelligence systems that use AI to help people understand and predict an uncertain future. For example, AI-enabled management simulators can help examine scenarios involving climate policy and greenhouse gas emissions, as pioneered by MIT's John Sterman. Research on AI algorithms has been moving quickly, especially since big data has been combined with statistical machine-learning algorithms.Narrow, task-driven AI techniques, already important in many industrial applications, are now working with big data to allow pattern recognition in unstructured text and images. The potential of deep learning using neural network architecture continues to grow -as computers become faster and big data becomes ever more prevalent -enhancing performance in fields such as language translation and autonomous cars.The latest advances in unsupervised deep reinforcement learning, from DeepMind's AlphaGo Zero research, show that in certain situations AI can be surprisingly powerful even without input data or labels. To date, reinforcement learning has been primarily used for AI gaming agents, but should also help in corporate strategic analysis, process optimization and many other domains where the rules and different states of play are well known. However, this is often not true for many systems encountered in the real world and a central research priority is to identify the real-world systems where reinforcement learning would be most useful.Experts expect that supervised and unsupervised learning techniques will become increasingly blended and that such hybrid techniques will open the way for human-machine collaborative learning and for AI to develop more advanced, human-like, capabilities.Progress in AI may accelerate as new techniques are developed to overcome existing challenges with machine learning (deep learning in particular) and to solve problems in the field. Two such techniques are synthetic data creation and transfer learning (transferring the model learnt from a task in a certain domain and applying it to a related problem in that domain). Both of these enable AI to \"learn\" more quickly, tackling a wider range of problems (particularly those for which there is less historical data available).In addition, the shift towards 'Explainable AI', which aims to create a suite of machine learning techniques that produce more explainable models whilst maintaining high performance levels, will facilitate wider adoption of machine learning techniques and potentially become best practice or even inform regulatory requirements.Ultimately, all this culminates in the quest for artificial general intelligence (AGI), at which point, the AI begins to master reasoning, abstraction, communication and the formulation and understanding of knowledge. Here the critical need for progress in AI safety becomes fully apparent. This will involve the development of algorithms with safety considerations at their core.Future advances in AI will need advanced computing power (currently it takes around 83,000 processors operating for 40 minutes to run the equivalent of one second of computations performed by just 1 percent of the human brain), The most important consideration in the development of AI is, arguably, to ensure that it benefits humanity, which includes being both \"human-friendly\" and \"Earth-friendly\".Figure Figure Increased demand for transport could offset some efficiency gains, but overall a smart transport system enabled by AI can be expected to lower emissions. Improved efficiency may also encourage car sharing and reduce car ownership, further reducing emissions from manufacturing and operating vehicles.Still, the transition to connected autonomous fleets in cities will be gradual and will vary from country to country. It may be decades before fully autonomous urban fleets are the norm. In addition to developing the technology, challenges related to public acceptance, legal and insurance liability questions, and the provision of charging infrastructure will need to be addressed. Furthermore, the vehicle replacement cycle takes approximately 15-20 years. While full \"Level 5\" vehicle autonomy (with no human intervention at all) may still be decades away, \"Level 4\" AVs (highly automated, but with driver takeover when needed) may be tested on roads as early as 2021. At this level, cars can drive in cities and provide mobility-on-demand services. More substantial emission-reduction benefits also begin to appear.Distributed energy gridsIn the energy grid, the application of machine learning, including deep learning, is increasingly widespread in industry. For the environment, the use of AI to make distributed energy possible at scale is critical for decarbonizing the power grid, expanding the use of (and market for) renewables and increasing energy efficiency. AI can enhance the predictability of demand and supply for renewables, improve energy storage and load management, assist in the integration and reliability of renewables and enable dynamic pricing and trading, creating market incentives. AI-capable \"virtual power plants\" (VPPs) can integrate, aggregate, and optimize the use of solar panels, microgrids, energy storage installations and other facilities. Distributed energy grids may also be extended to incorporate new sources such as solar spray or paint-coated infrastructure of vehicles, and to allow AI-enabled \"solar roads\" to expand, connect and optimize the grid further. In solar roads, for example, AI could allow a road to learn to heat up to melt snow, or to adjust traffic lanes based on vehicle flow. Smart grids will also use other Fourth Industrial Revolution technologies, including the Internet of Things, blockchain (for peer-to-peer energy trading) and advanced materials (to increase the number of distributed sources and optimize energy storage).All of this will require sufficient regulation to assure the security and integrity of the software, ownership and control of intellectual property rights (which may help unlock investment and innovation), management of, and responsibility for, operational elements that are powered by machine learning, and regulatory frameworks for transferring and trading energy, often virtually. As economies and settlements move away from \"heavy infrastructure\" towards \"smart\" infrastructure with a low environmental footprint, the decentralized nature of distributed energy grids mean they have the potential to be used globally.Smart agriculturePrecision agriculture (including precision nutrition) is expected to increasingly involve automated data collection and decisionmaking at the farm level -for example to plant, spray and harvest crops optimally, to allow early detection of crop diseases and issues, to provide timed nutrition to livestock, and generally to optimize agricultural inputs and returns. This promises to increase the resource efficiency of the agriculture industry, lowering the use of water, fertilizers and pesticides, which are creating runoff that currently finds its way into rivers, oceans and insect populations, causing damage to important ecosystems.Here the key Fourth Industrial Revolution technologies that will combine with AI include robot labour (such as Blue River Weather forecasting and climate modellingA new field of \"Climate Informatics\" is already blossoming, harnessing AI to fundamentally transform weather forecasting (including prediction of extreme events) and to improve understanding of the effects of climate change. Public agencies including the UK Met Office and NASA, and private-sector actors such as IBM and Microsoft, are using AI and machine learning to enhance the performance and efficiency of weather and climate models . Wider AI applications include simpler machine-learning techniques, combining weather models and ancillary impacts data, to help predict the effects of small-scale extreme weather events (such as windstorms and floods) on human systems, allowing better risk management. More broadly, however, the application of nascent deep reinforcement learning techniques is unchartered territory for climate and weather science. Investigation will be needed to identify the real-world physical systems in which these new tools will be most useful.We are already seeing how better weather and climate data helps decision-makers from the public and private sectors to improve climate resilience. The UK Met Office, for example, has developed a chatbot application to demonstrate how \"frictionless\" data or queries can be extracted from complex big datasets, using sophisticated AI in real time and communicating to the user through a simple interface. Another example involves artificial assistants, fed by forecasts data, that can help make everyday decisions, from what to wear to when to travel. Some companies are already working together, and with universities and government agencies, within the field of climate informatics. There is now an opportunity to formalize, organize and promote the emerging scientific discipline of AI for weather and climate science, including international coordination (for example, through the World Meteorological Organization and the Intergovernmental Panel on Climate Change), dedicated national R&D funding and cross-industry collaboration.A community disaster-response data and analytics platformThe speed and effectiveness with which organizations and people can respond to disasters has a substantial impact on the extent of economic losses and human suffering, particularly in the most catastrophic events. But delays often occur due to a lack of information, analytical insight and awareness of the best course of action. Often the necessary data exists in large part, but is segregated among various organizations and is thus mostly inaccessible to communities.Better resiliency planning is also an important component to mitigate the damage of future natural catastrophes. AI can be used to sort through multidimensional data about a region and identify which aspects have the biggest impact on resilience. AI can run and analyse simulations of different weather events and disasters in a region to seek out vulnerabilities and identify the resiliency plans that are most robust across a range of event types.New hybrid systems of rules and tools can use data and AI techniques to build a \"Community Distributed Data Escrow\" system that could enhance disaster preparation and response through coordination of emergency information capabilities. Harnessing AI to provide better disaster response and planning will require public-private partnerships. A community of technical, legal and accounting experts, for example, would need to specify key datasets and standardize approaches, define methodologies for leveraging APIs and machine learning tools to access vital data securely and accountably, and establish the terms and conditions for stakeholders to operate within the system.Decentralized waterMachine and deep learning could enable a step-change in the optimization of waterresource management. Increasingly, AI has the potential to create distributed \"offgrid\" water resources, analogous to decentralized energy systems.Household smart meters can produce large volumes of data that can be used to predict water flows, spot inconsistencies and check leaks. The next stage will be to combine machine learning, the Internet of Things and blockchain to create a truly decentralized water system, where local resources and closed-loop water recycling gain value. Water resources could even be traded via blockchain. Furthermore, machine learning, predictive modelling and robotics can be combined to transform current approaches to building and managing water infrastructure and to accelerate innovation in environmental engineering. Rivers, for example, could be engineered to autonomously adjust their own sediment flows. Coupled with AI-informed pricing, such approaches could optimize water usage and drive behaviour change by providing incentives for water conservation.AI-designed intelligent, connected and liveable citiesBeyond autonomous vehicles, deep learning also promises better urban planning, leading to resilient, human-centric cities with minimal air pollution and environmental impact. AI could also be used to simulate and automate the generation of zoning laws, building ordinances and floodplains. Combined with AR and VR, AI-generated data could be used by city planners and infrastructure investors, along with officials responsible for ensuring disaster preparedness and, when needed, reconstruction. AI, smart meters and the Internet of Things can also help forecast and optimize urban energy generation and demand -both city-wide and at the level of individual homes and buildings. Real-time AI-optimized energy efficiency can have an immediate and substantial impact on energy consumption (Google, for example, cut power use in its data centres by 40% by using DeepMind's reinforcement learning algorithms to optimize cooling. Combining real-time city-wide data on energy and water consumption and availability, traffic flows, people flows, and weather could create an \"urban dashboard\". With the addition of AI this could optimize water and energy use across the city, potentially reducing the need for costly additional infrastructure while reducing pollution and congestion -thereby reducing the city's environmental footprint and increasing its liveability.Oceans data platformReal-time monitoring with AI can improve decision-making in fields ranging from species management and protection to natural resource management to climate resilience. One early example is the Ocean Data Alliance, New processing capabilities could provide close-to-real-time transparency by enabling authorities, and even the general public, to monitor fishing, shipping, ocean mining and other activities. Vessel algorithmic patterns could identify illegal fishing, biological sensors could monitor the health of coral reefs and ocean current patterns could improve weather forecasting.One of the main challenges to realizing such a platform is the processing power required: ocean modelling is second only to astrophysics in its hunger for computing power. But as the cost of data storage and processing declines, new possibilities to model human activities and how they impact our oceans will become available. To prevent the emergence of multiple competing platforms, which could reduce effectiveness and increase the overall costs of collecting, managing, and using ocean data, an open-access platform could be created that enables data from different sources to continually be uploaded in a standardized format. Public-private partnerships may be needed to ensure trust, governance and accuracy.Earth bank of codesBio-inspired innovations (such as bloodpressure medication derived from viper venom) aim to replicate nature's products and processes. Historically, the revenues from such activities have not been shared with the indigenous and traditional communities from which the knowledge comes. For the first time in history, the fair sharing of benefits and a significant new stream of conservation finance is now possible using a combination of blockchain, artificial intelligence, advanced sensors and the Internet of Things.The Amazon Third Way initiative This project is building a coalition of willing stakeholders to co-design and co-implement the EBC in the Amazon Basin (called the Amazon Bank of Codes Further-off AI game changersBy the 2030s, further advances in AI and other Fourth Industrial Revolution technologies may bring us more innovations for the environment. These could include:A real-time digital dashboard of the EarthA real-time, open API digital geospatial dashboard for the planet would enable the monitoring, modelling and management of environmental systems at a scale and speed never before possible -from tackling illegal deforestation, water extraction, fishing and poaching to air pollution, natural disaster response and smart agriculture. We have the AI methods to do this, but we need more information, more frequently received and at greater resolution than at present. The challenge is to build something truly transformational, easy to use in real-time, open-access and data-dense (meaning that the information is high-resolution, scalable and aggregates environmental and human exposure data). This will require collaboration among entrepreneurs, industry, government and the nonprofit sector.Public and private systems that can help amass the necessary data include the European Space Agency's Copernicus, At least two steps are already being taken in this direction. The US National Science Foundation's EarthCube initiative uses machine learning and simulation modelling to create a 3D living model of the entire planet. And the US company Planet has put over 180 micro-satellites into orbit, to image the whole planet's landmass daily, at a resolution of 3-5 metres. Autonomous farming and end-to-end optimized food systemAI could enable farms to become almost fully autonomous. Farmers may be able to grow different crops symbiotically, using AI to spot or predict problems and to take appropriate corrective actions via robotics. For example, should a corn crop be seen to need a booster dose of nitrogen, an AIenabled system could deliver the nutrients. AI-augmented farms could also automatically adjust crop quantities, based on supply and demand data. This kind of production could be more resilient to earth cycles.Our understanding of human dietary needs is likely to improve in the coming decades, as we learn how individuals process their food intake, based on data from many individual bodies. Applying machine learning to this data could generate personalized nutrition plans optimized for individuals. When combined with autonomous farming, autonomous delivery vehicles, in-house robotic chefs and in-house vertical farming, entire food supply chains could be optimized and transformed, creating minimum-waste supply chains while providing high yields. The same principles could also be applied to livestock.Reinforcement learning for natural sciences breakthroughsDeep reinforcement learning could evolve to enable its application to real-world problems, including solving problems addressed by Earth scientists. This could enable scientific progress and discovery in scientific areas where the boundary conditions of a system are known but input data is lacking, and/or the complexity of a system is such that it requires access to currently infeasible computing architecture.Technically, step one is to understand what the optimal \"real world\" natural and human-natural systems are, in which we can most fully define the boundary conditions, to enable the application of reinforcement learning. A hybrid approach that combines supervised and unsupervised learning will likely be most successful, given the challenges of fully defining the boundary conditions of real world problems. Understanding which real world systems can be codified and optimizing for reinforcement learning will require collaboration between AI pioneers and domain experts including climate scientists, materials scientists, biologists, and engineers. For example, DeepMind co-founder, Demis Hassabis, has suggested that, in the materials science space, a descendant of AlphaGo Zero could be used to search for a room temperature superconductor -a hypothetical substance that allows electrical current to flow with zero lost energy, further allowing for incredibly efficient power systems. As was done with Go, the algorithm would start by combining different inputs (in this case, the atomic composition of various materials and their associated qualities) until it discovers something the humans had missed.Quantum and distributed computing to dramatically scale computational power for AI for the EarthInstead of using brute force to increase the computing power of AI, innovators are increasingly exploring other advances such as deep learning chips, harnessing the move to cloud, and the ability to use distributed computing and quantum computing. All of these advances that increase computing processing power will enable large scale optimization of big data analytics and AI, scaling and transforming their application and impact for environmental challenges. But advances in quantum computing, in parallel, could offer fundamentally new opportunities for scientific discovery. Classical computers cannot compute things the way nature does (which operates in quantum mechanics); they are limited to the human made binary code (of zeros and ones) rather than the natural reality of continuous variables. In other words, with classical computers we are currently modelling the Earth system in a way that it does not actually function. Quantum computers open the door to solving the quantum problems as they exist in nature and discovering ways in which the Earth system really works: from key applications in quantum chemistry, to quantum physics and mechanics. This could lead to the discovery of new advanced materials, new biological processes (e.g. energy transference, cellular growth, or ecosystem dynamics), and progress in the modelling of planetary physics.The home supercomputer and AI research assistants for democratized scientific progressEarth science is currently one of the most computational heavy fields of scientific discovery -with supercomputing systems currently in widespread use across the field and climate researches using some of the largest and most powerful systems available today. The cost of building, accessing and running supercomputers inhibits access to researchers and limits the pace at which new modelling and research can be undertaken. Over the coming decade or two, computational power and advances in AI algorithms will likely reach a point in which the average home computer will have as much power as today's supercomputers.In parallel, machine learning more broadly will also unlock faster and cheaper earth system and climate models, and AI will begin to replace many of the labour-intensive and time-consuming tasks that scientists now do (e.g., trawling through data archives, converting files) -acting in effect as an 'AI research assistant'. The result is that the pool of scientists and practitioners that have access to computing power and AI tools could increase vastly, progress in Earth science and its application could become democratized, and scientific productivity could be substantially boosted with a subsequent acceleration in discoveries. Again these could include breakthroughs in understanding of weather risk, future regional and local climate impacts, and more challenging areas including climate feedback loops and tipping points.For all the enormous potential AI offers for building a sustainable planet for future generations, it also poses short-and longterm risks. These can be divided, broadly speaking, into six categories with varying impacts on individuals, organizations, society, and the earth.AI unguided: Unintended consequences for the Earth Performance risksFor the most part, the outputs of AI systems are determined within a \"black box'\" and with little transparency, these outputs may not be trusted. By their nature, AI algorithms (which are self-learning and continuously adapting) are difficult to explain and in many cases may not be explainable to humans at all. An inability to understand the rationale behind AI outputs also makes it difficult to ascertain whether the performance or outputs of AI algorithms are accurate or desirable. Significant risks are therefore conceivable. The emerging field of explainable AI (XAI) research aims to create new AI methods that are accountable to human reasoning. But this field is still in its early days. Meanwhile, ongoing research aims to reduce \"model bias\" resulting from biases in training data, and to increase the stability of model performance. As AI solutions are deployed, one unintended consequence is the over-reliance on AI algorithms with variable performance. It is essential that humans stay \"in the loop\" on auditing algorithm outputs to mitigate these unintended biases and wider performance risks.Example: Early-warning systems for natural disasters such as flooding are trained using historical data on weather patterns. However, if there is a lack of understanding of factors driving model predictions due to poor explainability, there is a significant risk of false alarms or false negatives, particularly in situations that are not represented in the data used to train the AI model. Security risksMisuse of AI via hacking is a serious risk, as many algorithms being developed with good intentions (for example, for autonomous vehicles) could be repurposed for harm (for example, for autonomous weaponry). This raises new risks for global safety. Good governance is required to build explainability, transparency and validity into the algorithms, including drawing lines between beneficial and harmful AI. Machine-learning (especially deep-learning) models can also be duped by malicious inputs known as \"adversarial attacks\". For example, it is possible to find input data combinations that can trigger perverse outputs from machine-learning models, in effect hacking them.Performance risks Depending on circumstances, individual building decisions will interact with regional ones, potentially altering demand in ways that could crash regional energy systems.Economic risksAs companies adopt AI, it may alter the competitive landscape, creating winners and losers. Those able to improve their decision-making most quickly through AI may find the benefits accelerate very quickly, while slower adopters may be left behind. Companies that struggle in the AI transition may be forced to reduce investment, possibly impairing their sustainability performance. Tax-base erosion presents another economic threat as the current system, based on \"bricks-and-mortar\" and nation-states, struggles to keep pace with the globalized digital economy. Tax erosion could be a drag on public spending, including investment in, for example, programmes designed to reduce greenhouse gas emissions. Current tax systems may need re-evaluation as automation changes workplaces, potentially reducing the number of jobs available.Example: Increased productivity from automation, plus rising consumption from improved personalization, product design and AI-informed marketing, could increase resource use, waste and demand for energy.Social risksLarge-scale automation threatens to reduce employment in transportation, manufacturing, agriculture and the service sector, among others. Higher unemployment rates could lead to greater inequality in society. In addition, algorithms designed by a subset of the population at a national and global level have the potential for unconscious bias, possibly leading to results that marginalize minorities or other groups. Autonomous weapons also pose a significant threat to society, possibly permitting bigger, faster conflicts. Once unleashed, this might lead to rapid and significant environmental damage, even to a \"doomsday\" scenario where weaponized AI presents an existential risk to humanity. (Goldman Sachs estimates that the US alone will lose an estimated 300,000 jobs per year when AV saturation peaks). Ethical risksThe ethical and responsible use of AI involves three main elements: the use of big data; the growing reliance on algorithms to perform tasks, shape choices and make decisions; and the gradual reduction of human involvement in many processes. Together, these raise issues related to fairness, responsibility, equality and respect for human rights. Example: Autonomous emergency food-and disaster-relief delivery systems that are trained using reinforcement learning or historical demand patterns will route supplies to specific regions during natural disasters. This could create ethical dilemmas relating to accountability for delivery dysfunctions, prioritysetting and results.Conclusions and recommendations", "conclusions": "Conclusions AI systems, and their ability to control machines automatically and remotely, have caught the public's imagination. The opportunity for AI to be harnessed to benefit humankind and its environment is substantial. The intelligence and productivity gains that AI will deliver can unlock new solutions to society's most pressing environmental challenges: climate change, biodiversity, ocean health, water management, air pollution, and resilience, among others.However, AI technology also has the potential to amplify and exacerbate many of the risks we face today. To be sure that AI is developed and governed wisely, government and industry leaders must ensure the safety, explainability, transparency and validity of AI applications. It is incumbent on authorities, AI researchers, technology pioneers and AI adopters in industry alike to encourage deployments that earn trust and avoid abuse of the social contract.Achieving this requires a collaborative effort to ensure that as AI progresses, its idea of a good future is aligned to human values and encapsulates a future that is safe for humanity in all respects -its people and their planet.", "SDG": [13]}, "automatic_identification_of_oil_spills_on_satellite_images": {"name": "Automatic identification of oil spills on satellite images", "abstract": " A fully automated system for the identification of possible oil spills present on Synthetic Aperture Radar (SAR) satellite images based on artificial intelligence fuzzy logic has been developed. Oil spills are recognized by experts as dark patterns of characteristic shape, in particular context. The system analyzes the satellite images and assigns the probability of a dark image shape to be an oil spill. The output consists of several images and tables providing the user with all relevant information for decision-making. The case study area was the Aegean Sea in Greece. The system responded very satisfactorily for all 35 images processed. The complete algorithmic procedure was coded in MS Visual CCC 6.0 in a stand-alone dynamic link library (dll) to be linked with any sort of application under any variant of MS Windows operating system.", "keywords": "Oil spills,SAR,Remote sensing,Sea surface,Marine pollution,Fuzzy logic", "introduction": "Environmental protection is currently an important subject of increasing public concern and as a result, particular attention is being paid to the environmental damage caused by the creation of spills of hydrocarbon compounds over the sea surface created as a result of oil-tanker accidents or illegal cleaning of tankers. A successful combating operation to a marine oil spill depends on the rapid response from the time the oil spill is detected. In fact, the concept of oil spill contingency planning refers to several activities for developing an immediate response program and undeniably the most important one is oil spill detection . In fact, several studies have already been reported on oil spill contingency planning (Assilzadeh and Mansor, 2001)(Uthe, 1992;Monk and Cormack, 1992;Theophilopoulos et al., 1996;; all studies recognize the oil spill detection and surveillance issue as the most important one amongst all others including assessment and evaluation, spill evolution computer simulation, management and clean up. Oil spills depending on the exact hydrocarbon content and type involve normally extensive areas of film on sea surface, a fact which reduces water roughness and can therefore allow the detection by Synthetic Aperture Radar (SAR) images. It is important that oil spill detection algorithms provide the user with accurate information about specific features characterizing the oil spill, including location of its centroid, size, distance from the land etc. This information has to be available fast and subsequently fed into models which predict the trajectory and fate of a chemical spill (e.g. Assilzadeh et al., 1999)Skognes and Johansen, 2004; and/or statistical oil-spill risk analysis models, such as the ''Oil-spill Risk Analysis'' (OSRA) model, driven by analyzed sea surface winds and model-generated ocean surface currents French McCay and Isaji, 2004).(Price et al., 2004)SAR imagery is a common medium for detecting oil spills. In some cases, especially when a large number of SAR scenes has to be examined, the image processing and by eye discrimination between oil spills and lookalikes may be a time consuming as well as labor-intensive task . Several important efforts have already been reported so as to develop an automated detection system that would recognize an oil spill through a SAR image without the intervention of the expert. Such systems retrieve SAR images, referring to the sea regions under consideration, from a corresponding satellite platform and produce alarm notifications when an object on the image is identified as an oil slick. From previous experience, the automatic detection of oil slicks in SAR images is reported as a very complicated task because objects resembling oil spills (often called look-alikes) occur frequently in SAR images, especially in low wind conditions. Most frequently, look-alikes are produced by organic film, grease, wind front areas, land, plankton formations, rain cell, current shear zones and upwelling zones (Gade and Alpers, 1999). There are cases, where even the most experienced operator cannot discern between a possible oil spill and a look-alike. Actually, an experienced operator is trained to discriminate between oil spills and look-alikes based on experience and prior information on weather conditions, difference in shape, contrast to surrounding and background objects and proximity to land. Thus, a fully automated system should actually resemble the expert's decisions based on similar criteria, knowledge and rules.(Hovland et al., 1994)The development of automated or semi-automated systems for oil spill detection is a subject of several efforts reported in literature.  developed a neural network for the classification of dark regions detected in a series of nine SAR images that served as a training set of the system. The complexity of such a system as well as the appropriate actions that have to be taken into consideration by potential tool developers in such fields were analyzed in detail. Input to the classifier was straightforward, though image preprocessing was not automated. The classifier had an open architecture of rules so that it could embed user experience in several other fields apart from oil detection. Del Kubat et al. (1998) also used neural network architecture for semi-automatic detection of oil spills on SAR images using a set of features characterizing a candidate oil spill as input vector. Frate et al. (2000) and Solberg and Solberg (1996) produced a semi-automated classifier for oil spill detection, in which the objects with a high probability of being an oil spill were automatically detected. Three different categories of probability (low, medium and high) were recognized. A rational processing procedure was adopted for 84 SAR images utilized. It involved pixel local thresholding based on wind level information, clustering of small pixel objects or partitioning of large pixel objects based on sizing criteria and feeding each individual cluster to a classifier operating on a stochastic processing basis. Ten different object characteristics were identified and classification was based on a Bayesian inference procedure. Solberg et al. (1999) developed a stochastic classifier based on Mahalanobis statistical tests and classical compound probabilities. A preprocessing tool was used in order to extract pixel objects from SAR images and classified them according to statistical criteria implemented on a total of 14 different characteristics of extracted clusters. Fiscella et al. (2000) utilized RADARSAT data for oil slick detection and oil slick trajectory model in the coastal water of Malacca Straits. His approach involved two submodels: one is containing entropy and homogeneity texture algorithms for oil slick detection, and the second one is containing the oil slick trajectory forecasting model.Marghany (2004)In the present work a fully automated system for the identification of possible oil spills that resembles the expert's choice and decisions has been developed. The system comprises modules of supplementary operation and uses their contribution to the analysis and assignment of the probability of a dark image shape to be an oil spill. SAR images are read, located, land masked, filtered and thresholded so that the appropriate dark areas are extracted. Candidate oil spill objects are fuzzy classified to determine the likeness of each individual object to be an oil spill. The output images and tables provide the user with all relevant information for supporting decision-making. The case study area was the Aegean Sea. The system responded very satisfactorily for all 35 images processed.", "body": "Environmental protection is currently an important subject of increasing public concern and as a result, particular attention is being paid to the environmental damage caused by the creation of spills of hydrocarbon compounds over the sea surface created as a result of oil-tanker accidents or illegal cleaning of tankers. A successful combating operation to a marine oil spill depends on the rapid response from the time the oil spill is detected. In fact, the concept of oil spill contingency planning refers to several activities for developing an immediate response program and undeniably the most important one is oil spill detection SAR imagery is a common medium for detecting oil spills. In some cases, especially when a large number of SAR scenes has to be examined, the image processing and by eye discrimination between oil spills and lookalikes may be a time consuming as well as labor-intensive task The development of automated or semi-automated systems for oil spill detection is a subject of several efforts reported in literature. In the present work a fully automated system for the identification of possible oil spills that resembles the expert's choice and decisions has been developed. The system comprises modules of supplementary operation and uses their contribution to the analysis and assignment of the probability of a dark image shape to be an oil spill. SAR images are read, located, land masked, filtered and thresholded so that the appropriate dark areas are extracted. Candidate oil spill objects are fuzzy classified to determine the likeness of each individual object to be an oil spill. The output images and tables provide the user with all relevant information for supporting decision-making. The case study area was the Aegean Sea. The system responded very satisfactorily for all 35 images processed.Image preprocessingAutomatic oil spill detection for SAR images was implemented through a series of computational procedures involving retrieval and storage of SAR image content, locating, land masking, smoothing (filtering), thresholding, segmentation and classification.In almost each step a visual or tabular output is available to the user. The sequence of procedures and the available output in each step are presented in Fig. Low-resolution ERS-1 and -2 SAR images (pixel size at range and azimuth equal to 100 m and 79.5 m, respectively) are used for this study. The identification of patterns and shapes in SAR images require the evaluation of radar signal amplitude from complex pixel value as well as the corresponding geometrical corrections necessary for the image integrity. Low-resolution SAR images are derived from preprocessed 5/5 bit complex raw pixel images appropriately re-sampled for a pre-specified size of a non-overlapping moving window across the image. Such images are geometrically corrected and signal amplitude is introduced as a 16-bit integer number. The format of low-resolution SAR images comprises a leading header and a series of bytes representing pixel values in rows as scanned by radar beam. Header information is important to correctly import the file and appropriately locate it with respect to a pre-specified coordinate system, since several vital image components are included, such as the longitudelatitude pairs of the four corners bounding image quadrilateral as well as of its centroid, the exact image dimensions in pixels, the exact time that was taken and several more specific information.Information from leading file header is used to derive the exact coordinates of the image-bounding quadrilateral and therefore automatically locate the image in a coordinate map. To restore the image in its correct position with respect to the background, some further processing is required depending on the ascending or descending path of the satellite. Several different SAR images can be simultaneously viewed within the graphical user interface of the tool, as presented in Fig. The image projection on a latitude-longitude geographical coordinate system is a quadrilateral whose corner points are included in the image header. Let us consider an arbitrary pixel I(i, j ) located on the i-th column and j-th row of an image of N columns and M rows. In tool graphics procedures as well as in the landmasking algorithm, one has to estimate the exact longitude-latitude coordinate of the given image pixel I(i,j ) and vice versa, i.e. given the longitude-latitude geographical coordinates of a point C(x C ,y C ), to check whether it is included in the image and estimate its exact image coordinates. The derivation involved in mapping operations is given by a set of equations relating point projections on quadrilateral faces, based on the fact that these points must have similar distance proportions on face projections as the image point has with respect to its rows and columns.Land masking is a very important operation due to the fact that land regions that are present in the image involve several dark regions and thus, their existence may trick the classification process. In addition, in our case studies that involve mostly images of the Aegean Sea, the situation is more complicated because the images cover lots of small and medium sized islands usually very close to one another. Automatic land masking is performed by appropriately overlaying a polygonal GIS theme of the entire map of Greece and Turkey on the image. Each polygon of the collection is transformed in a regional MS Windows API application region and each of them is appropriately region-subtracted by the original image region. What remains is the sea mask. A coarse GIS theme can be safely used for this purpose for speeding-up calculations. The original polygon regions were created once and used for all images processed.The sea image pixels are smoothed using a standard Gaussian filter. The Gaussian smoothing operator is a 2-D convolution operator that is used to 'blur' images and remove detail and noise The result of thresholding was the partitioning of the initial sea part of the image into areas characterized as dark and bright. Extraction of pixel groups that would be candidate objects were fed to the system's classifier. This was automatically performed through appropriate segmenting of the initial group objects. Segmentation was performed by determining the k-groups within the extracted dark region objects Estimating the probability of an object to be an oil spillThe probability of each object extracted with the techniques mentioned in the previous section to be an oil spill was estimated using an artificial intelligence fuzzy logic modeling system. The system was initially developed by human experts based on their experience and a large database of available information.Fuzzy logic as a modeling toolFuzzy logic theory has emerged over the last years as a useful tool for modeling processes which are too complex for conventional quantitative techniques or when the available information from the process is qualitative, inexact or uncertain. Although it is almost four decades since Lotfi Traditional set theory is based on bivalent logic, where an object is either a member of a set or it is not. Contrary to that, fuzzy logic allows a number or object to be a member of more that one sets and most importantly it introduces the notion of partial membership 1. Fuzzification. It is the process of decomposing a system input variables into one or more fuzzy sets, thus producing a number of fuzzy perceptions of the input. 2. Fuzzy inference. After the inputs have been decomposed into fuzzy sets, a set of fuzzy if-then-else rules is used to process the inputs and produce a fuzzy output. Each rule consists of a condition and an action where the condition is interpreted from the input fuzzy set and the output is determined from the output fuzzy set. 3. Defuzzification. It is the process of weighting and averaging the outputs from all the individual fuzzy rules into one single output decision or signal. The output signal eventually exiting the system is a precise, defuzzified, crisp value.Fuzzy modeling methodologies are procedures for developing the knowledge base of the system, i.e. the set of fuzzy rules The most popular fuzzy model suggested in the literature, which is also used in this work, is the one proposed by where r is the fuzzy rule, R is the set of fuzzy rules, denotes the logical operator ''AND'', n is the number of input variables, m is the number of output variables, x i ; 1%i%n are the input variables, A r i ; 1%i%n are fuzzy sets defined on the respective universes of discourse, y j ; 1%j%m are the output variables and B r j ; 1%j%m are fuzzy sets defined for the output variables.Development of a fuzzy classifier for the detection of oil spills on SAR imagesThe fuzzy logic modeling architecture that was used to build a model for the estimation of the probability of an object to be an oil spill was based on important influencing factors. In order to develop the database and the rule base of the system, human experts were employed. The experts used their knowledge and experience, but also consulted a large database of information, consisting of:-Database of major oil spill events in the Aegean Sea between 1997 and 1999, provided by the Greek Ministry of Mercantile Marine. -Information on shape and size of oil spills detected by SAR sensors published in the literature The Mamdani type of fuzzy model was selected 1)Step 1 -Selection of input parameters: The probability of a dark object on an SAR image to be an oil spill is a function of many factors. The selection of the input parameters was made so that all the important influencing factors are considered, while maintaining the system at a reasonable size. The only source of data was the SAR image itself, making the system completely independent of any external information (e.g. weather and sea condition). Based on the above criteria, the list of selected input variables consisted of the following parameters:-The total number of objects identified in the image; -the number of the dark objects in the vicinity of a candidate dark object; -the area of the candidate dark object; -the eccentricity of the object's shape; -the proximity of the object to the land.It should be mentioned that the wind speed and sea roughness were not selected as input variables explicitly. However, the total number of objects on the image along with the number of dark objects in the vicinity of a candidate dark object are dependent on wind speed and sea roughness, as low wind conditions favor the development of many look-alikes. In moderate wind speed the number of look-alikes is small, and therefore the probability of an object to be an oil spill is higher.2)Step 2 -Development of the database: In this step, fuzzy sets were defined for all the input parameters, as well as for the only output variable, namely, the probability of a dark object to be an oil spill. More specifically, three fuzzy sets were defined for all the input variables, except for the area of the object, for which five fuzzy sets were appointed. An example of the fuzzy set defined for the input variable ''total number of objects identified on the image'' is given in Fig. The total number of objects identified on the image: Three fuzzy sets, namely ''Few'', ''Some'' and ''Many'' were defined on the input space which measures the number of objects from 0 to 200 (Fig. The proximity of the object to land: Three fuzzy sets were defined, namely ''Close'', ''Further'' and ''Away''. The distances cover the range of 0 to 55 km. In general, there are usually many dark objects along the coastline of the leeward side of the islands; therefore, the probability of such a dark object to be an oil spill is low. Probability of an object to be an oil spill: The only output variable is the probability of an object to be an oil spill and is measured from 0 to 1. For this variable, three triangular fuzzy sets were defined on the above mentioned output space (Fig. 3) Step 3 -Development of the rule base: During this step, the experts were employed to develop a number of fuzzy rules, based on their intuition and experience. The fuzzy rules were developed and tuned, so as to relate successfully the input conditions of nine out of the 35 Fig. available SAR images of the Aegean Sea, acquired under different weather conditions. The images are from the years 1998 and 1999. It has to be stressed that in each image a large number of dark objects are present, potentially oil spills or look-alikes. The number of these objects varies considerably with sea state from around ten to 200 per image. Therefore, the training and subsequent evaluation was based on hundreds of objects.The rules are constructed in simple language terms and can be understood at a common sense level. At the same time these rules result in specific and repeatable (same inputs gives same output) results. The experts developed 405 rules, one for each combination of fuzzy rules of the input parameters. All the rules use the logical AND operation. An example of a fuzzy rule is shown below: ''If the total number of dark objects on image is small AND the number of objects in the vicinity of the candidate object is small AND the area is small AND the eccentricity is high AND the distance from land is high THEN the probability of the candidate dark object to be an oil spill is HIGH.''The above three-step procedure defines the knowledge base of the fuzzy system. When the fuzzy model is to be applied to a set of input parameter values, the information flows through the fuzzification-inferencedefuzzification processes that are depicted in Fig. Fuzzification: During the fuzzification process, the triangular membership functions (fuzzy sets) defined on each input variables are applied to their actual values, to determine the degree of truth for each rule premise.Inference: During the inference process, the truth value for the premise of each rule is computed, and applied to the conclusion part of the rule. This procedure results in the assignment of one output fuzzy set for each rule. The min-max inferencing technique was used Defuzzification: The final output of the fuzzy system for the probability of an object to be an oil spill should be a crisp number, so as the fuzzy output needs to be defuzzified. The centroid defuzzification method The programming implementation of the fuzzification, rule-based and defuzzification part of the algorithm was based on numerical analysis approximations of the problem. More specifically, the area covered within the participation functions as calculated by rules was determined and computed as integral using a Simpson's rule. The fuzzy logic system developed using this approach gives very satisfactory results. The system was applied to the remaining 26 SAR scenes, not included in the training phase, and responded perfectly in 23 of them. The testing also proved that the system can be used to assign a probability that the observed object is an oil spill given any combination of input values within the specified ranges.Case studiesThe case study area was the Aegean Sea in Greece. A series of 35 ERS-1 and -2 SAR (acquired in 1998 and 1999) images were tested using the algorithm developed in this study and described above. The algorithms performed satisfactorily in scenes that contained verified oil spills but also in the ones that contained only lookalikes. A set of two images containing verified oil spills is presented (case 1 and 2) together with an example of a complex SAR scene which contained only look-alikes (case 3).Case 1The original SAR image is illustrated in Fig. Case 2The original SAR image is illustrated in Fig. Probability to be an oil spillvisual output of the algorithm. The third visual output is presented in Fig. Case 3The original SAR image is illustrated in Fig. Conclusion", "conclusions": "A fully automated system for the identification of possible oil spills has been developed. The software is a stand-alone application for windows. It is only fed by a RAW data satellite image file and returns an alarm as well as all information related to objects detected. The system comprises of modules of supplementary operation and uses the modules for the analysis and assignment of the probability of a dark image shape to be an oil spill. SAR images are read, located, land masked, filtered and thresholded so that the appropriate dark areas are extracted. Candidate oil spill objects are fuzzy classified to determine the likeness of each individual object to be an oil spill. The resulting images and tables provide the user with all relevant information for supporting decision-making. The system was developed using nine SAR images and was tested independently on 26 images of the Aegean Sea, yielding an overall performance of 88%.The system can be easily expanded to cover other geographical areas. The time required for the whole process (preprocessing and fuzzy classification) to be completed is of the order of 2-3 min per image, depending on computer speed. In addition, in case of an oil spill alarm, the system provides the operator with Probability to be an oil spill oil spill features necessary as input for trajectory and fate simulation models, such as size, shape, location, and distance from land. Automation of all the above makes the system autonomous; as a result, it can work continuously on a large amount of satellite images and alert the operator in case of an alarm. Therefore, the proposed system can be effectively used in real-time operations.", "SDG": [14]}, "a_feature_learning_and_object_recognition_framework_for_underwater_fish_images": {"name": null, "abstract": "", "keywords": "", "introduction": "I MAGE processing and analysis techniques for underwater cameras have drawn increasing attention since they enable a non-extractive and non-lethal approach to fisheries survey - [1]. For instance, by using a combination of cameras and mid-water trawl, known as the Cam-trawl [6], fish schools are sampled by capturing images or videos while they pass through the trawl. The camera-based sampling approach not only conserves depleted fish stocks but also provides an effective way to sample a greater diversity of marine animals. This approach, however, generates vast amounts of data very rapidly. An automatic image processing system is Manuscript received [7]; revised February 15, 2016; accepted February 21, 2016. Date of publication March 12, 2015; date of current version March 9, 2016. This work was supported in part by the National Marine Fisheries Services' Advanced Sampling Technology Working Group and in part by the National Oceanic and Atmospheric Administration, Seattle, WA, USA. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Peter Tay.February 26, 2016M.-C. Chuang and J.-N. Hwang are with the Department of Electrical Engineering, University of Washington, Seattle, WA 98195 USA (e-mail: mengche@uw.edu; hwang@uw.edu).K. Williams is with the Alaska Fisheries Science Center, National Oceanic and Atmospheric Administration, Seattle, WA 98115 USA (e-mail: kresimir.williams@noaa.gov).Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.Digital Object Identifier 10.1109/TIP.2016.2535342 thus critically required to make such a sampling approach practical. Toward this end, we have developed techniques that analyze the collected data by automatic object segmentation, size estimation, counting and tracking - [8]. Based on this, an automatic camera-based fisheries survey system can be realized by developing a reliable species identification algorithm that allows for monitoring the species composite and assessing fish stocks as well as the ecosystem.[10]While object recognition in various contexts has been well investigated in image processing and computer vision communities, there exist fundamental challenges to identifying live fish in an unconstrained natural habitat. Like most underwater imagery scenarios, one challenge is posted by the low image quality caused by fast attenuation of light in the water, poor control over illumination, the ubiquitous organic debris, etc. While capturing images for freely-swimming fish, there is a high uncertainty in many of the data due to low image quality, non-lateral fish views or curved body shapes. This seriously degrades the recognition performance since some critical information may be lost. Even without uncertainty, fish share a strong visual correlation among species. Common image features for object recognition are usually not sufficiently discriminative in this case.Another common challenge in applications of statistics or machine learning techniques is the existence of uncertain or missing samples. One strategy to handle this fact is partial classification, i.e., allowing indecision made by the classifier in certain regions in the data space. Partial classification has shown its effectiveness in various practical applications - [11]. However, the importance of rejected instances is gone since no information about the data is retrieved. Besides, there are yet no systematic methods proposed to determine the criteria of decision making. Since objects can be naturally categorized into higher groupings of classes based on either domain knowledge or visual similarity, the recognition algorithm would be favorable by obtaining a hierarchical relation among classes automatically and then providing a coarseto-fine categorization that can retrieve partial information from those uncertain data.[13]In this paper, we propose a novel feature learning and object recognition framework that addresses the challenges described above, as shown in Fig. . One advantage of the proposed framework is that it uses fully unsupervised algorithms to learn the features and class correlation, and thus provides an automatic solution for practical recognition systems. Specifically, the contributions of this paper include: 1) a novel nonrigid part model that represents both appearance and geometric attributes of the fish body; 2) an unsupervised learning algorithm of non-rigid part model based on systematic part initialization and an expectation-maximization-like (EM-like) alternating optimization algorithm; 3) a novel hierarchical partial classification that successfully handles data uncertainty and class imbalance; 4) a formal approach that determines the decision criteria based on an optimization formulation.1The rest of this paper is organized as follows. Section II gives a brief review of the related work. Section III describes the problem formulation. Section IV introduces the unsupervised non-rigid part model learning algorithm. Section V describes the hierarchical partial classification method. Section VI reports the experimental results on fish species recognition, and the conclusion is given in Section VII.", "body": "I MAGE processing and analysis techniques for underwater cameras have drawn increasing attention since they enable a non-extractive and non-lethal approach to fisheries survey M.-C. Chuang and J.-N. Hwang are with the Department of Electrical Engineering, University of Washington, Seattle, WA 98195 USA (e-mail: mengche@uw.edu; hwang@uw.edu).K. Williams is with the Alaska Fisheries Science Center, National Oceanic and Atmospheric Administration, Seattle, WA 98115 USA (e-mail: kresimir.williams@noaa.gov).Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.Digital Object Identifier 10.1109/TIP.2016.2535342 thus critically required to make such a sampling approach practical. Toward this end, we have developed techniques that analyze the collected data by automatic object segmentation, size estimation, counting and tracking While object recognition in various contexts has been well investigated in image processing and computer vision communities, there exist fundamental challenges to identifying live fish in an unconstrained natural habitat. Like most underwater imagery scenarios, one challenge is posted by the low image quality caused by fast attenuation of light in the water, poor control over illumination, the ubiquitous organic debris, etc. While capturing images for freely-swimming fish, there is a high uncertainty in many of the data due to low image quality, non-lateral fish views or curved body shapes. This seriously degrades the recognition performance since some critical information may be lost. Even without uncertainty, fish share a strong visual correlation among species. Common image features for object recognition are usually not sufficiently discriminative in this case.Another common challenge in applications of statistics or machine learning techniques is the existence of uncertain or missing samples. One strategy to handle this fact is partial classification, i.e., allowing indecision made by the classifier in certain regions in the data space. Partial classification has shown its effectiveness in various practical applications In this paper, we propose a novel feature learning and object recognition framework that addresses the challenges described above, as shown in Fig. The rest of this paper is organized as follows. Section II gives a brief review of the related work. Section III describes the problem formulation. Section IV introduces the unsupervised non-rigid part model learning algorithm. Section V describes the hierarchical partial classification method. Section VI reports the experimental results on fish species recognition, and the conclusion is given in Section VII.II. RELATED WORKA. Fish RecognitionLive fish recognition is one of the most crucial elements in camera-based fisheries survey systems On the other hand, unsupervised methods learn informative features directly from the images. Some approaches in this category can be found in the literature of fine-grained object recognition B. Classification With Data UncertaintyTraditional strategies in statistics for handling uncertain data include discarding these samples or performing imputation, where estimates are used to fill in the missing values. Some work integrated the classification formulation with the concept of robust statistics by assuming different noise distributions C. Comparison to Previous WorkThis paper is extended from our previous work on unsupervised feature extraction III. NON-RIGID PART MODELGiven a set of training images I = {I 1 , . . . , I N }, the goal is to discover discriminative features for the objects in terms of their subordinate categories. Let M = {P, X, S} denote a model that consists of part appearance P = {P 1 , . . . , P K }, part center locations X = {X 1 , . . . , X N } and part sizes S = {S 1 , . . . , S N }, where K is the number of object parts. For each image I m we denoteThe location and size of each part is normalized with respect to the image size, i.e., x i ,. The model M is referred to as a non-rigid part model since it describes common parts and allows for deformation in both position and scale. Based on this, the problem of learning such a model can be written as a constrained minimization programming problem:where J (P, X, S) is the objective function of the model. Note that ( A. FitnessImage regions corresponding to a certain object part have high appearance similarity with the model. The fitness cost is thus calculated by the distance between the part appearance P i and the appearance of a rectangular region in image I m defined by the center location x m i and size s m i . We denote this region by I m i , then the fitness cost is given by,where \u03c6(\u2022) denotes the feature descriptor for an image region, and d(P, Q) is the distance between two appearance feature vectors P and Q. For the following part of this paper, we denote I (x m i , s m i ) by I m i for convenience.B. SeparationThe separation cost enforces the parts to cover the maximum area of the whole object. This is achieved by minimizing the total overlapping rate of the image regions defined by the location-size tuples (x m i , s m i ) and (x m j , s m j ) for i = j .The overlapping rate is defined as the area ratio between the intersection and union of two rectangles, i.e.,We denote v(I m i , I m j ) by v m i, j for the following part of this paper for convenience.C. DiscriminationIt is desired that the non-rigid part model covers every representative part of the object. As a result, the discrimination cost is introduced to encourage the maximization of the distance between each pair of part features P i and P j , i.e.,where d is the same distance metric as the one in (4). D. Objective FunctionHaving the above cost functions, the final objective function is written asThe non-rigid part model, i.e., part features, locations and sizes are trained by minimizing (8) over the given training set I using the proposed unsupervised learning algorithm described in the following section.IV. UNSUPERVISED FEATURE LEARNINGNow we have a non-rigid part model -features, locations and size of each part -that represents the object local appearance and configuration, as well as an objective function A. Part InitializationThe effectiveness of alternating optimization guarantees only the convergence to local optima. To ensure a good solution can be obtained, we propose a systematic approach to initialize the part model. Note that most details that distinguish fine-grained categories match those parts which are prominent to humans' perception, such as the beak of a bird, the petals of a flower, or the tail fin of a fish. A saliency operator works perfectly for this purpose.There have been a variety of techniques investigated for estimating image saliency. For the efficiency in dealing with vast data amount, we adopt the phase Fourier transform (PFT) approach described in The saliency is obtained by taking the inverse Fourier transform of only the phase term, i.e.,where G \u03c3 (x, y) is a 2-D Gaussian filter with standard deviation \u03c3 . Non-maximal suppression is applied to extract local maxima from the saliency map. Here we use the object segmentation mask produced by B. Labeling-Based Part AssociationDue to the pose variation, one object part may appear in different locations in two images. To ensure the correctness of learning, it is important to align the extracted points from one image to another. In the proposed method, we formulate part identification as a one-to-one association problem and apply the relaxation labeling process as follows.Suppose a reference set contains K part locations, denoted by Y = {y 1 , . . . , y K }, and a candidate set also contains K part locations X = {x 1 , . . . , x K }. The goal of part identification is to find an optimal association from candidate parts to reference parts, which is similar to the matching problem between two sets of 2-D points that undergo some non-rigid deformation In relaxation labeling, the binary constraint \u03c0 i j \u2208 {0, 1} is relaxed to \u03c0 i j \u2208 [0, 1]. It has been proved that \u03c0 i j converges to either 0 or 1 We define a novel compatibility coefficient that imposes pairwise geometric constraints between two neighboring parts as follows. To handle pose variation among part sets, we first project all part coordinates to the basis formed by the object's two principal components. Denote the projected coordinates of candidate and reference parts by u i and v j , respectively. The associations (u i , v j ) and (u k , v l ) are more compatible if their distance between u i and u k is similar to the distance between v j and v l . Also, if the vector direction from u i to u k is similar to the one from v j to v l , they are more compatible. The distance and angle disparity is thus defined asNote here only the first principal component coordinates are considered to handle the left-right flipping of objects. The final compatibility coefficient betweenBased on this, the support function q i j in each iteration is given byEach entry \u03c0 i j is then updated byAlternated normalization is performed after each relaxation update. It has been shown that this normalization process ensures that each row and column of sum to one according to Sinkhorn's theorem C. Unsupervised Part Model DiscoveryThe non-rigid object part model is learned by solving the optimization problem (1)- 1) Part Localization: In this step, the part features P and sizes S are given. By updating X, we localize the sub-region that corresponds to each part in each image. The discrimination cost term in (7) becomes a constant since P is fixed  Here the mean-shift algorithm where k(\u2022) is the kernel function, n p is the number of pixels in the part and w j is the sample weight at z j :The iteration stops when the magnitude of mean-shift vector2) Part Size Fitting:The goal is to optimize the part size while fixing the appearance P and location X. Same as the part localization step, the discrimination cost term from ( Here we solve for S by adopting the scale-space meanshift algorithm where is the search range in the scale space centered at the current part size s m i (t), H is the scale kernel, n p is the number of pixels inside the current part size, and w(z a ) is the sample weight defined in 3) Part Model Learning: The goal here is to find the optimal part appearance P without changing location X and size S. When optimizing Note that if the discrimination cost term from V. HIERARCHICAL PARTIAL CLASSIFIERTo exploit the information from uncertain data without introducing misclassification, we develop a novel technique that learns a hierarchical structure for the classes and allows for indecision for ambiguous data. A class hierarchy, i.e., a binary decision tree with one classifier at each node, is generated to determine the grouping of classes in higher levels. The grouping labels can serve as coarse categorization results when the exact class label cannot be identified. In the testing phase, the input data instance is examined by layers of classifiers, each of which gives a prediction label. If the instance falls in the indecision range at any layer, the classification procedure stops and returns an incomplete sequence of class labels. In this way, misclassifications are avoided without losing the entire information provided by uncertain data. The concept of hierarchical partial classifier is illustrated in Fig. A. Unsupervised Construction of Class HierarchyThe class hierarchy follows a binary tree structure, i.e., each node separates data into two categories. The arrangement of class grouping is learned by an unsupervised recursive clustering procedure as follows. The EM algorithm for mixture of Gaussians (MoG) is applied to separate all data into two clusters, which can be viewed as \"positive\" and \"negative\" data respectively. For each species, data are relabeled based on which cluster the majority of this species belongs. A radial bases function (RBF) kernel support vector machine (SVM) is trained with these two super-classes. The above steps are then repeated separately within each cluster until there is only one species in each cluster.To handle the class imbalance issue, which is caused by the dominance of one or more species in the sampled habitats, a biased-penalty approach is adopted during the SVM training procedure where C is the original penalty parameter, N + and N \u2212 denote separately the number of positive and negative training samples, and N total = N + + N \u2212 .B. Benefit-Based Partial ClassificationAfter the SVM classifier is trained, one needs to define its indecision criterion in order to enable partial classification. In light of evaluating deferred decisions, our task is formulated as an optimization problem as follows. Given the data (x i , y i ), i = 1, . . . , N, and an SVM decision function f : R d \u2192 R trained by these data, the generalized benefit function of partial classification is defined as The goal is to find D that maximizes where a i := y i f (x i ) and 1[\u2022] denotes the 0-1 indicator function. It can be easily verified, as shown in Fig. Using ( An example of the exponential benefit function with respect to decision threshold is shown in Fig. where VI. EXPERIMENTAL RESULTSA. DatasetsThe proposed method is evaluated on the Fish4Knowledge (F4K) recognition dataset Extending our previous work on video-based fisheries surveys B. Implementation DetailsBefore the analysis, each image is scaled so that the bounding box is no larger than 200 \u00d7 200 pixels with its aspect ratio preserved. The number of parts is empirically determined as K = 6 in the experiments (more discussion in Section VI.E). Each part is initialized with a size of 48 \u00d7 48 pixels in the rescaled images. For part features P, the SIFT descriptors and weighted color histogram is used. SIFT descriptors are sampled densely every 4 pixels within the part region. After extraction, the dimensionality is reduced to 128 by applying the principal component analysis (PCA). The weighted histogram is HSV color histogram weighted by an isotropic kernel that is maximized at the center In addition to localized features, global features are also taken into account during classifier training. The same SIFT descriptors and weighted color histogram are extracted from the entire bounding box. The global features and local features from each part are concatenated to form the feature vector for one image.Parameters in the unsupervised non-rigid part model learning algorithm are set empirically as follows. For the stopping criteria in Algorithm 1, we set the convergence of object part models as K i=1 P i \u2264 0.5. The maximum iteration is set to 15 as we observed that the algorithm converges within 10 iterations in most cases. The part initialization algorithm is not very dependent on the Gaussian standard deviation \u03c3 . A typical value such as 0.5 or 1 (i.e., a 3 \u00d7 3 or 5 \u00d7 5 filter window) works in most cases. The convergence criteria for mean-shift update in part localization is set to \u03b5 x = 0.5. As for part size fitting, the coordinate base is set to b = 5 \u221a 2 and the search space as \u22122 \u2264 \u03c9 \u2264 2. The choices for parameters in our implementation depend highly on the characteristics of images, correlation between training and testing samples, as well as the computational consumption. To achieve better accuracy in feature descriptors, an object part is usually initialized with a square with each side at least 0.2 of the longest side of bounding box. Also the convergence criterion for training the non-rigid part model should be no more than 0.1 of the average norm of part feature vectors times the number of parts. In our experiments the parameters for part size fitting follows the work on scale-space mean-shift algorithm The method for solving In our experiments, we measure the distance metric between feature descriptors P and Q by the normalized correlation function, i.e., d(P, Q) = 1 \u2212 p T q, where p = P/ P and q = Q/ Q . Hence where p i = P i / P i and i m i = \u03c6(I m i )/ \u03c6(I m i ) . We use the LIBSVM C. Feature LearningIn this experiment, the proposed non-rigid part model learning algorithm is compared with several feature descriptors or learning methods. The template model The accuracy rates is shown in Table A visualization of some fish parts discovered by the proposed algorithm are shown in Fig. D. Hierarchical Partial ClassificationIn this experiment, the proposed hierarchical partial classifier algorithm is evaluated with both NOAA Fisheries and Fish4Knowledge datasets. The proposed algorithm is compared with several baseline algorithms including Flat SVM, principal component analysis (PCA-Flat SVM), taxonomy tree, BEOTR Results of Fish4Knowledge and NOAA Fisheries dataset are shown respectively in Table The proposed algorithm achieves an average F 1 -score of 93.4% and demonstrates consistency among fish species, as shown in Fig. E. DiscussionThe proposed non-rigid part learning and partial classification framework builds up an error-resilient object recognition system that is applicable \"in the field,\" where most of the images are noisy and with substantial degree of ambiguity. Low contrast in underwater imagery, for example, weakens the edges and texture and thus introduces error to part appearance. Pose variation gives a different view of the part and results in deviation when describing part size and appearance. Features with such error or deviation can be regarded as \"outliers\" from its species and are likely to fall on the opposite side of the SVM decision boundary, as shown by the white instances in Fig. In many cases, the performance of unsupervised learning algorithms depends highly on how well the variables are initialized. For the proposed non-rigid part model, one can decide the number of parts to be learned by the part model. This factor not only affects the power of discrimination but To investigate into this, the proposed algorithm is tested with different numbers of parts. As shown in Table As for classification, the proposed hierarchical partial classifier is able to handle uncertain or missing data, which is a common and challenging issue in practical applications of object recognition. For example, capturing images for freely-swimming fish in an unconstrained environment usually introduces a high uncertainty in many of the data due to poor capture quality and non-lateral fish. To see the effectiveness of handling uncertain data by partial classification, we compare the performance using a flat classifier, a hierarchical full classifier and the proposed method. The flat classifier is a multi-class one-against-all SVM, which classifies objects to all classes simultaneously. The hierarchical full classifier follows the proposed method from learning the class hierarchy to training every SVM, except the decision threshold is set to zero so that all instances are classified down to the lowest layer. The accuracy and partial decision rate (percentage of data not being classified to the lowest level) of the proposed algorithm are shown in Table VII. CONCLUSION", "conclusions": "In this paper, a novel framework for underwater fish recognition is proposed. The proposed framework is facilitated by unsupervised learning algorithms and thus reduces the requirement of human interference comparing to existing approaches. The non-rigid part model effectively discovers discriminative parts by adopting saliency and relaxation labeling. Fitness, separation and discrimination of parts are considered for finding meaningful representations of fish body parts in a fully unsupervised fashion. On the other hand, data uncertainty and class imbalance are two of the most common issues in practical classification applications. The proposed hierarchical partial classification successfully handled these issues by enabling coarse-to-find categorization and thus retrieving partial information from those ambiguous data which are possibly misclassified or rejected by other algorithms. We further develop a systematic optimization approach to selecting decision criteria for partial classifiers by introducing the exponential benefit function. Experimental results show a favorable performance of fish recognition on both large-scale public dataset and practical highly-uncertain dataset of live fish. Future work includes investigation of how to make stereo imaging and object part matching helpful to each other and conducting more tests of the proposed algorithm with different types of objects.", "SDG": [14]}, "bayesian_belief_network_model_for_community_based_coastal_resource_management_in_the_kei_islands_indonesia": {"name": "A Bayesian belief network model for community-based coastal resource management in the Kei Islands, Indonesia", "abstract": " Understanding the specific relationships between ecological and socioeconomic conditions and marine tenure is likely to contribute to successful functioning of self-governance institutions for common-pool resources. Complex interrelationships of factors influencing fishing activities of coastal communities and implementation of customary marine tenure over their waters can be represented in a Bayesian belief network model. We developed a Bayesian belief network model that includes the links between factors for fishing communities in the Kei Islands in Indonesia, based on indepth local surveys. Our results showed that the cumulative impacts of multiple factors on key social, economic, and environmental outcomes can be much larger than the impact from a single source, which implies that management or policy intervention could be more effective when addressing multiple factors simultaneously. The local community's perception of fish stock abundance trends was the single most important factor influencing social, economic, and environmental outcomes of their community-based management system. The frequency of which outsiders were sighted in territorial waters was strongly (negatively) linked to weak or strong implementation of a customary tenure (Sasi) and the occurrence of intervillage and intravillage conflict. Ecological variables also drive these conflicts, which illustrates the close connection between ecological and social outcomes, and the importance of considering social-ecological systems as a whole.", "keywords": "Bayesian belief network,community-based management,customary marine tenure,Indonesia,small-scale fisheries,socialecological systems", "introduction": "Marine and coastal resources in many parts of the world have been managed traditionally by community-based governance systems that involve ownerships or property rights, commonly referred to as customary marine tenure (CMT). Customary marine tenure provides a vehicle by which state agencies and customary stakeholders may work in partnership to share authority of, and responsibility for, resource management, in what is termed cooperative management or comanagement . Customary tenure regimes are the foundation of marine governance in much of the Pacific and have been documented throughout the world (Cooke et al. 2000)(Cinner 2005. In response to degradation of many inshore marine resources, CMTs and traditional community-based resource management have attracted great attention as cost-effective, decentralized ways of managing coastal fisheries , Aswani et al. 2013)(Hviding 1998. The viability of conservation strategies built on a foundation of marine tenure, however, remains ambiguous, as it is unclear whether marine tenure systems can withstand the profound economic development that much of the coastal states in the developing world are facing. A large body of literature has attempted to identify conditions that are important to successful functioning of self-governance institutions for common-pool resources , Ruddle 1998)(Cox et al. 2010. Studies specific to CMT have suggested that social and economic factors, such as poverty, dependency on resources, and human population size, affect the nature and functioning of marine tenure; however, specific relationships between socioeconomic conditions and marine tenure are still not well understood , Pollnac et al. 2010)(Cinner 2005). There has been little discussion that specifically focuses on interdependencies between factors that affect marine resource management under a CMT arrangement., Cinner et al. 2012bIn this article, we develop a Bayesian belief network (BBN) model that represents complex interrelationships of factors that influence small-scale fishing communities and implementation of customary management over their waters, and demonstrate the usefulness of the tool to identify the relative importance of factors to achieve social, economic, and environmental objectives. Bayesian belief networks are graphical models that represent a set of variables connected by directed, acyclic graphs which can be used to explore and display causal relationships between factors and assess the influences of each input variable on the output variables based on Bayesian principles. Bayesian belief networks have the ability to incorporate different types and sources of data, such as quantitative data, expert or local knowledge, and outputs from other models, and are capable of dealing with missing or incomplete data . Such features make them well suited for small-scale fisheries where availability of quantitative data is often limited. A number of BBNs have been developed to aid natural resource management decisions, including fisheries (Korb and Nicholson 2011)(Varis and Kuikka 1997, Kuikka et al. 1999, Little et al. 2004, Haapasaari and Karjalainen 2010, Levontin et al. 2011. The importance of integrating biological, economic, and sociological information into fisheries management plans was illustrated by , van Putten et al. 2013), who highlighted the link between commitment and management success. Most of the existing applications of BBNs are in industrial-scale fisheries, with limited application in small-scale fisheries. One notable http://www.ecologyandsociety.org/vol21/iss2/art16/ exception is van Levontin et al. (2011), who applied a BBN to investigate the factors affecting the participation of indigenous fishers under key economic and socio-cultural drivers, such as presence or absence of a government employment program. To the best of our knowledge, BBN has not been applied to assess the factors affecting small-scale fisheries under a CMT or comanagement setting.Putten et al. (2013)The Kei Islands, which are part of the Southeast Maluku Province in Indonesia, have well-established tenure systems where coastal resources have been communally owned and managed by Sasi, a set of traditional laws governing natural resources uses, including the spatial and temporal closure of fields, forests, reefs, and fishing grounds . Our aim is to develop a BBN for the Kei Islands to gain understanding of the nature of interrelationships between various factors that influence smallscale fisheries, whose fishery resource is managed under a CMT regime (Sasi, in this case), and livelihoods of coastal communities; i.e., the social, economic, and environmental outcomes of their fishing activities. The model developed can provide a practical tool to guide resource managers and policy-makers to identify priorities and/or points of intervention for sustainable coastal resource management.(Thorburn 2001)", "body": "Marine and coastal resources in many parts of the world have been managed traditionally by community-based governance systems that involve ownerships or property rights, commonly referred to as customary marine tenure (CMT). Customary marine tenure provides a vehicle by which state agencies and customary stakeholders may work in partnership to share authority of, and responsibility for, resource management, in what is termed cooperative management or comanagement In this article, we develop a Bayesian belief network (BBN) model that represents complex interrelationships of factors that influence small-scale fishing communities and implementation of customary management over their waters, and demonstrate the usefulness of the tool to identify the relative importance of factors to achieve social, economic, and environmental objectives. Bayesian belief networks are graphical models that represent a set of variables connected by directed, acyclic graphs which can be used to explore and display causal relationships between factors and assess the influences of each input variable on the output variables based on Bayesian principles. Bayesian belief networks have the ability to incorporate different types and sources of data, such as quantitative data, expert or local knowledge, and outputs from other models, and are capable of dealing with missing or incomplete data The Kei Islands, which are part of the Southeast Maluku Province in Indonesia, have well-established tenure systems where coastal resources have been communally owned and managed by Sasi, a set of traditional laws governing natural resources uses, including the spatial and temporal closure of fields, forests, reefs, and fishing grounds METHODSStudy siteIndonesia is the largest archipelago country in the world, with a population of approximately 250 million people. The study site is in the Kei Islands, a group of islands in the southeast part of Maluku in Indonesia (Fig. Fig. 1.General location of the Kei Islands (main islands) and study sites at the subdistrict level. Source: Southeast Maluku Regional Development Planning Agency The people of the Kei Islands come from diverse ethnic and religious backgrounds. Kei's local economy is a mixture of subsistence agriculture and fishing, with copra (dried coconuts) and topshell (Trochus niloticus, called Lola by locals) exports providing cash income. Other marine-based small-scale industries, such as seaweed and pearl cultivation, also provide important livelihoods. The Kei Islands are known as the center of cultural and marine tourism in the Maluku Province, as Kei society maintains many cultural traditions, and tropical coral reefs are located around the islands. Fishing activities around the Kei Islands are mostly artisanal. The most common species caught in the area include small tuna (komu), coral cod (kerapu), mackerel (kawalinya), scad (momar), bluefin trevally (bubara), ornate emperor (sikuda), and anchovy (puri).For a long time, daily activities of the people in Kei have been organized by a system of traditional rules that is called \"Larwul Ngabal.\" One aspect of these traditional rules is Sasi. The goals of implementing Sasi in Kei are: (1) to manage harmony in social relationship, particularly between women and men; and (2) to manage natural resources and the environment. However, particularly since the 2000 decentralization era, there has been misinterpretation of Sasi by village leaders or certain communal groups driven by their own social, cultural, economic, and political motivation (based on the interviews in this study and in Adhuri 2013).Rules for marine Sasi differ in the various villages in the Kei Islands. Some villages enact Sasi on the intertidal zone (meti), alternately closing portions of the coastal zone to allow reef organisms to replenish, while some villages in Kei close their entire meti for several weeks prior to the annual neap tide that occurs in September and October The heads of the villages typically lead the implementation of Sasi in their village, while the task of monitoring Sasi implementation relies on all village members. Breaking Sasi rules should be reported by village members to the head of their village. Those who break Sasi rule, or those who notice another person breaking the rules but who do not report the case, are punished by the head of village according to local customary rules. More examples of Sasi practices in the Kei Islands and Maluku Province are provides in SurveyHousehold and village leader surveys were conducted in four fishery villages in two subdistricts-Kei Kecil and Dullah Utara -between November and December 2013. Prior to the surveys, site visits were carried out by the research team members in order to identify main stakeholder groups in the region, as well as to develop and pretest the survey questionnaire in consultation with local experts in fisheries and Sasi. The experts included key personnel at the local technical/research institution (Tual State http://www.ecologyandsociety.org/vol21/iss2/art16/ Fisheries Politechnic), researchers at the University of Pattimura in Ambon, and local government officers responsible for fisheries. A total of 296 randomly selected households were interviewed using a structured questionnaire. The interviews focused on individuals in a household who spent most of their time on fishing activities (\"main fisher\"). The selected villages were known to have a high concentration of fishers, and were located close to the coast. The questionnaire consisted of seven sections. The first three sections focused on the characteristics of the fishing operation, such as years of fishing operation, average catch per trip over the last 12 months, fishing gears used and ownership of the gears, number of paid employees on board, cost of fishing, and whether the fishers felt that the catch amount had changed during the last 12 months. The next two sections of the survey contained a series of questions about fisheries management, including customary management practices (Sasi laut). Examples of questions included the following: \"Is there an area where fishing is temporarily prohibited as a practice of Sasi laut in your fishing area?\", \"In which months is fishing prohibited in your area?\", \"Which species are prohibited to be caught under the rules of Sasi laut?\", and \"Who needs to comply with the rules of Sasi laut?.\"The last two sections of the survey were comprised of questions concerning perceived states of their fishing grounds, including habitat condition or quality and current fish abundance trends, and questions regarding social structures, such as whether the respondent maintains a good relationship with other fishers, village or cultural leaders, and government fishery officers, and whether they are engaged in conflicts. Some demographic information was also collected in this section of the questionnaire.The interviews were undertaken by local researchers from the University of Pattimura, all of whom attended training and information sessions. Demographic information for survey respondents, and fishing operation details (e.g., fishing gear used) for the sampled villages are summarized in Table Bayesian belief network modelThe BBN modeling procedure consists of three steps: (1) identify the nodes representing system variables, such as those related to the environment (e.g., habitat quality and fish abundance trends), external stressors (e.g., presence of agriculture and population growth), and management systems (e.g., Sasi strength, and the presence of area/seasonal closures). Each variable can either be discrete or continuous and has a finite set of mutually exclusive states (e.g., high/low, present/absent); (2) create links representing causal relationships between these nodes (from parent node to child node; i.e., from cause to effect); and (3) generate probabilities attached to each node to quantify the relationships between connected links. The BBN we developed is based on multiple data sources, where key variables and causal links between different variables were identified through literature reviews, household and village leader surveys, and expert knowledge. Netica software was used to build the BBN and provide a sensitivity analysis by calculating the entropy and variance reduction scores to identify those parts of the model that most affect the output variables.In the BBN, causal links represent the relationship between nodes using conditional probability tables (CPTs). For example, high fish abundance will increase the likelihood of catch per unit effort being high. These causal relationships influence the likelihood of outcome states of the variables of interest depicted in the BBN where P(F) is the probability of selecting female (0.49 in this example), P(B|M) is the probability of getting someone who owns a boat, given that the person is a male (0.095), and P(B|F) is the probability of getting someone who owns a boat, given that the person is a female (0.017). After learning that the subject owns a boat, the probability is revised to be 0.853.When there was insufficient information on the probability distribution of events to populate CPTs from survey data, the CPTs in our research were either generated based on expert judgment (e.g., discussion with local scholars, fisheries managers, and village leaders), where feasible, or were given very simple scenarios (e.g., low/high) that were assigned equal probabilities (termed uninformative prior) when uncertainty was high. In the cases of expert judgment, a range of plausible alternative values was considered, and the resulting posterior probability distribution was also given a range (Table One notable restriction of applying a standard BBN to socialecological systems is that it does not allow for feedback loops to be represented in the model due to the mathematical properties of the joint probability distribution. This limitation can be overcome by including dynamic variables that represent two or more points in time that indirectly account for feedback between the current states and the future Three performance indicators representing economic, social, and environmental objectives of Sasi laut were considered: CPUE, conflicts, and future fish abundance, respectively. For each indicator, we considered scenarios where the values of major linked variables were changed to the extreme; for instance, changing the perception of habitat quality being damaged from 43.7% (base case) to 100% to see how these changes affected the indicator values. Since our main interest was on the influence on small-scale operators, the scenario analysis related to CPUE was examined for small-scale operators with up to four people on board (covering 67.8% of survey respondents). CPUE is generally much higher for pelagic species than for coastal species, and because our main interest was the small-scale operators and because Sasi is applied to nearshore coastal areas, the catch of the six most common pelagic species was excluded from CPUE. The excluded species were anchovies (puri), bigeye scad (kawalinya), small-tuna (koma), sardine (tembang), scad mackerel (momar), and Indian mackerel (lemma). A large proportion of these pelagic species is caught in water greater than 100 m in depth, although anchovies are caught mainly in relatively shallow water of about 10-15 m depth. Examples of coastal species included in CPUE are bluefin trevally (bubara), ornate emperor (sikuda), grouper or coral cod (kerapu), squid (cumi), snapper-like species (kakap merah, tenggiri, baronang), parrot fish (kakakutua), and small reef fishes, such as fusilier (lalosi).A sensitivity analysis was carried out to evaluate the influence of each node on the social (conflict), economic (CPUE), and environmental (future fish abundance) indicators for each subdistrict. A sensitivity analysis allows the effective determination of how much a finding at one node in the BBN will likely change the probability outcome of another node A common technique for validating BBNs is simply to ask the experts whether they agree with the model structure, discretization, and parameterization Another common validating test is to use sensitivity analysis (entropy/variance reduction scores). We used both sensitivity analyses and discussion with local scholars to explore alternative model structure. In order to test how sensitive the resulting posterior distributions of the key network variables were to the underlying assumptions made in the fixed (prior) distributions of the network, sensitivity tests were undertaken by altering the values of the priors between a low and high value. The resulting posterior probability was also given as a range in these cases. A workshop involving key stakeholder groups was organized in November 2015 to present our model results and obtain feedback to further improve/validate our model, although at the time of writing, such a meeting was still pending and it was not possible to include additional feedback in the results. The present model can benefit from inputs from additional stakeholder meetings.RESULTSThe variable names selected in the full BBN, and their base case states generated from the model, are shown in Table indicators and their entropy/variance reduction scores. As a result, they were excluded from the full model. The final BBN contained 34 nodes and 55 links, which generated 2827 conditional probabilities. The scenarios that tested the influence of changes in key variables on the performance indicators values are given in Table CPUE is an important indicator of economic productivity, as high CPUE suggests more fish are caught given the same amount of input. CPUE for coastal species ranges widely from zero to 920 kg/trip, with median 70 kg/trip and mean 138 kg/trip. Low CPUE in this study refers to catch less than 100 kg/trip based on the mean value as well as examples from similar tropical reef fisheries in southeast Asia One of the key social objectives of customary tenure systems and traditional comanagement is to reduce the number of conflicts probability of intervillage conflict (conflicts with other villagers) increased from 30.1% to 40.3% with lower catch trend, and from 30.1% to 33.0% with lower fish abundance trend. When both catch and abundance trends were worse, the magnitude of the impact on the presence of conflict was much larger, with 29.2% reduction in intravillage conflict, and 12.6% increase in intervillage conflict.Drawing on existing literature, we assumed that the states of future fish abundance, a key environmental objective of CMT, depended on the current fish abundance trend, habitat quality, and fishing pressure, which can be affected by the cost of fishing, availability of alternative employment, population growth, and implementation of CMT. The household survey results suggested that most respondents acknowledged the existence of Sasi rules, with 86.3% acknowledging the existence of temporal sea area closure, seasonal closure of approximately 1 month (on average), and protection of at least one species (in terms of harvest restrictions). Despite the high level of acknowledgement of Sasi rule existence, 92.3% of respondents felt that Sasi has weakened or totally disappeared in their village (Table Sensitivity analysesThe results of sensitivity analyses (Table Factors most influential to intervillage conflicts were perceived catch trend (whether catch is increasing/decreasing or unchanged), sighting of outsiders fishing in the respondents' territorial waters, perceived fish abundance, modernization of fishing gear, and presence of intravillage conflicts in both subdistricts, although their ranks were slightly different in each subdistrict. Tourism, perceived fish abundance trend, catch trend, and sighting of outsiders were influential in terms of intravillage conflicts in both subdistricts. The influence of relationships with village leaders affected the probability of intravillage conflicts more in Dullah Utara, while the relationships with fishery officers affected it more in Kei Kecil.In both subdistricts, fishing pressure was by far the most influential to the probability of future fish abundance, followed by perceived habitat quality, perceived current fish abundance trend, availability of alternative jobs, and Sasi implementation of http://www.ecologyandsociety.org/vol21/iss2/art16/ management rules. This suggests that a combination of measures to reduce fishing pressure may be effective in achieving higher future fish abundance. Other lesser but important factors (ranked 6 th -10 th ) to future fish abundance were population growth, cost of fishing, CPUE, enforcement, and sea area closure.Perceived fish abundance trend was listed as among the most important (top five) factors for all three indicators. This indicates that local knowledge about current fish abundance is a key driver of fishing activities that affect the successful outcome of achieving all social, economic, and environment objectives. Presence of outsiders fishing in the respondents' territorial water was another key driver that affected both CPUE and conflicts directly, and future fish abundance indirectly, since the frequency of sighting outsiders was linked to Sasi implementation.DISCUSSIONIt is well established that there are complex relationships, interactions, and feedbacks between factors that influence fishing activities Understanding exactly how the social, economic, and environmental outcomes of customary governance systems are influenced by different human and natural drivers is key to continued, and potentially improved, customary management. A first step to either reinstating or reinvigorating customary management of marine resources is to understand the relationships between factors that influence fishing activities in coastal communities and how these factors affect the economic, social, and environmental outcomes of customary governance systems. The Kei Islands in Indonesia have long-established tenure systems where coastal resources have been communally owned and managed by traditional laws (Sasi) governing natural resources Our results show that the cumulative impacts of multiple factors on key social, economic, and environmental outcomes can be much larger than the impact from a single source. For instance, the probability of a low CPUE for small-scale operators can be reduced much more when combining improved habitat quality, upward stock abundance trend, and absence of the sighting of outsiders fishing in their water (from 52% to 28%) rather than only improving habitat quality (from 52% to 41%). This implies that management or policy intervention could be more effective when addressing multiple factors simultaneously.The local community's perception of the upward or downward trend of fish abundance is a key factor influencing social, economic, and environmental outcomes of Kei's CMT. Perceptions are distinct from actual observed trends and might in fact be at odds with trends. For instance, if local communities are mis-or under-informed, they might believe fish abundance is declining, where scientific measurements or monitoring might indicate it is increasing (or the reverse). Previous studies in Fiji and the Solomon Islands suggest that indigenous ecological knowledge about the health of coral reels and fish stocks is not always supported by quantitative studies Perceptions of local people with respect to Sasi strength and catch trends were also important in meeting economic and social objectives, respectively. Perceptions can be changed, for instance, by improving the visibility of vessels monitoring and patrolling an area, which can in turn change the perception of Sasi strength and thus improve social outcomes for this CMT. Our results suggest that strengthening of Sasi alone has the most prominent impact on improving the economic (CPUE) indicator, which supports Our results also suggest that sighting outsider presence and outsider fishing activity in territorial water is another key driver that directly affects economic and social outcomes, and indirectly affects environmental outcomes. The frequency of which outsiders are sighted in territorial waters is strongly linked to the perceived weakness or strength of Sasi implementation, although results were found to be in opposing directions for intravillage (negatively affected by stronger Sasi) and intervillage conflicts (positively affected). The absence of outsiders can mean that stronger exclusive user rights are in place (i.e., due to stronger Sasi), which can in turn increase intervillage conflicts, while at the same time reducing intravillage conflicts. This can create complexity, as one of the key social objectives of customary tenure systems and traditional comanagement is to reduce conflicts, and lower levels of conflict suggest greater social capital, which is considered integral to successful common-pool fishery management.Similarly, both lower perceived catch and fish abundance trends reduced the probability of intravillage conflicts slightly where the probability of intervillage conflicts increased (Table Our results (Table Comanagement approaches and traditional customary tenure systems are highly vulnerable to discontinuities and asymmetries developing between the social, economic, and ecological system Interviews undertaken as part of this project revealed that there was a high level of local knowledge of Sasi and the existing governance rules. However, this intimate local knowledge of Sasi did not translate into general acceptance of the Sasi system in the sense that a high number of people believed the rules were unsuccessful and often not observed by marine resource users. This may at first glance seem problematic, but the high level of http://www.ecologyandsociety.org/vol21/iss2/art16/ knowledge and at the same time the large number of disaffected people may in fact provide an opportunity for improving the functioning of Sasi in the future. A recent study conducted in Ache Province of Indonesia suggests that people who had positive views of their customary management systems were generally more wealthy and had higher fortnightly expenditure CONCLUSION", "conclusions": "The core of a successful customary management system lies in the achievement of different environmental and socioeconomic objectives. For a coastal community with a CMT system to achieve its different objectives, the drivers that influence fishing behavior have to be well understood. A BBN model based on empirical data can be used to represent different factors that drive fishing activity and to assess the influence and impact of changing social, economic, and environmental conditions on meeting the community's CMT objectives.Bayesian belief network models are particularly useful in highlighting current information and knowledge gaps that are important for decision-makers, but as additional or more accurate resource user or community information becomes available, models can be further developed and expanded. Impacts following changes in fishery management, while accounting for uncertainties regarding costs and benefits of management decisions (e.g., effectiveness of introducing a monitoring program), can be trailed using these flexible models. Extended models are useful as decision support tools for resource management, as interactions between variables and the relative influence of variables on final management outcomes can be easily explored.In this study, we found that the current lack of empirical information available to quantify the level of fishing pressure, which is a key variable in its impact on future stock abundance, needs immediate attention to ensure that CMT objectives will be met in the foreseeable future. Aside from accurate fishing pressure information, the continued existence and potential future creation of CMTs to manage marine resources in coastal communities that depend on these resources for livelihoods and food security relies on government programs that carefully consider local cultural values. Ensuring congruence with local values and perceptions of CMT effectiveness, for instance through monitoring and patrolling, will likely determine the longevity of these management systems. Strong local support for the CMT is a key driver of lasting success. Aside from influencing factors that directly affect cultural and social perceptions of the CMT, governments also have to positively influence indirect factors such as job creation and reducing local poverty.", "SDG": [14]}, "compromises_between_international_habitat_conservation_guidelines_and_small_scale_fisheries_in_pacific_island_countries": {"name": "Compromises between international habitat conservation guidelines and small-scale fisheries in Pacific island countries", "abstract": " Wallis, Alofi, and Futuna are three small islands in the central Pacific Ocean, characterized by different reef geomorphologies. Following a request from the local Environment Service, we developed an indicative conservation plan for each island with two objectives: (1) representing 20% of the extent of each coral reef habitat within no-take areas while (2) keeping all subsistence fishing grounds open for extraction. The first objective was more ambitious than the current Convention on Biological Diversity (Aichi) targets. We found that both objectives could not be achieved simultaneously and that large compromises are needed. Due to the small size of these islands, and the dependence of local communities on coral reef resources, the fishery objective significantly limited the extent of most habitats available for conservation. The problem is exacerbated if the conservation plan uses larger conservation units and more complex habitat typologies. Our results indicate that international conservation guidelines should be carefully adapted to small Pacific islands and that incentives to make feasible the necessary reductions in available fishing grounds will probably be needed.", "keywords": "Conservation planning,marine protected areas,reef fisheries,habitat maps,Pacific islands,CBD,conservation targets,Millennium Coral Reef Mapping Project", "introduction": "As a response to increasing global and local threats to marine and coastal ecosystems, a worldwide system of plans of action with ambitious conservation guidelines has been established by the international community (Butchart et al. 2010;. These guidelines typically target percentage representation of marine and coastal habitats within marine-protected areas (MPAs), involving various levels of restrictions on extractive uses within the seven International Union for Conservation of Nature (IUCN)-protected area management categories Wabnitz et al. 2010)(IUCN 2008;. If reached successfully, these representation objectives are expected to help protect habitats, promote the viability of species, and ensure long-term and sustainable benefits to fisheries, thus sustaining economies and livelihoods as well as biodiversity.WCPA)In 2003, participants in the Cross-cutting Theme on Marine Issues at the 5th IUCN World Parks Congress in Durban, South Africa, called on the international community to include in networks of MPAs at least 20-30% of each marine habitat by 2012 . Since then, targets such as these have been identified for countries or whole regions. Regional action plans target protection in no-take areas of at least 20% of habitats associated with coral reefs (IUCN World Parks Congress 2005), or effective conservation of at least 30% of near-shore marine resources by 2020 (Coral Triangle Initiative 2008). The US national conservation strategy aims at protecting at least 20% of all coral reefs and associated habitats in each major island group and Florida (The Micronesia Challenge 2006). In Choiseul, in Solomon Islands, the local conservation strategy aims to protect 10% of the original extent of each terrestrial and marine ecosystem (United States Coral Reef Task Force 2000).(Lipsett-Moore et al. 2010)In 2010, based on the last meeting of the Conference of the Parties to the Convention on Biological Diversity in Nagoya, Japan, the previous 20-30% global targets were revised and the new objective is to protect \"10% of coastal and marine areas [. . .] through  systems of protected areas and other effective area-based conservation measures\" by 2020 (UNEP/CBD/COP/10/X/2 2010). Yet, there is increasing recognition that (1) percentages larger than 10% are likely needed in the longer term for effective conservation [. . .](Rodrigues & Gaston 2001;Svancara et al. 2005;, and (2) failing to frame targets in terms of individual habitats (e.g., rezoning of the Great Barrier Reef Marine Park, Gaines et al. 2010)) is likely to strongly bias conservation to habitats easiest to protect and perhaps least in need of protection, as demonstrated widely on land Fernandes et al. 2009.(Scott et al. 2001)Although quantitative conservation objectives are a foundation of systematic conservation planning  and a common tool in policy, many conservation plans based on such targets are either infeasible or ineffective in the short term (Margules & Pressey 2000)(Agardy et al. 2003;Mace et al. 2010;. Important reasons include limited funds, inadequate biological or socioeconomic data, and insufficient areas available for conservation. The question of the ecological relevance of targets and general conservation guidelines is being debated within the scientific community Wood 2011) but the difficulty of complying with these recommendations is not yet well-addressed.(Carwardine et al. 2009)How achievable are these global conservation objectives in regions such as in developing Pacific Islands, where people depend heavily on marine habitats and associated resources for day-to-day survival (Dalzell et al. 1996;Bell et al. 2009;)? Because of their small sizes and the dependence of local communities on coastal resources, these countries might be unable to balance food security with the closure of near-shore fisheries for protection of habitats. Finding areas available for conservation that are not used for resource extraction can be difficult in these countries and, if fishing grounds are to be closed, there is often limited scope for compensating resource users or finding them alternative livelihoods Govan et al. 2009. Consequently, tensions are likely in Pacific Island countries between resource users and proponents of conservation (Govan et al. 2009).(Hviding 2006)In this article, we demonstrate the conflict between conservation objectives based on the previous mid to upper range of international conservation guidelines and small-scale fisheries objectives for the three Pacific islands of Wallis, Alofi and Futuna, a French overseas territory where coral reef habitats are mainly exploited for subsistence. We focus on a conservation objective of 20% of each marine habitat for two reasons. First, we believe that the present 11th Aichi target (10%) will be seen as an underestimate of required protection of near-shore marine environments (Allison et al. 2003;. Second, we note that some jurisdictions have specified larger percentages (20-30, or even greater) of near-shore marine habitats to be protected Botsford et al. 2003)(Airame et al. 2003;Fernandes et al. 2009;. Specifically, we show the spatial extent of the trade-off between sets of objectives for conservation and fisheries. In this case, we considered the best protection option for marine habitats and associated resources (i.e., the implementation of no-take zones) and the best-case short-term scenario for fisheries (i.e., no restriction on take in current fishing grounds). Different sizes of potential no-take zones and different habitat data were tested to understand their influence on the trade-offs between conservation and fisheries objectives.Mills et al. 2011)", "body": "As a response to increasing global and local threats to marine and coastal ecosystems, a worldwide system of plans of action with ambitious conservation guidelines has been established by the international community In 2003, participants in the Cross-cutting Theme on Marine Issues at the 5th IUCN World Parks Congress in Durban, South Africa, called on the international community to include in networks of MPAs at least 20-30% of each marine habitat by 2012 In 2010, based on the last meeting of the Conference of the Parties to the Convention on Biological Diversity in Nagoya, Japan, the previous 20-30% global targets were revised and the new objective is to protect \"10% of coastal and marine areas [. . .] through Although quantitative conservation objectives are a foundation of systematic conservation planning How achievable are these global conservation objectives in regions such as in developing Pacific Islands, where people depend heavily on marine habitats and associated resources for day-to-day survival In this article, we demonstrate the conflict between conservation objectives based on the previous mid to upper range of international conservation guidelines and small-scale fisheries objectives for the three Pacific islands of Wallis, Alofi and Futuna, a French overseas territory where coral reef habitats are mainly exploited for subsistence. We focus on a conservation objective of 20% of each marine habitat for two reasons. First, we believe that the present 11th Aichi target (10%) will be seen as an underestimate of required protection of near-shore marine environments Materials and methodsStudy sites and contextWallis, Alofi, and Futuna are three Polynesian high islands belonging to the French Territory of Wallis and Futuna Islands, in the central South Pacific (Figures Because tourism is not well developed, demand for reef fish is confined to local communities, who exploit the reefs and lagoons for their own subsistence. Almost a third of the population practises artisanal small-scale fishing in Wallis, Alofi and Futuna, mostly with nets and speargun Thus far, there are no MPAs in Wallis, Alofi, and Futuna that have been integrated into a territorial management plan Objectives for the indicative conservation plan for this studyAn indicative conservation plan was set up for the three islands to reflect objectives of the Territorial Environment Service, with two sets of objectives based respectively on international habitat conservation guidelines and local fishery requirements. Wallis managers initially considered the guidelines of the previous 2002-2010 CBD Strategic Plan, which targeted a mid-to long-term protection of 20-30% of each habitat Here, the fishery objective was intended to avoid conflicts with local communities highly dependent on reef habitats both culturally and economically. It prevented the main current fishing grounds for nets and spearguns from being identified as no-take areas. The data and analyses below allowed an assessment of the extent to which these potentially conflicting objectives can be reconciled.Coral reef habitat mapsTwo types of habitat maps with two levels of detail were used (see detailed methods in Andr \u00e9fou \u00ebt & Dirberg 2006). First, a high-resolution geomorphic habitat map (hereafter the \"geomorphic\" map) of the three islands was derived from the Millennium Coral Reef Mapping Project Fishing grounds dataIn 2006, an environmental study was conducted in Wallis, Alofi, and Futuna to initiate the Plan de Gestion des Espaces Maritimes Conservation unitsEach island of interest was partitioned into manageable conservation units by superimposing a grid of square cells on all areas containing coral reef habitats on the maps. To understand whether the size of conservation units would affect the feasibility of reconciling conservation and fishery objectives, all analyses were conducted for two different sizes of conservation units (500 \u00d7 500 m and 200 \u00d7 200 m, hereafter \"large\" and \"small\", respectively).Reef area left available for conservationThe set of possible no-take areas or areas left for conservation considered in these analyses initially excluded all fished units. For each combination of island/habitat map/size of conservation unit, we first measured the proportion of conservation units available for conservation when all fishing areas were left open. Within all conservation units (fished, and unfished), we computed the number of habitats (excluding land features) and their extent, and the occurrence of net or speargun fishing. Then we measured the proportion of habitats that could potentially meet the 20% objective within unfished conservation units. All data were analysed using ESRI R ArcMap TM 10.0 and R (R Development Core Team 2008).Trade-off between objectives for habitat conservation and small-scale fisheriesConservation objectives were fully achieved when 100% of habitats could meet the 20% objective. Full achievement of the fishery objective meant that 100% of initial fished units were available for harvest. We measured the actual extent to which conservation objectives must be compromised to fully achieve the fishery objective, and vice versa. For this, we created trade-off curves with We applied Marxan iteratively, starting with the set of unfished conservation units (full achievement of the fishing objective) and progressively increasing the percentage of fished conservation units to be considered as potential reserves (allowing increasing achievement of the habitat conservation objectives). To do this, we defined a series of increasing cost thresholds by the percentages of the total fished conservation units that could be moved to no-take zones: from 0 to 100% in 10% increments. For each of these 11 cost thresholds, 1,000 different sets of conservation units were selected to meet the objectives. Across these 1,000 repeat runs, we selected the best solution (with the lowest total cost) for each percentage threshold. For each threshold, we recorded the proportion of habitats that would meet their objectives in the best solution.ResultsAvoiding a priori any fishing ground in no-take areas had a large impact on the number of conservation units available for conservation, and on the extent of habitats that could be protected. At best, for both sizes of conservation units, about 60% of the potential conservation units would be left available in both Wallis and Futuna, and about 20% in Alofi (Figure Figure The trade-off analyses showed that, if local communities harvested 100% of their fishing grounds (full achievement of the fishery objective), only 69% of all Wallis habitats and 75% of Alofi habitats, at best, can meet their conservation objectives (Figure Discussion", "conclusions": "Achieving 20% habitat conservation targets within no-take areas appears incompatible with the socioeconomicx context of small Pacific island countries such as Wallis, Futuna, and Alofi. Managers and conservation planners worldwide are already well aware of the tradeoff needed between conservation and fisheries objectives (see Proceedings of the Fourth World Fisheries Congress: Reconciling Fisheries with Conservation, 2004), and MPA designs now account for local socio-economic constraints . Here, we quantified the extent of trade-offs needed to reconcile two conflicting types of objectives, in the particular context of small tropical coral reef islands, rich in habitats, limited in extent, and with high levels of use for daily subsistence, and thus with limited scope to favor habitat protection over food security.(Klein et al. 2008)To our knowledge, no study has previously assessed the extent of the compromises needed between fisheries and strict habitat conservation in this tropical island context. Our analyses, based on immediately available data sets, provide quantitative and spatially explicit answers to questions about compromises between core objectives for managing marine regions. We found that achievement of both strict conservation and fishery objectives is generally not possible in Wallis, Alofi, and Futuna, regardless of the habitat maps and sizes of conservation units considered. However, although the 20% no-take objectives were not all achievable without reducing fishing areas, the achievement of objectives for specific habitats was greatly influenced by the type of habitat maps and the size of the conservation units, so choosing these variables carefully is fundamental to understanding such trade-offs.Our results show that using more detailed habitat maps makes the achievement of habitat objectives more difficult. Very detailed habitat maps provide a finer description of physical and biological variation across reef systems, and  suggested that finer resolution coral reef habitats might also be better surrogates of species diversity. However, finer resolution maps also contain more habitat classes that need to be represented in conservation designs, increasing the total area required to achieve objectives. The reason for this increase is the greater mismatch between the boundaries of conservation units and those of more detailed habitat classes, as demonstrated for terrestrial planning by Dalleau et al. (2010).Pressey & Logan (1995)Figure  Percentages of habitats that were under-represented or unrepresented in units available for conservation after exclusion of fished units. Under-represented habitats are those for which conservation objectives were only partially achievable (less than 20% of total extent available for conservation). Unrepresented habitats were those that could not be protected at all because their entire extents were fished. Figures are shown for the three islands (Wallis, Alofi, and Futuna), two sizes of conservation units (500 \u00d7 500 m and 200 \u00d7 200 m), and two types of maps (geomorphic in blue, and geomorphic + benthic in red). We considered 16 geomorphic habitats and 55 geomorphic + benthic habitats for Wallis, 4 geomorphic habitats, and 6 geomorphic + benthic habitats for Alofi, and 3 geomorphic habitats and 3 geomorphic + benthic habitats for Futuna.3Figure  Spatial patterns of under-represented and unrepresented habitat types in units available for conservation after exclusion of fished units, both shown in red. Under-represented and unrepresented habitats were those for which conservation objectives were only partially achievable (more than 80% of total extent unavailable for conservation, so less than 20% available) or not achievable, respectively. Effects of exclusion of fished conservation units are shown for each island, two sizes of conservation units, and two types of maps. The lower panels show the spatial footprint of net and speargun fishing in blue. For Alofi and Futuna Islands, only data on speargun fishing could be obtained.4Decisions about the sizes of conservation units also influence the potential to achieve both habitat conservation and fishery objectives. In general, smaller conservation units allow a more exact habitat representation, for any given spatial and thematic resolution of map, in the sense that fewer objectives are over-achieved (see  for coral reefs; Pressey & Logan 1995; 1998 for land systems). Our findings agree with these studies, showing that smaller conservation units lead to easier compromises. However, the appropriate size of conservation units must be decided taking account of other factors such as data resolution, manageability,    Mills et al. 2010). Prioritizing the fishery objective in Wallis, Alofi, and Futuna, as requested by the Territorial Environment Service, resulted in a deficit of reef areas available for conservation. The consequences were that several habitat types could not be protected at all, or could only partially achieve their objectives. These results were expected because several types of reef habitats are preferentially used by net and speargun fishermen for their accessibility, exposure, and resource abundance.Mills et al. 2010An extreme response to avoiding conflicts between conservation and fishery objectives is to abandon heavily fished or overfished areas to further extraction. However, failing to protect such areas from further fishing is likely to preclude their ability to restore or enhance stocks, and ignores the potential benefits of spillover of larvae and adults to supplement other fished areas. Therefore, avoiding conflicts at all costs might not be the best strategy in the long run, even to maximize benefits to fishermen themselves. Although subject to controversy, there is a now a wide body of work demonstrating the benefits of MPAs for adjacent fisheries (Roberts et al. 2001;Hilborn et al. 2004;Russ et al. 2004;Kaiser 2005;Harrison et al. 2012;. To incorporate these perspectives, our indicative conservation designs could be refined by considering past fishing activities, fishing pressure and yields, and the locations of potential MPAs relative to fished areas.)The achievement of objectives for both fisheries and conservation as we defined them here for small Pacific islands is clearly not feasible without compromises on both sides. Methods to identify achievable, realistic objectives early in the process of conservation planning are needed urgently for these countries. By achievable, we mean objectives that can actually be reached through effective conservation and management actions, not simply on paper. By realistic, we mean conservation objectives that converge towards the strict application of international objectives while allowing some flexibility to minimize socio-economic impacts on local communities heavily dependent on fishing. On the fisheries side, objectives must allow flexibility to minimize impacts on targeted species, other species, and physical habitats.Current international conservation guidelines such as the CBD targets have moderated their options through time to allow some flexibility for complying countries. If no-take areas are seen by some as the best protection option for marine habitats and associated resources (see  for a review on benefits for coral reefs), they are also the most difficult to implement.Graham et al. 2011Low compliance can be expected, especially in small Pacific countries that depend heavily on coastal habitats and associated resources. There is now a multitude of spatial arrangements for marine management that do not require total and permanent restrictions on harvest (see for example Cinner & Aswani 2007;Gaines et al. 2010;Agardy et al. 2011;. The IUCN-protected area management categories themselves offer a range of options to avoid the hard trade-offs demonstrated in this article.Mills et al. 2011)Incentives to accept marine conservation measures are also being investigated and include compensatory services such as schools or medical facilities (e.g., ) and buyouts or alternative livelihoods (e.g. Aswani & Weiant 2004Niesten & Gjertsen 2010;. For islands where most fishing activities provide food for subsistence, incentives might also need to include additional imports of food. Our analyses show that the extent of trade-offs between objectives and the need for such incentives can be demonstrated readily. Appropriate responses to trade-offs must then be formulated amongst the affected communities, decision makers concerned with fisheries and conservation, and conservation scientists.Jones & Qiu 2011)", "SDG": [14]}, "conflict_analysis_and_reallocation_opportunities_in_the_framework_of_marine_spatial_planning_a_novel_spatially_explicit_bayesian_belief_network_app": {"name": "Contents lists available at ScienceDirect", "abstract": " The competition for marine space is a recognized challenge, and the implementation of new activities, such as those emerging from Blue Growth initiatives, may amplify this competition. The marine spatial planning (MSP) framework requires decision makers to analyse spatially explicit environmental and socio-economic data to determine where user conflicts are or might emerge and consider several potential management scenarios. In the present research, a spatially explicit Bayesian belief network (BBN) was applied for this purpose. The BBN was developed to analyse the potential reallocation of artisanal fishing effort to alternative sites due to the introduction of a new, non-take area: an offshore aquaculture site along the Basque continental shelf. The constructed model combined discrete, operational fisheries data, continuous environmental data, and expert judgment to produce fishing activity suitability maps for three different m\u00e9tiers (longlines, nets and traps). The BBN was run with various effort reallocation scenarios for each metier, and the best alternative fishing locations were identified based on environmental suitability, past revenue, and past fishing presence. The closure had a lesser effect on net and longline activity, displacing 10% and 7% of local fishing effort respectively. Comparatively, 50% of all local effort by traps took place within the closed grounds, and few alternative sites were identified. Nets were found to have the greatest number of alternative fishing grounds surrounding the aquaculture site. The present research demonstrates how BBNs can support spatially explicit scenario building and user-user conflict analysis for sustainable and successful ecosystem-based marine spatial planning.", "keywords": "Blue,growth,Maritime,activities,Scenario,analysis,GIS", "introduction": "", "body": "Emerging conflicts and marine spatial planningMarine Spatial Planning (MSP) provides decision makers a general framework for managing activities over time and space. However, managers are confronted with the considerable task of designing, selecting and adapting spatial plans that are socially, economically and ecologically balanced. In MSP, maritime activities that would traditionally be regulated within independent sectors on a case-by-case basis are managed in the context of the entire ecosystem Within the framework of MSP, two types of spatial conflict are cited: user-environment conflicts, and user-user conflicts The European Commission defines Blue Growth as a strategy to support novel, sustainable uses of oceanic and coastal resources for new jobs and markets, while maintaining healthy marine ecosystems The Basque Country, like many regions throughout Europe, has continued to transform its use of the coastal ocean in recent decades. In addition to supporting more traditional sectors, such as commercial fisheries (purse seine, trawl and artisanal fisheries), tourism, and recreation, the Basque Country has committed to a number of Blue Growth initiatives, including marine renewable energy production Bayesian belief networks as a decision support toolIn view of the existing and emerging maritime activities, there is a demand for robust approaches that help users create scenarios, generate trade-off analysis and generate valuable information that can be used to resolve spatial and temporal conflicts between sectors. One innovative approach for building spatial scenarios is to use Bayesian belief networks (BBN). BBNs are probabilistic models that make use of causal reasoning and allow for limited or uncertain data. These models are simplified conceptual diagrams of a system, where the relationships between variables (nodes) are described graphically by arrows, and parameterized by conditional and joint probabilities In recent years, researchers have used BBNs for a variety of environmental applications Given this context, the objective of this research is to provide information about potential effort reallocation opportunities for artisanal fisheries when an area is closed to this activity for the development of a new aquaculture site. This analysis was undertaken by developing a spatially explicit BBN and by using the Basque continental shelf (SE Bay of Biscay) as case study. The research team created and employed a model that could be used to analyse spatially explicit management scenarios based on the fewest number of relevant environmental and fisheries activity variables. In order to achieve this aim, the following, specific objectives were established: (i) collation of available geospatial data; (ii) determination of the relevant variables for the model; (iii) creation of a conceptual diagram; (iv) implementation of the conceptual model in BBN software; (v) definition of scenarios according to present and future conditions; and (vi) analysis of the resulting fishing activity relocation options for the established scenarios.MethodsCase study areaThe Basque coast (SE Bay of Biscay) is approximately 150 km long, stretching from the French border to the province of Cantabria. The Basque section of the Cantabrian continental shelf is long and narrow, ranging from 7 km wide at Cape Matxitxako to only 20 km wide at the mouth of the Oria River Maritime activitiesThere is a diverse array of human activities taking place along the narrow Basque continental shelf, which makes the region especially prone to present and future spatial conflicts In 2016, the Basque Government declared a 3 km 2 Maritime-Terrestrial Public Domain (MTPD) zone in the littoral area of the Basque coast for the exclusive use of offshore aquaculture The artisanal fisheries sector has pronounced socio-economic and cultural importance in the region Based on the findings from the PRESPO and BATEGIN projects Development of the Bayesian belief networkThe development and application of the BBN required a series of steps Step 1. Collation and preparation of geospatial dataA 1 \u00d7 1 km grid covering the study area was adopted as reference for the subsequent analysis and to produce continuous coverage output maps. This grid yielded 2355 unique grid cells along the study site. Environmental data (e.g. marine habitat, distance variables) and fishing data were integrated into a single geographic information system (GIS).The European Union Nature Information System (EUNIS) benthic habitat map was obtained from Galparsoro et al. Fishing activity data were obtained from the BATEGIN project database Distance from port and distance from rock were calculated for both individual fishing events and grids. For the fishing events, distance from port was calculated as the geodesic distance (the shortest distance between two points) between each vessel's base port and the fishing event location. For individual grids cells, a Euclidean distance raster was calculated in QGIS based on the 14 major ports and the average distance in each cell. Similarly, the average Euclidean distance from rock in each grid cell was calculated based on a distance from rock raster layer (Supplementary Table The declared aquaculture site overlapped fully or partially with 13 grid cells. These cells were labelled as \"closed\" or \"open\" depending on the scenario analysis that was going to be performed.Step 2. Creation of the conceptual diagramThe model was designed to minimize the number of parameters needed for assessing fishing ground suitability in order to maximize its robustness given the limited environmental data available at the spatial resolution (1 km 2 ). The final conceptual model was designed to predict the fishing ground suitability of a given grid cell for each m\u00e9tier based on environmental suitability (primary habitat, distance from port and distance from rock), past fishing effort, and past catch revenue. The resulting model would be used to perform scenario analysis by reallocating fishing effort from the closed site to alternative grid cells. Expert judgement, contributed by local researchers with experience in habitat modelling, fisheries economics, fisheries management and Bayesian networks modelling, was used to develop the structure of the network. To create the conceptual diagram, the research team brainstormed a list of available predictor attributes that were most relevant to the model output and connected these variables to represent all expected relationships using their collective expert judgment.Step 3: model implementation in Bayesian belief network softwareThe conceptual diagram was converted into a BBN (Fig. Step 4. Discretization of geospatial dataPrior to training the BBN, continuous data (i.e. distance from port, distance from rock, revenue, and effort) were discretized according to Jenks natural breaks in R, to maximize variance between bins and minimize variance within bins Step 5. Definition of the conditional probability tablesBased on the availability of spatial data and to avoid creating an overly selective model, the team decided to use a combination of empirical data and expert judgement to inform the CPTs. First, the model was used to learn CPTs for the m\u00e9tier and their environmental predictor attributes through a dataset of fishing activity events. The research team then visualized each of the m\u00e9tier's environmental preferences based on these empirical data. Expert judgement was used to inform the remaining CPTs for overall suitability.Due to the grid resolution (1 km 2 ), there was a high degree of data variability within the grid cells. For example, within a single grid, there were tens of events occurring over different habitat types. So, although point data were defined by a single state of a given parameter, twodimensional spaces, like the grid cells, were defined by one or more states of a given parameter. For this reason, the team decided to train the environmental attribute CPTs with point data, but run the overall model with grid data to produce continuous coverage maps. The BBN implemented in Netica is shown in Fig. The total number of fishing events (14,506) was used to train the CPTs. The modelling software, Netica, uses Bayes' Theorem and machine learning algorithms After all fishing events were loaded into the model, the BBN was calibrated to each m\u00e9tier by moving the \"belief-bar,\" and model users visualized the conditional probabilities of each of the environmental parameters (Fig. A scoring system was implemented to capture the variability of the fishing activities. The CPTs learned from the empirical data were used to inform the scoring of the intermediate, environmental suitability node (Supplementary Table The CPT for the overall suitability node was calculated similarly. High scores were assigned to high previous effort and high catch revenue, and low scores to low previous effort and low catch revenue. The average score from the environmental suitability, effort, and revenue nodes was used to predict the overall suitability class. Finally, the model was trained to associate any grid cell classified as \"closed\" to the no suitability state.This step was repeated for each of the three m\u00e9tiers to produce distinct, spatially explicit scenarios.Scenario definitionsThe model was run for several scenarios to explore how fishing grounds suitability might change under new spatial management options. The scenarios were defined as follows:Scenario 0 (Baseline): the model was run for all grids in the case study, for each of the three m\u00e9tiers analysed. No grid cells were closed to fishing activity and thus, no effort was reallocated.Scenario 1a: this scenario was defined similarly to Scenario 0, but the 13 grid cells overlapped by the aquaculture site were closed, and fishing effort from those cells was removed (set to zero) and redistributed evenly to all available cells.Scenario 1b: the model was run for a subset of grids in the case study, for each of the three m\u00e9tiers analysed. This subset corresponded to the activity range of the 6 vessels that fished within the declared aquaculture area (Fig. Scenario 1c: this scenario was defined similarly to Scenario 1b, but fishing effort from the aquaculture site grid cells was removed and redistributed evenly to cells with predicted high environmental suitability and high previous fishing effort.ResultsCharacterisation of artisanal fishing activityBased on the activity records of the 79 artisanal vessels operating in Fig. the case study area, it was observed that nets were the most abundant and spatially widespread m\u00e9tier, followed by longlines (Fig. The majority of fishing events by netters took place over mud habitats (54%), in areas located on rocks (39%) or less than 250 m from rock (27%), and in locations between 2 and 5 km from their base port (38%) (Table Based on the records from 2010, the fishing activity taking place within the aquaculture site area was performed by just 6 vessels. These six vessels are directly affected by the closure, as their fishing grounds are located within the declared aquaculture site. Four of these vessels operated with nets throughout the declared aquaculture site, one vessel operated with longlines in a sub-section of the site, and the remaining vessel operated with traps within the site and in the immediately surrounding coastal waters. These vessels contributed 3% of the total net effort and just over 1% of total longline effort expended along the entire Basque coast. In contrast, the single affected vessel operating with traps contributed to over 30% of the total effort by traps.Artisanal fishing groundsThe predicted fishing ground suitability for nets, longlines and traps is shown in Fig. The predicted fishing ground suitability based on the baseline scenario mirrored the spatial distribution of m\u00e9tier activity along the Basque coast. The BBN results showed that there is a wide range of suitable sites for nets along the continental shelf. Nets were found to have the largest percentage of medium and high suitability sites in the study area (20%), compared to longlines (8%) and traps (< 1%) (Table Scenario 1a: reallocation opportunities for artisanal fishing effortIn this scenario, each m\u00e9tier's effort in the aquaculture area was summed and redistributed evenly to all open cells in the study site. All aquaculture site cells were reclassified as \"closed,\" and the model was trained to recognize these grid cells as unsuitable (Table Scenario 1b: effort reallocation for directly affected vesselsFor this scenario, the model was run with a subset of grid cells corresponding to the fishing grounds of directly affected vessels. The six vessels that had fished within the aquaculture site operated in 286 of the surrounding grid cells. Traps experienced the greatest degree of displacement by the closure, as 50% of the vessels' effort occurred within the closed area. The other m\u00e9tiers were less affected, as 10% of the local effort by nets and 7% of the local effort by longlines took place within the closed grounds. The BBN was used to highlight changes in local suitability when effort was redistributed to cells with predicted high suitability and low previous effort. In this scenario, the closure of the aquaculture site resulted in increased suitability for nets in two grid cells (Fig. Scenario 1c: effort reallocation to alternative sitesComparatively, when effort was redistributed to cells with predicted high environmental suitability and high previous effort, the suitability status of the study site changed considerably (Fig. Overall, the BBN results for the limited activity range in Scenario 1c showed the most noticeable changes in suitability, whereas the reallocation of a relatively small amount of effort to all potential areas in the case study did not produce an observable effect.DiscussionIn order to develop marine spatial plans that help reduce conflicts and optimise the use of available resources, managers must take into account the spatial and temporal patterns of all relevant maritime activities, as well as the interactions between them Table 1Results from the trained Bayesian belief network, calibrated for each m\u00e9tier. Scores were used to inform the conditional probability tables of the environmental suitability node. BBN for analysing conflict resolution opportunities for marine activities and space and resource optimisation under the framework of ecosystem-based marine spatial planning. To the authors knowledge, this is the first application of a spatially explicit BBN that can be used to analyse different management scenarios for fisheries when there is a possibility of grounds closure due to an emerging marine activity. This particular application is of high relevance for MSP and management plans, as artisanal fisheries make up 83% of the total EU fleets M\u00e9tierSimilarly, the model was used to identify a few grid cells west of the aquaculture site to which trap effort could be reallocated. Not only were traps the most affected by the closure in terms of effort, but also the model results showed that there were few grid cells in the surrounding area that were environmentally suitable. The limited relocation options for traps reflect both the reduced technical capacity of this particular fishing activity and the few number of fishing event data in this region. These results confirm that it may be more difficult to relocate vessels operating with traps, and that mitigating their potential losses due to the introduced aquaculture site should be prioritized.Model assumptionsDespite the simplicity of the BBN, several of its parameters and their underlying assumptions are in agreement with previous, spatial characterizations of fishing activity Model performance and underlying uncertaintiesThe model produced showed uncertainty in assigning low to medium suitability and did not capture extreme values. Stelzem\u00fcller et al. Aalders and co-workers Given the current availability of relevant spatial data, structured, expert and stakeholder elicitation may be the most appropriate means for further development and validation Application of Bayesian belief networks in the marine spatial planning processSome of the principal characteristics and capabilities of Bayesian belief networks make them valuable for ecosystem based MSP and integrated management-related applications. BBNs can be used to combine available empirical data with expert knowledge to increase the user's understanding of spatial phenomena, as was shown in the present research. Moreover, the resulting BBN can be amended and updated. The environmental parameters can be retrained with higher resolution spatial data, and the model could be updated with additional case files from more recent fishing activity reports and geodata obtained from Automatic Identification Systems (AIS) and Vessel Monitoring Systems (VMS) The key contribution of this research is that the produced BBN can be used to explore alternative management scenarios for a marine area supporting a high density of economic activities. Such scenarios can be created and run with modified versions of this BNN. The existing BBN could be applied to a large range of economic activities, such as the selected aquaculture site, proposed MPAs, or new offshore wind energy platforms. The model developed here can also be integrated with other BBNs to address marine spatial planning initiatives at a systems-wide scale. As this case study illustrates, implementing new objectives in one sector may introduce and change conflict patterns with another Conclusions", "conclusions": "This research helps to resolve a user-user conflict along the Basque Coast by identifying and analysing potential effort reallocation scenarios for artisanal fisheries, whose traditional grounds may be closed for aquaculture development. This analysis was accomplished through a novel approach: the creation and application of a spatially explicit Bayesian belief network. The results indicated that there are opportunities for traditional artisanal fishing activities to reallocate their effort and thus, coexist with the aquaculture initiative in areas immediately surrounding the aquaculture site. The results obtained revealed that there is added value in combining empirical data and expert knowledge when developing this kind of model, especially in the context of reaching marine spatial planning objectives. To our knowledge, the proposed model is still one of the only spatially explicit BBNs applied to artisanal fisheries and aquaculture in the context of MSP. Subsequent research should seek to develop models that incorporate additional maritime activities in order to address continued and newly emerging user-user conflicts and associated environmental effects. BBNs can be adapted to explore management scenarios for a range of actors and activities, thus proving their usefulness for ecosystem-based marine spatial planning implementation.", "SDG": [14]}, "design_of_autonomous_underwater_vehicles_for_cage_aquafarms": {"name": null, "abstract": "", "keywords": "", "introduction": "hough the consumption of fishery products has been ever increasing, fishery amounts from open wild habitats have encountered serious challenges due to several reasons including overfishing and environmental changes . One of solutions is to establish aquaculture facilities where fisheries are bred, raised, and caught under high productivity settings. It has been known that the proliferation, survival and growth of marine fishes are very sensitive to environmental factors such as temperature, acidity, dissolved oxygen concentration, and pathogenic microbes [1]. Unlikely to the open wild habitats where the fishes could migrate to their preferable regions autonomously, the aquaculture systems need to provide artificial monitoring and control functions to maintain such environmental factors within high productivity ranges.[2]Among several types of aquaculture systems, this paper focuses on cage-based aquaculture systems operated in coastal areas. Though cultivated fishes are kept inside the cages, the afore-mentioned environmental factors are highly affected by surrounding oceanic regions since the sea water is allowed to flow through the cages. For example, mass outbreaks of death can happen if cold water suddenly flows into the cages from the sea floor. Proliferation of pathogens inside or outside cages can put critical dangers on the aquafarms while the early detection is almost impossible. The current solution is the necropsy of dead fishes to identify pathogens after the dead outbreak prevails. Since the ranges of affecting oceanic regions amount to several thousand meters depending on the factors, it is hard to deploy sufficient number of fixed sensors over the regions both in technical and economical aspects.This paper suggests that autonomous underwater vehicles (AUV) equipped with essential sensors, communication devices, and navigational intelligence can provide acceptable solutions to this problem. After explaining the target aquaculture systems, we describe functional and performance requirements of autonomous underwater vehicles for surveillance of costal cage aquafarms. It also describes several design options considered for this ongoing project.", "body": "hough the consumption of fishery products has been ever increasing, fishery amounts from open wild habitats have encountered serious challenges due to several reasons including overfishing and environmental changes Among several types of aquaculture systems, this paper focuses on cage-based aquaculture systems operated in coastal areas. Though cultivated fishes are kept inside the cages, the afore-mentioned environmental factors are highly affected by surrounding oceanic regions since the sea water is allowed to flow through the cages. For example, mass outbreaks of death can happen if cold water suddenly flows into the cages from the sea floor. Proliferation of pathogens inside or outside cages can put critical dangers on the aquafarms while the early detection is almost impossible. The current solution is the necropsy of dead fishes to identify pathogens after the dead outbreak prevails. Since the ranges of affecting oceanic regions amount to several thousand meters depending on the factors, it is hard to deploy sufficient number of fixed sensors over the regions both in technical and economical aspects.This paper suggests that autonomous underwater vehicles (AUV) equipped with essential sensors, communication devices, and navigational intelligence can provide acceptable solutions to this problem. After explaining the target aquaculture systems, we describe functional and performance requirements of autonomous underwater vehicles for surveillance of costal cage aquafarms. It also describes several design options considered for this ongoing project.II. COASTAL CAGE AQUAFARMSA. Aquaculture TypesAquaculture is defined as the practice of using the sea, lakes, and rivers for cultivating aquatic animals and plants, especially for consumption as food. It is distinguished from fishing by active human efforts in maintaining or increasing the species involved, as opposed to simply taking them from the wild habitats. Aquaculture systems for marine products can be classified into three types according to the cultivation areas. (i) The indoor tank systems are using plastic or concrete tanks to raise fishes. The tank water is slowly circulated through pipes with pathogen filters, which are connected from the coast to the aquafarms, so that fishes can be provided with natural sea water regularly. (ii) The cage systems are using synthetic fiber cages installed several kilometers apart from the coasts. Fishes are kept inside the cages while natural sea water flows through the cages freely. Free flow of natural sea water is critical for cultivation of special fish species such as flat fishes. However, these fishes are always exposed to danger under water. (iii) Open marine ranches are designated areas in the middle of the sea where artificially enriched ecosystems are fostered. For example, massive marine plants and submarine structures are implanted to provide cultivated fishes with nutrients and shelters.B. Cage Aquaculture SystemsThis paper focuses on a specific type of cage aquaculture systems being used predominantly in Korea. Each farm consists of tens of beds, and each bed consists of four cells. The dimension of the cell is typically 5*5*5 meters. It is recommended that around 15 beds are deployed in a unit region of 10k square meters (Figure C. Environmental FactorsWe should consider several problems for cultivating fishes. Among them, growth promotion and prevention of mass death of fishes are absolutely important. For that, we should observe water quality at all times. To estimate water quality, we check temperature, dissolved oxygen (DO), ammonia, nitrites, nitrates, and pH and alkalinity of water. Between them, temperature is the important factor because fishes live within the optimal temperature range. If the temperature is below the threshold, they could not live any more.The red tide is frequently occurred in Korea for June through August. They are usually not harmful. However, the excessive proliferation of zooplankton results in extensive damage to fishes caught in gill nets. Thus, it triggers mass death of fishes.Recently, we acquire water quality data through the data logger system. It has several sensors to measure water quality. It is convenient to use within limited areas. However, we should cover relatively large area around aquafarms. Thus, it is inconvenient because users should move frequently to measure data around aquafarms.III. DESIGN REQUIREMENTS AND PRINCIPLESAutonomous underwater vehicles (AUVs) have been utilized for scientific, commercial and military underwater applications A. Requirements of AUVs for Cage AquafarmsWe divide Unmanned Underwater Vehicles (UUVS) into two parts according to their autonomy. One is autonomous underwater vehicles (AUVs) which can navigate under water without assistance from supervisors. The other is remotely operated vehicles (ROVs) which are supported by mother ship through a cable. Between them, AUVs are more adequate than ROVs because we need intelligent vehicles navigating autonomously around aquafarms. For that, we should consider several requirements of AUVs for surveillance of coastal cage aquafarms.First, the accuracy of AUVs should be sustained. (1) Exact handling in low speeds is important because AUVs should measure water quality at the target spot in low speeds.(2) The accurate transmission of position is necessary. We do not know whether AUVs navigate adequately or not. Second, the stability of AUVs should be obtained. (3) AUVs turn frequently their direction to perform their mission around aquafarms. As a result, they are affected by drag force or pressure and lose their attitude. It is important to keep attitude and position stable.Third, the efficiency of AUVs should be attained (4) Basically, AUVs need sufficient space to equip electrical devices and batteries. In addition, they need various sensors to measure water quality and control attitude. Thus, the adequate arrangement of device is important to reduce the unnecessary space.(5) AUVs should optimally move to the aiming point to minimize the consumption of batteries.Fourth, the invariability of AUVs should be maintained. ( Sixth, the accessibility of AUVs should be easy. (8) We frequently modify control program according to missions. We adjust operational depth, pathway and sensing rate. Thus, we should easily access control system.Seventh, the riskiness of AUVs is minimized. (9) When AUVs navigate around aquafarms, the fishes can get stress from the light or sound of AUVs. B. Cost Aspects of Design PrinciplesExisting AUVs have excellent performance. They can navigate under water from 100 to 6,000 meters. Expensive devices are equipped to navigate faster and deeper under water. However, we need not high-end AUVs for surveillance of aquafarms because our operational depth is approximately 10 meters. Thus, we can make AUVs by using devices of low price. In addition, we can minimize the cost of the waterproof hull fabrication.IV. THE DESIGN OF SYSTEMSA. Overall Surrounding of the Aquafarm AUV SystemAUVs collect data during their missions with respect to temperature, DO, PH. These data are delivered from the intelligent vehicle to the optimal buoy communication system. Buoy communication systems transfer these data to the underwater station (Figure B. AUV/SCA systems 1) Design of AUVs a) External shape of the hullGenerally, AUVs have several shapes sphere, cube or cylinder. According to their hull shape, they are affected differently by water. The best shape is a sphere to resist water pressure because water pushes equally from all sizes of the sphere (Figure The streamlining hull helps decrease the drag and lowers the amount of power needed to move the vehicle at a certain speed b) ThrustersWe will equip twin thrusters with AUVs. However, they are less effective than one thruster because they need more energy sources to cruise under water. Nevertheless, we use twin thrusters because they promise the high quality of rotation. In case of our systems, we should move frequently to measure the water quality data. Thus, two thrusters are more efficient than one thruster regardless of the weak point.c) Electrical devicesWe need several sensors for the control of attitude and position of AUVs and the measurement of water quality around aquafarms. Pressure sensors are used to calculate depth along with water pressure. Sonar sensors are used to detect obstacle and topography. The magnetic compass is used to detect attitude of AUVs. These sensors are contained the electronics housing (Figure The microcontroller and On board PC are also contained in electronics housing. We describe the circuit of the pressure sensor whose signals are conditioned by power and operational amp (Figure a) Control of AUVsThere are several complex factors and nonlinear forces under water. Thus, it is difficult to design the accurate control model of AUVs. Their attitude information is processed by On-board PC and AUVs can correct their attitude based on information. They need control algorithms to process information. Advanced control systems have shown that autonomous diving and steering of unmanned underwater vehicles can be controlled by multivariable sliding mode control ThE1.25Quasi-sliding mode systems have been adapted for control of autonomous underwater vehicles 2) Communication systems a)Underwater Buoy SystemsWe design underwater communication systems. We combine wireless communication systems with underwater communication systems. AUVs deliver acquired data to underwater buoy systems through ultrasound waves. These data are transferred to underwater stations (Figure b) Underwater StationsThey receive data from underwater buoy systems. These data are analyzed by monitoring systems. After that, they transfer necessary information to fishermen by the mobile phone.c) Underwater vehicles position tracking systemThere are several methods to detect position of AUVs. According to the length of transponder we classify them Long Base Line (LBL), Short Base Line (SBL), Super Short Base Line (SSBL). Among them, Long Base Line (LBL) is a basic method to estimate AUV's position 3) Software a) AUVs Control programThe underwater vehicle control program directs the movement of AUVs. Users select the vehicle operation parameters such as speed and depth, and to control the data collection rate of sensors. Users give instructions to the Driving AUV Block and Sensing Block through TCP/IP protocol. The Driving Block control thrusters and a rudder. Sensors measure the attitude of AUV and water quality. These data are delivered to the Sensing Block through RS232 protocol. Water quality data are transferred to underwater station. The microcontroller gives instructions to the Driving AUV Block by using attitude data (Figure b) Simulation for the control of AUVsThe Simulation is important because it give users chances to test the performance of AUVs. It can reduce the gap between the desired output and actual output. For example, PDI controller is the classical control method. It is difficult to find optimal Proportional, Integral and Derivative gain. C. System development alternative & method1) Electrical devices Basically, we use existing electrical devices or modify devices for our purpose. For example, there are many kinds of pressure sensors. Among them, the pressure sensor should satisfy high accuracy and stability against noise. In addition, it should be operated at sealed underwater vehicles. Thus, the M5100 pressure sensor of the Schaevitz firm is useful for our systems. It can be worked all-metal sealed system and supports extended temperature range, 1% total error band and has compact outline Ahlborn firm has various sensors to measure humidity, water quality, air velocity, and temperature 2) Hull design In designing the hull, we should consider pressure resistance and drag force. They are determined by the hull shape sphere, cube or cylinder. The sphere shape has the best pressure resistance and the cylinder shape with dome ends has low drag force. We should consider the design along with their purpose. For instance, if you want low drag force, you should choose cylinder shape. On the other hand, you need more robust hull against water, you should select cube shape. Thus, we choose the streamlining hull shape because it has low drag force and moderate pressure resistance (Figure 3) ThrustersWe select the Model 300 thruster of Technadyne firm To determine thrusters, we assume the value of CD (coefficient used in integrating forces and moments along hull due to local cross-flow), A (projection area of hull in xy-plane) according to the design.From equation (3), we need 12kg bollard output to satisfy a maximum velocity 2m/s. The Model 300 thruster has 8.2kg forward thrust force and we will use two thrusters. Thus, we can accomplish a maximum velocity by using two thrusters. (3) (4)4) Energy sourcesWe use Lithium Polymer batteries which are rechargeable and convenient. They are more expensive than lead storage batteries. On the other hand, they guarantee both lower weight and increased run times. In addition, they can be protected from overcharging.5) AUVs control programWe will develop control program by using C++ for the microcontroller. The C++ language possesses property of both the C language and the object oriented concept. The C language is adequate in the system programming Input-Output (I/O) control and object oriented concept help us more convenient. Object-Oriented Programming (OOP) provides reuse of code and minimizes time loss in debugging. Thus, the C++ language is more reasonable than C language.6) Communication SystemsWe divide underwater buoy systems into two parts. The upper device takes charge of wireless communication. It will be equipped with the Wireless Ethernet transceiver for data transmission to underwater stations. The lower device takes charge of underwater communication. It has been developing by ultrasound waves. We focus on the development of underwater communication systems. We should overcome some difficulties in developing systems. Scattering, diffraction and interference of waves exist under water. However, the operational depth of our system is up to 10 meters. We can minimize weak points of the wave under water. After considering that, the lower device will be equipped with Ultra sound receiver for data reception from AUVs.D. Design of Intelligent navigation methods adapted Clonal Selection Algorithms (CSA) 1) Clonal Selection Algorithms (CSA)The CSA is based on the artificial immune system. The  ThE1.25CSA is used in the field of optimization and pattern recognition. It establishes the idea that only those cells that recognize the antigens are selected to proliferate. The selected cells are subject to an affinity maturation process, which improves their affinity to the selective antigens 2) Intelligent navigation AUVs should move to the every target point to measure water quality and they occasionally move to the same tpoint. There exist many kinds of possible pathways to move around aquafarms. However, it is difficult to find the optimal pathway. Thus, we can find the best solution through CAS. First, generate possible pathways randomly from start point and calculate all affinities with respect to navigation time. Then, select pathways of high affinity. Make a lot of clones of them, and mutate. Recalculate affinities and select pathways of high affinity. Some randomly generated parameters are mixed together at this moment. These processes are iteratively performed until it converges. Finally, pathways are optimally selected by CSA. It will minimize time of navigation and the consumption of energy sources of AUVs.V. CONCLUDING REMARKS", "conclusions": "This paper has suggested autonomous underwater vehicle (AUV)-based surveillance systems for effective and broad-range monitoring missions of cage-based aquaculture system surroundings. Unlikely to the state-of-the-art AUVs, originally developed for military missions, the AUVs for aquafarms have their own engineering compromises and opportunities. It has also described artificial immune techniques for intelligent navigation and situation monitoring of the aquafarm AUVs.Successful completion of this project will open new ways of aquafarm surveillance as well as provide cost-effective models of intelligent underwater vehicles for mission-specific applications.", "SDG": [14]}, "marine_litter_prediction_by_artificial_intelligence": {"name": "Marine litter prediction by artificial intelligence", "abstract": " Artificial intelligence techniques of neural network and fuzzy systems were applied as alternative methods to determine beach litter grading, based on litter surveys of the Antalya coastline (the Turkish Riviera). Litter measurements were categorized and assessed by artificial intelligence techniques, which lead to a new litter categorization system. The constructed neural network satisfactorily predicted the grading of the Antalya beaches and litter categories based on the number of litter items in the general litter category. It has been concluded that, neural networks could be used for high-speed predictions of litter items and beach grading, when the characteristics of the main litter category was determined by field studies. This can save on field effort when fast and reliable estimations of litter categories are required for management or research studies of beaches--especially those concerned with health and safety, and it has economic implications. The main advantages in using fuzzy systems are that they consider linguistic adjectival definitions, e.g. many/few, etc. As a result, additional information inherent in linguistic comments/refinements and judgments made during field studies can be incorporated in grading systems.", "keywords": "Marine litter prediction,Neural network,Fuzzy systems,Beach grading,Artificial intelligence,Turkey", "introduction": "Litter in the marine environment leads to numerous problems, which adversely affects coastal development sectors. Marine litter is defined as ''solid materials of human origin that are discarded at sea or reach the sea through waterways or domestic or industrial outfall'' . Because of beach litter, possible adverse effects can occur for human health and wild life on beaches. Prevention at source is one of the most important strategies in enabling litter pollution reduction, but for this aim to be achieved strong links between measurement and management need to be realized (Williams et al., 2000)(Earll et al., 2000;Tudor and Williams, 2001;Balas et al., 2001;. Due to the high variability of beach characteristics and sources of beach litter, there is yet no widely accepted approach or standardized methodology to litter pollution. Some areTudor et al., 2002)\u2022 Individual items of beach marine debris are counted and classified or recorded as presence or absence . \u2022 The whole beach is surveyed from splash zone to waters edge (Rees and Pond, 1996). \u2022 Transects--used to represent a sub section of a beach, may be used of varying width. The optimum transect width is one which provides a reliable sample width (Dubsky, 1995). \u2022 Transect line quadrates or randomly dispersed quadrates (Williams et al., 2000)). \u2022 Strand line counts (Dixon and Hawksley, 1980.(Williams and Simmons, 1997)\u2022 The EA/NALG (2000) approach--see later.\u2022 Sourcing the litter .(Williams et al., 2003)\u2022 Postal surveys .(Dixon, 1992)In addition, various indices, both qualitative and quantitative, have been used in order to assess litter . In this paper, artificial intelligence techniques such as neural network and fuzzy systems are applied as alternative methods for the determination of beach litter grading by using litter survey results obtained from the Antalya coastline, which is known as the Turkish Riviera.(Williams et al., 2000)Artificial intelligence consists of different techniques such as neural networks and fuzzy logic, utilized to solve complex problems based on human intelligence . Neural networks and fuzzy systems represent two methodologies that deal with uncertainty arising from system complexity. An artificial neural network (neuronet) is a non-linear computing system consisting of a large number of interconnected processing units (neurons), which simulates human brain learning. In recent years, neuronets have been successfully used for analysis of coastal environments, such as for time series processing (Pham and Pham, 1999), tidal level forecasting (Deo and Kumar, 2000), pattern classification (Tsai and Lee, 1999)(Deo and Naidu, 1999;, wave data assessment Balas, 2001) and structural failure predictions (Tsai et al., 1999).(Mase et al., 1995)Fuzzy systems are the collection of ''if-then'' rules defining the fuzzy relations of fuzzy variables in the systems by utilizing fuzzy logic or fuzzy set theory (Zadeh, 1997;. Fuzzy systems make effectively use of ''additional information'' such as knowledge and experience of humans, i.e. it is different from standard modeling approaches. It simulates management ability regarding complex tasks under significant uncertainties. Fuzzy systems can be used in modeling, analyzing, predicting and/or controlling of complex system behaviors associated with inherent non-linearity, when adequate knowledge and reliable methods of measuring system variables are not available Zadeh, 1999).(Moshgbar et al., 1995)Available methodologies of beach litter grading cannot predict the number of litter items and categories based on previous measurements of litter data. Therefore, these are litter assessment methods rather than a prediction model. Furthermore, available methods require the tasks of counting, classification, surveying and evaluation carried out at various beaches. Field campaigns generally necessitate well-trained measurement teams and extensive disbursements over long time. The litter prediction model developed in this study provides the prediction of litter categories based on artificial intelligence. Therefore, this model will save time--only one category is measured, so it is easier for the field worker, and provides fast and reliable estimations of litter categories--a necessity for successful beach management. The constructed neural network satisfactorily predicted the beach gradings and litter categories, based on the number of litter in general litter category for beaches in Antaya. As a result, future predictions of litter items and categories can be performed by this model, which will lead to better management scheme concerning the health, safety and economic implications of beaches.The artificial neural network was chosen as the prediction model for litter grading, since it is a non-linear robust prediction method that can satisfactorily handle the randomness and uncertainty inherent in data sets and complex natural systems. Neuronets exhibit the characteristics of ''biological'' networks to simulate the human brain learning and they have been successfully used nowadays for the prediction of environmental processes.In addition, a fuzzy system was developed to obtain the classification of the beaches, since uncertainty is generally inherent in beach work due to the high variability of beach characteristics and the sources of litter categories. This resulted in effective utilization of ''the judgment and knowledge of beach users'' in the evaluation of beach gradings. Frequently, linguistic descriptions, such as ''very good'', ''above average'' have been used to grade a beach, ''many/few items'' etc. have been used for litter counts. Available methods cannot include qualitative knowledge of human to this extent.In summary, artificial neural networks can demonstrate the learning and adaptation capabilities of biological neural systems by predicting litter grades from new data sets considering the change in environmental conditions. Therefore, they are flexible and robust nonlinear prediction models. On the other hand, fuzzy systems can effectively utilize the uncertainties inherent in human knowledge, but they do not have the capability of learning. Therefore, neural networks have more ''generalization ability'' (functional approximation capability) than the fuzzy systems in using a database. However if the database is limited and contains qualitative information, fuzzy systems can be alternatively used as litter assessment models. The advantage of fuzzy systems is that, they can handle human based information such as experience and judgment, and can consider qualitative data described by language.", "body": "Litter in the marine environment leads to numerous problems, which adversely affects coastal development sectors. Marine litter is defined as ''solid materials of human origin that are discarded at sea or reach the sea through waterways or domestic or industrial outfall'' \u2022 Individual items of beach marine debris are counted and classified or recorded as presence or absence \u2022 The EA/NALG (2000) approach--see later.\u2022 Sourcing the litter \u2022 Postal surveys In addition, various indices, both qualitative and quantitative, have been used in order to assess litter Artificial intelligence consists of different techniques such as neural networks and fuzzy logic, utilized to solve complex problems based on human intelligence Fuzzy systems are the collection of ''if-then'' rules defining the fuzzy relations of fuzzy variables in the systems by utilizing fuzzy logic or fuzzy set theory Available methodologies of beach litter grading cannot predict the number of litter items and categories based on previous measurements of litter data. Therefore, these are litter assessment methods rather than a prediction model. Furthermore, available methods require the tasks of counting, classification, surveying and evaluation carried out at various beaches. Field campaigns generally necessitate well-trained measurement teams and extensive disbursements over long time. The litter prediction model developed in this study provides the prediction of litter categories based on artificial intelligence. Therefore, this model will save time--only one category is measured, so it is easier for the field worker, and provides fast and reliable estimations of litter categories--a necessity for successful beach management. The constructed neural network satisfactorily predicted the beach gradings and litter categories, based on the number of litter in general litter category for beaches in Antaya. As a result, future predictions of litter items and categories can be performed by this model, which will lead to better management scheme concerning the health, safety and economic implications of beaches.The artificial neural network was chosen as the prediction model for litter grading, since it is a non-linear robust prediction method that can satisfactorily handle the randomness and uncertainty inherent in data sets and complex natural systems. Neuronets exhibit the characteristics of ''biological'' networks to simulate the human brain learning and they have been successfully used nowadays for the prediction of environmental processes.In addition, a fuzzy system was developed to obtain the classification of the beaches, since uncertainty is generally inherent in beach work due to the high variability of beach characteristics and the sources of litter categories. This resulted in effective utilization of ''the judgment and knowledge of beach users'' in the evaluation of beach gradings. Frequently, linguistic descriptions, such as ''very good'', ''above average'' have been used to grade a beach, ''many/few items'' etc. have been used for litter counts. Available methods cannot include qualitative knowledge of human to this extent.In summary, artificial neural networks can demonstrate the learning and adaptation capabilities of biological neural systems by predicting litter grades from new data sets considering the change in environmental conditions. Therefore, they are flexible and robust nonlinear prediction models. On the other hand, fuzzy systems can effectively utilize the uncertainties inherent in human knowledge, but they do not have the capability of learning. Therefore, neural networks have more ''generalization ability'' (functional approximation capability) than the fuzzy systems in using a database. However if the database is limited and contains qualitative information, fuzzy systems can be alternatively used as litter assessment models. The advantage of fuzzy systems is that, they can handle human based information such as experience and judgment, and can consider qualitative data described by language.Artificial neural networksArtificial neural networks (neuronets) are based on a simplified modeling of the human brain's biological functions. Therefore, they are very effective in computational systems where complex real world problems are modeled A multi-layer feed-forward neural network (MFF) has a layered structure as given in Fig. In the supervised learning algorithm, weight and bias factors were determined by minimizing the convergence criteria, i.e. the performance index defined as:The weight and bias updates are proportional to the performance index (rJ) bywhere N is the number of input and output vectors, n is the epoch number, z is the number of neurons at the output layer, rE\u00f0w; n\u00de is the gradient vector of total instantaneous errors that have components associated with the weights of the hidden and output layers W h and W y , respectively.In the error minimization of the conjugate gradient (CG) learning algorithm, subsequent weight factors were calculated in the steepest descent direction (P 0 \u00bc \u00c0g 0 ) as followswhere w\u00f0k \u00fe 1\u00de is the value of the weight vector at the iteration step (k \u00fe 1), g k is the step size adjusted at kth iteration, b k is the scalar Fletcher-Reeves factor Fuzzy systemsFuzzy systems or fuzzy rule based systems are formal representations of informal linguistic descriptions by means of if-then rules or fuzzy rule base. A fuzzy rule base represents the relationship between two or more fuzzy variables in the general form of ''if'' antecedent proposition; ''then'' consequent proposition or a set of consequences that can be inferred. In the linguistic fuzzy system the antecedent and consequent propositions are always fuzzy propositions. For example, a fuzzy rule base consists of: if x is A i then y is B i i \u00bc 1; 2; 3; . . . ; N , where, x and y are linguistic variables or antecedent (input) and consequent (output) fuzzy variables, respectively; A i and B i are antecedent and consequent linguistic terms or primary values of the fuzzy variables. The values of the primary values A i and B i are fuzzy sets given by membership functions. A number between 0 and 1 indicates the degree of membership to a set. For example a membership function maps every element of the universe discourse X to an interval \u00bd0; 1 and this mapping can be written as l A \u00f0x\u00de : X ! \u00bd0; 1. Fuzzy logic differs from binary logic in the way that the membership function in binary logic suddenly jumps from 0 to 1, while the membership function in fuzzy logic smoothly varies between the values of 0 and 1. The main advantage of fuzzy sets for solving real world problems is the ability to capture non-linear relationship between inputs (antecedents) and outputs (consequents) without oversimplification. In order to use the fuzzy systems, an algorithm or fuzzy inference mechanism can be generally applied to compute the output value for the given input values, as conceptual units of fuzzy rule base.Fuzzification is to transform the input fuzzy sets (the input information) into an appropriate form to be handled by the fuzzy rule based system. In the fuzzy rule based system, logical relationship between the fuzzy input and output sets are revealed and quantified. The fuzzy rule base consists of conditional statements that  Fuzzy rules are connected by an aggregation operator of either union (_) or intersection (^) depending on the implication operator. Results obtained from the fuzzy rule based system are retransformed from internal fuzzy quantities (consequent) into numerical quantities or crisp outputs (y 0 ) by the defuzzification methods. The center of gravity method is the most commonly used defuzzification method and is given as4. A case study on beaches in Antalya, TurkeyIn the first stage, litter categorization for the Antalya coast was obtained using artificial neural networks trained by litter measurement data obtained at Antalya beaches. In the second stage, these measurements were assessed by fuzzy systems and an alternative litter categorization system developed.Field litter studies were conducted on some of the most attractive tourist beaches of the Turkish Riviera (Antalya) coast, namely Cirali, Konyaalti, Kemer, Side and Belek. For a 100 m stretch of beach located on the normal access points (Fig. The amount of litter in the other categories was in correlation with the general litter category and as numbers in the general litter category were amplified an increase in other litter items was commonly observed. For example, from Table Therefore, this relationship between general litter (when the number of litter items in the general litter category can be measured or identified) and other litter categories, was used for the construction of neural networks in order to predict the number of litter items in other categories and the beach grade. Input to the neural network is the number of litter items in the general litter category. The neural network predicts the classification of the beaches (A-D) for other litter categories given in Table For the neural network training stage, a single hidden layer consisting of 40 neurons was utilized. The CG learning algorithm applied and the minimum mean square errors of computations (MSE) was defined as the performance index for the training stage. The unibipolar sigmoid and linear functions were allocated as activation functions of hidden and output layers, respectively. The initial value of the step size was taken as g \u00bc 0:01 and the number of iteration steps was selected as 1000. The minimum mean square errors of computations at the end of iterations were calculated as 0.0928. Input and output values of the neural network were normalized between 0 and 1 by using the maximum and minimum values of the ranges. Grading of litter categories (A-D) which are the output values of the neural network, were coded as numerical values of 1, 2, 3 and 4, respectively.Testing stage results of the trained neural network are given in Fig. In the second stage of this study, a fuzzy system of artificial intelligence was developed, which had input parameters of general litter and sewage related debris, and an output parameter of the grading of litter categories. In this study, the uncertainty inherent in litter data that has not a standardized assessment methodology, was appraised by using the fuzzy system. In the   fuzzification process of the system inputs, which were the number of general litter and sewage related debris items, the grading criteria and approach of EA/NALG (2000) was utilized. The fuzzy input sets and membership functions of these variables were obtained for the grading of A-D, from the best to worst case depending on the number of litter items measured on beaches as shown in Figs. An example for the fuzzy rules is the conditional statement in Fortran computer coded program developed for this study, describing the following dependence of linguistic variables:\u2022 If the beach category for general litter is excellent and the beach category for the sewage related debris is good; then the grading of that beach is considered as good.Other pollution categories were also considered in the analysis, since fuzzy systems can handle additional refined linguistic definitions of beach grades. The linguistic definitions of beach users, were aggregated with information obtained by field teams researching public perception, in which attitudes of the public on beach usage and grading were determined from questionnaires distributed to 381 beach users The fuzzy sets of grades given in Figs. As a result, litter measurements were categorized and assessed by artificial intelligence techniques, and it was concluded that they were practical and fast methods in handling available litter data obtained by such field studies.Conclusions", "conclusions": "An alternative categorization system of beaches, to the EA/NALG (2000), was obtained. EA/NALG (2000) litter categories for Antalya beaches, Turkey (Cirali, Konyaalti, Kemer and Belek) were satisfactorily predicted from field studies using artificial neural networks and fuzzy logic systems.Available methodologies of beach litter grading cannot predict the number of litter items and categories based on previous measurements of litter data. Therefore, they are only litter assessment methods rather than a prediction model. The litter prediction model developed in this study provided the prediction of litter categories based on artificial intelligence. Therefore, this model will save on field effort when fast and reliable estimations of litter categories are required for management purposes. The developed model, which would lead to better management schemes concerning beach health, can perform future predictions of litter items, categories and safety and it also has economic implications.In addition, a fuzzy system was developed to obtain a beach classification, since uncertainty is generally inherent in marine environment litter management due to the high variability of beach characteristics and sources of litter categories. Litter measurement techniques, excess of litter sources and variability in the coastal characteristics of beaches, were observed as the main uncertainties inherent in the assessment and prediction of litter data for Antalya beaches. The model resulted in effective utilization of the ''judgment and knowledge of beach users'' by adding linguistic adjectival descriptions, such as ''very good'', ''above average'' ''few items, many items'', etc. Available methods cannot include qualitative knowledge to this extent. Therefore, artificial intelligence techniques, which take into account uncertainties inherent in litter data, could be considered as robust alternatives for assessment of EA/NALG (2000) results and prediction of litter data.", "SDG": [14]}, "mobile_autonomous_process_sampling_withi": {"name": "Mobile autonomous process sampling within coastal ocean observing systems", "abstract": " Predicting when and where key oceanic processes will be encountered is problematic in dynamic coastal waters where diverse physical, chemical, and biological factors interact in varied and rapidly changing combinations. Defining key processes often requires efficient sampling of specific water masses and prompt sample return for subsequent analyses. This compound challenge motivated our efforts to develop mobile autonomous process sampling (MAPS) for use with autonomous underwater vehicles (AUVs). With this system, features are recognized by artificial intelligence that integrates AUV sensor data to estimate probabilistic states for adaptive control of survey navigation and triggering of targeted water samplers. To demonstrate the utility of the MAPS/AUV system, we focused on intermediate nepheloid layers (INLs), episodic transport events that may play a role in zooplankton ecology. During multiple field tests in Monterey Bay, California, the MAPS/AUV system recognized, mapped, and sampled INLs. Invertebrate larvae contained in the water samples were subsequently characterized with molecular probes developed for high-throughput screening. Preliminary results support the hypothesis that INLs function as vehicles for episodic larval transport. Applying MAPS within a greater coastal ocean observing system permitted description of regional oceanographic dynamics that influenced the patterns and scales of INL and larval transport.", "keywords": "", "introduction": "", "body": "Predicting when and where key oceanic processes will be encountered is problematic in dynamic coastal waters where diverse physical, chemical, and biological factors interact in varied and rapidly changing combinations. Defining key processes often requires efficient sampling of specific water masses and prompt sample return for subsequent analyses. This compound challenge motivated our efforts to develop mobile autonomous process sampling (MAPS) for use with autonomous underwater vehicles (AUVs). With this system, features are recognized by artificial intelligence that integrates AUV sensor data to estimate probabilistic states for adaptive control of survey navigation and triggering of targeted water samplers. To demonstrate the utility of the MAPS/AUV system, we focused on intermediate nepheloid layers (INLs), episodic transport events that may play a role in zooplankton ecology. During multiple field tests in Monterey Bay, California, the MAPS/AUV system recognized, mapped, and sampled INLs. Invertebrate larvae contained in the water samples were subsequently characterized with molecular probes developed for high-throughput screening. Preliminary results support the hypothesis that INLs function as vehicles for episodic larval transport. Applying MAPS within a greater coastal ocean observing system permitted description of regional oceanographic dynamics that influenced the patterns and scales of INL and larval transport.Understanding the hydrodynamic and biological processes affecting the connectivity of natural populations is one of the great challenges facing scientists and policy-makers involved in coastal ocean management, defining marine protected areas, and regulating sustainable fisheries Intermediate nepheloid layers (INLs) have the potential to influence geographical connectivity of benthic species having pelagic larval stages. Forced by diverse physical processes, INLs develop episodically from transport of the turbid bottom boundary layer (BBL) Materials and proceduresWe employed the MBARI AUV Dorado, an operational system with an extensive suite of physical, optical, and chemical sensors for interdisciplinary research. Specifics of AUV sensors and methods of data processing have been published The AI advancements of this research employ a newly developed Teleo-Reactive EXecutive (T-REX) control system that integrates probabilistic state estimation, planning, and execution Deliberation in T-REX utilizes methods developed for applying mature computational techniques to command NASA space missions To detect INLs using T-REX, we use Hidden Markov Models (HMM) INLs can be detected optically by elevated attenuation and backscattering of light caused by their relatively high concentrations of suspended particulate matter (SPM). Although phytoplankton cells also cause elevated attenuation and backscattering, they are distinguishable from INLs by the fluorescence of their chlorophyll. Therefore, INLs are characterized by high optical backscattering and low chlorophyll fluorescence. We used optical backscattering and chlorophyll fluorescence measurements from a HOBI Labs HS2 sensor for training and realtime application of the INL cluster analysis MAPS missions were conducted on 10 Jan 2008 and 10 and 13 Nov 2008. Modifications of sample triggering control were made between the January and November field tests to acquire samples closer to the center of INLs. For the January test, samples were acquired as soon as the minimum probability of INL detection (set to 0.4) was reached. For the December tests, the learned model was improved by including information on INL vertical position, and sample triggering control was separated into two steps: (1) activation of a ready state when the minimum probability of INL detection (increased to 0.48) was reached, and (2) triggering of the sample acquisition only when the INL signal strength began to decrease, as would occur when passing the layer peak. Following each ~7-h survey, water samples were immediately returned to shore and transferred into clean carboys. Samples were then concentrated to 50 mL using a 100 \u00b5m mesh filter and sterile seawater for rinsing. All water samples were processed and analyzed using previously described molecular probes and sandwichhybridization assays Remote sensing data from satellite sensors and shore-based HF radar were used to examine event-driven transport phe-nomena related to the observed INL and larval variability. Methods for processing these remote sensing data have been published AssessmentInterpretation and assessment of MAPS results will be aided by first examining synoptic examples of INLs in the study region. INL observations from two transects in Monterey Bay are presented (Fig. In the first example along transect T1 (Fig. With introductory perspective provided by synoptic examples of INLs, we now turn to assessment of the MAPS testing. During the first experiment, INL waters were autonomously detected in the pycnocline in all AUV profiles, and five INL samples were acquired (Fig. The second experiment advanced INL sampling and hypothesis testing. INL samples were acquired on 2 d within a    Without AUV data over the shelf (e.g., Fig. The general crustacean probe results were significant for all samples from both November surveys (Table Of the organisms targeted with molecular probes, polychaete larvae exhibited the clearest patterns. Polychaetes comprise the largest component of soft-sediment infauna, yet the complete life history is known for only about 5% of the > 8000 described species Discussion", "conclusions": "The aspect of marine ecology that we approached with the newly developed MAPS system, population connectivity, is extremely challenging. By their very nature as episodic trans- port events that couple near/benthic habitat to the greater pelagic environment, INLs have a potentially significant role in the population connectivity of benthic species having pelagic larval stages. MAPS detected, mapped, and sampled INLs in each of the field studies. Recognizing the boundary sampling inclination of the initial INL model in the first survey, we improved the model for better sample acquisition control and more consistently acquired samples from INL centers. Because of the effective INL sampling by MAPS, we could apply molecular methods to precisely targeted water samples and identify larval constituents. Because these samples were acquired within the environmental context provided by AUV sensors and a greater coastal ocean observing system, we could link larval patterns to oceanographic processes. The preliminary data set produced by this integration of autonomy, oceanography, and molecular ecology yielded insights into the constituents, dynamics, and scales of INL transport, and thereby a compelling glimpse of the role of INL transport in population connectivity. The MAPS system is being applied for more extensive INL surveying to more thoroughly study this multidisciplinary, multifaceted process.The methodological requirements of the MAPS application demonstrated here are representative of the requirements for advancing studies of many other important coastal ocean processes, for example episodic harmful algal blooms (HABs) or development of riverine and estuarine plumes that follow intense flushing of a land drainage basin. The stochastic nature of complex marine ecosystems and the inability to adequately sample these dynamic environments necessitate advanced methods that intelligently focus limited observing and sampling resources when and where collection of data and samples are most needed. The MAPS system represents a widely applicable approach to meeting this fundamental need.", "SDG": [14]}, "setting_the_stage_for_marine_spatial_planning_ecological_and_social_data_collation_and_analyses_in_canada\u2019s_pacific_waters": {"name": "Setting the stage for marine spatial planning: Ecological and social data collation and analyses in Canada's Pacific waters", "abstract": " Canada's Pacific coast is one region where there is a renewed commitment to pursue marine spatial planning (MSP). The British Columbia Marine Conservation Analysis (BCMCA) project aimed to set the stage for MSP, and was designed to provide resource managers, scientists, decision-makers, and stakeholders with a new set of resources to inform coast-wide integrated marine planning and management initiatives. Geographic Information Systems and the decision support tool Marxan were used to develop two main products: (1) an atlas of known marine ecological values and human uses; and (2) analyses of areas of conservation value and human use value. 110 biophysical datasets and 78 human use datasets were collated and refined where applicable, as identified through five ecological expert workshops, one expert review of physical marine classification and representation, and guidance from the human use data working group. Ecological data richness maps and Marxan results show the importance of nearshore and continental shelf regions. Data richness maps for the six categories of human uses show that all, except shipping and transport, are also closely linked to the shoreline and continental shelf. An example ecological Marxan solution identifying areas of conservation value overlapped human use sector footprints by percentages ranging from 92% (i.e., 92% of planning units selected by Marxan also contain commercial fisheries) to 3%. The experience of the BCMCA project has the potential to provide valuable guidance to regions seeking to jump-start planning processes by collating spatial information and carrying out exploratory analyses.", "keywords": "Marine,spatial,planning,Marine,conservation,Stakeholders,Marxan,Decision-support,tools,Marine,use,planning", "introduction": "Declining marine resources and ecosystem services , and evidence that sector-based approaches to management have been inadequate at achieving sustainability [1], have led to increased global interest in marine spatial planning (MSP) [2][2,. MSP is a framework that informs the spatial distribution of marine activities to support current and future uses, and maintain delivery of ecosystem services to meet ecological, economic and social objectives 3]. Complementary literature on systematic conservation planning emphasises the importance of rigorous process, transparency and efficiency (e.g., through setting quantitative targets) throughout the planning process [2][4,. One example of combined systematic conservation planning and MSP is the rezoning of the Great Barrier Reef in Australia, which assigned six different zones, allowing a range of uses, in a region about 350,000 km 2 5]. Many regions are following suit, instigating systematic MSP processes [e.g., Belgium, 7, California, 8, Australia, 9].[6]As illustrated in MSP exercises worldwide, a critical component to its efficacy is comprehensive ecological and social data to support the process . Ecological data are necessary to identify areas of importance for biodiversity conservation and delivery of ecosystem services. Data on human activities are useful for identifying areas of importance to marine industries and other uses. The combination of ecological and human use data is particularly valuable in explicitly identifying overlapping interest to multiple users and/or biodiversity conservation, and investigating tradeoffs [2]. Spatial data are also necessary to use decision-support tools, such as Marxan [8][10, or Marxan with Zones 11]. Such decision-support tools can aid MSP by identifying options for areas requiring special management [e.g., marine protected areas, 6], or human use areas [e.g., designated fishing areas, 13].[12]Canada's Pacific coast (province of British Columbia, BC) is one region where there is a renewed commitment to carry out MSP, also referred to as ''Integrated Management'' in Canada . In particular, the provincial government is partnering with some coastal First Nations to create marine plans, through what is called the Marine Planning Partnership for the North Pacific Coast (www.mappocean.org), and the federal government is working on a planning process called the Pacific North Coast Integrated Management Area. In other areas, such as the west coast of Vancouver Island, MSP has been taking place via local community, First Nations and government partnerships (i.e., West Coast Aquatic, http://westcoastaquatic.ca/plans/). While these initiatives are promising, previous discussions about MSP have been slow to get started, which has significantly impeded progress to date [14][15][16].[17]The British Columbia Marine Conservation Analysis (BCMCA) project emerged from the interest of a multitude of stakeholders to set the stage for MSP in British Columbia. The BCMCA (www. bcmca.ca) is a collaborative project designed to provide resource managers, scientists, decision-makers, and those with a vested interest in the marine environment with a new set of resources to inform coast-wide integrated marine planning and management initiatives. Furthermore, the BCMCA project set out to spatially identify marine areas of high conservation value and areas important to human use in Canada's Pacific Ocean. The BCMCA is not a planning process as it does not have the ability or mandate to implement management actions, and it does not seek to replace planning initiatives that are underway or in preparation. Rather, the results are intended to inform and help advance marine planning initiatives in BC by providing collaborative analyses based on the best available ecological and human use spatial data at scales relevant to a BC coast-wide analysis. The BCMCA project is coordinated by a Project Team, comprised of representatives from the Canadian government, BC government, First Nations (self-defined as observers), academia, marine users and environmental organisations, which is responsible for coordinating, organising and implementing the project. The BCMCA project's ecological objectives were to represent the diversity of BC's marine ecosystems across their natural range of variation, maintain viable populations of native species, sustain ecological and evolutionary processes within an acceptable range of variability, and build a conservation network that is resilient to environmental change. The history and approach of the project has been described by Ban et al. , and supporting documents can be found online (www.bcmca.ca).[18]The purpose of this paper is to report the process and results of the multi-year BCMCA effort, and discuss its relevance to BC and beyond. With increasing global popularity of MSP, the impetus for the BCMCA project, an interest by a diversity of stakeholders to set the stage for MSP is likely emerging in many regions of the world. The experience of the BCMCA project has the potential to provide valuable guidance to those regions seeking to jump-start planning processes by collating spatial information and carrying out exploratory analyses.", "body": "Declining marine resources and ecosystem services As illustrated in MSP exercises worldwide, a critical component to its efficacy is comprehensive ecological and social data to support the process Canada's Pacific coast (province of British Columbia, BC) is one region where there is a renewed commitment to carry out MSP, also referred to as ''Integrated Management'' in Canada The British Columbia Marine Conservation Analysis (BCMCA) project emerged from the interest of a multitude of stakeholders to set the stage for MSP in British Columbia. The BCMCA (www. bcmca.ca) is a collaborative project designed to provide resource managers, scientists, decision-makers, and those with a vested interest in the marine environment with a new set of resources to inform coast-wide integrated marine planning and management initiatives. Furthermore, the BCMCA project set out to spatially identify marine areas of high conservation value and areas important to human use in Canada's Pacific Ocean. The BCMCA is not a planning process as it does not have the ability or mandate to implement management actions, and it does not seek to replace planning initiatives that are underway or in preparation. Rather, the results are intended to inform and help advance marine planning initiatives in BC by providing collaborative analyses based on the best available ecological and human use spatial data at scales relevant to a BC coast-wide analysis. The BCMCA project is coordinated by a Project Team, comprised of representatives from the Canadian government, BC government, First Nations (self-defined as observers), academia, marine users and environmental organisations, which is responsible for coordinating, organising and implementing the project. The BCMCA project's ecological objectives were to represent the diversity of BC's marine ecosystems across their natural range of variation, maintain viable populations of native species, sustain ecological and evolutionary processes within an acceptable range of variability, and build a conservation network that is resilient to environmental change. The history and approach of the project has been described by Ban et al. The purpose of this paper is to report the process and results of the multi-year BCMCA effort, and discuss its relevance to BC and beyond. With increasing global popularity of MSP, the impetus for the BCMCA project, an interest by a diversity of stakeholders to set the stage for MSP is likely emerging in many regions of the world. The experience of the BCMCA project has the potential to provide valuable guidance to those regions seeking to jump-start planning processes by collating spatial information and carrying out exploratory analyses.MethodsGeographic Information Systems (GIS) and the decision support tool Marxan were used to develop two main products: (1) an atlas of known marine ecological values and human uses; and (2) analyses to identify areas of high conservation value and areas important to human use. The BCMCA Project Team guided and implemented the methods, informed by ecological and human use experts who provided overarching direction and advice about the collation, use and analyses of data. All data layers were stored, mapped and documented using ArcGIS (versions 9.0-10). Key steps of the Marxan analyses, after data were collated, were to create planning units, develop targets, carry out calibration, run analyses, and draft reports explaining results.Collation of existing dataDiffering approaches were used to identify ecological and human use data to incorporate in the BCMCA project. Ecological features and datasets recommended by experts via workshops were collated and prepared for use in Marxan. Individual workshops were held for seabirds, marine plants, marine mammals, marine and anadromous fish, and marine invertebrates. Approaches used, and other details of the workshops, are described in Ban et al. Human use datasets were first sourced by BCMCA Project Team members within each of their organisations (e.g., federally held fisheries data, provincially held recreation data). Example maps were drafted and a review of these data was sought through a two-pronged strategy of group-by-group engagement and the formation of a human use data working group to advise on the collation, mapping and analysis of human use data. Six sectors or categories of human use were identified (i.e., commercial fisheries, recreational fisheries, ocean energy, shipping and transportation, tenures, and recreation and tourism), and a nomination process was held for each sector to self-identify two representatives to participate in the working group. The working group was lead by a neutral facilitator and was designed to be broadly representative of user groups, but participants were not expected to represent a constituency in any formal capacity. The working group held thirteen meetings over almost 2 years. Two of the working group representatives -with the support of the rest of the working group -also participated on the Project Team.Due to data limitations, it was not possible to create spatial data for some recommended features, while other datasets not specifically mentioned at workshops were developed from available data (e.g., general kelp). While the focus of the BCMCA project was to collate existing data, opportunities arose to create or update some ecological and human use datasets. Known gaps in digital datasets for four ecological features were filled by digitising data (e.g., central coast Marbled Murrelet surveys). For the purposes of the BCMCA, the existing provincial benthic classification scheme was replaced by a new benthic classification developed by Parks Canada using methods published by the Nature Conservancy (TNC) Once all features were compiled and reviewed, maps and descriptive information were combined into atlas pages, available online (www.bcmca.ca). Maps showing the number of features found in each planning unit (i.e., data richness maps) were created for each ecological group and human use sector, counting only datasets designated for use in Marxan.Marxan analysesMarxan (version 2.1.1) was used to identify areas of high conservation value and areas important to human use. Marxan is a free decision support tool that finds efficient solutions to the problem of selecting a set of areas that meet a suite of conservation Defining targetsTargets for ecological features are intended to quantify the amount required to meet ecological objectives. At the ecological workshops, experts were requested to recommend a range of targets for each feature, spanning a minimum to preferred amount (see Ban et al. To incorporate human use features, the Project Team initially suggested running Marxan for ecological features, using human uses as a 'cost', as is commonly done in Marxan analyses Alternately, an option was to set targets for human use features, which tells Marxan how much of each feature to include in the solution (i.e., to identify areas of important for all human uses, as per CalibrationCalibration was conducted to ensure that Marxan was behaving in a robust and logical manner, following guidance from the BCMCA Marxan expert workshop and Marxan Good Practices handbook Ecological Marxan analysesOnce Marxan parameters were finalised through calibration, the BCMCA explored a range of ''What ify?'' scenarios designed to identify areas of high conservation value. Eighteen ecological scenarios were used: High, medium and low target scenarios for the targets set by experts during the workshops as well as those identified by the Project Team. Each of these six scenarios had three sub-scenarios with different BLMs. The best and summed solutions were mapped for all scenarios.Human use Marxan analysesMarxan was used to produce a range of solutions for the human use scenarios. In this case, the scenarios were designed to explore the most efficient reduction of footprint for each human use sector. For each of the six human use sectors, five separate scenarios were performed to explore how a range of reductions in each sector's use would affect that sector's footprint. Reduction values of 5%, 10%, 15%, 20%, and 25% were applied resulting in a range of corresponding Marxan targets (95%, 90%, 85%, 80%, and 75%) and a total of 30 unique scenarios. Various metrics were used in Marxan for characterising the human use data. Marxan runs for commercial fishing targeted total catch; ocean energy used a combination of area and relative importance; shipping and transport, and tourism and recreation, used relative intensity; and sport fishing and tenures used area. The best and summed solutions for all scenarios were mapped but are not shown here.Overlap analysesArcGIS was used to identify the per cent of overlap between the six human use sectors and one example solution from an ecological Marxan scenario. The scenario with the Project Team medium targets and medium clump size was chosen for this overlap analysis because it illustrates a middle-of-the-road scenario. For each of the human use sectors, the combined footprint of all uses within each sector was used. Some caveats regarding the footprint data are that they only reflect the mapped footprint (which may or may not represent the most current footprint), and not the relative importance for any particular human use.ResultsCollation of existing data110 biophysical datasets were collated and refined, where applicable, to create 200 features, many of which were targeted by class or region in the Marxan analyses (see Supplementary Table Seventy-eight human use datasets were collated and refined where applicable (see Supplementary Table Marxan analysesTargetsLow, medium and high values for ecological targets were identified from the ranges recommended at expert workshops (as described in Section 2.2.1) (see Supplementary Table Human use targets were set based on the human use working group recommendation of conducting analyses where the use declines by 5% for each scenario, and the metric for that use depends upon the sector. Therefore scenarios consisting of these five target values: 95%, 90%, 85%, 80% and 75% were run for each of the six human use sectors.CalibrationSensitivity tests uncovered a problem with the initial plan of using two different-sized planning units (smaller nearshore and larger offshore) in the same Marxan analysis. Marxan solutions for runs using a BLM equal to zero, area as cost, and a single feature filling all planning units equally but targeted at 30%, significantly favoured the smaller planning units (Fig. Other calibration tests included number of iterations, boundary length modifier, and feature penalty. We determined that 750 million or 1 billion iterations effectively and efficiently produced solutions that adequately considered the solution space (Fig. illustrate results with no BLM and possible solutions to the range of ''What ify?'' scenarios that might be recommended by planners. The human use runs used a BLM of 1000, accepted by the human use data working group as the most appropriate BLM suitable for use across all six sectors. A consistent feature penalty factor of 8 was used for ecological features, and 500 for human use features.Ecological Marxan analysesEcological data and Marxan results show the importance of nearshore and continental shelf regions. Overlaying all ecological datasets (i.e., displaying data richness, Fig. Human use Marxan analysesData richness layers for the six categories of human uses show that all, except for shipping and transport, are closely linked to the shoreline and continental shelf (Fig. Overlap analysesOverlapping the footprint of one example solution of an ecological Marxan scenario with the footprint of each of the six human use sectors showed that all sectors utilise areas that appear in the Marxan solution as areas of high conservation value. The percentage of the Marxan solution that overlapped the sector use footprints ranged from 92% (i.e., 92% of planning units selected by Marxan also contain commercial fisheries) to 3%. Conversely, the area of each sector footprint that overlapped with the example Marxan solution ranged from 18% to 23% (Table DiscussionThe BCMCA project's multi-year effort to collate existing data, augment existing datasets by making additional and new data available, and provide examples of Marxan analyses, has made available an impressive resource for marine planners and stakeholders in British Columbia and elsewhere. The project attempted to follow best practices for data analysis While data collation is time-intensive, although relatively straightforward, the experience of the BCMCA project with Marxan analyses highlights several lessons that may be of interest to similar endeavours elsewhere. First, setting conservation targets is notoriously difficult Second, the creation of a human use data working group was a key strength of the BCMCA project, but more could have been done to involve human users earlier and more effectively Table 1Overlap between human use sector footprints and one example of an ecological Marxan solution illustrating areas of conservation value (Project Team medium target, medium clump size, which selected 22% of Pacific Canada waters). terms of reference and clarified terminology based on comments from marine users. Additional time was needed to build the new relationships with marine users within the Project Team. Furthermore, members of the working group had a different background than most of the Project Team, and for this reason a concerted effort was made to introduce Marxan and systematic conservation planning concepts to them. Terminology used in conservation planning did not always directly translate to the day-to-day lexicon of members of the working group, highlighting the importance of regular, two-way communication. A facilitator, independent of the process, was hired to ensure the working group process was meaningful to participants, that all participants' input was obtained, concerns recognised, addressed where possible, and documented where they could not be addressed. Third, experts, including human use sector representatives, were crucial to identifying the limitations of existing human use data for use in Marxan, and data gaps. Challenges are similar for ecological and human use data. For example, some species distributions and human uses are seasonal, and where spatially explicit seasonality data are missing, Marxan results do not capture such nuanced information. Similarly, some areas may be particularly important for some species or human uses (e.g., spawning grounds, shipping traffic to small communities, fishing areas close to communities), but this level of detail may not be represented in available data. Thus, although analyses were designed to identify areas important to ecosystems and human users, with little or no relative value information in the data sets, Marxan uses data density to determine areas of importance. Another data limitation is that the time period over which data were originally collected was not consistent. Some data are older, even though they may be the best-available data, and data sets for different features used in a single analysis may have been compiled for different time periods. Furthermore, data illustrated for some features may not reflect current or future reality in terms of the various measures of relative importance. Both ecological and human use features shift spatially over time due to ongoing changes in the environment and management. Thus feedback from the human use sectors was that the Marxan results for human uses were of limited value, and may not represent actual areas of importance.Per cent of ecologicalFourth, as highlighted in the Marxan Good Practices Handbook Fifth, unfamiliarity with Marxan was an obstacle, but a surmountable one. Education about Marxan, how it works, what it does, and what it is used for, was necessary with most participants-e.g., ecological experts, data providers, government employees, non-profit groups and marine users. Much effort was put into educating participants about this tool and its potential uses and limitations. An ancillary benefit to the marine based community in British Columbia is a better understanding of Marxan, both its strengths and limitations. This may prove useful for marine planning processes in the future.Finally, while the BCMCA project was made possible in part because of the commitment and dedication of many people who volunteered their time, having adequate funding ultimately made the project possible. Some groups needed funding to participate, workshops cost money, GIS contractors were needed for preparation of the many datasets, and so on. Thus, while volunteer efforts can go a long way to instigating a data collation and analysis project, to realise its full potential, the BCMCA required financial resources to be completed.Ultimately, one of the most important benefits of a project such as the BCMCA is the development and maintenance of working relationships among stakeholder groups. As shown by the exploratory overlap analyses, marine areas of conservation value in the Canadian Pacific are also important to a variety of stakeholders. The process that the BCMCA project developedincluding development of a Project Team, human use working group, user group outreach, presentations to planners -served to get parties to work together, strengthen relationships, raise awareness about the need for data collation and analysis, and educate marine users and others on the value of quality data and Marxan analyses. Communication with collaborators was a part of the project throughout. Such benefits are difficult to substantiate, yet anecdotal feedback from participants indicates that communication and collaboration among stakeholders has improved because of the project, and that the BCMCA's data products are in high demand.This project affirmed the importance of several issues discussed in the marine spatial planning and conservation planning literatures. Involving stakeholders early in the process is important for their support for the project Conclusion", "conclusions": "The BCMCA project has been invaluable in supporting MSP initiatives in British Columbia. The BCMCA project has received additional funding for a period ending May 2013 for product support, updates to select datasets, and support (if requested) to marine planning processes that are now underway. Additional communication and outreach are also planned to help people understand and build trust in the data products and supporting analyses. Furthermore, the Project Team is interested in exploring tradeoffs and win-win solutions for human uses and conservation. The simple overlap analyses reported here were illustrative only; more sophisticated and informative analyses would be useful. For example, possible use of a sister tool of Marxan, Marxan with Zones , to develop trade-off curves between different human uses and ecological features, is under discussion. This is one way to explore analysing and visualising overlap amongst users and between human uses and biodiversity hotspots.[12]to the many people who participated in workshops, contributed data and knowledge and helped to steer this work to completion.", "SDG": [14]}, "systematic_conservation_planning_within_a_fijian_c": {"name": "Systematic conservation planning within a Fijian customary governance context", "abstract": " Although conservation planning research has influenced conservation actions globally in the last two decades, successful implementation of systematic conservation plans in regions where customary marine tenure exists has been minimal. In such regions, local community knowledge and understanding of socioeconomic realities may offer the best spatially explicit information for analysis, since required socioeconomic data are not available at scales relevant to conservation planning. Here we describe the process undertaken by the Kadavu Yaubula Management Support Team, a team of researchers from The University of the South Pacific and the local communities to assess whether systematic conservation planning tools can be effectively applied and useful in a customary governance context, using a case study from Fiji. Through a participatory approach and with the aim of meeting local-scale conservation and fisheries needs, a spatial conservation planning tool, Marxan with Zones, was used to reconfigure a collection of locally designed marine protected areas in the province of Kadavu in order to achieve broader objectives. At the local scale, the real value of such tools has been in the process of identifying and conceptualising management issues, working with communities to collate data through participatory techniques, and in engaging communities in management decision making. The output and use of the tool has been of secondary value. The outcome was invaluable for developing marine protected area network design approaches that combine traditional knowledge with ecological features in a manner appropriate to a Melanesian context.", "keywords": "Pacific,Conservation,Biology,C", "introduction": "Conservation planning is the process by which conservation areas are located, configured and managed in some way to promote the persistence of natural features . Systematic approaches to conservation planning (Bottrill and Pressey 2009), which involve working through a structured, transparent and defensible process of decision-making, increasingly underpin the conservation and management of marine and coastal ecosystems worldwide (Margules and Pressey 2000). However, whilst the Pacific islands have a long history of community-based marine resource management (Bottrill and Pressey 2009), systematic conservation planning has had very little influence in this region.(Johannes 2002)There are substantial challenges to undertaking systematic conservation planning in the Western Pacific : local communities are highly dependent upon natural resources; management of marine resources is typically decentralised and often under customary systems of governance; few ecological or socioeconomic datasets are available across the regional extents required by systematic conservation planning; and in-country capacity to undertake planning is lacking. Nevertheless, new approaches are needed to address the growing threats to the region's resources, and conservation planning might provide a means to better coordinate communitybased management actions to increase their overall impact.(Weeks et al. 2014)In Pacific Island Countries increased local subsistence harvests and commercial fishing have left most coastal waters in the region overfished (Gillett 2009;. In response, a growing number of villages have begun to regulate the use of their marine areas through the establishment of locally-managed marine areas (LMMAs). To date there are hundreds of these LMMAs in operation around the region (http://www.lmmanetwork.org/). Management measures employed within LMMAs include permanent no-take marine protected areas (MPAs), periodically harvested fisheries closures, species-specific regulations such as size limits, fishing gear and access restrictions Bell et al. 2011).(Jupiter et al. 2014)The benefits of permanent, no-take MPAs have been widely demonstrated in the literature. Over time, MPAs have shown conservation benefits through increased abundance, biomass, density, body size, species diversity and richness of targeted fishes and invertebrates (Russ and Alcala 2003a;. MPAs have also offered fisheries benefits through spillover effects outside MPA boundaries McClanahan and Graham 2005)(McClanahan and Kaunda-Arara 1996; and may contribute towards poverty alleviation and increase the quality of life of surrounding communities McClanahan and Mangi 2000)(Allison et al. 1998;Leisher et al. 2007;.Gurney et al. 2014)In Fiji, as the number of LMMAs and MPAs has increased, there has been a shift in focus towards management of these sites at the provincial level. The Province of Kadavu has been leading the way in this regard. The Kadavu Provincial Administration (with support from the Institute of Applied Science of the University of the South Pacific, IAS-USP) established the Kadavu Yaubula (living-wealth) Management Support Team (KYMST) to promote the sustainable development and utilisation of Kadavu's natural resources. Since its establishment, the KYMST has taken a lead role in resource management initiatives, environmental awareness and protection activities in the Province.Since 1997, 60 MPAs have been established within LMMAs with support from the provincial office. The boundaries of these MPAs were selected by the local communities to meet localscale conservation and fisheries needs, as determined through community-based adaptive-management processes ) and informed by community-level political boundaries and governance considerations. Each iqoliqoli (traditional fishing ground) in Kadavu is now under some form of management, with many having at least one MPA within their boundaries.(Govan et al. 2008As this ground-swell of management has grown, greater consideration has been given to principles of MPA placement and the nature of management interventions being undertaken. Many management interventions are believed to contribute towards food security at the individual community level (Tawake et al. 2005;; UNEP/GRID-Arendal 2008). For instance, results from five LMMA sites across Kadavu saw a 55% increase in fish abundance within MPAs compared with 30% increase in harvested areas between 2003 and 2007 Leisher et al. 2007. Similarly, there was a 30% increase in invertebrates within MPAs and 10% increase in adjacent harvest areas (Tawake et al. 2005). However, individual MPAs lack the coordinated island-wide outcomes sought by the province and the biodiversity conservation benefits associated with an integrated network of MPAs. Furthermore, some of the existing MPAs were located far offshore and were thus subject to poaching, others had disputed boundaries, and some communities had lifted their MPAs after observing no change in fish populations after one or two years of protection.(Tawake et al. 2005)Here we describe the process undertaken by the IAS-USP, the KYMST and the communities of Kadavu to strengthen the MPA network design to better achieve both local and provincial objectives. This represents one of the first attempts to use systematic conservation planning tools to inform customary governance of natural resources, providing an opportunity to evaluate the effectiveness of these tools in this context.", "body": "Conservation planning is the process by which conservation areas are located, configured and managed in some way to promote the persistence of natural features There are substantial challenges to undertaking systematic conservation planning in the Western Pacific In Pacific Island Countries increased local subsistence harvests and commercial fishing have left most coastal waters in the region overfished The benefits of permanent, no-take MPAs have been widely demonstrated in the literature. Over time, MPAs have shown conservation benefits through increased abundance, biomass, density, body size, species diversity and richness of targeted fishes and invertebrates In Fiji, as the number of LMMAs and MPAs has increased, there has been a shift in focus towards management of these sites at the provincial level. The Province of Kadavu has been leading the way in this regard. The Kadavu Provincial Administration (with support from the Institute of Applied Science of the University of the South Pacific, IAS-USP) established the Kadavu Yaubula (living-wealth) Management Support Team (KYMST) to promote the sustainable development and utilisation of Kadavu's natural resources. Since its establishment, the KYMST has taken a lead role in resource management initiatives, environmental awareness and protection activities in the Province.Since 1997, 60 MPAs have been established within LMMAs with support from the provincial office. The boundaries of these MPAs were selected by the local communities to meet localscale conservation and fisheries needs, as determined through community-based adaptive-management processes As this ground-swell of management has grown, greater consideration has been given to principles of MPA placement and the nature of management interventions being undertaken. Many management interventions are believed to contribute towards food security at the individual community level Here we describe the process undertaken by the IAS-USP, the KYMST and the communities of Kadavu to strengthen the MPA network design to better achieve both local and provincial objectives. This represents one of the first attempts to use systematic conservation planning tools to inform customary governance of natural resources, providing an opportunity to evaluate the effectiveness of these tools in this context.MethodsStudy siteKadavu is a volcanic island arc in the Fiji Island group located in the South-west Pacific (Fig. BPacific Conservation Biology H. K. Community engagement and resource-use mappingIn Fiji, the customary marine tenure system qualifies the people of Kadavu to be the primary stakeholders because of their customary fishing rights over the iqoliqolis and the resources within In 2009, one-day workshops were conducted by the team from the IAS-USP in each of the eight districts around the main island of Kadavu. Key representatives were invited from each district to gather information on resource-use patterns, governance and cultural considerations across iqoliqolis Where socioeconomic data are not available at scales relevant to conservation planning In order to assess how well the existing system of communitybased MPAs represented coral reef habitats, we undertook extensive field surveys to ground-truth the distributions of key shallow reef habitats derived from satellite imagery, using methodology adapted from Spatial prioritisation using Marxan with ZonesMarxan is a decision support tool that has been used in the design of marine reserves worldwide As a result of the 2009 socio-cultural and resource-use workshops, six important geographic information systems (GIS) layers were identified: three layers of significant sites that should be included in the MPA network, and three layers associated with costs of MPA establishment. Significant sites identified by the communities included: (1) SPAGs, (2) turtle nesting sites, and (3) sites of cultural importance. Information on significant sites gained from community traditional knowledge was a priority inclusion as input conservation features in the Marxan Z analysis and in the redesign process of the community-based MPA network.The cost layers included: (1) fishing use intensity (i.e. opportunity cost), (2) disputed areas, and (3) enforceability (Fig. The study region covering the entire inshore iqoliqoli areas was divided into 29 728 1.5-ha hexagonal planning units, to which information on the conservation features they contained and costs of inclusion in the MPA network were assigned. Conservation targets were to include in the MPA network 30% of the total extent of identified habitat types (continuous corals, patchy corals, sparse corals, patchy coralline algae, dense seagrass, sparse seagrass and patchy mangrove), and 80% of SPAGs, cultural sites and turtle nesting sites identified by communities. Targets were set for both the MPA and fishing zones with the aim to distribute costs and benefits more equitably across all iqoliqolis. In addition to minimising opportunity, enforceability, and disputed area costs, we specified fisheries objectives to ensure that at least 60% of each iqoliqoli remained open to fishing. Conservation targets for the MPA zone were set across the entire network, whilst the fishing targets were set for each iqoliqoli. We used Marxan's boundary length modifier to define a degree of clumping that produced compact areas that were more or less the same size as existing MPAs.We produced two Marxan Z outputs, based on two different scenarios. First, the 'best' run solution Redesigning the MPA networkIn February 2011, the team from IAS-USP worked with the KYMST and Kadavu Fisheries Department to conduct four community-level workshops to communicate the outputs from the reserve design scenarios and use these, through a collaborative planning process, to redesign the existing system of community-based MPAs to better protect identified conservation features and fisheries targets ResultsThe existing system of community-based MPAs protected 12% of key shallow reef habitats, 41% of SPAG sites, 24% of the area of turtle nesting sites and 7% of culturally important areas identified by communities (Table The number of MPAs increased from 60 to 77 (Fig. Discussion", "conclusions": "Although conservation planning research has influenced conservation actions on the ground in the last two decades (e.g. Fernandes et al. 2005; very few systematic conservation planning processes have been successfully implemented in regions with customary marine tenure. Thus, our study provides important insights into the utility of conservation planning tools and processes in such contexts. We first discuss three factors that might limit the use of conservation prioritisation tools such as Marxan Z: the difficulty of using computerised software in remote and rural regions; the need for technical capacity to use tools effectively; and the small scale of natural resource management decision-making. We then discuss lessons learnt in Kadavu regarding the process of planning: on constraints to 'scaling up' local management, and incorporating local knowledge of socioeconomic costs. Finally, we outline how the Kadavu MPA network might influence future conservation efforts in Fiji.Bottrill and Pressey 2009)The utility of systematic conservation planning tools in customary governance contexts Recent applications of conservation planning have been characterised by the use of planning support tools such as GIS and decision support software (e.g. Marxan). Our study   demonstrates that such planning tools can be used 'offline' with local communities. With unreliable power supply and very little capacity within communities to operate computerised tools, workshop participants were provided with colour-printed maps of satellite imagery and iqoliqoli boundaries, with the MPA design scenarios on tracing paper overlays (Fig. ). We believe that using this offline approach, whilst diminishing the possibility of on-site iterative planning, was critical to ensuring local ownership of the process and the success of the study. Other examples of non-computer-based community-based management tools have been used extensively in the region for years. Participatory mapping, threat analysis and Participatory Learning and Action rely on simple instruments such as paper and pens but could also be considered as planning 'tools' 3.(Veitayaki et al. 2003)Nevertheless, our study did employ the use of GIS and Marxan Z by the research team from the IAS-USP. Technical capacity to implement computer-based spatial planning tools is very limited in the Pacific Islands region. Many examples of tool use have relied on the work of researchers who are unlikely to have a sustained presence in the region (e.g. Game et al. 2011;. It is considered unlikely, though not impossible, that capacity to use specialist conservation planning software can be mainstreamed into government agencies. One example of this is the Marine and Coastal Biodiversity in Pacific Island Countries (MACBIO) project, which is currently providing technical support to the governments of Fiji, Kiribati, Solomon Islands, Tonga and Vanuatu to undertake marine spatial planning.Weeks and Jupiter 2013)In regions with customary marine tenure, management of natural resources is typically undertaken at small geographic scales equivalent to individual tenure units , in this case iqoliqolis. After several demonstrations of conservation prioritisation tools at the local scale, there is a feeling that the use of computer-based tools is perhaps not necessary. We found that, at the local scale, the real value of conservation planning was in the process of identifying and conceptualising the management issue, working with communities to collate data through participatory techniques, and engaging communities in management decision-making. The Marxan Z outputs were of secondary value. However, as many countries in the region begin, under relevant national and international commitments, to examine the possibilities and strategies for scaling up from individual site-based management to national networks of conservation areas, the value of applying computer-based tools may be realised. We suggest that computerbased spatial planning tools are likely to be most useful in identifying conservation priorities at the subnational and national scale, rather than directing management actions at the sitespecific scale.(Foale and Manele 2004)", "SDG": [14]}, "aerial_mapping_of_forests_affected_by_pathogens_using_uavs_hyperspectral_sensors_and_artificial_intelligence": {"name": "Aerial Mapping of Forests Affected by Pathogens using UAVs, Hyperspectral Sensors and Artificial Intelligence", "abstract": " The environmental and economic impacts of exotic fungal species on natural and plantation forests has been historically catastrophic. Recorded surveillance and control actions are challenging because they are costly, time-consuming, and hazardous in remote areas. Prolonged periods of testing and observation of site-based tests have limitations to verify the rapid proliferation of exotic pathogens and deterioration rates in hosts. Recent remote sensing approaches have offered fast, broad-scale and affordable surveys, and additional indicators that can complement on-ground tests. This paper proposes a framework that consolidates site-based insights and remote sensing capabilities to detect and segment deteriorations by fungal pathogens in natural and plantation forests. This approach is illustrated with an experimentation case of myrtle rust (Austropuccinia psidii) on paperbark tea trees (Melaleuca quinquenervia) in New South Wales (NSW), Australia. The method integrates unmanned aerial vehicles (UAVs), hyperspectral image sensors, and data processing algorithms using machine learning. Imagery is acquired using a Headwall Nano-Hyperspec R camera, orthorectified in Headwall SpectralView R , and processed in Python programming language using eXtreme Gradient Boosting (XGBoost), Geospatial Data Abstraction Library (GDAL) and Scikit-learn third-party libraries. In total, 11,385 samples were extracted and labelled into five classes: two classes for deterioration status and three classes for background objects. Insights reveal individual detection rates of 95% for healthy trees, 97% for deteriorated trees and a global multiclass detection rate of 97%. The methodology is versatile to be applied to additional datasets taken with different image sensors, and the processing of large datasets with freeware tools.", "keywords": "Creative Commons: Attribution 2.5 Austropuccinia psidii,drones,hyperspectral camera,machine learning,Melaleuca quinquenervia,myrtle rust,non-invasive assessment,paperbark,unmanned aerial vehicles (UAV),xgboost", "introduction": "Exotic pathogens have caused irreversible damage to flora and fauna within a range of ecosystems worldwide. Popular outbreaks include the enormous devastations of chestnut blight (Endothia parasitica) on American chestnut trees (Castanea dentata) in the U.S. [1][2], sudden oak death (Phytophthora ramorum) on oak populations (Quercus agrifolia) in Europe, California and Oregon [3][4][5], dieback (Phytophthora cinnamomi) on hundreds of hosts globally [6][7][8] rust (Austropuccinia psidii) on Myrtaceae family plants in Australia [9][10][11][12]. The effects of the latter case have raised national alerts and response programmes given the extensive host range and the ecological and economic importance of Myrtaceae plants in the Australian environment [13][14][15][16]. As a result, various surveillance and eradication programmes have been applied in an attempt to minimise the impacts invasive pathogens cause on local hosts such as dieback in the Western Australia Jarrah forests [17], sudden oak death in the tan oak forests of the U.S. [18] and rapid ohia death (Ceratocystis fimbriata)[19]on ohia trees (Metrosideros polymorpha) in Hawaii .[20]Modern surveillance methods to map hosts vulnerable to and affected by exotic pathogens can be classified in site-based and remote sensing methods, according to Lawley et al. . Site-based approaches are commonly small regions used to collect exhaustive compositional and structural indicators of vegetation condition with a strong focus on biophysical attributes of single vegetation communities [21][22,. These methods, nonetheless, require deep expertise and time to conduct experimentation, data collection and validation that, along with their limited area they can cover, represent a challenge while assessing effects on a broad scale 23]. Research has also suggested the design of decision frameworks to monitor and control the most threatened species [24][17,25,. Although these models can determine flora species that require immediate management control, limitations on the amount of tangible, feasible and broad quantified data of vulnerable host areas 26] have resulted in lack of support from state and federal governments [21].[27]The role of remote sensing methods to assess and quantify the impacts of invasive pathogens in broad scale has increased exponentially [28,. Standard approaches comprise the use of spectral and image sensors through satellite, manned and unmanned aircraft technology 29]. Concerning sensing technology by itself, applied methods by research communities range from the use of non-imaging spectroradiometers, fluorescence, multi-, hyperspectral and thermal cameras, and light detection and ranging (LiDAR) technology [30][31][32][33]. These equipment are usually employed for the calculation of spectral indexes [34][35][36][37] and regression models in the host range [38][39,. Nevertheless, these methods are mainly focused on the quantification, distribution, among other physical properties of flora species.40]Satellite and manned aircraft surveys have reported limitations concerning resolution, operational costs and unfavourable climate conditions (e.g. cloudiness and hazard winds) . In contrast, the continuous development of unmanned aerial vehicles (UAVs) designs, navigation systems, portable image sensors and cutting-edge machine learning methods allow unobtrusive, accurate and versatile surveillance tools in precision agriculture and biosecurity [41][42][43]. Many studies have positioned UAVs for the collection of aerial imagery in applications such as weeds, diseases and pests mapping, and wildlife monitoring [44][21,[45][46]. More recently, unmanned aerial systems (UASs) have been deployed in cluttered and global positioning system (GPS)-denied environments [47].[48]Approaches to the use of UAVs, hyperspectral imagery and artificial intelligence are gaining popularity. For example, Aasen et al.  deployed UAS to boost vegetation monitoring efforts using hyperspectral three-dimensional (3D) imagery. Nasi et al. [49] developed techniques to assess pest damages at canopy levels using spectral indexes and k-nearest neighbour (k-NN) classifiers, achieving global detection rates of 90%. Similar research focused on diseases monitoring, however, has been limited. Calderon et al. [50] for instance, evaluated the early detection and quantification of Verticillium Wilt in Olive plants using support vector machines (SVM) and linear discriminant analysis (LDA), obtaining mixed accuracy results among the evaluated classes of infection severity (59% to 75%) [51]. Albetis et al. [52] presented a system to discriminate asymptomatic and symptomatic red and white vineyards cultivars by Flavescence doree, using UAVs, multispectral imagery and up to 20 data features, collecting contrasting results between the cultivars and maximum accuracy rates of 88%.[53]In sum, the integration of site-based and remote sensing frameworks have boosted the capabilities of these surveillance solutions by combining data from abiotic and biotic factors and spectral responses, respectively . However, this synthesis is still challenging due to the high number of singularities, data correlation and validation procedures presented in each case study.[54]Considering the importance of site-based and remote sensing methods to obtain reliable and broader assessments of forest health, specifically, for pest and fungal assessments , this paper presents an integrated system that classifies and maps natural and plantation forests exposed and deteriorated by fungal pathogens using UAVs, hyperspectral sensors and artificial intelligence. The framework is exemplified by a case study of myrtle rust on paperbark tea trees (Melaleuca quinquenervia) in a swamp ecosystem of north-east New South Wales (NSW), Australia.[55]", "body": "Exotic pathogens have caused irreversible damage to flora and fauna within a range of ecosystems worldwide. Popular outbreaks include the enormous devastations of chestnut blight (Endothia parasitica) on American chestnut trees (Castanea dentata) in the U.S. on ohia trees (Metrosideros polymorpha) in Hawaii Modern surveillance methods to map hosts vulnerable to and affected by exotic pathogens can be classified in site-based and remote sensing methods, according to Lawley et al. The role of remote sensing methods to assess and quantify the impacts of invasive pathogens in broad scale has increased exponentially Satellite and manned aircraft surveys have reported limitations concerning resolution, operational costs and unfavourable climate conditions (e.g. cloudiness and hazard winds) Approaches to the use of UAVs, hyperspectral imagery and artificial intelligence are gaining popularity. For example, Aasen et al. In sum, the integration of site-based and remote sensing frameworks have boosted the capabilities of these surveillance solutions by combining data from abiotic and biotic factors and spectral responses, respectively Considering the importance of site-based and remote sensing methods to obtain reliable and broader assessments of forest health, specifically, for pest and fungal assessments System FrameworkA novel framework was designed for the assessment of natural and plantation forests exposed and potentially exposed to pathogens as presented in  Data AcquisitionThe data acquisition process involves an indirect data collection campaign using an airborne system, and direct ground assessments of the studied pathogen through exclusion trials, which are controlled by biosecurity experts. Airborne data is compiled using a UAV, image sensors and a ground workstation in the site to acquire data above the tree canopy. Similarly, ground data is collected through field assessment insights by biosecurity experts. Field assessments bring several factors such as growth, reproduction and regeneration on coppiced trees exposed to any specific pathogen. A database is created by labelling, georeferencing and correlating relevant insights into every tested plant. Details on the studied area, flight campaign and field assessments can be found in Sections 3.1, 3.2 and 3.3.UAV and Ground StationThis methodology incorporates a hexa-rotor DJI S800 EVO (DJI, Guangdong, China) UAV. The drone features high-performance brushless rotors, a total load capacity of 3.9 kg and dimensions of 118 \u00d7 100 \u00d7 50 cm. It follows an automatic mission route using the DJI Ground Station 4.0 software, controlling the route, speed, height above the ground, and overlapping values remotely. It is worth mentioning, however, that other UAVs with similar characteristics can also be used and included into the airborne data collection process of Figure SensorsSpectral data is collected using a Headwall Nano-Hyperspec R hyperspectral camera (Headwall Photonics Inc., MA, USA). This visible near infra-red (VNIR) sensor provides spectral wavelength responses up to 274 bands, a wavelength range from 385 to 1000 nm, a spectral resolution of 2.2 nm, a frame rate of 300 Hz, spatial bands of 640 pixels and a total storage limit of 480 GB. The camera is mounted in a customised gimbal system that augments data quality by minimising externaldisturbances such as roll, pitch and yaw oscillations, as depicted in Figure Data preparationImagery is downloaded and fed into a range of software solutions to transform raw data into filtered and orthorectified spectral bands in reflectance. Using Headwall SpectralView R software, raw hypercubes from the surveyed area were automatically processed. The preprocessing operations included radiance, orthorectification (through ground control points) and the white reference illumination spectrum (WRIS). The WRIS spectrum comes from the extraction of the radiance signature of a Spectralon target from an acquired image in the surveyed area. Using the orthorectified imagery and the WRIS, reflectance data and scene shading are calculated through CSIRO | Data61 Scyven 1.3.0. software From a recovered red, green, blue colour model (RGB) image of the cropped hypercube, each tree is graphically labelled using their tracked GPS coordinates in Argis ArcMap 10.4. To handle all the data processing from the proposed system framework, Algorithm 1 was developed. These tasks were conducted with Python 2.7.14 programming language and several third-party libraries for data manipulation and machine learning, including Geospatial Data Abstraction Library (GDAL) 2.2.2 eXtreme Gradient Boosting (XGBoost) 0.6 where K is the kernel of the filter and w the size of the window. Here, up to three kernels for w = 3, w = 7 and w = 15 are calculated per vegetation index. All the hypercube bands in reflectance, as well as calculated vegetation spectral indexes, are denominated data features. In Step 3, an array of features is generated from all the retrieved bands I and the calculated indexes S.Training and PredictionThe Experimentation SetupSiteAs displayed in Figure Flight campaignThe acquired data from flight campaign incorporated a single mission route. The UAV was operated with a constant flight height of 20 m above the ground, an overlap of 80%, side lap of 50%, mean velocity of 4.52 km/h and a total distance of 1.43 km. The acquired hyperspectral dataset had a vertical and horizontal ground sample distances (GSD) of 4.7 cm/pixel.Field AssessmentsIn order to evaluate the effects and flora response of myrtle rust on paperbark trees, a biological study in an exclusion trial on the mentioned site was conducted. Several on-ground assessments were conducted in individual trees within a replicated block design using four treatments: trees treated with fungicides (F), insecticides (I), fungicides and insecticides (F + I) and trees without any treatment action. Figure PreprocessingAs mentioned in Section 2.2, the entire surveyed area included an exclusion trial. As a result, an orthorectified hyperspectral cube in reflectance with spatial dimensions of 2652 \u00d7 1882 pixels sized 5.4 GB was generated. This area was cropped from the original hypercube, reducing computational costs and discarding irrelevant data. Eventually, a 1.7 GB cube of 1200 \u00d7 1400 pixels as depicted in  Training and PredictionThe XGBoost classifier contains several hyper-parameters to be set. Following a grid search technique, in which the classifier performance and detection rates were tracked with a set of possible hyper-parameters, the optimal values found for this case study were: estimators = 100, learning rate = 0.1, maximum depth = 3where \"estimators\" is the number of trees, \"learning rate\" is the step size of each boosting step and \"maximum depth\" is the maximum depth per tree that defines the complexity of the model.Results and DiscussionTo visualise the benefits of inserting an optimisation scheme in Step 8 of Algorithm 1, detection rates were tracked by training and running the classifier multiple times with only a set of filtered features per instance. The features were ranked with their relevance by the XGBoost classifier and sorted consequently, as illustrated in Figure showed here is relevant to the fitted XGBoost model only and results may differ if the same features are processed through other machine learning techniques. It is recommended, therefore, to perform individual analyses for every case study.A total of 11, 385 pixels contained in 23 features filtered by their relevance were read again inStep 14 of Algorithm 1. Data was divided into a training array D E with 9108 pixels and a testing array D T with 2277 pixels. The generated confusion matrix of the classifier and its performance report is shown in Table The performance of Algorithm 1 was tested in a computer with the following characteristics:Processor Intel R Core TM i7-4770, 256 GB SSD, 16 GB RAM, Windows 7 64bit and AMD Radeon TM HD 8490. It contains a report of the elapsed seconds for the application to accomplish the primary data processing, training and prediction tasks, as illustrated in Table A hypercube covering the entire area flown was also processed using the trained model with resultsshown in Figure Nevertheless, it is suggested to revise their prediction performance with new data. Like any model based on decision trees, over-fitting may occur, and misleading results might be generated. In those situations, labelling misclassified data, aggregating them into the features database and rerunning the algorithm is suggested.The availability to process and classify data with small GSD values demonstrates the potential of UASs for remote sensing equipment compared with satellite and manned aircraft for forest health assessments on forest and tree plantations, and traditional estimation methods, such as statistical regression models in regions of study. In comparison with similar approaches of non-invasive assessment techniques using UAVs and spectral sensors, this framework does not provide general spectral indexes that can be applied with different classifiers and similar evaluations. In contrast, this presented method boosts the efficiency of the classifier by receiving feedback from the accuracy scores of every feature and transforming the input data in consequence. The more explicit the data for the classifier is, the better the classification rates are. Furthermore, it is also demonstrated that a classifier which processes and combines data from multiple spectral indexes provides better performance than analysing individual components from different source sensors.Conclusions", "conclusions": "This paper describes a pipeline methodology for effective detection and mapping of indicators of poor health in forest and plantation trees integrating UAS technology and artificial intelligence approaches. The techniques were illustrated with an accurate classification and segmentation task of paperbark tea trees deteriorated by myrtle rust from an exclusion trial in NSW, Australia. Here, the system achieved detection rates of 97.24% for healthy trees and 94.72% for affected trees. The algorithm obtained a multiclass detection rate of 97.35%. Data labelling is a task that demands many resources from both site-based and remote sensing methods, affecting smoothly, the accuracy and reliability of the classifier results due to human errors.The approach can be used to train various datasets from different sensors to improve detection rates that single solutions offer as well as the capability of processing large datasets using free-ware software. The case study demonstrates an effective approach that allows rapid and accurate indicators, and alterations of exposed areas at early stages. However, understanding disease epidemiology and interactions between the pathogen and host is still required to enable the effective use of these technologies.Future research should discuss the potential of monitoring the evolution of affected species through time, the prediction of expansion directions and rates of the disease, and how data will contribute on improving control actions to deter their presence in unaffected areas. Technologically, future works should analyse and compare the efficacy of unsupervised algorithms to label vegetation items accurately, integrate the best approaches in the proposed pipeline and evaluate regression models that predict data based on other biophysical information offered by site-based methods. ", "SDG": [15]}, "conservation_biology_2014_stephanson_putting_people_on_the_map_through_an_approach_that_integrates_social_data_in": {"name": "Putting People on the Map through an Approach That Integrates Social Data in Conservation Planning", "abstract": " Conservation planning is integral to strategic and effective operations of conservation organizations. Drawing upon biological sciences, conservation planning has historically made limited use of social data. We offer an approach for integrating data on social well-being into conservation planning that captures and places into context the spatial patterns and trends in human needs and capacities. This hierarchical approach provides a nested framework for characterizing and mapping data on social well-being in 5 domains: economic well-being, health, political empowerment, education, and culture. These 5 domains each have multiple attributes; each attribute may be characterized by one or more indicators. Through existing or novel data that display spatial and temporal heterogeneity in social well-being, conservation scientists, planners, and decision makers may measure, benchmark, map, and integrate these data within conservation planning processes. Selecting indicators and integrating these data into conservation planning is an iterative, participatory process tailored to the local context and planning goals. Social well-being data complement biophysical and threat-oriented social data within conservation planning processes to inform decisions regarding where and how to conserve biodiversity, provide a structure for exploring socioecological relationships, and to foster adaptive management. Building upon existing conservation planning methods and insights from multiple disciplines, this approach to putting people on the map can readily merge with current planning practices to facilitate more rigorous decision making.", "keywords": "conservation planning,ecoregion conservation,political empowerment,social indicators,social well-being", "introduction": "Conservation planning is integral to strategic and effective operations of governmental and nongovernmental conservation organizations. Spatial and strategic conservation planning provide direction regarding which elements of biodiversity to protect and how to do so, allowing conservation organizations to set priorities and allocate scarce resources more efficiently (O'Connor et al. 2003;. (We use the term conservation planning broadly, referring to discrete decision making processes designed to identify options and to set priorities among them in pursuit of specific conservation objectives.) The goals of conservation planning are to protect the most important areas of biodiversity Pressey & Bottrill 2008), maximize returns on conservation investments (Margules & Pressey 2000), and promote effective conservation interventions (Brooks et al. 2006).(Knight et al. 2006)The field of conservation planning draws heavily upon biological sciences. Researchers and practitioners have developed sophisticated methods to identify global and regional conservation priorities (Brooks et al. 2006;Moilanen et al. 2009;. Once these priorities have been delineated, methods to select biodiversity targets (i.e., focal species, habitats, ecological processes), assess their viability (i.e., likelihood of persistence), and identify threats further narrow the geographic and thematic focus of future conservation interventions Pressey & Bottrill 2009)(Salafsky et al. 2002;. Finally, operational planning guidelines (e.g., Parrish et al. 2003)Knight et al. 2006; and standard conservation strategies CMP 2007) provide guidance for protection of prioritized biodiversity targets.(Salafsky et al. 2008)Less attention in conservation planning has been paid to the social factors that influence (and are influenced by) an organization's choice of strategic action (O'Connor et al. 2003;, though the need to incorporate sophisticated and diverse social data into conservation planning is increasingly recognized Cowling & Wilhelm-Rechmann 2007)(Knight & Cowling 2007;Polasky 2008;. Given that social variables (e.g., values, norms, institutions, human well-being) underpin most opportunities and constraints for effective conservation action Pressey & Bottrill 2008), understanding social phenomena that affect conservation action and biodiversity targets is fundamental to conservation success (Cowling & Wilhelm-Rechmann 2007)(Cowling et al. 2004;Sheil et al. 2006;).Polasky 2008Traditionally, social data in conservation planning have focused upon direct and indirect threats to biodiversity. These direct and indirect threats are represented by direct measures of human resource use, proxies for human behavior (e.g., population density), and by indicators of human impacts (e.g., tons of fish harvested) (Gorenflo & Brandon 2006;CMP 2007;. Novel approaches that rely on a wider range of social data and analyses include incorporating spatially explicit information about the equity Salafsky et al. 2008)) and economic costs of conservation (Halpern et al. 2013(Naidoo et al. 2006;); mapping social assets (del Campo & Wali 2007) and local concerns and preferences Wilson et al. 2007(Sheil et al. 2006; as foundations for conservation action; predicting conservation return on investment based on social and ecological factors Knight et al. 2010)(O'Connor et al. 2003;; and integrating data on social institutions and governance structures in conservation planning Wilson et al. 2007).(Pressey & Bottrill 2008)We extended this recent work by devising a framework and process for integrating data on social wellbeing into conservation planning. This approach, which we termed Putting People on the Map (P-MAP), captures and places into context the patterns and trends in human needs and capacities and, thus, complements other biophysical and social data used in conservation planning. The P-MAP approach builds upon in-country collaboration with conservation practitioners at 7 landscape conservation programs in 8 countries (2006)(2007)(2008)) (2009). We introduce the P-MAP approach and highlight 4 key questions that can be addressed by integrating data on social well-being into conservation planning processes. (For detailed technical guidance, see (Morrison et al. 2009)Stephanson & Mascia 2009a.) Though we draw upon existing social monitoring frameworks and conservation planning approaches, P-MAP is a novel and robust approach intended for use in any conservation planning process that would benefit from a richer understanding of social well-being as 1 component of social context., 2009bcomponents by comparing and identifying emergent concepts from approaches developed by , the Millennium Ecosystem Assessment (MEA) Sen (1999), and a review of social indicators used to monitor protected areas (A. Khurshid & M.B.M., unpublished data). Sen's capabilities approach, which defines well-being based on human capabilities \"to lead the kinds of lives that they have reason to value . . . \" (2005), was the first to extend the definition of poverty beyond simplistic measures of gross domestic product or per capita income (1999) Despite their disparate origins, Sen, MEA, and the social indicators review coalesce around a similar definition of social well-being as the human capabilities and conditions associated with a productive, fulfilling life (Sen 1999)(MEA 2005;. Given that social well-being inherently comprises multiple components (Table CMEPSP 2009)), we derived a hierarchical framework for P-MAP with 5 primary domains: economic well-being, health, political empowerment, education, and culture. Economic wellbeing represents the resources people use to meet basic needs and access other sources of well-being 1. Health is a state of complete physical, mental, and social well-being, not merely the absence of disease or infirmity (Sen 1999). Political empowerment refers to people's ability to participate in and influence decision making processes that affect their lives (WHO 1946). Education refers to formal and informal structures, systems, and practices used to transfer knowledge and skills in a society. Culture refers to the \"distinctive spiritual, material, intellectual and emotional features of society or a social group, . . . encompass[ing] . . . lifestyles, ways of living together, value systems, traditions and beliefs\" (UNDP et al. 2005).(UNESCO 2002)", "body": "Conservation planning is integral to strategic and effective operations of governmental and nongovernmental conservation organizations. Spatial and strategic conservation planning provide direction regarding which elements of biodiversity to protect and how to do so, allowing conservation organizations to set priorities and allocate scarce resources more efficiently The field of conservation planning draws heavily upon biological sciences. Researchers and practitioners have developed sophisticated methods to identify global and regional conservation priorities Less attention in conservation planning has been paid to the social factors that influence (and are influenced by) an organization's choice of strategic action Traditionally, social data in conservation planning have focused upon direct and indirect threats to biodiversity. These direct and indirect threats are represented by direct measures of human resource use, proxies for human behavior (e.g., population density), and by indicators of human impacts (e.g., tons of fish harvested) We extended this recent work by devising a framework and process for integrating data on social wellbeing into conservation planning. This approach, which we termed Putting People on the Map (P-MAP), captures and places into context the patterns and trends in human needs and capacities and, thus, complements other biophysical and social data used in conservation planning. The P-MAP approach builds upon in-country collaboration with conservation practitioners at 7 landscape conservation programs in 8 countries components by comparing and identifying emergent concepts from approaches developed by Framework for Measuring Social Well-BeingTo operationalize this framework within P-MAP, we looked to conservation practice for inspirationparticularly the flexible approach developed by the Conservation Measures Partnership (CMP). The Open Standards for the Practice of Conservation Similar to the Open Standards and UN Human Development Index (UNDP 2007), we used a hierarchical structure to categorize and quantify social well-being in P-MAP. In this structure, the domain represents a broad component of social well-being. Each domain contains numerous social attributes. Each attribute can be measured with 1 or more social indicators (Table Through P-MAP, we examined indicators that represent the well-being of local residents (i.e., construct validity), which is a fundamental component of the social context of conservation. Social well-being is tightly linked to biodiversity conservation through complex and reciprocal relationships, both as variables that influence human interactions with the environment and as a manifestation of conservation interventions. Contextual variables such as degree of water security (health attribute) and strength of resource tenure (political empowerment attribute), for example, may influence resource use patterns (e.g., harvest rates, locations). A village without secure access to water might dam a river, whereas residents with secure hunting rights might guard wildlife against poaching. Conversely, conservation interventions may (a) directly impact social well-being; (b) directly impact the flow of ecosystem services, with indirect effects on social well-being; or, (c) by affecting one component of social well-being, induce secondary impacts on other attributes. For example, establishment of a marine protected area (conservation intervention) might directly reshape local residents' resource rights (political empowerment attribute); indirectly influence food security (health attribute) by directly protecting fish populations for harvest (ecosystem services); and indirectly enhance educational attainment (education attribute) by enhancing food security and, thus, allowing children to attend school more regularly (secondary impact). Moreover, the social equity of conservation plans may influence the likelihood of plan implementation and success Open StandardsTarget Features of a place that are chosen to represent and encompass the biodiversity found in a conservation area.Targets can be focal species, habitats, or ecological systems and processes.Key ecological attributesAn aspect of a target's biology or ecology that, if present, defines a healthy target and, if missing or altered, would lead to the loss or extreme degradation of that target over time.IndicatorA specific, measurable characteristic of the attribute or a collection of such characteristics combined into an index.Viability assessmentRating to classify the state of the target (based on indicator measurements) as Poor, Fair, Good, Very good.P-MAP DomainThe specifiable components of social well-being: economic well-being, health, political empowerment, education, and culture.AttributesCharacteristics or qualities that describe each domain (e.g., Health = Food security + access to medical care + access to clean water)IndicatorA specific, measurable representation of the attribute (e.g., Food security is measured by: Proportion of population below minimum level of dietary energy consumption).BenchmarkingRating to compare the value of an indicator against national or global average (where possible).a Putting People on the Map. b Sources: CMP (2007); authors.Where appropriate, for efficiency and comparability, we look to globally accepted indicators to measure social well-being. For example, MDGs (UN DESA 2008) provide numerous conservation-relevant indicators of social well-being that are measured by governments globally.Social Well-Being DataThe P-MAP approach explicitly recognizes that social well-being is heterogeneous in space and time. To in-form conservation planning, social data must reflect this spatial and temporal heterogeneity at the finest practicable resolution. Commonly available social data sets (e.g., government censuses, demographic and health surveys, and MDG indicator data), which tend to track social well-being at district-level spatial scales or larger, are a default source of social well-being data for P-MAP. Though secondary source data may not always provide sufficient spatial and temporal resolution to inform conservation planning at extremely local levels or in particularly dynamic social settings, existing data can inform conservation planning without the costs or complexities associated with primary data collection. For example, data on economic activity collected at the scale of village development committees (VDCs, each roughly analogous to a municipality) in Nepal's Terai region provides information that is useful for planning village-level interventions (Fig. The disconnect between ecological boundaries (e.g., ecoregions, landscapes) and political boundaries commonly associated with social data (e.g., districts, provinces) presents a challenge to integration of social and ecological data in conservation planning The spatial and temporal scale of conservation planning and management decisions influence the extent and resolution at which data on social well-being are collected and analyzed. If one is setting priorities among countries, national-level data on social well-being may be sufficient; if allocating resources among districts within a country, then data at the district scale (or finer) are required. In practice, as a rule of thumb, conservation planning at the ecoregion scale necessitates social data at district or subdistrict levels; landscape-level planning requires subdistrict or community-level data; and community-scale planning requires social data at the household level. Similarly, iterative conservation planning processes would benefit from data on social well-being that correspond to planning timescales (e.g., a 3-year management plan benefits from new data every 3 years). Government agencies Social Data in ContextIn conservation practice, biological data are sometimes rated according to viability criteria that put numbers into context and provide insights for interpretation (Table Process as KeyA framework helps organize data into conceptually distinct categories, but cannot replace processes for selecting representative and locally relevant indicators; determining scales at which to measure indicators; or for applying data in a conservation planning system. Selecting indicators generally benefits from a participatory process, in which diverse stakeholders with regional expertise (e.g., knowledgeable social scientists, government officials, local residents, conservation and development practitioners) determine indicators to describe social well-being within the conservation planning unit. The process to define indicators represents an opportunity to explore relationships between conservation priorities and social well-being and to begin identifying appropriate conservation interventions. A portfolio of strategically chosen indicators can represent the multiple dimensions of social well-being. In some cases, it may be possible to measure a portfolio of such indicators by drawing upon consistently available, accessible data (e.g., government statistics, UN statistics). Drawing upon secondary sources is particularly important for conservation planning at large spatial scales, where primary data collection is often impractical or impossible for conservation organizations (Morrison The specific steps required to structure and integrate data on social well-being into a conservation planning tool or process depend upon the planning system itself. In spatially explicit optimization tools such as Marxan (www.uq.edu.au/marxan/), for example, social well-being data must be transformed into one or more cost layers that shape the relative importance (i.e., priority) of a given location for conservation investment. In strategic conservation planning tools such as the Open Standards, by contrast, indicators characterizing raw or benchmarked data on social well-being may be considered targets (i.e., what conservation interventions seek to achieve), threats or opportunities, enabling conditions (i.e., macroscale social variables that foster or hinder intervention effectiveness), or exogenous variables that simply enrich understanding of the context within which conservation planning and action occur Improved Conservation PlanningThe P-MAP approach can inform conservation decisions by providing a rigorous yet flexible method for integrating the multiple dimensions of social well-being into conservation planning. Characterizing social context provides conservation planners with a starting point for addressing 4 sets of key questions: Conservationists debate the appropriateness of social targets as legitimate goals for conservation planning and action Regardless of one's perspective on this debate, integration of social well-being data into conservation planning provides a richer understanding of conservation context. Examining patterns and trends in social well-being allows decision makers to determine if social issues are cause for concern, where problems are acute, and how social dynamics are changing over time. This knowledge permits conservation organizations to make informed choices about the social contexts in which they wish to operate, and the ecological (and possibly social) targets upon which they wish to focus. All else being equal, some organizations may be drawn to areas where social wellbeing is a particularly acute concern, while others may choose to avoid places characterized by, for example, extreme poverty, poor human health, or widespread political disenfranchisement. Conservation plans that assess and address social equity when siting conservation interventions may increase the likelihood of plan implementation Designing StrategiesIncorporating data on social well-being into conservation planning can facilitate selection of conservation strategies that respond to local human capacities and needs, resulting in more sustainable and effective conservation action Socioecological RelationshipsIntegrating social and biological data through conservation planning can facilitate novel analyses that advance conservation science. Global analyses examining spatial patterns of linguistic diversity and biological diversity, for example, reveal sites of high biocultural diversity and highlight common threats facing endangered species and indigenous peoples Adaptive ManagementIncorporating social well-being data in conservation planning provides a foundation for adaptive management in the face of shifting social contexts. (We use adaptive management broadly, to mean learning from management experiences and-based on these experiences and shifting contexts-modifying management activities to deliver desired outcomes more effectively.) Declining food security, for example, may increase pressure on natural resources, requiring shifts in conservation strategies and new partnerships to address local livelihood concerns. Similarly, spatial and temporal data on land tenure may highlight areas vulnerable to appropriation and exploitation by extractive industries (von Braun & Meinzen-Dick 2009), suggesting the need for conservation strategies that strengthen local land rights and, thus, prevent industrial exploitation of priority conservation areas. Equipped with such information, managers can creatively adapt conservation strategies to respond to shifting challenges Relation of P-MAP to Conservation PracticeThe P-MAP approach builds upon conservation practice, complementing other protocols for assessing biological and social context. Similar to the Open Standards approach Despite sharing analogous participatory processes and hierarchical approaches for conceptualizing data in conservation planning, P-MAP and the Open Standards (CMP 2013) differ in their overall intent, their conceptualization of social well-being, and use of social data. First, the Open Standards are a conservation planning tool (CMP 2013), whereas P-MAP is an approach to characterizing social well-being and embedding it within diverse conservation planning tools (including the Open Standards but not limited to them). Second, while the Open Standards are explicitly designed to address planning after geographic priority setting has occurred (CMP 2013), P-MAP is designed to inform both spatial priority setting among geographies and strategic planning within priority geographies. Third, the Open Standards use the MEA framework for categorizing attributes of human well-being (CMP 2013), whereas P-MAP draws upon MEA (2005), Sen (1999), and conservation practice (A. Khurshid and M.B.M., unpublished data). Fourth, the Open Standards principally consider human well-being a target (i.e., desired conservation end) and secondarily as a factor (i.e., means to a desired conservation end) Consistent with Though scientists and practitioners increasingly recognize that every conservation plan and every conservation action occur within a social context, social data and social considerations represent a frontier in conservation planning. With its focus on social well-being, P-MAP complements other recent innovations in the use of social data for conservation planning, but many important aspects of social context remain poorly developed or unexplored. Arenas ripe for further exploration and operationalizing within conservation planning include equity; laws and policies; environmental beliefs, values, and place attachment; social networks; and more systematic, evidence-based approaches to operationalizing threats (e.g., direct measures of discrete human behaviors). At a more fundamental level, the potential of appreciative inquiry Figure 1 .Figure 2 .Table 1 . Social domains characteristic of the Putting People on the Map (P-MAP) approach and the established frameworks for characterizing social well-being upon which it is based. a", "conclusions": "", "SDG": [15]}, "estimating_animal_acoustic_diversity_in_tropical_environments_using_unsupervised_multiresolution_analysis": {"name": "Estimating animal acoustic diversity in tropical environments using unsupervised multiresolution analysis", "abstract": " Ecoacoustic monitoring has proved to be a viable approach to capture ecological data related to animal communities. While experts can manually annotate audio samples, the analysis of large datasets can be significantly facilitated by automatic pattern recognition methods. Unsupervised learning methods, which do not require labelled data, are particularly well suited to analyse poorly documented habitats, such as tropical environments. Here we propose a new method, named Multiresolution Analysis of Acoustic Diversity (MAAD), to automate the detection of relevant structure in audio data. MAAD was designed to decompose the acoustic community into few elementary components (soundtypes) based on their time-frequency attributes. First, we used the short-time Fourier transform to detect regions of interest (ROIs) in the time-frequency domain. Then, we characterised these ROIs by (1) estimating the median frequency and (2) by running a 2D wavelet analysis at multiple scales and angles. Finally, we grouped the ROIs using a model-based subspace clustering technique so that ROIs were automatically annotated and clustered into soundtypes. To test the performance of the automatic method, we applied MAAD to two distinct tropical environments in French Guiana, a lowland high rainforest and a rock savanna, and we compared manual and automatic annotations using the adjusted Rand index. The similarity between the manual and automated partitions was high and consistent, indicating that the clusters found are intelligible and can be used for further analysis. Moreover, the weight of the features estimated by the clustering process revealed important information about the structure of the acoustic communities. In particular, the median frequency had the strongest effect on modelling the clusters and on classification performance, suggesting a role in community organisation. The number of clusters found in MAAD can be regarded as an estimation of the soundtype richness in a given environment. MAAD is a comprehensive and promising method to automatically analyse passive acoustic recordings. Combining MAAD and manual analysis would maximally exploit the strengths of both human reasoning and computer algorithms. Thereby, the composition of the acoustic community could be estimated accurately, quickly and at large scale.", "keywords": "Ecoacoustic,monitoring,Acoustic,community,Unsupervised,machine,learning,Wavelets,Nocturnal,soundscape", "introduction": "The diversity of life forms is an invaluable biological resource threatened by anthropogenic environmental change (Pimm et al., 1995;. Given the pace of this change, there is an imperative need to develop quantitative indicators that provide specific information on the state of biodiversity Thomas et al., 2004). With the advent of new sensor technology it is possible to remotely collect environmental data, assisting to determine, and eventually buffer, the pressures on biological diversity and ecosystem services (Pereira et al., 2013). In particular, the use of passive acoustic sensors in ecological research, or ecoacoustics (Petrou et al., 2015), has proved to be a viable method for biodiversity assessment that can be scaled up at multiple spatial and temporal scales (Sueur and Farina, 2015). The environmental sounds collected by these automated sensors usually include a large combination of both biotic and abiotic sounds, which are mixed down into a single time series. Such interlaced audio data needs to be unravelled in order to extract and to decipher ecological meaningful information, which represents to date a prominent bottleneck for the application of acoustic sensors in biodiversity monitoring.(Towsey et al., 2014)A significant proportion of animal species produce sounds for social interaction, navigation or predator-prey encounters .(Fletcher, 2014)Most of these acoustic signals have a species-specific signature that can be exploited for the remote identification of species. The use of these signatures is a direct way to retrieve ecological data about species presence, abundance, status and distribution. Manual species identification by experts can be carried on audio datasets, but for large collections, the analysis can be facilitated by automatic pattern recognition methods such as supervised learning . Supervised learning is a method to build a statistical classifier based on labelled training data (Kershenbaum et al., 2016). An increasing number of supervised learning tools have been adapted to identify automatically single species (Webb and Copsey, 2011)(Dugan et al., 2013;Ganchev et al., 2015; or several species Ulloa et al., 2016)(Briggs et al., 2012;Potamitis, 2014;Heinicke et al., 2015;Dong et al., 2015;Xie et al., 2016;. The application of supervised learning is limited by the large reference datasets required to 'train' the classifiers and the high acoustic similarity sometimes observed between closely related taxa. The available sound libraries, even if providing thousands of samples, still cover only a small fraction of the animal sound diversity, at both population and species scales.Ruiz-Mu\u00f1oz et al., 2016)An alternative to species identification consists in characterising the acoustic community or the soundscape with the use of acoustic indices . Rather than focusing on target species, acoustic indices aim to describe the global structure of the soundscape. A variety of indices have been proposed and applied to terrestrial (Sueur et al., 2014)(Lellouch et al., 2014;Farina et al., 2015; and underwater habitats Fuller et al., 2015)(Parks et al., 2014;Desjonqu\u00e8res et al., 2015;Harris et al., 2016;. These indices revealed, for example, changes in bird species richness among woodland habitats Buscaino et al., 2016) or dynamics of the soundscape across different temporal scales (Depraetere et al., 2012). However, they also showed to be sensitive to transitory or permanent background noise, variation in the distance of the animals to the sensor, and the relative sound amplitude or the calling rate of the signalling animal (Rodriguez et al., 2014)(Gasc et al., 2015;.Kendrick et al., 2016)More recently, methods based on unsupervised learning have been adapted to audio recordings achieved in natural environments. Unsupervised learning searches for structures or patterns in a dataset without using labels. This approach has been extensively used to draw inferences in areas where labelled data is inaccessible or too expensive, such as astronomy (Way, 2012), genetics and genomics . In an innovative work, (Libbrecht and Noble, 2015) adapted sparse-coding and source separation algorithms to extract shift-invariant spectro-temporal \"atoms\" from environmental recordings. However, the authors did not establish a clear link between the spectrotemporal \"atoms\" and ecological or biological processes. Unsupervised learning has also been used as a pre-processing step for the classification task, significantly improving the classification performance on species recognition Eldridge et al. (2016). In their approach, (Stowell and Plumbley, 2014) first decomposed the sounds into \"atoms\" with spherical k-means, and then used the \"atoms\" as features for the supervised learning framework. Thus, unsupervised learning offers new means to characterise sounds and may provide insights on the acoustic communities of diverse and threatened ecosystems, such as those of tropical regions Stowell and Plumbley (2014)(Pekin et al., 2012;.Rodriguez et al., 2014)The present work emerges from the question: how to best measure, quantify and characterise environmental sounds (from biotic and abiotic sources) in passive acoustic recordings to get valuable ecological indicators? We propose a new data-driven method, named Multiresolution Analysis of Acoustic Diversity (MAAD), to automate the discovery of plausible and interpretable patterns in passive acoustic recordings. To build a generalized method for multiple conditions and environments, we adapted methods from the unsupervised learning field. We estimated acoustic diversity by detecting regions of interest in sound recordings and grouping them into soundtypes based on the value of their time-frequency attributes. To test the flexibility and robustness of the method, we applied MAAD to two distinct night tropical environments in French Guiana, a lowland high rainforest (HF) and a rock savanna (RS). The RS is inhabited by a distinct and likely less diverse animal community in comparison with the HF  so that it was expected to find contrasting acoustic communities between these two tropical environments. We compared manual and automated annotations to (1) evaluate the model selection procedure;(Bongers et al., 2001)(2) assess the relevance of different features in the clustering process; and (3) quantify the overall similarity between manual and MAAD soundtypes. To conclude, we give practical advices and discuss how MAAD can potentially be transferred to other environments in order to track the state and dynamics of animal communities for biodiversity studies.", "body": "The diversity of life forms is an invaluable biological resource threatened by anthropogenic environmental change A significant proportion of animal species produce sounds for social interaction, navigation or predator-prey encounters Most of these acoustic signals have a species-specific signature that can be exploited for the remote identification of species. The use of these signatures is a direct way to retrieve ecological data about species presence, abundance, status and distribution. Manual species identification by experts can be carried on audio datasets, but for large collections, the analysis can be facilitated by automatic pattern recognition methods such as supervised learning An alternative to species identification consists in characterising the acoustic community or the soundscape with the use of acoustic indices More recently, methods based on unsupervised learning have been adapted to audio recordings achieved in natural environments. Unsupervised learning searches for structures or patterns in a dataset without using labels. This approach has been extensively used to draw inferences in areas where labelled data is inaccessible or too expensive, such as astronomy (Way, 2012), genetics and genomics The present work emerges from the question: how to best measure, quantify and characterise environmental sounds (from biotic and abiotic sources) in passive acoustic recordings to get valuable ecological indicators? We propose a new data-driven method, named Multiresolution Analysis of Acoustic Diversity (MAAD), to automate the discovery of plausible and interpretable patterns in passive acoustic recordings. To build a generalized method for multiple conditions and environments, we adapted methods from the unsupervised learning field. We estimated acoustic diversity by detecting regions of interest in sound recordings and grouping them into soundtypes based on the value of their time-frequency attributes. To test the flexibility and robustness of the method, we applied MAAD to two distinct night tropical environments in French Guiana, a lowland high rainforest (HF) and a rock savanna (RS). The RS is inhabited by a distinct and likely less diverse animal community in comparison with the HF (2) assess the relevance of different features in the clustering process; and (3) quantify the overall similarity between manual and MAAD soundtypes. To conclude, we give practical advices and discuss how MAAD can potentially be transferred to other environments in order to track the state and dynamics of animal communities for biodiversity studies.Material and methodsThe workflow of the proposed method (MAAD) followed four main steps: (1) passive acoustic recordings were transformed into the timefrequency domain using the windowed short-time Fourier transform and the Fourier coefficients were filtered to remove noise and to highlight sounds that can be delimited in time and frequency, here defined as regions of interest (ROIs); (2) each ROI was then characterised by features in the time-frequency domain using 2D wavelets;(3) the ROIs with their attributes were used to automatically estimate clustering hyper-parameters; and (4) the hyper-parameters and the attributes of the ROIs were passed to a clustering algorithm that formed homogenous groups of ROIs, namely soundtypes (Fig. Audio datasetEnvironmental sounds were gathered using automated acoustic sensors (Songmeter SM2, Wildlife Acoustics Inc., Concord, MA, USA) equipped with omnidirectional microphones (PUI Audio POM-3535L-3-R, frequency response 50 Hz-20 kHz \u00b1 4 dB). A single acoustic sensor was placed at each environment, HF (04\u00b005\u203215\u2033N; 52\u00b040\u203242\u2033W) and RS (04\u00b005\u203233\u2033N; 52\u00b040\u203240\u2033W), and recorded one minute every 30 min from sunset to sunrise for 10 consecutive nights (5-15 December 2014). Each sensor was set to sample the audio at 44.1 kHz with a 16-bit resolution (mono, WAV format). This audio database was subsampled by selecting two one-minute samples per night, one four hours after sunset (22 h 17 min UTC/GMT-3 h) and one four hours before sunrise (02 h 24 min UTC/GMT-3 h). These environmental audio recordings were deposited at the sound library of the Mus\u00e9um national d'Histoire naturelle (www. sonotheque.mnhn.fr, Table Detection of regions of interest (ROIs)A region of interest is an isolated region in the time-frequency domain with a high density of energy. The automated detection of ROIs followed a four-step process computed with MATLAB (The MathWorks, Inc., Natick, MA) using the signal processing toolbox. First, we computed a spectrogram of the audio signal using the windowed short-time Fourier transform, (1024 FFT length, 50% overlap, Hamming window). Second, we applied a denoising method, namely spectral subtraction Thereby, each detected ROI was a frame of variable size in the timefrequency domain, delimited by a start and end time, and a minimum and maximum frequency. The number of ROIs found in the RS and the HF audio files were respectively 4028 and 5375, for a total of 9403.Characterization of ROIsAutomated measurements on the frequency and the time-frequency shape of each ROI were performed. To measure the frequency, a single feature was calculated: the median frequency, which is the value that divides the ROI into two frequency intervals of equal energy. This is a robust measurement that does not vary much based on the exact timefrequency bounds.To measure the shape of the ROI in the time-frequency domain a wavelet analysis was used. The purpose of this procedure was to decompose the signal into coefficients that can be saved and manipulated to better represent the information in the signal. The wavelet transform is the result of filtering the signal with a bank of specific filters (or 'wavelets'). Each analysing wavelet can be visualised as a kernel of fixed scale that moves along the data. When the wavelet encounters a feature in the data with similar shape and scale, the analysis returns a high value for the wavelet coefficient. Then, the operation is repeated at a different scale with a new dilated or contracted wavelet. In this way the wavelet transform allows a multiresolution analysis and can represent hierarchical structures of the data. This scale-by-scale analysis is particularly suited for the detection of local features in aperiodic data. Wavelets can be extended to the two dimensional case (2D), in particular to process images First, the high frequencies were recovered by convolution with the wavelet filters. By rotating and dilating the wavelet, we obtained rotation and scale covariant coefficients, which allowed discriminating the differences in shape of the different ROIs. Then, each filtered signal was averaged with a rotation-invariant low-pass filter. The rotationinvariant low-pass filter removed small differences between similar ROIs, forming homogeneous groups. The operation on a 2D signal x is formalised as:where the symbol * denotes spatial convolution, \u03d5 is a gaussian lowpass filter and \u03c8 j \u03b8 , is a wavelet dilated by 2 j and rotated by an angle \u03b8. The filter bank used consisted of wavelets of the Morlet family, at 16 scales and 8 angles: horizontal (0\u00b0), vertical (90\u00b0) and diagonals (22.5\u00b0, 45\u00b0, 67.5\u00b0, 112.5\u00b0, 135\u00b0, 157.5\u00b0). In this way, a total of 128 shape features were calculated. An illustrative subset of the 2D filters is presented in Fig. High dimensional clusteringClustering is an unsupervised learning analysis that aims at grouping objects into homogenous groups or clusters. As opposed to supervised learning, clustering is more flexible since no groups need to be defined a priori, i.e. the groups are formed based on the value of the attributes of the data. If available, labelled data can be used to estimate whether the groups found are suitable classes. To group the ROIs in homogeneous groups, a method suited to the multidimensional attributes of the ROIs was used. This method, named High Dimensional Data Clustering (HDDC), is a clustering technique based on a family of twelve parsimonious Gaussian mixture models adapted to multivariate highdimensional data The models proposed in HDDC have different regularizations that control the complexity of the clustering. The most complex model is a kj b k Q k d k , all the parameters are class-specific and the dimension is specific to each cluster. The simplest model is abQ k d, all the parameters are common between classes and the dimension of the class subspace is common. The properties of the parsimonious models in HDDC are detailed in Text S6.A model selection procedure was implemented to estimate the hyper-parameters that control the complexity of the model. These hyper-parameters are the model M, the number of groups K, and thethreshold value th to find the intrinsic dimensionality of each class. Classical model selection methods, namely AIC where \u03b8 is the set of parameter values that maximize the log-likelihood function l \u03b8 ( ), \u03be M ( ) is the number of free parameters of the model, and \u015d is the slope of the linear part of l \u03b8 ( ) with regard to the number of parameters. SHC follows the same rationale than other model selection criteria such as BIC and AIC, the likelihood of the fitted model is penalised by a function. Yet, SHC criterion has been found to be more consistent than BIC for model selection in HDDC Slope heuristics were calculated for the twelve models implemented in HDDC (see Text S6), at ten different thresholds (0.0001, 0.0005, 0.001, 0.01, 0.03, 0.05, 0.07, 0.1, 0.15, 0.2), for 39 values of K (from 2 to 40, by steps of one). Since HDDC has a random initialization, the returned log-likelihood can vary between executions. Hence, the slope heuristics value was calculated ten times for each combination of hyper-parameters. The mean value was stored and the maximum was selected to find the hyper-parameters of the HDDC models.With the hyper-parameters fixed, the model was fitted ten times with random initialisation. Random initialisation is a standard method to initiate the Expectation-maximization algorithm. This method correctly explores the parameter space to reach the global maximum of the likelihood Validation of system performanceTo evaluate and determine the performance of MAAD, proper ground truth was established and a quantitative method to compare ground truth and the system output was used. ROIs (n = 9403) of sound recordings, which were automatically detected, were examined manually using the software Raven The automatic annotations output by MAAD were compared with the manual annotations using the adjusted Rand index (ARI). The ARI is a similarity measure between two partitions where a denotes objects in a pair placed in the same group in U and in the same group in V; b denotes objects in a pair placed in the same group in U and in different groups in V; c denotes objects in a pair placed in the same group in V and in different groups in U and; and d denotes objects in a pair placed in different groups in U and in different groups in V. This index, bounded between \u00b1 1, was derived from the popular Rand index but has the advantage of being adjusted for chance with respect to the null hypothesis and can be interpreted as the difference between probabilities of concordance and discordance. Independently of the number of clusters and samples, the ARI has a value of -1 when the partitions are opposed, close to 0 for random labelling, and exactly 1 when the two partitions are identical. Clustering analyses, cluster validation and graphs were achieved with R version 3.2.0 (R Core Team, 2017).ResultsWe manually identified 35 soundtypes in the HF and 18 in the RS dataset. The relative soundtype abundance was unbalanced in both datasets (Fig. Model selectionTo begin with the cluster analysis, the most adequate model hyperparameters were identified by observing the trend of the slope heuristics criterion. On both datasets, RS and HF, slope heuristics attained a maximum value with the model a kj b k Q k d k , the most complex one (a full covariance matrix for each group), and a threshold value of 0.0005. As expected, the suitable number of clusters K was different for each habitat. The curve showing the evolution of the slope heuristics value for different K peaked between 10 and 15 with a maximum at 11 on the RS dataset and peaked between 15 and 20 with a maximum at 17 on the HF dataset (Fig. Using the manual annotations, different settings of the hyper-parameter K (i.e. the number of clusters) were tested to analyse the response of MAAD. The response at different values of K was similar in both datasets (Fig. Feature relevanceUsing the hyper-parameters found with the slope heuristics, the ROIs were automatically clustered based on their computed time-frequency attributes (129 features). Before evaluating the clustering results, the weight of the features estimated by the clustering process were analysed. Interestingly, a single feature, the median frequency, accounted for most of the variation in the data, 39.4% and 51.0% for the RS and HF respectively (Fig. In light of the weights of the features estimated during the clustering process, different subsets of features were tested (diagonal wavelet orientations, perpendicular wavelet orientations, all wavelets, median frequency and the full set) and contrasted with the manual annotations to further examine the response of MAAD. The global return on both datasets, RS and HF, was the same (Fig. Clustering resultsROIs were grouped into soundtypes through an unsupervised framework using the hyper-parameters returned by the slope heuristics criterion and providing the full set of spectro-temporal features. Comparative analysis showed a high concordance between manual and automatic partitions with an ARI of 0.77 and 0.85 for the RS and the HF environments respectively (Fig. The average computing time to process a one-minute file through the complete pipeline was 45.67 s on a desktop computer (3.4 GHz Intel Core i5 processor, 8 GB memory). Automatic annotation was on average forty times faster than human annotation.Discussion", "conclusions": "The animal acoustic diversity is known to potentially carry relevant ecological information related to the species diversity (Riede, 1993;. However, it is still challenging to use automated statistical tools to analyse and extract ecological meaningful information from passive acoustic recordings. MAAD was designed to overcome this barrier enabling to analyse environmental audio recordings by automatically decomposing the acoustic community into few elementary components based on their time-frequency attributes. Our experiments showed that the partitions derived by MAAD in distinct tropical acoustic communities were highly similar to the ones obtained by meticulous manual (aural and visual) inspection. In addition, MAAD showed that some specific features were more informative for the clustering model, revealing potential structures that partition the acoustic community.Krause and Farina, 2016)", "SDG": [15]}, "linking_biophysical_models_and_public_preferences_for_ecosystem_service_assessments_a_case_study_for_the_southern_rocky_mountains": {"name": "Linking biophysical models and public preferences for ecosystem service assessments: a case study for the Southern Rocky Mountains", "abstract": " Through extensive research, ecosystem services have been mapped using both survey-based and biophysical approaches, but comparative mapping of public values and those quantified using models has been lacking. In this paper, we mapped hot and cold spots for perceived and modeled ecosystem services by synthesizing results from a social-values mapping study of residents living near the Pike-San Isabel National Forest (PSI), located in the Southern Rocky Mountains, with corresponding biophysically modeled ecosystem services. Social-value maps for the PSI were developed using the Social Values for Ecosystem Services tool, providing statistically modeled continuous value surfaces for 12 value types, including aesthetic, biodiversity, and life-sustaining values. Biophysically modeled maps of carbon sequestration and storage, scenic viewsheds, sediment regulation, and water yield were generated using the Artificial Intelligence for Ecosystem Services tool. Hotspots for both perceived and modeled services were disproportionately located within the PSI's wilderness areas. Additionally, we used regression analysis to evaluate spatial relationships between perceived biodiversity and cultural ecosystem services and corresponding biophysical model outputs. Our goal was to determine whether publicly valued locations for aesthetic, biodiversity, and life-sustaining values relate meaningfully to results from corresponding biophysical ecosystem service models. We found weak relationships between perceived and biophysically modeled services, indicating that public perception of ecosystem service provisioning regions is limited. We believe that biophysical and social approaches to ecosystem service mapping can serve as methodological complements that can advance ecosystem services-based resource management, benefitting resource managers by showing potential locations of synergy or conflict between areas supplying ecosystem services and those valued by the public.", "keywords": "ARIES,Cultural ecosystem services,Hotspot analysis,Modeling,Social values,SolVES K. J. Bagstad (&),J. M. Reed,D. J. Semmens", "introduction": "A large and rapidly growing body of research seeks to quantify and value ecosystem goods and services-the benefits that ecosystems provide to people -in support of better resource management and decision making (Millennium Ecosystem Assessment 2005)(Ruhl et al. 2007;). This has included spatially explicit biophysical modeling of ecosystem services, both through modeling tools intended to be applicable across diverse geographic contexts Daily et al. 2009(Kareiva et al. 2011;) and locally developed and applied ecosystem service models Villa et al. 2014. The Millennium Ecosystem Assessment (Martinez-Harms and Balvanera 2012) advanced the now well-known typology of supporting services (ecological processes that underpin the provision of other types of ecosystem services), regulating services (control of environmental conditions within optimal ranges for the survival of people and other species on which we depend), provisioning services (goods supplied by ecosystems), and cultural services (nonmaterial benefits that enhance individual and social well-being). Many biophysical ecosystem service models have developed out of past ecological, hydrologic, and other physical process models, and have proven useful in quantifying supporting, regulating, and provisioning services. By contrast, cultural services have often remained more difficult to quantify. With the exception of a limited number of cultural ecosystem services, such as the viewshed component of aesthetic values (MA 2005); ARIES Consortium 2014), biophysical models are poorly suited to quantifying cultural services. Their intangible nature and the limited collaboration between ecologists and social scientists outside of the field of economics have historically limited the opportunities for cultural services to inform decision making (Kareiva et al. 2011(Chan et al. 2011(Chan et al. , 2012;;.Daniel et al. 2012)Scientists have, however, mapped public and expert perceptions of ecosystem services, through Public Participatory Geographic Information Systems (PPGIS) approaches (Brown 2005;Sieber 2006;Dunn 2007;. They have also developed tools to systematize the mapping of social values for ecosystem services, i.e., using surveys of the public's values and attitudes to map their perceived values for ecosystem services Raymond et al. 2009). We define social values as those assigned by people to places in the world (Sherrouse et al. 2011), expressed as nonmonetary preferences. These approaches ask the public, which could include residents, visitors, focus groups, and/or online panels, to allocate value across a series of social-value types and/or to place points on maps that correspond to the locations where they feel the landscape provides these values. Social-values mapping largely focuses on understanding values for cultural ecosystem services, including non-use values, although most social-value typologies have included biological diversity (a supporting service) and life-sustaining values (defined by past surveys as ''help (Ives and Kendal 2014) produce, preserve, clean, and renew air, soil, and water,'' roughly corresponding to multiple regulating services). Social-values mapping thus offers a means of quantifying cultural and other services to inform environmental planning and management decisions [ing](Brown and Reed 2009;.Brown 2012)Multiple methods exist for eliciting social values. An extensive peer-reviewed literature has been developed using point-based, social-value elicitation methods and representative sampling of visitors to or residents of an area (Alessa et al. 2008;Bryan et al. 2011;Brown et al. 2012;van Riper et al. 2012;. Others have used polygons or fuzzy boundaries Sherrouse and Semmens 2014)(Evans and Waters 2008;) and/or semi-structured interviews, which often seek to answer different, though related, questions than representative sampling Carver et al. 2009(Carver et al. 2009;Fagerholm et al. 2012;. Additional work has been undertaken to use internet-based mapping surveys and understand trade-offs in responses based on paper and internet surveys Klain and Chan 2012). (Pocewicz et al. 2012) compared the results of polygon versus point-based studies. Theoretically, PPGIS results obtained using points should converge to polygons given an adequate sample size. Brown and Pullar found this to be the case, though they caution that this convergence may not occur for value types that are infrequently mapped. Brown and Pullar recommend the use of polygons for focus groupbased research and points when surveying individuals and note that polygons covering all or nearly all of an area must usually be discarded, as they give little distinguishing information and can introduce error.Brown and Pullar (2012)While some survey-based studies have asked the public or experts to specifically map supporting, regulating, and provisioning services in addition to cultural services (Raymond et al. 2009;Bryan et al. 2011;, those same studies acknowledge the difficulty in asking the public to map complex ecosystem processes and services of which they may have only limited understanding. Regulating and supporting services are likely to be the most cognitively challenging landscape attributes for the public to map Brown et al. 2012)(Brown 2012. In a study conducted in coastal Wales, only more technical respondents (i.e., academics and representatives of environmental groups, but not representatives of business, fisheries, or recreational groups) chose to map supporting and regulating services at all (Brown , 2013)). In another case study in South Australia, cultural services were most frequently mapped by respondents, followed by provisioning, then regulating, then supporting services (Ruiz-Frau et al. 2011).(Bryan et al. 2011)Indeed, in a study that asked the public to map 22 ecosystem service types,  noted that ''Although the purpose of [our] exploratory research was not to scientifically validate the ecosystem service data generated through PPGIS, future research should undertake this challenge. While few would question the validity of using PPGIS to generate maps for identifying cultural ecosystem services, many would question the utility of consulting the 'public' to identify more complex and 'invisible' ecosystem services (p. 647).' Brown et al. (2012) suggestion is to have the general public and experts independently map ecosystem services. Carefully combined use of social-values mapping and biophysical modeling may, however, be a better strategy to integrate cultural services into ecosystem services assessments ' Brown et al.'s (2012)(Chan et al. 2012;. In this study, we thus compare survey-derived maps from the general public against outputs of biophysical models designed to quantify and map corresponding ecosystem services, rather than against maps generated solely by expert opinion.Plieninger et al. 2013)Although biophysically modeled ecosystem services have not, to our knowledge, been mapped against corresponding social values, others have used spatial statistics to explore the strength of relationships between social values and diverse types of ecological data.  found moderate overlap between public perceptions and expert opinion in locating biodiversity hotspots in Prince William Sound, Alaska. They also found that commercial fishermen, who make their living from the ocean's biodiversity, were better able to identify biodiversity hotspots than the general public. Brown et al. (2004) mapped social values against biodiversity in the Palouse region of Idaho and Washington, and found that the public was able to identify meaningful places for ''natural diversity'' that overlapped with forest and prairie remnants. Finally, Donovan et al. (2009) compared public perceptions of biological value to net primary productivity on the Kenai Peninsula, Alaska, and found a moderately significant positive relationship. Others have demonstrated the potential value of combining mapped social values and ecological data in support of land use and conservation planning in Australia Alessa et al. (2008)(Bryan et al. 2011;.Whitehead et al. 2014)Aside from these past studies combining social values mapping and ecological data, biophysical modeling and social-values mapping of ecosystem services have largely taken place independently. Yet they could function as complementary approaches to support resource management-pairing social-values data to quantify and map cultural services with biophysically derived approaches to assess regulating and supporting services. As public agencies seek to incorporate ecosystem services information into planning (36 CFR 219;McIntyre et al. 2008;Zhu et al. 2010;U.K. National Ecosystem Assessment 2011;), information about cultural ecosystem services will be as important to consider as other services. The difficulty in monetizing cultural services has led to the development and use of multicriteria analysis for decision support Bagstad et al. 2013a). Yet concurrent mapping of biophysically modeled services and social values could provide another approach to synthesizing such information for management. (Hermans and Erickson 2007 identified ''social-ecological hotspots''-areas of high ecological and/or social value and their converses-''coldspots'' with low ecological and/or social value. A 2 9 2 matrix of social values and biophysically modeled ecosystem services (hereafter social value and/or ecosystem service hotspots and coldspots) can describe potential public land management implications for these hotspots and coldspots (Table Alessa et al. (2008)). Our approach is also analogous to 2 9 2 decision matrices used in the social values mapping literature to help guide mixed public-private land management 1(Bryan et al. 2011;) and climate change adaptation Whitehead et al. 2014. By identifying social-ecological hotspots and coldspots, we might conduct more complete ecosystem service analysis than by using biophysical models or social-values mapping alone, better informing resource managers about potential synergies, trade-offs, and conflicts between existing or planned uses, management strategies, and the provision of ecosystem services. This approach also offers a way to conceptualize trade-offs and synergies between cultural and other ecosystem services and to consider some difficult-to-monetize cultural ecosystem services in quantitative, spatial ecosystem service assessments.(Raymond and Brown 2011)In this study, we extend on these themes from past analyses, using the Pike-San Isabel (PSI) National Forest in Colorado as a study area. We first map hotspots for summed social values and ecosystem services and their overlap, to provide a mapped example of the management implications hypothesized in Table . If social-values mapping and biophysical modeling of ecosystem services are indeed complements and not substitutes, joint mapping may thus more comprehensively map and value a broader array of ecosystem service types. Second, we statistically model the relationship between three individual social values and five corresponding biophysically modeled ecosystem services to determine whether survey respondents are capable of mapping those services that we can concurrently assess using biophysical models (Table 1). If corresponding social values and ecosystem service maps align well, then we may be collecting redundant information, suggesting that it may only be necessary to use one approach rather than both. If they align poorly, one or the other type of map may be inappropriate, suggesting that the less scientifically trusted approach could be dropped. Based on the findings of 2 and Bryan et al. (2011)Brown (2012, we hypothesize that non-expert survey respondents will best be able to map cultural services, which public preferences are integral to understanding, next best able to map provisioning services, i.e., ecosystem goods used by the public (provided they are willing to identify the locations on which they depend for basic resources, Brown ( , 2013)), and least able to map more complex regulating and supporting services (Table Klain and Chan 2012)).2", "body": "A large and rapidly growing body of research seeks to quantify and value ecosystem goods and services-the benefits that ecosystems provide to people Scientists have, however, mapped public and expert perceptions of ecosystem services, through Public Participatory Geographic Information Systems (PPGIS) approaches Multiple methods exist for eliciting social values. An extensive peer-reviewed literature has been developed using point-based, social-value elicitation methods and representative sampling of visitors to or residents of an area While some survey-based studies have asked the public or experts to specifically map supporting, regulating, and provisioning services in addition to cultural services Indeed, in a study that asked the public to map 22 ecosystem service types, Although biophysically modeled ecosystem services have not, to our knowledge, been mapped against corresponding social values, others have used spatial statistics to explore the strength of relationships between social values and diverse types of ecological data. Aside from these past studies combining social values mapping and ecological data, biophysical modeling and social-values mapping of ecosystem services have largely taken place independently. Yet they could function as complementary approaches to support resource management-pairing social-values data to quantify and map cultural services with biophysically derived approaches to assess regulating and supporting services. As public agencies seek to incorporate ecosystem services information into planning In this study, we extend on these themes from past analyses, using the Pike-San Isabel (PSI) National Forest in Colorado as a study area. We first map hotspots for summed social values and ecosystem services and their overlap, to provide a mapped example of the management implications hypothesized in Table Materials and methodsStudy areaThe PSI is located on the eastern side of the Continental Divide in the Southern Rocky Mountains and covers 9,011 km 2 , about 3.3 % of Colorado's land area (Fig. Social-values mapping and ecosystem service modelingWe generated social-values maps for this analysis using the Social Values for Ecosystem Services (SolVES) 2.0 tool, a GIS application intended to quantify and map perceived social values for ecosystem services Hotspot analysisTo identify social value and ecosystem service hotspots, coldspots, and regions of overlap (Table We equally weighted then summed the modeled surfaces for the 12 social values and 4 biophysically modeled ecosystem services (excluding biodiversity) and ran the  Getis-Ord Gi* tool on both summed layers, using a fixed distance band specified to ensure that all features had at least one neighbor. We excluded biodiversity from the ecosystem service hotspot analysis because, by definition as a supporting service, it benefits humans indirectly. The maximum summed value index for a given cell across all 12 social-value types was 61. We normalized ecosystem service values by setting each service (scenic views from housing and recreation sites, carbon sequestration and storage, sediment deposition, and water yield) from 0 to 1, rescaling the individual maps for the two viewshed and carbon metrics (views provided to recreation sites and residences, carbon sequestration and storage) between 0 and 0.5, and summing all layers. This gave equal weight to each of the four biophysically modeled ecosystem services.Although a maximum value of four could thus theoretically be achieved, the actual maximum actual value was 1.79, indicating relatively little overlap between areas of maximum provision of each of the four services.Regression analysisOur regression analyses build on recent comparisons between spatially explicit social values and ecological data, such as expert-derived biodiversity maps The only spatially explicit biodiversity data for the region were for vertebrate species richness, which does not perfectly align with the survey's description of biological diversity value. We compared the aesthetic value survey results to spatially explicit viewshed quality model results (themselves calibrated based on visual preference studies for the Rocky Mountain region); obviously viewshed models capture only the visual component of aesthetic quality and exclude other sensory elements important to aesthetics. The survey's description of life-sustaining value comes close to definitions of regulating ecosystem services; we thus compared this value to modeled results for clean air (carbon sequestration and storage), soil (soil erosion and sediment deposition), and water (water yield). Despite the imperfect alignment of the description of social values and biophysically modeled services, we feel that the biophysically modeled services come close to matching the defined social values.We used point locations for our regression models that were marked by survey respondents for each value type (n = 238 life-sustaining points, 242 biodiversity points, and 466 aesthetic points). These points were located at an average minimum distance of 1.24 km from each other, helping to avoid inflation of test statistics due to spatial autocorrelation of values. Rather than using the statistically modeled raster surface generated by SolVES using the MaxEnt algorithm (which could also yield unacceptably high spatial autocorrelation), we divided the number of dollars allocated by each respondent to a given value type during the value allocation exercise by the number of points they marked for that value type. For instance, if a respondent allocated 60 dollars to aesthetic value, then marked three points on their map, each point would be assigned a value of 20. We then extracted the corresponding biophysically modeled ecosystem service values for each point layer. Jarque-Bera tests revealed a highly non-normal distribution of residuals for the initial OLS results, so we applied Box-Cox power transformations to transform the dependent variable in each model using the lambda value recommended by the Box-Cox test ResultsHotspot analysisJust over 20 % of the PSI was classified as statistically significant hotspots for social values and/or ecosystem service at the a = 0.05 significance level, and less than 2 % of the PSI was classified as hotspots for both (Table Regression analysisUsing a linear functional form with Box-Cox transformation of the dependent variable, all OLS results were nonsignificant at the a = 0.05 level (Table Discussion", "conclusions": "Hotspots and coldspots: Implications for planning and resource management Wilderness areas were generally found at higher elevation and despite their relative inaccessibility were perceived by the public as valuable locations (Fig. ). Colorado is home to 54 ''Fourteeners,'' which are highly visible landmarks and popular locations for hiking, mountaineering, and backcountry skiing. Thirty of these Fourteeners are located within or adjacent to the PSI, including the highest point in the state, Mt. Elbert (14,439 ft/4,401 m). Recreation sites were relatively well distributed throughout the PSI and were found within, near, and distant from social-value/ ecosystem service hotspots. The location of wilderness areas and Fourteeners thus seems to better explain concentrations of social values than the location of general purpose recreation sites.2Wilderness boundaries were marked on the maps that respondents used to locate social value points. Notable social values hotspots outside of wilderness areas included the South Platte River corridor and southern end of the Collegiate Peaks. Non-wilderness ecosystem service hotspots included the Pikes Peak area, Wet Mountains, and the southernmost part of the PSI located west of the Spanish Peaks Wilderness. Using summed social-values data, PSI wilderness areas, on average, were valued 32 % more than non-wilderness areas. Based on the results of parallel studies in more rural northwest Wyoming that also included wilderness boundaries on maps marked by respondents , however, the summed value index for non-wilderness areas was 15 % greater than wilderness areas in the Bridger-Teton National Forest (BTNF) and 100 % greater than in the Shoshone National Forest (SNF). In their study of nearby residents' attitudes about forest management in Colorado and Wyoming, (Sherrouse et al. 2014) found that PSI respondents favored wilderness more than BTNF or SNF respondents, whereas BTNF and SNF respondents were more favorable to fishing and hunting, motorized recreation, and oil/gas drilling. Wilderness areas provide important ecosystem services Clement and Cheng (2011), though efforts to catalog and quantify them have generally tended toward isolated case studies rather than systematic, quantitative, spatial analyses. Additional studies to map social values and biophysically modeled ecosystem services for wilderness and non-wilderness areas would improve our understanding of ecosystem services generated by wilderness areas, particularly for a greater variety of services and geographic contexts.(Watson and Venn 2012)Ecosystem services are increasingly entering into the decision processes of public agencies, particularly those charged with planning and resource management (McIntyre et al. 2008;; U.K. National Ecosystem Assessment 2011). In Colorado's national forests, joint mapping of cultural and biophysically modeled ecosystem services has implications for resource management by the USDA Forest Service and adjacent land management agencies such as the National Park Service and Bureau of  Zhu et al. 2010. Hotspot analysis results enable spatial and visual comparisons between cultural and other ecosystem services, putting difficult-to-monetize cultural services on a level playing field for decision making with biophysically modeled services that are more amenable to monetary valuation. In planning contexts, hotspot results can be used to identify potential resource management synergies, trade-offs, and conflicts between existing or planned uses, management strategies, and the provision of ecosystem services (Table (36 CFR 219)).1Given that resource extraction and motorized recreation are prohibited within wilderness areas, we expect that threats to ecosystem services there would originate more from either global change (e.g., climate change) or accessrelated environmental degradation. Risk and value can be incorporated into resource management, with resources directed to higher-value and/or higher-risk landscapes . Outside these wilderness areas, managers could overlay the spatial extent of  potential management actions atop social-values and ecosystem service maps to better visualize human/landscape relationships (Raymond and Brown 2011)) and areas of potential management synergies or conflicts. Coldspots were prevalent outside of wilderness areas (Fig. (Alessa et al. 2008). While coldspots have lower total ecosystem service values than hotspots, managers should not assume that these areas are devoid of value. Coldspot management strategies may include raising awareness of their value or distributing human use and related impacts to underutilized areas, while being aware that greater use can degrade sensitive environments 2.(van Riper et al. 2012)", "SDG": [15]}, "machine_learning_approaches_for_estimating_forest_stand_height_using_plot_based_observations_and_airborne_lidar_data": {"name": "Machine Learning Approaches for Estimating Forest Stand Height Using Plot-Based Observations and Airborne LiDAR Data", "abstract": " Effective sustainable forest management for broad areas needs consistent country-wide forest inventory data. A stand-level inventory is appropriate as a minimum unit for local and regional forest management. South Korea currently produces a forest type map that contains only four categorical parameters. Stand height is a crucial forest attribute for understanding forest ecosystems that is currently missing and should be included in future forest type maps. Estimation of forest stand height is challenging in South Korea because stands exist in small and irregular patches on highly rugged terrain. In this study, we proposed stand height estimation models suitable for rugged terrain with highly mixed tree species. An arithmetic mean height was used as a target variable. Plot-level height estimation models were first developed using 20 descriptive statistics from airborne Light Detection and Ranging (LiDAR) data and three machine learning approaches-support vector regression (SVR), modified regression trees (RT) and random forest (RF). Two schemes (i.e., central plot-based (Scheme 1) and stand-based (Scheme 2)) for expanding from the plot level to the stand level were then investigated. The results showed varied performance metrics (i.e., coefficient of determination, root mean square error, and mean bias) by model for forest height estimation at the plot level. There was no statistically significant difference among the three mean plot height models (i.e., SVR, RT and RF) in terms of estimated heights and bias (p-values > 0.05). The stand-level validation based on all tree measurements for three selected stands produced varied results by scheme and machine learning used. It implies that additional reference data should be used for a more thorough stand-level validation to identify statistically robust approaches in the future. Nonetheless, the research findings from this study can be used as a guide for estimating stand heights for forests in rugged terrain and with complex composition of tree species.", "keywords": "forest stand height,plot-level to stand-level expansion methods,airborne LiDAR,machine learning", "introduction": "Sustainable forest management has been identified as a key part of one of the 17 Sustainable Development Goals from the 2015 United Nations Sustainable Development Summit. Others have mentioned the importance of sustainable forest management in association with climate change mitigation, biodiversity, water resources, and the sustainable supply of forest products [1][2][3][4]. At the government level, effective sustainable forest management for broad forest areas requires country-wide forest inventory data. A stand-level inventory is appropriate as a minimum unit for local and regional forest management. In South Korea, stand-level forest type maps have been produced at a national scale since 1972 to characterize the status of forests across the country. These maps have been widely used to support various tasks such as generating forest statistics, forest-land management, fostering forest resources, and forest protection. Since 2008, the scale of Korean forest type maps has been enhanced from 1:25,000 to 1:5000. The attributes of the forest type maps consist of tree species, age, diameter-at-breast-height (DBH), and crown density as categorical variables. Other nations have also been producing forest type maps. For example, forest services in the United States and Canada generate Vegetation Resources Inventory (VRI) maps with attributes recorded based on the interpretation of aerial photos and ground samples. The thematic maps can contain various parametric information such as cover pattern, crown closure, tree layer, vertical complexity, species composition, age, height, basal area, density of living trees, dead tree density, confidence indices, and data source codes. The maps provide these forest parameters as geospatial layers to facilitate monitoring of overall forest condition. Finland produces forest thematic maps using multi-source data, which include forest inventory data, satellite images, land-use maps, and digital elevation models. The thematic maps contain information such as biomass, crown closure, stand-level mean DBH, stand height, basal area, and soil productivity.[5]Unlike the forest maps produced by other countries that include many properties, the forest type maps in South Korea contain only four categorical parameters. Therefore, there is a strong need to supplement the forest type maps of South Korea with additional forest information to enhance monitoring and management of forest ecosystems. Among forest attributes, forest stand height is a representative index of vertical forest structure that informs how an ecosystem works, including cycles of carbon, water and nutrients . In addition, it is a useful index to estimate forest stand quality, site index, forest volume, and aboveground biomass.[6]In the context of remote sensing, Light Detection and Ranging (LiDAR), an active sensor that can be used to obtain three-dimensional information of geographical features, has been widely used for forest characterization. Advances in LiDAR technology make it possible to consistently and semi-automatically extract forest parameters when compared to traditional photogrammetry whose data quality depends on the scale and resolution of photographs, and the experience and skills of the photographic interpreter. In particular, airborne LiDAR has been used as a robust data source for obtaining forest inventory data [7][8][9]. The definition of canopy or stand height is a representative height of the tallest tree stratum in a forest patch or stand [10]. Different from gridded forest patches, a forest stand consists of adjacent trees with same tree species, age and DBH class distribution, site quality, spatial arrangements, structure and conditions that are distinguished from neighboring communities. Estimation of forest stand height using LiDAR data can be conducted using two types of approaches: individual tree-based and plot-based approaches [11]. The individual tree-based approach calculates forest stand heights by averaging tree heights extracted based on individual tree crown delineation [12][13,. This approach works well over relatively flat and homogeneous forests, but may not work well for rugged or heterogeneous areas 14]. The plot-based approach estimates stand heights with various descriptive statistics at the plot level. Most previous studies modeled forest stand heights based on the plot-based approach [7][15,. Some studies used linear regression to relate field-surveyed stand/plot heights and LiDAR-based statistical variables. Means et al. 16] estimated ground-based stand heights with airborne LiDAR-derived metrics using stepwise regression analysis. The average maximum height, maximum height, average mean height, height percentiles, and canopy cover percentiles were used as independent variables. The model accuracy for all 19 plots showed coefficient of determination (R 2 ) of 0.93 and root mean squared error (RMSE) of 3.4 m; model accuracy for 11 old-growth plots showed an R 2 of 0.98 and RMSE of 1.7 m. Similarly, other studies used multiple regression analysis between field measurements and airborne laser data on young forests, mature forests with poor site quality, and mature forests with good site quality [17][18][19]. The height percentiles, maximum values, mean values, and variations were used as independent variables with R 2 of about 0.8-0.95. Other recent studies estimated forest stand heights using linear regression with LiDAR data and optical satellite images. Donoghue and Watt [20] compared linear regression-based stand height models with three different datasets: airborne LiDAR, Landsat ETM+ and IKONOS. Wulder and Seemann [21] used an empirical model to estimate airborne LiDAR-derived stand heights from Landsat5 TM data after delineating forest stands with an image segmentation. Mora et al. [22][23, estimated LiDAR-based stand heights with high resolution satellite images (i.e., QuickBird-2, Worldview-1) using diverse regression techniques including linear regression, k-nearest neighbor, regression trees, and random forest. However, prior studies were generally carried out on non-rugged terrain with relatively analogous tree species composition, which makes it difficult to apply to mountainous forests in South Korea. Vega et al. 24] also described difficulty in extracting tree biophysical parameters where complex forest terrains affect height normalization, which causes distortions to the canopy height model (CHM). Khosravipour et al. [25] showed that tree top detection on slope-distorted CHMs is strongly influenced by individual tree crown shape, which usually depends on tree species. Korean forests have rugged terrain with small patches of tree species. Therefore, there is a need for developing an improved model to estimate forest stand height under such complicated conditions.[26]The primary goal of this research was to develop novel forest stand height estimation models for rugged terrain with highly mixed tree species, considering their operational use and adding a new attribute (i.e., stand height) to the Korean forest type maps. We extracted area-based descriptive statistics of forest structure from airborne LiDAR data and then estimated forest canopy heights at the plot level using three machine learning approaches-support vector regression, modified regression trees, and random forest. We also proposed two approaches for expanding the plot level model to estimate stand level forest height, which was validated using field survey data.", "body": "Sustainable forest management has been identified as a key part of one of the 17 Sustainable Development Goals from the 2015 United Nations Sustainable Development Summit. Others have mentioned the importance of sustainable forest management in association with climate change mitigation, biodiversity, water resources, and the sustainable supply of forest products Unlike the forest maps produced by other countries that include many properties, the forest type maps in South Korea contain only four categorical parameters. Therefore, there is a strong need to supplement the forest type maps of South Korea with additional forest information to enhance monitoring and management of forest ecosystems. Among forest attributes, forest stand height is a representative index of vertical forest structure that informs how an ecosystem works, including cycles of carbon, water and nutrients In the context of remote sensing, Light Detection and Ranging (LiDAR), an active sensor that can be used to obtain three-dimensional information of geographical features, has been widely used for forest characterization. Advances in LiDAR technology make it possible to consistently and semi-automatically extract forest parameters when compared to traditional photogrammetry whose data quality depends on the scale and resolution of photographs, and the experience and skills of the photographic interpreter. In particular, airborne LiDAR has been used as a robust data source for obtaining forest inventory data The primary goal of this research was to develop novel forest stand height estimation models for rugged terrain with highly mixed tree species, considering their operational use and adding a new attribute (i.e., stand height) to the Korean forest type maps. We extracted area-based descriptive statistics of forest structure from airborne LiDAR data and then estimated forest canopy heights at the plot level using three machine learning approaches-support vector regression, modified regression trees, and random forest. We also proposed two approaches for expanding the plot level model to estimate stand level forest height, which was validated using field survey data.Data and MethodsStudy AreaThe study area is located on the southwestern region of Mt. Maehwa in Gangwon-do, South Korea, and covers 2500 ha (centered at 37 \u2022 38 N, 127 \u2022 50 E) (Figure The study area is in the temperate mid-latitude zone, which is hot and humid in the summer influenced by the North Pacific air mass, and cold and dry in the winter influenced by the Siberian air mass. Compared to the east coast, the difference between summer and winter temperature at this inland mountainous area is large. The annual mean temperature is 10.1 \u2022 C, January mean temperature is \u22125.6 \u2022 C, and August mean temperature is 24 \u2022 C. The rainfall pattern shows topographic variation and the annual mean precipitation is 1291.3 mm. DataAirborne LiDAR data were acquired during 4 to 17 September 2013 using a Leica ALS60 sensor (Leica Geosystems, Heerbrugg, Switzerland) (Table DataAirborne LiDAR data were acquired during 4 to 17 September 2013 using a Leica ALS60 sensor (Leica Geosystems, Heerbrugg, Switzerland) (Table MethodsStand height is defined as the mean height or the top height of trees within a stand The proposed forest stand height estimation models in this study consist of two parts: (1) the development of height estimation models at the plot level (Figure In this study, among many kinds of machine learning algorithms available, support vector regression (SVR), modified regression trees (RT) and random forest (RF) were used for estimating forest height at the plot level. SVR is a regression version of widely used support vector machines (SVM), which is a supervised non-parametric statistical technique A typical regression tree partitions input data into relatively homogeneous groups at nodes through recursive binary splits to predict a target variable. In this study, a modified regression tree (RT) approach based on Cubist software (RuleQuest, Pomona, Australia) was used RF is an ensemble approach based on classification and regression trees (CART) Since the mean height models were developed at the plot level, there is a need to expand these machine learning-based plot height models into the stand-level. Naesset et al. Scheme 2: The 20 input variables (Table The estimated stand heights over the study site were compared by model and scheme using R 2 and root mean square difference (RMSD). ANOVA tests were also conducted to see if there is any significant difference between each pair of estimated stand heights. The models were finally validated against field-survey stand heights for three stands (A, B and C).  Results and DiscussionComparison of Plot-Level Height ModelsThe arithmetic mean height was modeled at the plot level using SVR, RT and RF (Figure Other researchers have attempted similar analysis under different forest conditions. Naesset Results and DiscussionComparison of Plot-Level Height ModelsThe arithmetic mean height was modeled at the plot level using SVR, RT and RF (Figure Other researchers have attempted similar analysis under different forest conditions. Naesset We also analyzed the input variable importance for RF and RT (Figure Forests 2018, 9, x FOR PEER REVIEW 9 of 15 our study site include a few deciduous trees, which made it difficult to estimate the canopy height at the plot level. Donoghue and Watt Inter-Comparison of Stand-Level Height ModelsWe evaluated the relative performance of our models in terms of the three machine learning algorithms and two expansion methods (i.e., Schemes 1 vs. 2) using 575 stands in the study area. Figure Based on forest reports for the Maehwa mountain region and communications with forest managers, it is expected that the typical range of dominant tree heights is between 10 m and 35 m. Although it is not possible to quantitatively assess all of the stand-level heights estimated using the proposed models, the SVR-based models tend to over-estimate the stand heights (Figure Since we trained the machine learning algorithms with plot-level input variables, there is a need to compare whether the choice of expansion method impacts the estimation of stand heights. We explored the difference in the results of the two different expansion schemes when the same machine learning algorithm and stand height type were used. Most of the pairwise comparisons (Figure scheme are not significantly different except for the pair between the RT and RF models with Scheme 2 (p-value = 0.01) (Figure Based on forest reports for the Maehwa mountain region and communications with forest managers, it is expected that the typical range of dominant tree heights is between 10 m and 35 m. Although it is not possible to quantitatively assess all of the stand-level heights estimated using the proposed models, the SVR-based models tend to over-estimate the stand heights (Figure Validation of Forest Stand Height ModelsField surveys of forest stands are typically time-consuming and require huge efforts over large areas. We investigated three relatively small forest stands for exhaustive validation. Each stand height was calculated using individual tree heights. We additionally compared the proposed approaches to calculation of stand heights based on individual tree crown delineation. For delineating individual tree crowns, we used the algorithm proposed in Fang et al. Validation of Forest Stand Height ModelsField surveys of forest stands are typically time-consuming and require huge efforts over large areas. We investigated three relatively small forest stands for exhaustive validation. Each stand height was calculated using individual tree heights. We additionally compared the proposed approaches to calculation of stand heights based on individual tree crown delineation. For delineating individual tree crowns, we used the algorithm proposed in Fang et al. It is challenging to directly compare the results to other studies presented in the literature not only because of the influence that different environmental characteristics have on the results, but also because of the differences in data being applied. Nonetheless, our results are similar to previous stand-level height estimation. For example, Ahmed et al. Table Mean Difference (Mean AbsolutePotentials and LimitationsThe novelty of this study is that the stand height models were developed for areas with very rugged terrain composed of highly mixed tree species. In particular, the comparison of the proposed two expansion methods (Schemes 1 and 2) gives us an insight on how to conduct stand-level forest height estimation from plot-level data and models for an operational purpose. The stand height map for the whole study area is depicted in Figure However, this study has several limitations. First of all, due to small sample size in both plotand stand-level analyses (42 and 3, respectively), statistically robust approaches were not identified. A more thorough plot-and stand-level validation should be conducted with more samples to further improve the approaches. In addition, the generalization capability of the proposed approaches should be further investigated for different forested areas. Based on the testing performed, it is not possible to say that the proposed approaches would work for forests in the US or China.A forest stand is defined as an adjoining community which has similar species composition, structure, age class, size class, and site qualities. However, since forests in South Korea are very heterogeneous, many deciduous trees are often found in coniferous stands and vice versa, which makes it difficult to accurately estimate stand heights. In addition, it should be noted that field-surveyed tree heights contain measurement errors especially for dense forests with rugged terrain. Conclusions", "conclusions": "This study used airborne LiDAR data to estimate stand heights for forests with rugged terrain and highly mixed tree species. To calculate stand heights, the arithmetic mean height was used. Three machine learning algorithms were used for empirical modeling based on 20 statistical variables at plot level. Then two expansion schemes (i.e., central plot-based expansion and stand-based expansion) from the plot level to the stand level were examined. Plot-level models using SVR, RT and RF algorithms were first developed with the results showing varied performance metrics (i.e., coefficient of determination, root mean square error, and mean bias) by model for forest height estimation at the plot level. There was no statistically significant difference in performance among the three machine learning models based on the ANOVA test results (p-values > 0.05). The standlevel validation using all tree measurements for the three selected stands showed varied results by scheme and model used, which implies that more rigorous validation is required with more samples. Thus, it is not possible to suggest a statistically robust method among the tested approaches that can be used to quantify stand heights in forests with rugged terrain in South Korea. The proposed approaches should be further investigated with more samples over other areas with complex forest and terrain conditions so that they can be considered for use to operationally update stand-level forest inventories.Author Contributions: J.L. led manuscript writing and contributed to the data analysis and research design. ", "SDG": [15]}, "mapping_tropical_forest_structure_in_southeastern_madagascar_using_remote_sensing_and_artificial_neural_networks": {"name": "Mapping tropical forest structure in southeastern Madagascar using remote sensing and artificial neural networks", "abstract": " Tropical forest condition has important implications for biodiversity, climate change and human needs. Structural features of forests can serve as useful indicators of forest condition and have the potential to be assessed with remotely sensed imagery, which can provide quantitative information on forest ecosystems at high temporal and spatial resolutions. Herein, we investigate the utility of remote sensing for assessing, predicting and mapping two important forest structural features, stem density and basal area, in tropical, littoral forests in southeastern Madagascar. We analysed the relationships of basal area and stem density measurements to the Normalised Difference Vegetation Index (NDVI) and radiance measurements in bands 3, 4, 5 and 7 from the Landsat Enhanced Thematic Mapper Plus (ETM+). Strong relationships were identified among all of the individual bands and field based measurements of basal area ( pb0.01) while there were weak and insignificant relationships among spectral response and stem density measurements. NDVI was not significantly correlated with basal area but was strongly and significantly correlated with stem density (r=\u00c00.69, pb0.01) when using a subset of the data, which represented extreme values. We used an artificial neural network (ANN) to predict basal area from radiance values in bands 3, 4, 5 and 7 and to produce a predictive map of basal area for the entire forest landscape. The ANNs produced strong and significant relationships between predicted and actual measures of basal area using a jackknife method (r=0.79, pb0.01) and when using a larger data set (r=0.82, pb0.01). The map of predicted basal area produced by the ANN was assessed in relation to a pre-existing map of forest condition derived from a semiquantitative field assessment. The predictive map of basal area provided finer detail on stand structural heterogeneity, captured known climatic influences on forest structure and displayed trends of basal area associated with degree of human accessibility. These findings demonstrate the utility of ANNs for integrating satellite data from the Landsat ETM+ spectral bands 3, 4, 5 and 7 with limited field survey data to assess patterns in basal area at the landscape scale.", "keywords": "Remote sensing,Forest structure,Artificial neural networks,Madagascar,Basal area", "introduction": "Tropical forests play crucial roles in the functioning of our planet and the maintenance of life . They serve as regulators of global and regional climate systems (Myers, 1996), act as carbon sinks (Gedney & Valdes, 2000), are rich in biodiversity, containing over half of the planet's life forms (Grace et al., 1995), provide valuable ecosystem services, and serve as vital resources for human populations (Wilson, 1988). Thus, monitoring the state and condition of tropical forests can also provide indications of the health of our planet and its inhabitants. A considerable amount of research has investigated the use of satellite imagery for measuring tropical deforestation, a process which can be readily observed by comparing pixels that have changed from forest to non-forest in images collected on different dates (Laurance, 1999)(Green & Sussman, 1990;. However, this crisp, binary approach is unable to capture and describe the great variety of processes that reduce or alter forest cover without eliminating it Skole & Tucker, 1993)(Sgrenzaroli et al., 2002;. This more subtle process of dforest degradationT refers to the temporary or permanent decrease in the density, biomass and/or overall structure of vegetation cover and/or its species composition Stone & Lefebvre, 1998)(Grainger, 1993;. Because forest degradation involves internal or vertical change within the forest, it is more difficult to detect with remote sensing than forest clearance. It is important to investigate methods for monitoring these subtle changes since forest modification is generally a more prevalent process than deforestation Sgrenzaroli et al., 2002)(Lambin, 1999;.Nepstad et al., 1999)A degraded or modified forest may also be classified as a secondary forest. Tropical secondary forests are excluded from many forest assessments and conservation plans despite the fact that they constitute approximately 40% of tropical forest cover . Conservation and management of these forests are crucial since they often provide valuable resources to human communities, retain significant amounts of biodiversity and may relieve pressure on primary forests (Brown & Lugo, 1990). Sustainable management of tropical secondary forests will require more scientific knowledge and a better understanding of human impact on these forests (Cadotte et al., 2002)(Brown & Lugo, 1990;.Moran et al., 1996)Multi-spectral satellite images available at high spatial and temporal resolutions can provide a useful means for monitoring and assessing forest condition, forest modification and dtop downT secondary forest formation (i.e., the conversion of old growth forest to secondary forest through continual degradation processes). A key step towards using remotely sensed images for this purpose is to determine the relationship between spectral information contained within an image and forest structural properties that are indicative of forest condition. The spectral response of a forest is determined by the structure of the canopy through its relationships with leaf area index or canopy cover , which controls the amount of understory vegetation, leaf litter and soil that are visible to the sensor (Danson, 1995). The spectral response is indirectly determined by features that shape the structure of the canopy such as biomass, age, density, mean tree height and basal area (Franklin, 1986)(Lee & Nakane, 1996;Peterson et al., 1987;. Many of these features, such as basal area and stem density, are also indicators of forest condition. For example, basal area and density of large stems has been shown to be higher in protected areas and old growth forests and tend to decrease with increasing levels of disturbance Rock et al., 1986)(Bhat et al., 2000;Bhuyan et al., 2002;Chittibabu & Parthasarathy, 2000;. Reduction in basal area or tree biomass due to human disturbance, such as selective logging, may be attributed to a preference for trees of larger size classes for construction purposes Macedo & Anderson, 1993)(Medley, 1993;, although these patterns may vary across sites. Similarly, most tropical secondary forests are characterized as having a high density of trees b10 cm diameter at breast height (dbh), short trees with small diameters, low overall basal area and a high leaf area index Vermeulen, 1996). The distinct structural characteristics of disturbed and secondary forests and the correlative relationships with canopy reflectance support the utility of remote sensing as a useful tool for assessing forest condition and forest disturbances, which are often patchily distributed across a landscape both spatially and temporally (Brown & Lugo, 1990).(Cannon et al., 1994)Multiple studies have compared the application of different satellite sensors for monitoring forest structural features (Brockhaus & Khorram, 1992;Hyyppa et al., 2000;. These studies have shown that the Landsat Thematic Mapper (TM) provides comparable and, in some cases, stronger predictions of certain forest structural features, such as basal area, when compared to radar satellite systems Lefsky et al., 2001)(Hyyppa et al., 2000; or other optical sensors of similar spectral and spatial resolution Lefsky et al., 2001). The Landsat data have clear practical advantages over the spectrally comparable SPOT imagery, which include lower costs (Brockhaus & Khorram, 1992). In comparison to hyperspectral or hyperspatial resolution sensors, the Landsat data are less expensive, have lower storage requirements, higher spatial coverage and comparative ease of processing, which is aided by a substantial body of published literature concerning Landsat image processing methods. Due to the strong results derived from Landsat imagery for monitoring forest structural features as shown in previous studies combined with the practical advantages of the sensor, this research utilized Landsat ETM+ imagery for the assessment of tropical forest basal area and stem density in southeastern Madagascar. The practical advantages of a sensor are especially important to consider when determining suitable imagery to use for research or monitoring in developing countries where a high percentage of tropical forests are located and where resources for conservation and environmental management are often limited.(Hyyppa et al., 2000)Previous studies have found significant relationships among spectral information within Landsat TM or ETM+ imagery and variables such as forest age, successional status, basal area, height, biomass, density and volume (Brockhaus & Khorram, 1992;Foody et al., 2001Foody et al., , 2003;;Jakubauskas, 1996;Olsson, 1994;Puhr & Donoghue, 2000;. Research on remote sensing of tropical forest attributes has indicated that the optimal spectral bands for estimating forest structural features may vary across studies and across sites within the same study Steininger, 2000)(Foody et al., 2001(Foody et al., , 2003;;. These issues are at the crux of a central problem in remote sensing applications, which is the inability to generalize across studies in both space and time Steininger, 2000).(Woodcock et al., 2001)To overcome these obstacles, more knowledge is needed on the relationships among spectral response and forest structural characteristics at different sites. Addition-ally, there is a need to apply and test similar methodologies so as to increase comparability of results from separate investigations.Various methods exist for utilizing and analyzing spectral information to assess vegetation or forest structural parameters. One widely used approach is to combine spectral information from multiple bands into a composite value known as a spectral vegetation index . Of the many vegetation indices that exist, the Normalised Difference Vegetation Index (NDVI) is among the most common in remote sensing studies and provides an estimate of vegetation greenness or biomass per pixel (Cohen & Goward, 2004). However, several factors limit the applications of NDVI in tropical forest studies. One limitation of the NDVI is that vegetation greenness within a pixel saturates at a threshold level, beyond which NDVI values are insensitive to increasing vegetation amount (Goward et al., 1985). Furthermore, NDVI provides measures of vegetation greenness and soil reflectance, which may be more sensitive to topographic variation than to actual soil or vegetation properties (Ripple, 1985)). An additional disadvantage of using the NDVI alone is that it utilizes a limited amount of the total spectral information available within an image (Cohen & Goward, 2004. Methods that integrate a broader range of spectral data may provide more information on vegetation cover than possible with the use of a single vegetation index.(Foody et al., 2001)The statistical analyses used for understanding the relationships among spectral data and forest attributes should accommodate for the possibility that these relationships may be non-linear and complex. Regression and correlation analyses have commonly been used within remote sensing studies (Jensen et al., 1999;. However, these approaches typically assume linear relationships among variables of interest while plant biophysical characteristics often do not conform to these criteria Lawrence & Ripple, 1998). For this reason, nonparametric statistical methods may be more useful for describing the relationship between remotely sensed imagery and environmental variables since these tests make no a priori assumptions about the data. An artificial neural network (ANN) offers a powerful method for analysing complex relationships among variables without making assumptions about the data. ANNs are capable of handling non-normality, nonlinearity and collinearity in a system (Jensen et al., 1999). This capability is a major advantage of ANNs for assessing the relationships between forest structural attributes and spectral reflectance values, which are frequently non-linear and complex and, in turn, may vary across the different wavebands.(Haykin, 1994)An ANN is defined by an assemblage of bneurons,Q a protocol for the way the neurons are networked, organized, weighted and connected, and a learning rule . An ANN is typically composed of an input layer, one output layer and one or more hidden layers (Baret, 1995). The system dlearnsT by predicting output data from patterns learned from a set of input training data (Jensen et al., 1999). By comparing the current output layer to a desired output response, the difference between the two can be obtained and used to adjust weights within the network. The goal is to achieve a set of weights that produce results that closely resemble the target output. This adaptive learning process is repeated until the difference between predicted and training values drop below a predetermined threshold of user-defined accuracy (Pearson et al., 2002). Once constructed and after patterns in the data have been learned, the ANN can be used to estimate or predict values for similar but unexperienced instances of the data (Jensen et al., 1999).(Carvalho, 2001)ANNs have recently been shown to provide useful alternatives to traditional statistical analyses in forest remote sensing research.  observed stronger relationships among the age of coniferous forests in Brazil and spectral data from bands 3,4 and 5 of the Landsat TM when using artificial neural networks versus multiple regression. They suggest that this reflects the ability of ANNs to handle collinearity among the spectral bands, which may degrade the predictive power of the multiple regression. ANNs have also been shown to provide stronger relationships between actual and predicted estimates of biomass when using ground data and Landsat TM data of moist tropical forests in Brazil, Malaysia and Thailand when compared to predictions derived from multiple regression or vegetation indices Jensen et al. (1999). In a comparison of the predictive power of neural networks to multiple vegetation indices, including NDVI, (Foody et al., 2003) found that the neural networks provided the strongest relationship between predicted and actual estimates of above ground biomass (derived from dbh, basal area and tree height) using the 6 non-thermal bands of the Landsat TM in a tropical rain forest in Borneo.Foody et al. (2001)In this study, we used Landsat ETM+ imagery to assess structural attributes of coastal forests in southeast Madagascar. Madagascar is one of the world's foremost biodiversity hotspots due to its combination of exceptional amounts of endemic species and high estimates of forest loss . The island's forests are also important for human livelihoods since approximately 80% of the island's population is rural and villagers are often largely dependent upon forests for ecosystem services and resources such as fuel wood and construction materials (Myers et al., 2000)(Shyamsundar & Kramer, 1997;. Despite considerable research on the clearance and extent of Madagascar's rain forests World Bank, 2003)(Green & Sussman, 1990;Mayaux et al., 2000;, there has been surprisingly little research on the application of remote sensing for estimating forest structural features on the island. An initial assessment of the utility of remote sensing data for estimating forest structure in Madagascar is a necessary first step towards developing a program for monitoring forest degradation. Thus, the aims of this study are (i) to assess the utility of using remotely sensed data from the Landsat ETM+ for the assessment of forest structural features in southeast Madagascar, (ii) to assess patterns in forest structure at a landscape scale and (iii) to determine if remote sensing can provide a useful quantitative alternative to commonly used semi-quantitative surveys of forest condition. In this study, basal area and stem density are the primary structural features assessed because they have been shown to be useful indicators of forest disturbance Nelson & Horning, 1993)(Bhat et al., 2000;Bhuyan et al., 2002;Chittibabu & Parthasarathy, 2000; and because basal area can be directly related to biomass and collected with relative ease Macedo & Anderson, 1993).(Salvador, 2000)", "body": "Tropical forests play crucial roles in the functioning of our planet and the maintenance of life A degraded or modified forest may also be classified as a secondary forest. Tropical secondary forests are excluded from many forest assessments and conservation plans despite the fact that they constitute approximately 40% of tropical forest cover Multi-spectral satellite images available at high spatial and temporal resolutions can provide a useful means for monitoring and assessing forest condition, forest modification and dtop downT secondary forest formation (i.e., the conversion of old growth forest to secondary forest through continual degradation processes). A key step towards using remotely sensed images for this purpose is to determine the relationship between spectral information contained within an image and forest structural properties that are indicative of forest condition. The spectral response of a forest is determined by the structure of the canopy through its relationships with leaf area index or canopy cover Multiple studies have compared the application of different satellite sensors for monitoring forest structural features Previous studies have found significant relationships among spectral information within Landsat TM or ETM+ imagery and variables such as forest age, successional status, basal area, height, biomass, density and volume To overcome these obstacles, more knowledge is needed on the relationships among spectral response and forest structural characteristics at different sites. Addition-ally, there is a need to apply and test similar methodologies so as to increase comparability of results from separate investigations.Various methods exist for utilizing and analyzing spectral information to assess vegetation or forest structural parameters. One widely used approach is to combine spectral information from multiple bands into a composite value known as a spectral vegetation index The statistical analyses used for understanding the relationships among spectral data and forest attributes should accommodate for the possibility that these relationships may be non-linear and complex. Regression and correlation analyses have commonly been used within remote sensing studies An ANN is defined by an assemblage of bneurons,Q a protocol for the way the neurons are networked, organized, weighted and connected, and a learning rule ANNs have recently been shown to provide useful alternatives to traditional statistical analyses in forest remote sensing research. In this study, we used Landsat ETM+ imagery to assess structural attributes of coastal forests in southeast Madagascar. Madagascar is one of the world's foremost biodiversity hotspots due to its combination of exceptional amounts of endemic species and high estimates of forest loss MethodsSite descriptionThe study site is comprised of approximately 2800 ha of tropical littoral rain forest in southeastern Madagascar and is distributed across three sites: Ste. Luce, Mandena and Petriky (Fig. The forests in the three areas are used by local people for subsistence purposes, which include fuel wood, construction materials, food and medicine. Due to long-term human pressure, the forests are considered as degraded or secondary forests. The future persistence of the forests is under pressure not only from local use but also from the potential establishment of a large mining operation, which will progressively exploit each of the three forest blocks for ilmenite deposits.These forests have been mapped and preclassified by degradation level as determined from a semi-quantitative ground assessment of canopy closure conducted from 1999 to 2001 as part of an Environmental Impact Assessment of mining in the region (QMM, 2001, Fig. Field surveysIn November 2001, twenty-one belt transects were surveyed in the three study sites. Samples were taken within six forest stands belonging to four of the five different degradation classes (2 strongly degraded, 2 moderately degraded, 1 in good condition, 1 in very good condition). The more degraded stands were smaller in size and, for this reason, two stands were sampled within the strongly degraded and moderately degraded classes. No samples were collected in the extreme degradation class since there was often no intact forest at these sites. Each transect was 100 m by 4m and the site of each transect was randomly selected within each stand and sited at least 100 m from the forest edge when stand size permitted. The cardinal direction of each transect was randomly selected unless the selected direction of the transect would extend the transect into a land cover type other than forest (i.e., swamp or matrix). At each site, geographic coordinates were recorded with a Geographical Positioning System (GPS) allowing geo-referencing to the satellite data. Within each transect, all trees with a dbh (dbh taken at 1.3 m from the forest floor) N5 cm within the 4 m by 100 m transect were identified and the dbh recorded. Where there was more than one stem, all stems N5 cm dbh were recorded, enabling basal area and stem density to be calculated. An earlier field campaign was conducted in the region during July 2000 and measurements of stem density and dbh within 23 transects were collected. This data set was used to test the predictive power of the neural networks when trained with the ground data collected in 2001. However, the 18-month time lag between the collection of the 2000 test data and satellite image acquisition in January 2002 is less than ideal since these forests are used daily by local people and, thus, there could easily have been significant change in the forest structure in the interim. This is a problem characteristic of many remote sensing studies in which it is often difficult to obtain appropriate satellite imagery and ground data from identical time periods To determine the compatibility between the measurements of total basal area/m 2 and stem density derived from the different methods, the ground data collected in 2001 were transformed into the VAT data format. This transformation was done by dividing the data collected in 2001 into the three size classes designated in 2000 and recalculating the basal area and stem density. The relationship between the estimates of forest structure using the VAT method of 2000 and the fixed transect method of 2001 was very strong for basal area (r=0.98, n=21, pb0.01) and strong for stem density (r=0.79, n=21, pb0.01). Thus, the potential error associated with the use of these slightly different methods is negligible for basal area but more of a concern for stem density.Landsat ETM+ dataA Landsat ETM+ scene (path 158, row 077) covering the study site was obtained for January 2002. Preprocessing requirements of satellite imagery may include atmospheric, radiometric, topographic and geometric correction. Atmospheric and radiometric corrections are necessary when comparing multiple images from different locations or time periods. However, atmospheric corrections were not performed in this study since single date imagery was used, all of the sites of interest are located within the same image The radiance of the red (band 3), near-infrared (NIR)(band 4), mid-infrared (MIR) (band 5) and MIR (band 7) bands and standard deviations (SD) for each band at each transect site were extracted using a 3\u00c23 pixel window. Each window was centered on the GPS recorded position of each transect in order to minimize any potential mislocation errors (similar to The standard deviation of the spectral response of each band can provide a textural assessment of canopy structure Statistical analysesCorrelation analysesPreliminary analyses of the structural, spectral reflectance and NDVI data were conducted within the SPSS statistical package using descriptive statistics and correlation analyses. A Kolmogorov-Smirnov test was conducted to determine the normality of the data Artificial neural networksArtificial neural networks (ANNs) were used in this research (i) to determine the relationship of Landsat ETM+ bands 3, 4, 5 and 7 and textural information in those bands to basal area and stem density and (ii) based on this relationship, to use the spectral data within the image to predict structural attributes of pixels comprising littoral forest where no ground data had been collected. NDVI was excluded from the ANN analysis because of the weak correlations observed among NDVI and structural measures. Multi Layer Perceptrons (MLPs) are the most common type of ANN used for remote sensing studies where N is the number of neurons and i is equal to the number of inputs in the input layer. There were eight inputs consisting of the spectral response values for each of the four spectral bands and the SD values of each band. The desired output was either the basal area or stem density. These structural parameters were tested separately in relation to the spectral information but the same architecture was used for each network: one input layer, one hidden layer and one output layer. For all of the networks created, a sigmoid axon transfer function, which is a non-linear transfer function, was used with a momentum learning rule, a step size of 1.00 and a momentum set to 0.700. It is recommended to standardize input variables to values between 0 and 1 where SV is the standardized value, R is the real value, min is the minimum value in the training data and max is the maximum value of the training data. Two different approaches were used for testing the relationship between spectral response and forest structure within the neural networks, although the network architecture and structure was identical for both approaches. were used as the testing data. After creating and training the network, the model was tested and actual measures of basal area and stem density were plotted against the values for each feature as predicted by the network for each transect site. The predictive power of the model was tested using a linear regression and a Bland-Altman test where d is the mean difference, SD is the standard deviation of the differences and LA represents the upper and lower limits of agreement. The degree of difference deemed suitable to permit the substitution of one method for another must be judged by the researcher and will depend on the specific applications of the method. jackknifing technique was also used to test the predictive power of the network. This analysis utilized only the ground data collected in November 2001. In this approach, all but one of the 21 transects from 2001 were used as training data and the excluded transect was used as the testing data. This process was repeated 21 times so that all values of stem density and basal area were used as testing data once, which produced a predicted value of stem density and basal area for each of the 21 transects.The relationship between predicted and actual values of stem density/ basal area were assessed using a linear regression and the Bland-Altman test.Predictive map of basal areaThe littoral forests were isolated from other vegetation types by creating a littoral forest mask developed from previous land cover classifications of the region. Using the basal area data set collected from the field survey conducted in 2001 as the training data, the neural network was used to predict the basal area for all unsurveyed littoral forest pixels from the spectral information in bands 3, 4, 5 and 7. The predictions of basal area made by the neural network were spatially displayed across the littoral landscape. The resultant predictive map of basal area was compared to the map of forest condition derived from the semiquantitative ground survey.ResultsRelationship of forest structure to spectral reflectance values using correlation analysesThe results showed that the 2001 ground data were strongly and significantly correlated with spectral response in all wavebands while the relationships varied in strength and significance when using the ground data collected in 2000 (Table Stem density was not strongly related to spectral response in any of the individual bands and demonstrated widely divergent relationships with NDVI between the two data sets. Stem density from the 2000 data was strongly and significantly related to NDVI (r=\u00c00.69, pb0.01) but there was no strong or significant relationship between the 2001 stem density data and NDVI (r=\u00c00.09, pN0.05). To test if the difference in the relationship between the two data sets was due to the different ground survey methods, the 2001 data were transformed into the VAT format and then reanalysed with a Spearman's test in relation to NDVI. The results showed no improvement in the strength of the relationship (r=0.03, pN0.05) suggesting that the methodological difference does not account for the variability in the correlations between the two years of ground data. A plot of the negative relationship between NDVI and stem density measurements from the year 2000 revealed a trend in which very high values of NDVI were tightly clustered at sites of low stem density and scattered values of NDVI were observed at sites of mid to high stem density (Fig. 3.2. Results from the artificial neural networks 3.2.1. The 2001 Jackknife ANN The 2001 Jackknife ANN produced stronger predictions for basal area than for stem density. The relationship between actual and predicted values of stem density was weak and insignificant (r=0.02, n=21, pN0.05). The values of basal area predicted by the 2001 Jackknife ANN were strongly and significantly related to actual measurements of basal area (r=0.79, n=21, pb0.01; Fig. The 2001/2000 ANNThe 2001/2000 ANN proved to be more powerful for predicting basal area than for predicting stem density. The relationship between predictions of stem density and actual values of stem density was weak and insignificant (r=0.04, n=16, pN0.05). The 2001/2000 ANN produced a strong and significant relationship between actual and predicted values of basal area (r=0.69, n=16, pb0.01). There was one outlier for which the predicted values of basal area produced by the 2001/2000 ANN were considerably lower than the actual measure of basal area. A plot of the residuals identified this as an outlier, having a residual value greater than two standard deviations from the mean residual value. It is likely that this low prediction could be due to tree removal, which has occurred at this transect since the ground data were collected in July of 2000 and which was not detectable in the visual comparison of aerial photography from December 2000 and satellite imagery from 2002 and 1999. This sampling station was located in a forest fragment that is close to the main village in St. Luce (S9) and is frequently accessed by villagers for resources. Once this outlier was removed, the relationship improved (r=0.82, n=15, pb0.01; Fig. Textural informationThe contribution of textural values to basal area predictions was tested by running each network, the 2001/ 2000 ANN and the 2001 Jackknife ANN, with the radiance values within each band but without the textural information. The relationship between predicted and actual basal area did not change significantly when the textural data were removed from the input data within either the 2001/ 2000 The predictive map of basal areaThe predictive map of basal area (Fig. In Petriky, pixels predicted to have the highest basal area occur within the middle of the forest fragment. Although these high predictions of basal area occur within pixels proximate to the road, they were at a maximal distance from the three villages located on the periphery of the forest.A high degree of heterogeneity in the basal area predictions across the Mandena site. Two fragments, M15 and M16, have been designated by the mining company as conservation zones and were predicted to have a relatively high basal area when compared to the other forest stands in the site. Forest fragments that lie proximate to roads, such as M13 and M3, are predicted to have very low basal area. These predictions of low basal area have been confirmed by ground observations within these forest parcels (JCI, the author's observations). In comparison, fragments M4, M5, M6 and M7, which are set slightly back from the road and are bordered by a river on the northern boundaries of the fragments, were predicted to have more inter-stand heterogeneity in basal area values and a higher abundance of high basal area pixels when compared to unprotected fragments within Mandena. The pixels predicted to have the lowest basal area within M4, M5, M6 and M7 fragments were located predominantly along the edge of these fragments. Multiple pixels across the Mandena site were predicted to have very high basal area values (N20 cm/m 2 ), notably in the area between stands M15 and M16. However, ground observations have revealed that most of these areas are wetland-like forest habitats (JCI and TD, authors' observations). These swampy areas are characterized by a high abundance of palm-like trees, with large leaves, which may have a spectral signal similar to dense vegetation.In St. Luce, there is one main road that passes by the three surveyed forest parcels (S9, S8 and S7). Three villages are located towards the end of this road, one of which is extremely close to the forest stand of S9. The forest pixels located closest to this village were predicted to have a relatively lower basal area than pixels at greater distances from the village and the road, most likely due to the ease of access at these locations. The core areas of the forest parcels were generally comprised of pixels with higher predicted values of basal area. Within the three larger fragments, S9 and S6/S7 (which is bisected by a stream into S6 and S7), there was a higher overall basal area when compared to the smaller fragments such as S8 and other forest stands included in the region.DiscussionThe relationships among spectral values and basal area derived from the correlation analyses and those between predicted and actual measures of basal area derived from the ANNs demonstrate that the selected wavebands from the Landsat ETM+ are useful for estimating forest basal area for the study site. The correlation analyses provided descriptive information on the strength of each band in relation to stem density and basal area while the ANNs made use of these relationships to predict basal area across the landscape with reasonable accuracy. The spatial depiction of basal area across the landscape displayed climatic impacts on basal area as well as trends associated with distance from human population centres and accessibility to forest fragments. The findings in this study support observations from similar studies and contribute to the growing body of knowledge on the application of remote sensing for assessing tropical forest condition.Correlations of NDVI to stem density and basal areaThere were inconsistent relationships between stem density measures and NDVI. The stronger relationship between the stem density measurements collected in 2000 and NDVI in comparison to the very weak relationship between stem density measures collected in 2001 and NDVI could be due to the careful pre-stratification of sites within each forest fragment during the 2000 field survey.During the 2000 field survey, each transect site was chosen to represent the deterioration status of the forest fragment. In contrast, during the 2001 survey, transect sites were randomly selected within each fragment in order to assess the range of internal fragment structural diversity. Because of the stratification during the 2000 survey, more extreme differences in stem density among fragments were captured when compared to the 2001 data on stem density (as seen in Fig. There was no significant relationship between NDVI and basal area using data from either of the two field campaigns. Previous studies in tropical forests have also reported weak and insignificant relationships between Landsat TM-derived NDVI values and biomass Correlations of spectral response with stem density and basal areaThe lack of strong and significant relationships of individual bands with stem density is similar to findings in other studies, which have failed to identify a strong relationship between stem density and spectral response (e.g., All correlations of the spectral values in the individual wavebands with basal area were negative. Negative correlations of TM bands 3, 4, 5 and 7 with forest structural parameters have also been observed in other studies investigating canopy spectral reflectance The strength of the relationship between basal area collected in 2000 and spectral response varied from the correlations observed from the analysis of the 2001 ground data. The decrease in the strength of the relationship between basal area and spectral response in MIR band 5, MIR band 7 and the red band 3 could be due to physical alterations to the forest vegetation at sites surveyed in 2000 and the sensitivity of these bands to forest change. The MIR band 5 has been shown to have high sensitivity to canopy changes during tropical forest succession The NIR band was the only band in which the relationship between spectral response and basal area did not change significantly when using the ground data collected in 2000. A plot of the NIR values versus basal area measurements from the 2000 survey revealed a tight cluster of relatively high NIR values at sites with low values of basal area (Fig. Neural networks for predicting stem density and basal areaAlthough the correlation analyses showed that the correlations of the MIR bands 5 and 7 with basal area were very strong and, thus, potentially useful individually for predictive purposes, the ANN approach was robust across the two data sets and allowed inclusion of multiple wavebands also known to provide useful information on forest structure and, thus, providing more predictive power than one or two bands used alone. Using a neural network, the radiance values within the four spectral bands produced predictions of forest basal area, which were strongly and significantly correlated with the actual measures of basal area. The strong and significant relationships between predicted and actual measures of basal area using both the 2001/2000 ANN and 2001 Jackknife ANN support the utility of neural networks for predicting patterns of basal area from the spectral bands used in this study. However, the two networks did differ in the prediction tendencies. The predictions of basal area using the 2001 Jackknife ANN were fairly close to a 1:1 relationship and were consistent in variability with respect to the field measurements. The 2001/2000 ANN produced a strong and significant relationship between actual and predicted measures of basal area but, in contrast to the 2001 Jackknife ANN, the relationship deviated substantially from the 1:1 line as indicated from the results of the Bland-Altman test. The model derived from the 2001/2000 ANN had a tendency to underpredict values of basal area at sites with medium to high actual values of basal area and to overpredict values of basal area at sites with relatively low actual values of basal area. This trend can be attributed to changes in basal area since the data were recorded in July 2000, which would alter the relationship between actual and predicted estimates of basal area derived from the 2002 satellite image. The nature of the model's systematic error, due largely to observed physical changes occurring in this forest, supports the utility of the method. However, the results from the two ANN models demonstrate the importance of obtaining a satellite image as close as possible to the date of the ground survey for more accurate estimations of basal area.Remote sensing alone may not be able to produce exact values of basal area, but when used in combination with ground data, the bias and limits of agreement can be calculated to give an indication of the deviation from a 1:1 agreement and, thus, the error associated with the predictions. The strong relationship between actual and predicted values of basal area for both ANNs demonstrate the usefulness of remote sensing for assessing patterns in basal area across a landscape and its potential as a tool for first-order assessments of forest condition when ground survey data are not available. Contribution of textural information for predicting basal areaTextural information appears to be insignificant in this study since the relationships between predicted and actual measures of basal area did not change significantly when textural data were removed from the network. Textural measures derived from the standard deviation of pixels within a 3\u00c23 window have been useful for discriminating heterogeneity in vegetation structure using imagery of higher spatial resolutions than Landsat TM or ETM+ images Predictive map of basal areaThe map derived from the ANN predictions of basal area displayed patterns of forest condition comparable to those presented within the semi-quantitative map, captured intersite differences in basal area associated with environmental variability and portrayed intra-site trends in basal area associated with human impact. The ANN predictive map provided finer spatial detail on the heterogeneity of basal area within a forest fragment and displayed trends in basal area that were lacking within the semi-quantitative preexistent map of forest condition (Fig. In the absence of humans, spatial patterns of forest basal area will be determined by environmental variables Within each of the three forest sites where environmental variables are relatively constant, predicted patterns of basal area appear to be associated with centres of human population combined with ease of accessibility to the forests. Changes in forest structure and decreases in basal area have been observed with distance from human settlements or roads and represent the impact of human pressure on forests The application of this method for monitoring forest conditionThe relationship between spectral information and ground-collected forest structural data can be combined with other indicators of forest change detectable throughout a time series of satellite images to monitor forest modification Conclusion", "conclusions": "This study has demonstrated the potential for using a limited amount of ground-collected data and Landsat ETM+ spectral information from bands 3, 4, 5 and 7 for estimating the basal area of tropical forests in southeastern Madagascar. The strong and significant relationships between predicted and actual measurements of basal area derived from ANNs and the compatibility of the ANN-derived map of basal area (Fig. ) with a preexistent map of forest condition (Fig. 4) support the use of the methods presented here for assessments of basal area across a forest landscape. Future research should build upon these relationships and investigate how the predictive relationships between spectral information and basal area observed here can be generalized temporally and, thus, incorporated into long term monitoring of forest condition for the region.1", "SDG": [15]}, "marxan_with_zones_software_for_optimal_conservation_based_land_and_sea_use_zoning": {"name": "Marxan with Zones: Software for optimal conservation based land-and sea-use zoning", "abstract": " Marxan is the most widely used conservation planning software in the world and is designed for solving complex conservation planning problems in landscapes and seascapes. In this paper we describe a substantial extension of Marxan called Marxan with Zones, a decision support tool that provides landuse zoning options in geographical regions for biodiversity conservation. We describe new functions designed to enhance the original Marxan software and expand on its utility as a decision support tool. The major new element in the decision problem is allowing any parcel of land or sea to be allocated to a specific zone, not just reserved or unreserved. Each zone then has the option of its own actions, objectives and constraints, with the flexibility to define the contribution of each zone to achieve targets for pre-specified features (e.g. species or habitats). The objective is to minimize the total cost of implementing the zoning plan while ensuring a variety of conservation and land-use objectives are achieved. We outline the capabilities, limitations and additional data requirements of this new software and perform a comparison with the original version of Marxan. We feature a number of case studies to demonstrate the functionality of the software and highlight its flexibility to address a range of complex spatial planning problems. These studies demonstrate the design of multiple-use marine parks in both Western Australia and California, and the zoning of forest use in East Kalimantan.", "keywords": "Biodiversity,Zoning,Marxan,Systematic,conservation,planning,Decision,support,Optimization,Simulated,annealing,Natural,resource,management", "introduction": "Systematic conservation planning involves finding cost-efficient sets of areas to protect biodiversity. One goal of systematic conservation planning is to meet quantitative conservation objectives, such as conserving 30% of the range of each species, as cheaply as possible . This is referred to as the minimum-set problem (Carwardine et al., 2009). It can be expressed as an integer linear programming problem if the cost and constraints are linear functions of the number of sites in the system (Possingham et al., 2006)(Cocks and Baird, 1989;Possingham et al., 1993;Underhill, 1994;Willis et al., 1996;.McDonnell et al., 2002)Recent research efforts have focused on developing computer software to solve the minimum-set problem . Numerous algorithms can find solutions to the minimum-set problem (Sarkar, 2006)(Margules et al., 1988;Rebelo and Siegfied, 1992;Nicholls and Margules, 1993;Csuti et al., 1997;. The large number of feasible solutions to the minimum-set problem makes some iterative and optimizing algorithms unsuitable for large conservation planning problems. They often are slow to find solutions, find only inefficient solutions or find only single solutions. Marxan is the most widely used conservation planning software in the world with over 2600 individuals and 1500 organisational users in 110 countries. It uses a simulated annealing algorithm because of its ability to find many near-optimal solutions to large problems fairly quickly Pressey et al., 1997)(Ball and Possingham, 2000;.Possingham et al., 2000)A major limitation of the approach to spatial planning employed in existing systematic conservation planning software such as Zonation, ResNet, C-Plan and Marxan, is the inability to simultaneously consider different types of zones to reflect the range of management actions or conservation activities being considered as part of a conservation plan . Indeed, in terrestrial environments, conservation practitioners implement a diversity of management actions, ranging from fire management and predator control, to restoration and reservation (Moilanen et al., 2009). Furthermore, conservation activities occur in a matrix of alternative land and sea uses, many of which are contrary to conservation objectives. While the outer boundaries of protected areas are often identified using systematic conservation planning software (e.g. (Wilson et al., 2007)Fernandes et al., 2005;, the actual process of zoning is often performed outside of a systematic framework because of software limitations.Airame et al., 2003)Zoning is a common management practice to spatially and temporally designate areas for specific purposes (Anon, 1977;Korhonen, 1996;Liffmann et al., 2000;Day et al., 2002;Russ and Zeller, 2003;Airame, 2005;. Zoning plans provide an explicit approach to resolving conflicts between activities and determining trade-offs when balancing these competing interests Foster et al., 2005). Zoning is implemented around the world as an approach to support the multiple objectives of marine parks (notably in Australia's Great Barrier Reef Marine Park, (Halpern et al., 2008). The purpose of zoning in a conservation context is typically to accommodate potentially conflicting activities including conservation, non-consumptive recreation (e.g. scuba), and consumptive uses (e.g. fishing) within an integrated system of management Fernandes et al., 2005).(Day, 2002)The original Marxan software could only include or exclude a planning unit from being reserved, implicitly assuming two zones: reserved or not reserved. Furthermore all conservation features (such as vegetation types or species) are assumed to be fully protected in a reserve and all conservation features outside a reserve are lost -an assumption which does not match reality. Multiple zoning could be achieved by iterative application of the software , but that is clumsy and sub-optimal. Forest managers have used simulated annealing algorithms to harmonize site suitability for forestry regimes (Loos, 2006), and planners have internally zoned protected areas based upon spatial attributes such as compatibility and connectivity (Bos, 1993). Our zoning approach differs by explicitly targeting representation, complementarity and constraints of conflicting objectives in the problem definition (Sabatini et al., 2007).(Lourival, 2008)In this paper we introduce Marxan with Zones, an analytic tool that expands on the basic reserve design problem to allow for zones. This enables users to move from the binary decision framework of conservation planning to multi-use landscape and seascape planning by allowing for the efficient allocation of planning units (i.e. the units of land or sea available for selection) to a range of different management actions that may offer different levels of protection. We present Marxan with Zones as a tool for systematic zoning; not only to improve planning for reserve systems but also with application to a wider range of natural resource management and spatial planning problems. Two particular limitations of Marxan are overcome in Marxan with Zones: 1) the ability to spatially separate multiple and potentially conflicting activities (e.g. fishing and non-consumptive recreational activities) and 2) the ability to explicitly address multiple objectives (e.g. conservation and socioeconomic) in a systematic way. This is the first land-use zoning software with a particular focus on conservation.First, we describe the mathematical formulation of the problem for which Marxan finds good and feasible solutions as well as the new decision problem addressed by Marxan with Zones. Next, we discuss additional data requirements and example applications that illustrate the new functions of the software. We conclude by discussing some challenges and potential applications of Marxan with Zones in conservation planning. The software should be of interest to systematic conservation planning practitioners, policy makers and natural resource managers.", "body": "Systematic conservation planning involves finding cost-efficient sets of areas to protect biodiversity. One goal of systematic conservation planning is to meet quantitative conservation objectives, such as conserving 30% of the range of each species, as cheaply as possible Recent research efforts have focused on developing computer software to solve the minimum-set problem A major limitation of the approach to spatial planning employed in existing systematic conservation planning software such as Zonation, ResNet, C-Plan and Marxan, is the inability to simultaneously consider different types of zones to reflect the range of management actions or conservation activities being considered as part of a conservation plan Zoning is a common management practice to spatially and temporally designate areas for specific purposes The original Marxan software could only include or exclude a planning unit from being reserved, implicitly assuming two zones: reserved or not reserved. Furthermore all conservation features (such as vegetation types or species) are assumed to be fully protected in a reserve and all conservation features outside a reserve are lost -an assumption which does not match reality. Multiple zoning could be achieved by iterative application of the software In this paper we introduce Marxan with Zones, an analytic tool that expands on the basic reserve design problem to allow for zones. This enables users to move from the binary decision framework of conservation planning to multi-use landscape and seascape planning by allowing for the efficient allocation of planning units (i.e. the units of land or sea available for selection) to a range of different management actions that may offer different levels of protection. We present Marxan with Zones as a tool for systematic zoning; not only to improve planning for reserve systems but also with application to a wider range of natural resource management and spatial planning problems. Two particular limitations of Marxan are overcome in Marxan with Zones: 1) the ability to spatially separate multiple and potentially conflicting activities (e.g. fishing and non-consumptive recreational activities) and 2) the ability to explicitly address multiple objectives (e.g. conservation and socioeconomic) in a systematic way. This is the first land-use zoning software with a particular focus on conservation.First, we describe the mathematical formulation of the problem for which Marxan finds good and feasible solutions as well as the new decision problem addressed by Marxan with Zones. Next, we discuss additional data requirements and example applications that illustrate the new functions of the software. We conclude by discussing some challenges and potential applications of Marxan with Zones in conservation planning. The software should be of interest to systematic conservation planning practitioners, policy makers and natural resource managers.Mathematical formulation of MarxanThe original Marxan software aims to minimize the sum of the site-specific costs and connectivity costs of the selected planning units, subject to the conservation features reaching predetermined targets in the reserve system. The Marxan minimum representation problem is: minimizewhere there are m planning units under consideration. The first term of equation ( where there are n features under consideration. This penalty is zero if every feature j has met its representation target in the selected reserve system. It is greater than zero if the targets are not met, and gets larger as the gap between the target and the conserved amount increases. The terms FPF j and FR j are the feature penalty factor and feature representation respectively, which are the scaling factors used when a feature has not met its representation targets. FPF j is a scaling factor which determines the relative importance of meeting the representation target for feature j. FR j is computed as the representation cost of meeting the representation target of feature j. A zone configuration that satisfies the target for feature j only is computed, and then FR j for feature j is set as the cost of this zone configuration. This representation cost is given in terms of the configuration cost plus the connectivity cost, and is computed for each feature by using that features representative zone configuration as the control variable x in equation (1). The shortfall s is the amount of the representation target not met and is given by s \u00bc t j \u00c0 P m i \u00bc 1 a ij x i . The Heaviside function, H(s), is a step function which takes a value of zero when s 0 and 1 otherwise. The feature specific parameter t j is the target representation for feature j. The expression \u00f0s=t j \u00de is the measure of the shortfall in representation for feature j. It is reported as a proportion and equals 1 when feature j is not represented within the configuration and approaches 0 as the level of representation approaches the target amounts. The Heaviside function ensures the whole equation becomes zero when the representation is greater than the target amount. We use the shortfall as a weighting factor of the total cost to meet the target, which assumes the cost associated with the shortfall is a linear proportion of the total cost to meet the target. This is a simplification as the total cost may vary non-linearly as the shortfall changes. We use the simplification because the actual cost of meeting the target for each iteration of Marxan is computationally expensive to find and provides little improvement in the final answer.We combine equations ( Marxan uses the simulated annealing algorithm Marxan with Zones generalizes this approach by increasing the number of states or zones to which a planning unit can be assigned. Each term of the objective function is increased in complexity. Furthermore, two types of representation targets are allowed and consequently the representation shortfall penalty reflects two types of shortfall.Mathematical formulation of Marxan with ZonesThe aim of the Marxan with Zones software is to minimize the sum of costs and connectivity costs of the zone configuration of planning units, subject to meeting the representation targets and zone targets. A zone configuration is a solution and it fully specifies the type of zone in which each planning unit is placed. This is the Marxan with Zones minimum representation problem, formally defined as:and subject toIn this case there are m planning units and p zones. The first term of equation ( We define a cost matrix c ik , which is the cost of placing each planning unit i in zone k. The second term of equation ( In equation ( In equation ( We use a feature penalty equation below to implement the two target constraints in the Marxan with Zones objective function:This is the sum of two different representation targets where there are n features under consideration. The shortfalls s1 and s2 are the amount of the two different representation targets not met and are given by s1Both shortfalls are used as weightings for the feature dependent factors of FPF j and FR j in the same way they were in the Marxan problem formulation.Combining equations ( This is identical to the Marxan objective function in equation ( Additional information requirements for Marxan with ZonesMarxan with Zones has a number of information requirements beyond those used in Marxan. Individual planning problems will determine the amount of additional information required. At a minimum, the number of zones and the costs of assigning each planning unit to each zone must be defined. In this section, we describe the additional information requirements and introduce three case studies. The case studies demonstrate how the new capabilities of Marxan with Zones can be applied to a variety of multi-zone problems. A detailed description of how to use Marxan with Zones, and the interaction between its many input parameter files, is provided in the on-line manual Multiple zonesA list of all possible zones must be defined. These can range from high quality conservation zones (e.g. well managed national park) to extractive use zones (e.g. intensive agriculture, forestry, or fishing). The user can specify zone-specific targets to prescribe how feature targets are achieved. For example, given an overall target of 20% for each habitat type, which could be met across three different conservation zones, the user may require at least 10% of the overall target to be met in the zone offering the highest level of protection. Not specifying a zone-specific target means that the overall target for a feature can be achieved across all zones (see Case Study 1).Furthermore, Marxan with Zones allows the user to prescribe preferred spatial relationships between zones using a zone boundary cost CostsThe cost of allocating each planning unit to each zone must be defined. Marxan with Zones can accommodate multiple costs for individual planning units (see Case Study 2), with the total cost of Case Study 1. Zoning multiple-use marine parks: achieving multiple objectives and minimising conflict of use Several potential conflicting uses within the marine park were identified:  These interactions informed the definition of a zoning framework with the flexibility to deliver across competing objectives. The three zones defined for this case study are presented in Table Marxan with Zones provides a systematic approach to the problem of managing multiple uses by allowing spatial separation of activities into zones. This reduces the potential conflict between the different uses. An additional function within Marxan with Zones is the zone boundary cost. This serves to prescribe the spatial relationship between zones and is useful to encourage further separation of conflicting uses (by setting a high zone boundary cost) or to cluster zones which share compatible management objectives (a low zone boundary cost). We set this feature so that the high protection zone is preferably buffered by partial protection zone to further reduce the potential conflict between conservation objectives and fishing activities.To successfully manage the multiple activities within the marine park, explicit objectives must be defined that identify the target level of reservation for each activity (Table Each zone makes a different contribution towards these objectives, according to whether activities are included or excluded. Table Marxan with Zones generated many efficient solutions to this problem, each of which meet all of the objectives. An example map illustrating the spatial configuration of one of the solutions is shown in Fig. Marxan with Zones outperforms standard Marxan for this particular problem by delivering a zonation scheme which supports multiple objectives within an individual reserve, and configuring the zones to accommodate a range of uses that can be spatially separated. In this example these functions serve to minimize conflict of use between resource protection and resource extraction. Standard Marxan would have been limited to meeting one of the objectives (traditionally the conservation objective) and could only spatially separate two activities. Using Marxan with Zones, marine park managers can systematically approach the zoning of multiple-use marine parks, separate potentially conflicting activities, and achieve multiple objectives.assigning a planning unit to a particular zone measured as the sum of the individual costs. Costs can also be zone-specific. For example, there may be purchasing, opportunity, and management costs associated with designating a planning unit as a national park FeaturesFeatures may be defined as elements that the user would like to occur in particular zones We planned for five zones, each restricted to different fisheries: 1) No-take marine reserve (all fisheries restricted); 2) Conservation area, high (7 fisheries restricted); 3) Conservation area, high/medium (4 fisheries restricted); 4) Conservation area, medium (3 fisheries restricted); 5) Commercial fishing zone (no fishing restrictions).We used the same spatial data representing habitats, depth zones, and commercial fishing value used in the Initiative. Habitats included coastal marshes, eelgrass, estuaries, hard bottom, kelp forests, soft bottom, surfgrass, and tidal flats. We subdivided these features into three biogeographic regions (North, South, and the Farallon Islands) and three depth zones We implemented Marxan with Zones for two different scenarios, each with different zone-specific targets. In scenario 1, represented 10% of the distribution of each biodiversity feature in a no-take reserve (zone 1) and an additional 20% in any of the four protected area zones (zone 1-4). We evaluated the results of scenario 1 to determine the proportion of lost value overall and for each of the fisheries. With the aim of more equitably affecting the fisheries, in scenario 2 we also targeted a percentage of each fisheries total value, where the fishing targets could only be achieved in zones where the given fishery was not restricted. We targeted the same proportion for each fishery and incrementally increased the target by 1% until 100% of the fishing grounds were placed in a zone without spatial fishing regulations. In addition we compare the results of our scenarios to those produced using Marxan (without zoning), where we minimized lost fishing value subject to the constraint that 30% of each biodiversity feature is protected in a no-take reserve. Given that Marxan can only select areas important for one type of protected area, we assume that selected areas are a cost to all fisheries.We found that Marxan with Zones outperforms Marxan in two ways. First, the overall impact on the fishing industry is reduced. Second, there is a more equitable impact on different fishing sectors (Fig. Results from this application of Marxan with Zones will inform California's Marine Life Protection Act Initiative's stakeholders, staff, and scientific advisors in designing marine protected areas that efficiently achieve the biodiversity conservation and socioeconomic objectives. allocated to forestry, or the expected total timber production is above a certain level.Relationship between zones and featuresIn some cases, it may be useful to define the relationship between each zone and feature. The contribution of a zone towards achieving feature targets can be indicated by the user (see Case Studies 1 and 3). Feature targets can be achieved across a combination of zones, with the potential for some zones to contribute more than others to feature targets than others. For example, in Case Study 1, the fishing and recreational zones represent management regimes offering different levels of protection to biodiversity features. This information determines how much of each feature in each zone is needed for target achievement.Software evaluationSystem testing of Marxan with Zones used a staged approach. Multiple scenarios were constructed, starting with the standard Marxan dataset, and then incrementally adding new zoning and cost data structures. Using this method, we determined the influence of each new data structure on the resulting spatial configurations and summary outputs. This simplified the sensitivity analysis and identification of the cause for observed software bugs and discrepancies. The software was tested on a range of problems relating to biosphere reserves The number of zones, planning units, features, and costs that can be input into Marxan with Zones is limited by the applications memory address space, which is currently 2 GB due to our use of a 32 bit computer operating system and 32 bit C compiler. A relatively simple conversion of the C code to available 64 bit operating systems and 64 bit C compilers would result in an increase of the potential memory address space to around 512 GB. A dataset with 80,000 planning units and 6000 features uses only 20% of the 32 bit application address space, and the amount of memory used scales approximately linearly with changes in the number of planning units and features. The number of zones and costs has little impact on the amount of memory used.We developed a systematic validation software system for Marxan with Zones that reproduced every computation at each step of the algorithm in an alternative software system (Zonae Cogito). This involved implemented reporting functionality in theStudy RegionFig. Zonae Cogito decision support system Marxan with Zones is significantly more complicated than the original Marxan software. The software was implemented very carefully to efficiently use computing resources. As a result, the performance of Marxan with Zones at runtime is approximately equivalent to the performance of an equivalent dataset running in the original Marxan. We have found that the number of iterations used needs to be scaled according to the number of zones used for efficient operation of Marxan with Zones. For example, the original Marxan software has two zones, so if we are using six zones for a Marxan with Zones problem, we need to use three times as many iterations for equivalent efficiency, resulting in the software taking three times longer to run.Discussion", "conclusions": "Marxan with Zones offers key improvements to the Marxan software by extending the range of problems to which the software can be applied. The in-built flexibility for users to define multiple objectives, multiple zones and accept multiple costs makes the software versatile and suitable for a wide range of resource management problems. Effective conservation zoning plans must often integrate the management of multiple uses and account for the different types of interactions between and among activities. They must not only separate conflicting activities but explicitly balance competing interests in a way that delivers acceptable trade-offs. Brokering trade-offs is a challenging task and will most likely be guided by government policy. Marxan with Zones provides a systematic planning framework to evaluate the consequences and trade-offs of alternative zoning configurations, which is critical for informed decision making.The ability to specify zone-specific planning unit costs presents a number of potential uses. It could support the design of conservation landscapes and seascapes that include both communally and privately managed areas, where the costs of conservation actions Case Study 3. A zoning configuration of multiple conservation strategies in East Kalimantan  Tropical rainforest habitat is used for a diversity of land uses ranging from protected areas to production forests. Each alternative land use makes a different contribution to the conservation of biodiversity (Wilson et al., in review). The degree of protection offered by different land uses varies, and a high protection status may not be necessarily synonymous with a large contribution to biodiversity conservation (Meijaard and Sheil, 2008)(Curran et al., 2004;. The contribution of different land uses to the conservation of species varies depending on the relative sensitivity of species to habitat modification and degradation DeFries et al., 2005).(Nakagawa et al., 2006)We applied Marxan with Zones to prioritize conservation investments in East Kalimantan by accounting for the relative costs and benefits of three conservation strategies across four primary land uses (Fig. ). We obtained data on the distribution of 170 mammal species that occur in the study region and evaluated their relative sensitivity to forest conversion and degradation 4. We assigned species-specific conservation targets and determined through expert evaluation the contribution of each land-use zone to achieving targets for each species depending on their sensitivity to forest loss and degradation. While some land uses contribute to the representation targets for all species, some land uses make no contribution to the target achievement of some species. This variable contribution was specified in the problem formulation.(Catullo et al., 2008)We prioritized investments in each alternative strategy in a spatially explicit manner, in order to achieve the conservation targets cost-effectively. Our results revealed the potential for the costs of conservation to be grossly overestimated if we assume that conservation goals can only be met through establishing new protected areas and if we assume that the unprotected matrix makes no contribution to conserving biodiversity. If we would have accounted only for the contribution of protected areas to our conservation goals we would have overestimated the required expenditure by an order of magnitude, and the area requiring protection would have been overestimated by almost five orders of magnitude. The effective and sustainable management of the unprotected matrix is revealed to be essential in East Kalimantan in order to achieve our conservation goals. This case study illustrates the economic and ecological imperative of considering the contribution of the unprotected matrix in planning analyses and the full suite of conservation strategies available for implementation. We found that traditional approaches deliver pessimistic estimates of the costs of achieving conservation goals, and similarly a conservative estimate of our conservation progress. By applying Marxan with Zones and making use of the expanded land-use planning functionality we have been able to evaluate not only where to act, but how to act in order to effectively and efficiently conserve biodiversity in East Kalimantan. This research provides an important step towards the development of an integrated conservation plan in East Kalimantan, and highlights the value of enhanced political and industry support for sustainable forest management, along with the need for an improved understanding of the contribution of sustainably managed forest to biodiversity conservation goals. differ, but the conservation outcomes are equivalent. It also allows for complex natural resource management situations where costs and biodiversity benefits vary depending on the land and sea use or management action. For example, conservation actions such as weed control, protected area establishment, and the creation of conservation easements could be spatially assigned in a zoning configuration. Moreover, this approach could prioritize actions based on ecosystem services, where biodiversity benefits and management costs of the delivery of one ecosystem service such as carbon sequestration could differ from others such as pollination or water filtration services . Marxan with Zones could help identify which parts of the planning region are most suitable for providing each ecosystem service.(Chan et al., 2006)Marxan with Zones can support many types of conservation focused decision making including; land-use planning, marine planning, urban and regional planning, and support for group decision making in a multi-stakeholder context. More generally, the software can solve spatial resource allocation problems involving multiple actions, objectives and constraints . The objectives and constraints can be based on economic, social, cultural or biological spatial features. The software is a decision support tool and is meant to support rather than replace decision making processes. The outputs can be useful in a decision making process through identification of priority areas that must be allocated to a particular zone (Wilson et al., 2009), and through the generation of alternative options for use in a negotiation setting (Klein et al., 2008a). We hope the novel functionality of Marxan with Zones will attract wide use in a range of conservation planning problems beyond those solvable by Marxan.(Airame, 2005)", "SDG": [15]}, "understanding_and_overcoming_the_barriers_for_cost_effective_conservation": {"name": "Understanding and Overcoming the Barriers for Cost-effective Conservation", "abstract": " Despite extensive research demonstrating the benefits of applying cost-effective conservation techniques, such as optimization, a large gap remains between the evidence from research and the actions of professions as they design and implement conservation programs. This study examines this gap through an international survey of conservation professionals who are familiar with cost-effective conservation techniques. The primary results of this study, replicate previous results from a smaller sample of agricultural preservation professionals, and show that the vast majority of survey respondents viewed cost-effectiveness as a virtue, but ultimately do not consider it as important as other program design criteria. These results reinforce the idea that advocates of cost-effective conservation need to address concerns about fairness and transparency and remedy gaps in the knowledge and expertise of professionals involved. Finally, the lack of incentive to conservation professionals to change their practices is a challenge that calls for public pressure and encouragement for experimentation and evidence-based policy to improve the cost effectiveness of conservation.", "keywords": "Cost-effective,conservation,Replication,Conservation,professionals,Conservation,planning,Optimization", "introduction": "Research has consistently shown that organizations with the most severely limited budgets have the most to gain from adopting cost-effective conservation (CEC). CEC is a project-selection process that incorporates both benefits and costs to maximize the conservation outcomes generated by available funds (see for instance, Underhill, 1994;Babcock et al., 1997;Balmford et al., 2000;Polasky et al., 2001, Naidoo et al., 2006;. Over the past couple of decades, a substantial literature has developed that advocates for applying CEC techniques, such as optimization through mathematical programming, to enable conservation professionals to select a set of projects that maximizes the organizations objectives for a given budget (see for instance, Sarkar et al., 2006)Babcock et al., 1997;Polasky et al., 2001;Ferraro, 2003;. Despite the many studies that have identified the benefits of CEC, conservation professionals remain wary Wu et al., 2001)(Arponen et al., 2010; and its application is limited. This lack of application is perplexing to economics, which has a core principal of studying the behavior given limited resources. Previous work by Gowdy et al., 2010) suggests that part of the problem is that conservation professionals do not consider cost effectiveness a priority and lack incentives to adopt new practices that could improve the cost effectiveness of conservation efforts. This research seeks to replicate the approach used by Messer et al. (2016a) by expanding the sample to an international group of professionals engaged in a variety of conservation activities. This research thus addresses the question of why conservation practitioners, who have dedicated their professional lives to environmental conservation, have not adopted cost effective selection techniques that would enable limited budgets to be further stretched to achieve greater environmental benefits.Messer et al. (2016a)In general, CEC methods consider both the benefits and the costs associated with each potential project and identify a set of projects that provides the greatest aggregate benefit possible (\"the most bang for the buck\"). Optimization delivers CEC by using a set of mathematical programming algorithms adopted from operations research, including binary linear programming (BLP) and goal programming, to systematically address complexities (see for instance, Underhill, 1994;Babcock et al., 1997;Balmford et al., 2000;Polasky et al., 2001;Naidoo et al., 2006;. Despite extensive research demonstrating the advantages of applying optimization techniques and efforts to acquaint conservation organizations with them, conservation professionals generally have not adopted cost-effective methods of project selection.Sarkar et al., 2006)Currently, conservation programs throughout the world rely mostly on benefit-targeting (BT), also referred to as rank-based method (Babcock et al., 1997, Messer and. BT involve constructing an index of potential benefits and associated weights from offered projects. For example, US federal conservation efforts have typically used BT-such as the selections for acquisition to the national parks system and for forest preservation Borchers, 2015)(Babcock et al., 1997., Wu et al., 2017)BT ranks each project according to the environmental benefits provided and sequentially selects the highest-ranking projects until the budget is exhausted . BT performs best when the benefits of various projects vary more than the costs of those projects (Ferraro, 2003). This benefit-only method is not cost-effective because it ignores cost as a selection criterion. BT can result in budgets quickly being exhausted by a few high-ranking but relatively expensive projects.(Babcock et al., 1997)While optimization through mathematical programming will always achieve the highest aggregate benefits for a given budget (assuming that the benefits are measured accurately), another CEC technique is benefit-cost targeting (BCT) which selects projects with the largest benefit-cost ratio until the budget is exhausted. BCT computes the greatest benefit per dollar and achieves greater cost-effectiveness than BT (Babcock et al., 1997;Ferraro, 2003;. In most cases, CEC and BCT will yield identical selection sets, except in cases involving large budget remainders Duke et al., 2015)(Duke et al., 2013;. For instance in the context of farmland conservation, Messer, 2006) found that BCT and optimization yielded very similar outcomes when focusing on associated benefits and costs, parcel numbers, and number of acres. These authors note that when a government or agency wants to achieve more specific goals optimization is usually better than BCT. A handful of conservation programs at the state level Wang and Swallow, 2016 and the federal level (Messer et al., 2016b)(Wu et al., 2000; have used variants of BCT to achieve CEC.Wu et al., 2001)This research seeks to understand why CEC does not have widespread use by conservation professionals. Researchers have pointed out several obstacles for CEC. For instance,  identified political process and perceptions of fairness by various groups as major obstacles. Sullivan et al. (2004) argued for the importance of accurately measuring the external benefits when designing conservation policies while other researchers have raised a variety of concerns about the difficulty in accurately capturing and quantifying these environmental amenities in the context of CEC Gardner (1977)(Arponen et al., 2010;Bryan, 2010;Gowdy et al., 2010;. Conservation professionals may resist adopting CEC methods because they are not familiar with the mathematics used in optimization or lack computer and software tools needed to implement them Bryan et al., 2011). (Ferraro and Pattanayak, 2006) pointed out that there exists a knowledge gap as conservation professionals do not have access to easily understandable scientific information. Pullin et al. (2004) also noted that conservation professionals often resist change and prefer to plan as they have in the past instead of incorporating further information into their decision making process. Pullin et al. (2004) identified the lack of awareness of optimization methods and lack of understanding of how they function as major obstacles to adoption. Prendergast et al. (1999) surveyed agricultural land professionals in Maryland and found that conservation professionals value transparency and fairness more than cost-effectiveness. This study seeks to replicate this earlier research by expanding the sample by surveying conservation professionals from organizations that have a variety of conservation objectives, not just agricultural preservation. Additionally, this study includes a broader sample of geographic regions including conservational professionals who work in international contexts. This research also helps address the so-called \"replication crisis\" in behavioral and social sciences. A growing number of scholars have expressed concerns that published results in the behavioral sciences, including economics, are frequently false Messer et al. (2016a). Replication is a cornerstone of science (Ioannidis and Doucouliagos, 2013) and is something that needs to occur more often in peer-reviewed articles (Moonesinghe et al., 2007). Replication is particularly important in the context of research that has policy implications.(Hamermesh, 2007)This study also contributes to the literature about the gap between the practices of conservation professionals and the recommendations of researchers interested in CEC. Importantly, this research studies the attitudes of conservation professionals who have been educated about CEC techniques, yet still generally do not use them in their work. Thus, this research provides important insights beyond the basic 'knowledge gap' arguments and suggests that other factors need to be overcome before CEC will occur on a widespread basis. We designed this study to address three primary objectives:1. Evaluate attitudes of conservation professionals about CEC.2. Evaluate whether these attitudes varied by the type of conservation activity. 3. Identify barriers that discourage conservation professionals from adopting CEC and determine what, if anything, can be done to overcome them.The results of this study demonstrate that the vast majority of survey respondents viewed cost-effectiveness as a virtue in program design but did not consider it as important as other program design criteria. In particular, respondents emphasize the important of fairness and transparency of the selection process. A major obstacle for adoption CEC is the lack of incentives to change existing programs as respondents seem to receive little public pressure to be cost-effective nor receive additional recognition or financial award in their work for making their conservation programs more cost-effective. Finally, respondents indicated that their likelihood of adoption of cost-effective conservation would increase if they can receive additional training and software to facilitate adoption of CEC.", "body": "Research has consistently shown that organizations with the most severely limited budgets have the most to gain from adopting cost-effective conservation (CEC). CEC is a project-selection process that incorporates both benefits and costs to maximize the conservation outcomes generated by available funds (see for instance, In general, CEC methods consider both the benefits and the costs associated with each potential project and identify a set of projects that provides the greatest aggregate benefit possible (\"the most bang for the buck\"). Optimization delivers CEC by using a set of mathematical programming algorithms adopted from operations research, including binary linear programming (BLP) and goal programming, to systematically address complexities (see for instance, Currently, conservation programs throughout the world rely mostly on benefit-targeting (BT), also referred to as rank-based method BT ranks each project according to the environmental benefits provided and sequentially selects the highest-ranking projects until the budget is exhausted While optimization through mathematical programming will always achieve the highest aggregate benefits for a given budget (assuming that the benefits are measured accurately), another CEC technique is benefit-cost targeting (BCT) which selects projects with the largest benefit-cost ratio until the budget is exhausted. BCT computes the greatest benefit per dollar and achieves greater cost-effectiveness than BT This research seeks to understand why CEC does not have widespread use by conservation professionals. Researchers have pointed out several obstacles for CEC. For instance, This study also contributes to the literature about the gap between the practices of conservation professionals and the recommendations of researchers interested in CEC. Importantly, this research studies the attitudes of conservation professionals who have been educated about CEC techniques, yet still generally do not use them in their work. Thus, this research provides important insights beyond the basic 'knowledge gap' arguments and suggests that other factors need to be overcome before CEC will occur on a widespread basis. We designed this study to address three primary objectives:1. Evaluate attitudes of conservation professionals about CEC.2. Evaluate whether these attitudes varied by the type of conservation activity. 3. Identify barriers that discourage conservation professionals from adopting CEC and determine what, if anything, can be done to overcome them.The results of this study demonstrate that the vast majority of survey respondents viewed cost-effectiveness as a virtue in program design but did not consider it as important as other program design criteria. In particular, respondents emphasize the important of fairness and transparency of the selection process. A major obstacle for adoption CEC is the lack of incentives to change existing programs as respondents seem to receive little public pressure to be cost-effective nor receive additional recognition or financial award in their work for making their conservation programs more cost-effective. Finally, respondents indicated that their likelihood of adoption of cost-effective conservation would increase if they can receive additional training and software to facilitate adoption of CEC.Research MethodsWe developed the survey using the Qualtrics software (Qualtrics, Provo, UT). We identified valid email addresses for 246 conservation professionals from around the world who had attended lectures on CEC techniques presented by the study co-authors. This list of participants (see Appendix A for the organizations represented by the participants) was derived from attendance lists of 47 presentations that were made at the National Conservation Training Center, the Land Trust Alliance Rally, the American Farmland Trust conference, and offices of nonprofit and government agencies. 1 .Since the sample population came from people who had previously attended lectures on CEC techniques, this sample is certainly not a representative sample of all conservation professionals worldwide. Instead, we selected this sample because it helps address the question of why, even once educated about the virtues and techniques of cost effective, conservation professionals are not adopting these approaches. Consequently, this research can move beyond the simple explanation that conservation professionals are not adopting these approaches because they are do not know they exist, but can instead dig deeper about what organizational and attitudinal barriers continue to exist to CEC.The recruitment process consisted of emailing a survey that consisted of 43 questions to 246 individuals. To encourage people to respond to the survey, we incentivized participation in the form of a raffle of one $250 Amazon gift cards and four $50 Amazon gift cards. Recipients of the gift cards were to be chosen randomly after the survey had been completed. All respondents were also offered the opportunity to donate the money to a nonprofit organization of their choice, this option was made available as we anticipated that some government employees would not be able to receive direct financial payment, but might still be motivated to participate by donating the money.We based the survey structure on Twenty-four of the original email addresses proved to be nonfunctional, reducing the number of professionals contacted to 246. The rate of response to the initial survey request was 26.4%. After completion of all of the follow-up emails, we had obtained responses from 85 individuals, representing a final response rate of 34.6%. The 85 responses consist of 65 from the long survey and 20 from the short survey.ResultsAt the beginning of the survey, respondents were asked to rate their knowledge of their own conservation programs on a scale of 1 (not knowledgeable) to 5 (expert). All of the respondents reported having expert or near expert knowledge with an average rating of 4.52. In terms of familiarity with optimization generally, which was rated on a scale of 1 (not at all) to 5 (very well), the average response was 3.14. Respondents who had heard a presentation on optimization rated their retention of the material presented fairly high-an average of 3.58 on a scale of 1 (remember nothing) to 5 (remember most of the information). Similarly, respondents who had read information on optimization techniques reported an average retention rating of 3.32.A very high percentage of respondents viewed optimization as a good idea (91%). However, only 55.4% thought it was applicable to their organizations, while 39% said that they did not know whether optimization would be applicable to their organization. Respondents were asked to rate the importance of five criteria on a scale of 1 (not important) to 5 (very important) in their project selection processes:(1) Knowledge (knowledge of the staff in how to use the selection process to identify good projects);(2) Fairness (fairness to applicants);(3) Transparency (ease of explanation to public, advisory board, potential applicants, etc.);(4) Cost-effectiveness (achieve the largest possible social benefit for a relatively low price), and(5) Ease (ease of administration).As shown in Fig. The results showed statistically significant differences between fairness and cost-effectiveness (p = 0.021). In addition, there are statistical significant differences between transparency and cost-effectiveness (p = 0.014). Other differences in means between the criteria were found to not be statistical significant.Several survey questions sought to evaluate the degree of difficulty of the challenges that have been associated with adopting optimization: Respondents rated the difficulty of each on a scale from 1 (not difficult) to 5 (very difficult) and the results are shown in Table The survey also asked questions designed to measure how planners' willingness to adopt optimization would be influenced by the availability of additional resources, such as user-friendly software and training. From an initial willingness of 2.95 (on a scale of 1 to 5), access to software raised average willingness to adopt to 3.34, a 13% increase, and access to both software and training raised willingness to adopt an additional 9% to 3.63 (Table We also conducted an analysis of factors that influence conservation professionals' willingness to adopt optimization as their primary selection process. Ordered probit models were used to analyze the relationships between willingness to adopt optimization and independent variables. As shown in Table LSP is a technique originally developed in computer science to help develop project selection criteria and corresponding weightings. LSP incorporates fundamental properties of human reasoning and seeks to develop a means for measuring project benefits in a way that corresponds to the intent of decision makers In Model 1, we estimate the impact of eight independent variables on conservation professionals' willingness to adopt optimization. The independent variables in Model 1 are Understand Optimization, Lack Incentives, Lack Experience, Initial Cost, Staff knowledge, Fairness Importance, Transparency Importance, and Forgo Best Project. The Understand Optimization variable is the response to the question that asked respondents to rate their understanding of optimization from 1 (not at all) to 5 (very well). We included these independent variables in the model, because we expected them to have an impact on conservation professionals' willingness to adopt optimization. For instance, as the understanding of optimization increases (Understand Optimization), we would expect to observe a greater willingness to adopt optimization.All seven other independent variables included in the model came from a survey question asking respondents to rate each item on a scale of 1 (not difficult/not very important) to 5 (very difficult/very important). The Lack Incentives variable addresses the perceived difficulty of changing the project selection process to optimization due to a lack of incentive to justify this switch. We expect that having a lack of incentive to change will be common in government organizations and will decrease the likelihood of adopting optimization. The Lack Experience variable addresses the perceived difficulty of adopting optimization due to lack of previous experience using this method. Similarly, we expect if lack of experience is a large obstacle, than there will be less willingness to adopt optimization. The Initial Cost variable addresses the potential that participants perceive CEC optimization as being more expensive for the organization due to the costs of staff training and related software. We expect to see initial cost of technical resources to be a significant reason why conservation professionals are not adopting optimization. The Staff Knowledge Importance variable addresses how important it is that the staff understand how to use the selection process. The Fairness Importance variable addresses how important fairness to applicants is for selection process. We expect willingness of adoption to increase if fairness is an important criterion to many conservation professionals. The Transparency Importance variable addresses how important transparency is for selection process. The concept of transparency was explained to the survey respondents as the ease of explanation to the public, advisory board, and potential applicants. Since optimization generally does not take into count transparency, we expect willingness of adoption to decrease if transparency is an important criterion. The Forgo Best Project variable addresses the perceived difficulty of adopting optimization since, unlike Benefit Targeting, optimization can frequently forgo the highest scoring conservation project, especially when those projects have relatively high costs. We expect that conservation professionals will be reluctant to forgo high costs, especially those facing imminent development As shown in Model 1 in Table First, willingness to adopt optimization (Understand Optimization) increases with respondents understanding of optimization. Recall that the respondents had a generally good understanding of optimization (the average rating of understanding was 3.14), which is not surprising considering our international sample of professionals had all been exposed to information about the value of CEC techniques relative to BT.Second, respondents who emphasized staff knowledge of the selection process (Staff Knowledge Importance) are more willing to adopt optimization. In the survey, the average rating of staff knowledge importance was 4.07. Thus, respondents seemed to be relatively confident that their organizations could easily learn to incorporate optimization by teaching their staff about the selection process.Third, respondents who emphasized a fair process as important (Fairness Importance) (average rating was 4.23) were more willing to adopt optimization. In this context, fairness to applicants can be defined as the organization showing no bias and have the conservation project given the same consideration to each project and applicant. It makes sense that individuals who want to be fair are more likely to use optimization, which eliminates political considerations and biases and analyzes each potential project using the same mathematical method.Fourth, a surprising positive coefficient was the coefficient related to the relative difficulty of foregoing the highest-ranking projects (Forgo Best Project). This suggests a rather contradictory idea that the less willing they were to forego the best available projects, the more likely respondents were to adopt optimization. The average rating of the difficulty of this obstacle was 3.22 (representing \"somewhat\" difficult). A possible explanation for this result is that respondents may have  already been passing on the highest-ranking projects due to political pressures and thought that switching to CEC might make the process less political and thus better outcomes would result. As noted in The coefficients of the independent variables for the lack of incentives (Lack Incentives) to switch and transparency of the process (Transparency Importance) are significant and negative. Lack of incentives was rated as most challenging of the obstacles listed (average rating of 3.55). This could be because the organizations, the public, and/or their donors are not demanding that these programs be more cost-effective. Alternatively, the program administrators may be reflecting the incentive structures common to most government agencies and non-governmental organizations that do not reward staff for being more cost-effective. When asked to evaluate how cost-effective their organization's current selection processes, the average rating was 3.76 out of 5 (somewhat cost-effective); therefore, many of the respondents viewed their current processes as less cost-effective than they could be. Change tends to be difficult and thus is unlikely to occur without some kind of incentive provided to the staff involved with implementing CEC.The coefficient for transparency in the selection process was negative so willingness to adopt optimization declined with the importance of having a transparent process. Recall that this criterion had a relatively high average rating of 4.14. This result may be related to the perceived complexity of optimization methods, which could be viewed as confusing by the organization's staff and thus difficult to explain to stakeholders. These concerns might be alleviated by using BCT, since it only uses simple ratios of benefits and costs, instead of mathematical programming as a means of achieving CEC.The results of Model 1 show that the lack of experience and the initial cost of implementing optimization were not significant. These results are particularly interesting since conservation programs often have limited budgets. Since all of the respondents in this data set had been exposed to at least one presentation on optimization, their awareness of the method and availability of free or inexpensive software packages may have reduced their concerns about cost.Table Conclusion", "conclusions": "Despite extensive research demonstrating the advantages of applying cost-effective conservation (CEC) techniques, such as optimization, conservation organizations generally have not adopted such methods and continue to use less cost-effective techniques such as benefit targeting (BT). This lack of application is perplexing to economics, which has a core principal of studying the behavior given limited resources. In response to this situation, researchers have identified a number of potential obstacles to adoption including concerns about the fairness and transparency of such methods, political considerations, challenges of measuring environmental benefits, adverse incentives within public bureaucracies, and lack of awareness and understanding of optimization.This research surveyed conservation professionals who had been educated about CEC. The advantage of this sample is that it enabled the research to focus on a group of education conservation professionals that has otherwise not been studied; however, this sample does limit the generalizability of the results to the broader conservation professionals who have not attended trainings. Future research on this broader population and what is inhibiting these professionals from adopting cost effective conservation is warranted.Similar to the results found in , in this study while the vast majority of the survey respondents indicate that cost-effectiveness is a virtue in conservation programs, they do not consider it as important as other program design criteria. The results point not to one particular barrier that predominantly impedes adoption, but to a handful of significant issues that need to be addressed. We find that concerns about fairness and transparency of the process, a lack of confidence in the organization's ability to understand and use optimization, and a lack of incentives to change the method currently used to a more cost-effective approach all have an impact on willingness to adopt optimization. These results also replicate the findings of Messer et al. (2016a) that suggest that expanding training efforts to introduce optimization to conservation professionals and providing user-friendly software are likely to be crucial in promoting CEC methods. The replication of the results of the earlier study, suggests that these findings are robust and that philanthropic foundations and government agencies should consider investing in trainings and software development.Messer et al. (2016a)These results also suggest that public pressure may need to be applied to conservation professionals to make them more responsive to concerns about cost effectiveness. Interestingly, this pressure could come from either side of the political spectrum as environmental advocates want to see more on-the-ground conservation given the limited funds available and good governance advocates want to see taxpayer money used as effectively as possible. Since some of the statutes that created government conservation programs call for the efforts to be conducted in a way that maximizes conservation benefits, the continued failure of some of these groups to use CEC methods may make them vulnerable to legal challenges. Finally, given recent federal efforts to encourage federal agencies to develop evidence-based policy and programs, efforts should be undertaken to use randomized controlled trials to test various selection methods and see how best to overcome the identified obstacles currently inhibiting the adoption of cost effective conservation. Supplementary data to this article can be found online at http://dx. doi.org/10.1016/j.ecolecon.2017.03.027.", "SDG": [15]}, "unmanned_aerial_vehicles_uavs_and_artificial_intel": {"name": "Unmanned Aerial Vehicles (UAVs) and Artificial Intelligence Revolutionizing Wildlife Monitoring and Conservation", "abstract": " Surveying threatened and invasive species to obtain accurate population estimates is an important but challenging task that requires a considerable investment in time and resources. Estimates using existing ground-based monitoring techniques, such as camera traps and surveys performed on foot, are known to be resource intensive, potentially inaccurate and imprecise, and difficult to validate. Recent developments in unmanned aerial vehicles (UAV), artificial intelligence and miniaturized thermal imaging systems represent a new opportunity for wildlife experts to inexpensively survey relatively large areas. The system presented in this paper includes thermal image acquisition as well as a video processing pipeline to perform object detection, classification and tracking of wildlife in forest or open areas. The system is tested on thermal video data from ground based and test flight footage, and is found to be able to detect all the target wildlife located in the surveyed area. The system is flexible in that the user can readily define the types of objects to classify and the object characteristics that should be considered during classification.", "keywords": "Unmanned Aerial Vehicle (UAV),wildlife monitoring,artificial intelligence,thermal imaging,robotics,conservation,automatic classification,koala,deer,wild pigs,dingo,conservation", "introduction": "Effective management of populations of threatened and invasive species relies on accurate population estimates . Existing monitoring protocols employing techniques such as remote photography, camera traps, tagging, GPS collaring, scat detection dogs and DNA sampling typically require considerable investment in time and resources [1][2,. Moreover, many of these techniques are limited in their ability to provide accurate and precise population estimates 3]. Some of the challenges in wildlife monitoring include the large size of species' geographic ranges [4], low population densities [5], inaccessible habitat [3][6,, elusive behaviour 7] and sensitivity to disturbance [8].[9]The increase in availability of inexpensive Unmanned Aerial Vehicles (UAVs) provides an opportunity for wildlife experts to use an aerial sensor platform to monitor wildlife and tackle many of these challenges to accurately estimate species abundance [10][11]. In recent years, the use of UAVs that can perform flight paths autonomously and acquire geo-referenced sensor data Sensors 2016, [12], 97; doi:10.3390/s16010097 www.mdpi.com/journal/sensors has increased sharply for agricultural, environmental and wildlife monitoring applications 16[13][14]. Some issues restricting the wider use of UAVs for wildlife management and research include UAV regulations [15][9,[15][16], operational costs and public perception. One of the most important restrictions, however, is the need to develop or apply advanced automated image detection algorithms designed for this task. Current examples of the use of UAVs for wildlife management include monitoring sea turtles [17], black bears [18], large land mammals (e.g., elephants [8]), marine mammals (e.g., dugongs [19]) and birds (e.g., flocks of snow geese [20]), wildlife radio collar tracking [21], and supporting anti-poaching operations for rhinos [22]. UAVs with digital and thermal imagery sensors can record high resolution videos and capture images much closer to the animals than manned aerial surveys with fewer disturbances [23][9,10,18,. Jones et al. 22] for example conducted a test that involved gathering wildlife video and imagery data from more than 30 missions over two years, and concluded that a UAV could overcome \"safety, cost, statistical integrity and logistics\" issues associated with manned aircraft for wildlife monitoring. Other advances in this field include autonomous tracking of radio-tagged wildlife [24][13,25,.26]Overall, UAVs have proven to be effective at carrying out wildlife monitoring surveys however in many cases, the extensive post-processing effort required negates any convenience or time savings afforded by UAVs in the field compared to conventional survey methods. Therefore, for UAVs to become truly efficient wildlife monitoring tools across the entire workflow of data collection through to analysis, improved capabilities to automate animal detection and counting in the imagery collected by UAVs are required. Research into automatic classification of UAV images for wildlife monitoring is emerging. For example, van Gemert et al.  evaluated the use of UAVs and state-of-the-art automatic object detection techniques for animal detection demonstrating a promising solution for conservation tasks. Although using an elevated structure rather than a UAV, Christiansen et al. [14] used thermal imagery and a k-nearest-neighbour classifier to discriminate between animal and non-animal objects, achieving 93.3% accuracy in an altitude range of 3-10m. In this paper, we further address the issue of automated wildlife detection in UAV imagery by describing a system composed of a UAV equipped with thermal image acquisition as well as a video processing pipeline to perform automated detection, classification and tracking of wildlife in a forest setting to obtain a population estimate within the area surveyed.[4]", "body": "Effective management of populations of threatened and invasive species relies on accurate population estimates The increase in availability of inexpensive Unmanned Aerial Vehicles (UAVs) provides an opportunity for wildlife experts to use an aerial sensor platform to monitor wildlife and tackle many of these challenges to accurately estimate species abundance Overall, UAVs have proven to be effective at carrying out wildlife monitoring surveys however in many cases, the extensive post-processing effort required negates any convenience or time savings afforded by UAVs in the field compared to conventional survey methods. Therefore, for UAVs to become truly efficient wildlife monitoring tools across the entire workflow of data collection through to analysis, improved capabilities to automate animal detection and counting in the imagery collected by UAVs are required. Research into automatic classification of UAV images for wildlife monitoring is emerging. For example, van Gemert et al. Experimental DesignSystem ArchitectureThe system used in this experiment can be divided into airborne and ground segments as presented in Figure Unmanned Aerial Vehicle (UAV)The aerial platform weighs approximately 6 kg, including flight and communications systems. It has a recommended maximum take-off weight of 8 kg, thus allowing 2 kg for sensor payload. The UAV has four main sub-systems: the airframe, the power and propulsion subsystem, the navigation subsystem and the communications subsystem. These subsystems are integrated to provide navigation and power during the UAV flight operations.AirframeThe airframe used in this platform is an S800 EVO Hexacopter Power and PropulsionThe UAV uses a 16,000 mAh Lipo 6 cell battery. This provides a maximum hover time of approximately 20 mins with no sensor payload. The maximum motor power consumption of each motor is 500 W operating at 400 rpm/V. These are running in conjunction with 15 \u02c65.2 inch propellers.NavigationThe main component of the navigation system is a WooKong-M (WK-M) flight controller autopilot, which comes with a GPS unit with inbuilt compass, stabilization controller, gimbal stabilizer, position and altitude hold, auto go home/landing, with enhanced fail-safe. The system has an IMU located in the centre of the UAV to reduce the vibrations and reduce the risk of damage or failure. The autopilot's role in the aircraft is to navigate towards the desired location by altering the altitude, direction and speed. The autopilot has three main operating modes. The first mode is autonomous which allows the UAV to fly a predefined flight path that is designed using the ground control station (GCS). The second mode is stabilized mode which is designed for pre-flight checks of the control surfaces and autopilot. This mode allows the aircraft to maintain a level flight when no pilot input is received. The final mode is full manual which is generally used for take-off and landings, as well as any emergency situations. The GPS connects directly to the autopilot multi-rotor controller as seen in Figure FLIR Camera, Gimbal System and Video TransmissionThe FLIR camera used is a Tau 2-640 Ground Control Station and DatalinkThe ground station software together with the datalink enables flight path planning with predefined waypoints and real-time telemetry reception in the ground station. The datalink communication between the UAV and the ground station is performed with a LK900 Datalink that consists of an airborne transmitter and a receiver connected to the ground station laptop. With a frequency of 900 MHz, the range of the datalink is estimated to be up to 1 km outdoors in optimal conditions. This hardware and software combination ensures stable flight performance while providing real-time flight information to the UAV operations crew. Although it is possible to take-off and land autonomously, it is recommended that these flight segments are completed manually via radio transmitter to guarantee safety standards.Remote Display for VisualizationThe ground station is equipped with a First Person View (FPV) system, to visualize the thermal video in real-time. The FPV goggles (Fatshark Dominator) may be used by UAV crew members, ecologists and wildlife experts to monitor aircraft motion in relation to wildlife as the UAV flies over the tree canopy (Figure Algorithms for Counting and TrackingWe implemented two algorithms on the ground control station computer that automatically count, track and classify wildlife using a range of characteristics. Different approaches are required depending on the information provided by the image or video. For instance, using a clear image of a koala, deer or a kangaroo, colour, size and position thresholds are applied to determine the object of interest. More complex algorithms are required if the image is less clear, for example if the object is an irregular shape, with no apparent colour, of variable size or in multiple positions. The algorithms were written in the Python programming language using the SimpleCV framework for ease of access to open source computer vision libraries such as OpenCV.Algorithm 1: Pixel Intensity Threshold (PIT)This algorithm approaches the problem by using the wildlife's heat signature which creates a good contrast between the background and the target wildlife. This contrast enables an intensity threshold to be applied which in turns eliminates the background and brings the object of interest to the front. Intensity threshold, also known as binarization or segmentation of an image, assigns 0 to all pixels under or equal to the threshold and 255 to all the pixels above the same threshold where 0 represents the black colour and 255 represents the white colour (Figure where x and y are the coordinates of a pixel within the image and f (p) is a function that changes in value with respect to the threshold T.A Graphical User Interface (GUI) was implemented to change this threshold using Tkinter libraries and to assist in finding the most appropriate value for T (Figure As seen in Figure Algorithm 2: Template Matching Binary Mask (TMBM)Using a reference image template of the target object, the TMBM algorithm searches for a match in each frame of the video and labels it. The template reflects characteristics of the object of interest. Multiple templates that reflect changes in size, shape and colour of the object can increase the probability of finding the object in the image. The template matching provides the starting point to detect the species of interest. Very clear and distinct differences from the thermal heat signature and the environment at close range are then used to improve the detection.The algorithm shown in Figure 1.Load templates: In this first step template images are selected (e.g., koala, kangaroo, deer, pigs or birds). The templates are taken from the original footage or a database of images and then saved as small images of the object of interest. The algorithm is able to search for multiple templates in each frame.2.Processes Templates: The contrast of the template is increased in order to enhance the possibility of finding this template in the footage; white is made lighter and black darker by adding or subtracting a constant, C, from each pixel value, p, depending on a threshold, T.p \" image px, yq f ppq \"The values of T and C may be changed in the program code, depending on the template quality and animal size, and can be determined by experimenting with different cases.3.Search for each template in the video frame: For a detected animal to be recorded as a match it must be able to pass a scoring threshold. The searching function returns a score from 1 to 10, based on the proximity of the template to the matched object, where 1 indicates the smallest chance of finding the target and 10 indicates a perfect match. A score of 7 was designed to reflect a high quality match that gave an acceptable chance of avoiding false positives. In addition, to avoid false positives, any match found has to be present for at least 10 consecutive frames before it is considered to be a true match.4.Assign coordinates: once a match has been found, the pixel coordinates (x, y) of the location within the frame are stored for later use.5.Create a mask using coordinates: A new image is created with the same dimension as the source footage with a black (pixel with a value of 0) background. Using the coordinates within the image of the match found in the previous step, a white (pixel with a value of 255) bounding box or circle is drawn with a variable area (Figure 6.Logical operation with the mask: In this step a logical AND is applied using the current frame and the mask. As a result the background is eliminated leaving only the regions of interest at the front.7.Pixel intensity threshold: In this step the function described in Step 2 is used to assign a 0 if the pixel value is less than or equal to the threshold and 255 otherwise.8.Tracking: After obtaining an image comprising only the objects, a function to track is implemented. This function is capable of identifying multiple objects within the same frame and can also distinguish one from another by using their coordinates. The coordinates of the current objects are compared to the previous mask obtained, therefore making it possible to recognize if an object of interest has moved. 9.Counting: This function is able to number, count and display matches in the current frame and throughout the video. This is established by using the object's coordinates within the frame to differentiate multiples objects and to count the number of consecutive frames in which those objects have appeared. If this number increases, it means the object is still in the frame and this is counted as a match. If the object leaves the scene for a specified number of frames after being identified as a match, it is included in the total count. 10.Last frame Loop: In this last step the algorithm then checks if the current frame is the last. If not the algorithm restarts at Step 3; otherwise the process ends.  Validation TestFocal Species: KoalaThe koala (Phascolarctos cinerus) was chosen as the focal species for several reasons: it is an iconic native marsupial species of Australia whose status is listed as vulnerable in parts of the region Study AreaThe site selected for this experiment was located on the Sunshine Coast, 57 km north of Brisbane, Queensland, Australia. This site is a large rehabilitation enclosure where the number of koalas is determined in advance, which enables the accuracy of our counting algorithms to be assessed (ground-truthing).The elevation of the treetop canopy varies between 20 m and 30 m, and is more densely populated in the western side than the eastern side (Figure Data AcquisitionBoth RGB video and thermal video were obtained simultaneously during the flights over the study area. Survey flights were performed at 60 m and 80 m in autonomous mode following a\"lawn mowing\" pattern (e.g., Figures Results and DiscussionThe detection algorithms described in Sections 2.2.1 and 2.2.2 were applied post-flight on the recorded imagery. The test flights successfully located koalas of different shapes and sizes. Figure The steps of TMBM algorithm were applied as follows. For Step 1, we defined a reference template to search for matching templates within the video. In this case the template was based on a sample of images of koalas seen from above at given altitudes. Processing was then applied to the template to improve the contrast between the shape and the background, as per Step 2. Step 3 involved execution of a search for the templates in the video. Due to the absence of local features to describe the koalas at different altitudes (20 m, 30 m, 60 m and 80 m), it was not possible to apply other types of approaches such as key point descriptors, parameterized shapes or colour matching.Following Steps 4 and 5, the coordinates of the possible matches were saved as a vector for a follow-up check and a circle or a square box was drawn with the centre of the coordinates creating an image mask (Figure 1.If the target wildlife (e.g., koala) is found in at least 10 consecutive frames it is counted as a match 2.The target wildlife (e.g., koala) that has been identified cannot make big jumps in location (coordinates) between consecutive frames.3.A horizontal displacement of the target wildlife (e.g., koala) is expected to be within the circle surrounding the target in the mask 4.The size of the target wildlife (e.g., koala) cannot suddenly increase drastically (area in pixels).5.If the target wildlife (e.g., koala) being tracked is not found for 10 consecutive frames it is considered lost or out of the frame.    In order to evaluate the accuracy of detection, the GPS locations of the detected koalas (Figure In order to evaluate the accuracy of detection, the GPS locations of the detected koalas (Figure Conclusions", "conclusions": "UAVs have proven to be effective at carrying out high-resolution and low-disturbance wildlife aerial surveys in a convenient and timely fashion, especially in habitats that are challenging to access or navigate at ground level . However detecting or counting the species of interest from the large volumes of imagery collected during the flights has often proven to be very time consuming [9]. Other important issues are UAV regulations, operational costs, public perception [9][9,, despite increasing effort in this area 17][15,.16]This paper addresses the challenge of automated wildlife detection in UAV imagery by describing a system that combines UAVs with thermal imaging capabilities and artificial intelligence image processing to locate wildlife in their natural habitats. The TMBM algorithm was able to detect the target koalas at the various locations and from various altitudes and produced an orthomosaic thermal image with GPS location of the species of interest (i.e., koala). The accuracy of detection at altitudes of 20 m, 30 m and 60 m was compared with ground truth detections, showing 100% accuracy at these altitudes. In cases where the species is difficult to locate, vertical traveling of the UAV above the canopy is able to give the camera a new angle of vision and hence improve detection.  At altitudes of 20 to 30 m the average detection time was 1.5 s and the false positives were completely filtered by the algorithm. At altitudes above 30 m the algorithm took more time to detect the koala since the UAV had to fly over and then past the koala to detect it. This fractionally increased the number of false positives, although these only appeared in the detection task and was removed in the tracking task.", "SDG": [15]}, "using_footprints_to_identify_and_sex_giant_pandas": {"name": "Using footprints to identify and sex giant pandas", "abstract": " Data on numbers and distribution of free-ranging giant panda are essential to the formulation of effective conservation strategies. There is still no ideal method to identify individuals and sex this species. The traditional bite-size method using bamboo fragments in their feces lacks accuracy. The modern DNA-based estimation is expensive and demands fresh samples. The lack of identifiable individual features on panda pelage and no apparent sexual dimorphism impede reliable estimation from camera trap images. Here, we propose an innovative and non-invasive technique to identify and sex this species using a footprint identification technique (FIT). It is based on a pairwise comparison of trails (unbroken series of footprints) using discriminant analysis, with a Ward's clustering method. We collected footprints from 30 captive animals to train our algorithm and used another 11 animals for model validation. The accuracy for individual identification was > 90% for individuals with more than six footprints and 89% with fewer footprints per trail. The accuracy for sex discrimination was about 84% using a single footprint and 91% using trails. This cost-effective method provides a promising future for monitoring wild panda populations and understanding their dynamics and especially useful for monitoring reintroduced animals after the detachment of GPS collars. The data collection protocol is straightforward and accessible to citizen scientists and conservation professionals alike.", "keywords": "Giant,panda,Non-invasive,monitoring,technique,Footprint,identification,technique,Individual,identification,Endangered,species,conservation", "introduction": "The giant panda (Ailuropoda melanoleuca) is one of the world's most iconic threatened species, with an estimated 1864 pandas surviving in the wild . Although protected areas cover 54% of the suitable habitat (State Forestry Administration, 2015), this species still faces serious threats such as habitat loss and fragmentation (State Forestry Administration, 2015)(Loucks et al., 2001;. The giant panda now lives in six mountain ranges and is isolated into 33 subpopulations. Of these, 22 have fewer than 30 individuals, and 18 have fewer than ten individuals and some of them are on the brink of extinction Li and Pimm, 2016). For their long-term survival and management, understanding giant panda population dynamics is crucial. To date, there are no ideal methods for individual and sex discrimination. Direct observation and counts are impossible because of low population densities, complex topography, and elusiveness of the species (State Forestry Administration, 2015). Unlike tigers or leopards, the similar appearance of individual pandas, with no identifiable features such as stripes or spots, makes them difficult to differentiate from camera trap images. Here, we suggest a practical field method to sex and identify individual pandas.(Zhan et al., 2006)Currently, there are two primary methods to identify individual giant pandas: the bite-size technique and DNA-based approaches. The bite-size technique was originally used to differentiate age groups of pandas  and then was extended to identify individuals (Schaller, 1985). Studies of giant pandas in the wild and captivity have shown individual differences in \"bite size\" and \"chew rates\" of the bamboo stems in their droppings (Garshelis et al., 2008)(Schaller, 1985;. The bite size is usually derived from measuring 100 stem/leaf fragments in droppings Yin et al., 2005). This method has been used for the third (Yin et al., 2005)(1999)(2000)(2001)(2002) and fourth (2003)(2011)(2012)(2013) national survey of giant pandas (2014), but it lacks scientific rigor (State Forestry Administration, 2015)(Wei et al., 2002;. It is less reliable in denser population areas or within mating clusters because many individuals may have similar bite sizes. Moreover, some significant variation in bite sizes within individuals could result in overestimating numbers Zhan et al., 2006). Finally, this method requires field staff to make very precise measurements to apply the threshold of 2 mm (Zhan et al., 2006). Human and measurement tool errors are often unable to meet this level of precision (Yin et al., 2005).(Zhan et al., 2006)The alternative is using microsatellite analysis with fecal DNA . This non-invasive DNA sampling was also used in the fourth national giant panda survey (Zhan et al., 2006). Believed to be more accurate than the traditional bite-size estimate (State Forestry Administration, 2015), its accuracy requires the sample to be very fresh to exclude potential degradation and contamination of DNA. The extensive survey effort required and challenges in finding sufficient samples have prevented applying this method successfully in large-scale studies. The cost of processing samples in the laboratory has impeded the use of DNA individual identification for most conservation practitioners.(Wei et al., 2015)There is no apparent sexual dimorphism in the giant panda. Because the external sexual organs are small and cryptic, it is difficult to identify the sex of giant pandas in the field, or even in captivity, without a DNA test. Adult males are 10-20% larger than adult females . There is much variation, however, and it is particularly difficult to identify the sex of a solitary, free-ranging animal, outside the breeding season. This problem is exacerbated when it comes to identifying the sex of sub-adults (Smith et al., 2010).(Yang et al., 1999)Reintroduction has been a crucial part of panda conservation, especially to revive the small and isolated local populations. GPS collars are only used for these reintroduced pandas and are set to drop off after two years. Reintroduction needs to be evaluated in the long term and requires novel non-invasive methods to monitor these individuals.These challenges have motivated the development of a robust and cost-effective technique to balance the accuracy required of a population estimate with the need for a low-cost field tool. The Footprint Identification Technique (FIT) has become a promising and cost-effective tool in wildlife conservation in recent years . This non-invasive technique was first developed for black rhinos (Pimm et al., 2015). More recently it has been successfully adapted and applied for cheetah (Jewell et al., 2001), white rhinos (Jewell et al., 2016), Amur tiger (Alibhai et al., 2008), mountain lions (Gu et al., 2014)(Alibhai et al., 2017; and other endangered species.Jewell et al., 2014)Footprints have been used as signs of giant panda presence for many years (Fan et al., 2011;Wang et al., 2014;. Their footprints are characteristic of the species, and if the substrate permits, easily found.Li et al., 2015)We report the development of the giant panda FIT for individual and sex identification, a potentially powerful tool to assist with the management and conservation of this endangered species. FIT can play an important role in monitoring the demographics of giant panda populations. China now has around 375 captive giant pandas and an active re-introduction programme is underway . Since FIT requires the initial establishment of a training database with known individuals to extract the necessary algorithms, the captive-bred population proved to be an ideal resource. The development of this technique for the giant panda could help establish an individual database of footprints for the free-ranging populations.(State Forestry Administration, 2015)", "body": "The giant panda (Ailuropoda melanoleuca) is one of the world's most iconic threatened species, with an estimated 1864 pandas surviving in the wild Currently, there are two primary methods to identify individual giant pandas: the bite-size technique and DNA-based approaches. The bite-size technique was originally used to differentiate age groups of pandas The alternative is using microsatellite analysis with fecal DNA There is no apparent sexual dimorphism in the giant panda. Because the external sexual organs are small and cryptic, it is difficult to identify the sex of giant pandas in the field, or even in captivity, without a DNA test. Adult males are 10-20% larger than adult females Reintroduction has been a crucial part of panda conservation, especially to revive the small and isolated local populations. GPS collars are only used for these reintroduced pandas and are set to drop off after two years. Reintroduction needs to be evaluated in the long term and requires novel non-invasive methods to monitor these individuals.These challenges have motivated the development of a robust and cost-effective technique to balance the accuracy required of a population estimate with the need for a low-cost field tool. The Footprint Identification Technique (FIT) has become a promising and cost-effective tool in wildlife conservation in recent years Footprints have been used as signs of giant panda presence for many years We report the development of the giant panda FIT for individual and sex identification, a potentially powerful tool to assist with the management and conservation of this endangered species. FIT can play an important role in monitoring the demographics of giant panda populations. China now has around 375 captive giant pandas and an active re-introduction programme is underway MethodsStudy populationWe collected footprint images from 41 captive giant pandas in the China Conservation and Research Centre for the Giant Panda (CCRCGP) in Sichuan, China. It has three major captive bases; Ya'an, Du Jiang Yan, and Wolong. The Wolong base is located in the heart of Wolong National Nature Reserve, which is one of 67 reserves designated by China's government to protect wild giant pandas Study periodWe collected images from captive animals from March 2014 to April 2016, mostly on a prepared sand substrate since snowfall was infrequent at the lower altitudes where captive pandas are held. Fresh sand was used for each animal to avoid any possible disturbance of behaviors from olfactory cues. At the same time, we collected footprints on snow from captive animals at Wolong when enough snow had accumulated in the higher-altitude enclosures.Foot anatomy and data collectionIn addition to the five digits, the giant panda has an unusual feature on the front feeta 'sixth finger' or 'sesamoid pad'. This structure acts as an opposable digit and is an adapted and enlarged radial sesamoid bone from the wrist. This exaptation enables giant pandas to grab bamboos more efficiently and to facilitate feeding Initial trials to investigate the clarity of the prints left by each of the four feet also indicated that front foot impressions were more distinctive, detailed, and clearly outlined. This was likely due to a combination of greater weight at the front of the animal and less fur on the front feet. We arbitrarily chose the left front foot for the FIT model development. In common with bear species, pandas tend to over-step or side-step. That is, instead of registering the hind foot impression on that made by the front foot, the hind foot usually falls in front of the front foot print or to one side, leaving a clear front foot impression.We define a trail to be an unbroken series of footprints from one animal. We took images of each left front footprint from directly above with a carpenter's scale in the trail according to the protocol described in Extracting a geometric profileIn total, we collected 521 usable footprints along 76 trails from 41 individuals (see Supplementary Table We imported each digital footprint image into a customized FIT addin in JMP software from SAS, resized and rotated for standardization Data analysisIndividual identificationThe FIT customized model for classifying trails employs pairwise Fig. The distance between the centroids is relative, depending on the matrix of within-group variations and the relative-position vector of the centroids. Thus, any changes of a testing set (adding or removing individuals) would alter the positions of the centroid values as well the ellipses. To solve this problem, we applied two modifications proposed by When testing the accuracy of a species FIT algorithm it is necessary to optimize the values of three features within the FIT model construct: the number of variables in the model, the size of the confidence intervals around the ellipse, and the threshold value of the distances between the means. The supplementary materials discuss the process of identifying the optimal combination of these three parameters.To test the robustness of our model, we ran sequential holdback trials with random portioning of the dataset into test and training sets. By varying the number of individuals in the training set, we tested the accuracy of the model in predicting the number of individuals in the test set. We used this process to get an overview of the most effective combination of the three parameters (see supplementary materials).Then we ran a more detailed sequential holdback trial. We started with a test/training set ratio of 3/27 randomly selected individuals with the optimal combination of the three parameters and increased the test size at intervals of 3. For each test/training set ratio, we iterated the process ten times and plotted the predicted means for each set against the actual test set size.The final output in FIT is in the form of a cluster dendrogram giving a predicted number of individuals We analyzed the data in three stages. First, of the 41 individuals, 30 had trail(s) with a minimum of 6 footprints per trail. Footprint images for these individuals were from a sand substrate and we used these to extract the algorithm in FIT for individual identification. Table We divided our independent test dataset into two sets. The first test set had two individuals from the enclosure with natural habitat. The second test set had nine individuals with fewer than six footprints and was used to test a limited sample size.Sex discrimination and age-class distributionWe analyzed the data using discriminant analysis to generate a predictive model to discriminate sex in the giant panda. We performed linear discriminant analysis (LDA) sequentially using an increasing set of measurements selected stepwise to identify the asymptote for the accuracy. The stepwise measurement selection was used to generate a parsimonious set of measurements, based on F-ratios, which provides the most power to discriminate sex ResultsIndividual identificationSystematic holdback trialThe optimal combination of three parameters for the model is 12Fig. Model validationUsing the above algorithm for the giant panda, we tested its efficacy in three stages. First, we ran the FIT analysis for the data set of 30 individuals with 477 footprints and 67 trails. Fig. Second, we iterated the analysis with a total of 32 individuals including two trails collected from semi-enclosures in a snow substrate from two known individuals. Fig. Third, we ran the accuracy assessment with the additional trails from the nine individuals that had fewer footprints than the requisite minimum number ( Sex discriminationSince age of individuals could affect the analysis, we compared the mean ages of females (n = 20, x = 10.18, SE = 1.03) and males (n = 20, x = 7.5, SE = 1.03) (the age for one female was not known). There was no significant difference (F Ratio = 3.35, p > 0.05). Fig. Sex and age-class interactionWe subjected the data to LDA using different sets of variables selected stepwise and it became clear that while the other age classes for the sexes showed a clear difference in a two-way canonical plot, the youngest age classes for males and females appeared to be very similar. This suggests that female and male pandas under the age of three years appear to have very similar foot shape and size. Fig. Discussion", "conclusions": "The literature now recognizes the importance of developing robust, reliable and cost-effective non-invasive techniques for censusing and monitoring populations of endangered species (Jewell et al., 2016;Pimm et al., 2015;. Invasive methods requiring capture, handling and tagging of animals can have negative effects ranging from subtle changes such as alteration of sex ratios Alibhai et al., 2017) to dramatic reductions in fertility (Moorhouse and MacDonald, 2005).(Alibhai et al., 2001)The Footprint Identification Technique (FIT) is non-invasive. We show that metrics derived from footprint images of the giant panda successfully identify individuals, determine sex and, to some extent, classify them according to age. In particular, the sequential holdback trial gave predictions for estimated numbers of individuals in the samples very similar to the actual numbers. This analysis with 30 individuals (Fig. ) indicated that even with a training set of 12 individuals, the predicted mean of the ten iterated values for the 18 individuals in the test set was very close to the actual test set size. However, the high variation around the mean suggests that a training set of around 12 individuals for building the model would lead to inconsistencies in prediction accuracies. For the giant panda, we would suggest a training set of 20 individuals with an even sex ratio as a minimum number for individual identification using FIT.6Free-ranging giant panda sub-populations exist at relatively low population densities where FIT can provide the opportunity to monitor individuals or simply provide regular population estimates. This is necessary because the technique requires that the variation in shape and size of footprints due to extraneous factors such as gait, weight distribution, substrate type etc. be taken into account in the development of the model. Previously we have shown that for the FIT model, six to eight footprints per trail is the optimum number . When we tested the efficacy of the FIT model for the giant panda on trails with fewer than six footprints, the predicted accuracy (eight individuals) was still very close to the actual number (nine). The two trails from two different known individuals which were incorrectly identified as belonging to the same individual had only two and three footprints per trail. This suggests that with relatively smaller sub-populations of about ten individuals or where only a part of a population is being sampled, even three to four footprints per trail would give accurate identification of individuals.(Alibhai et al., 2008)We examined the possibility of classification interaction between age and sex. Although there is very little sexual dimorphism in the giant panda, males and females show different growth patterns in their footprint morphometrics. In particular, the disparity between the sexes shown by age class changes for the measurement 'Area 8' was quite marked (see supplementary materials). Using multiple variables, once again, a two-way canonical plot, generated using discriminant analysis, showed that footprints for the sexes differed. However, our sample sizes for the age class groups were too small to draw any definitive conclusions. Since there was no significant difference between the mean ages of the sexes, linear discriminant analysis using a varying set of measurements selected stepwise, showed that even with as few as five variables, the prediction accuracy was quite high. A five-fold trial (training/test % ratio of 80/20) with five variables gave a mean accuracy for the five trials of 82%. For a repeat trial with 35 variables, the mean accuracy was 86%.The models in this paper are already scripted into an add-in in JMP software. The software can be made available free-of-charge to conservation organizations after participation in a training workshop and by application to JMP. Basic level digital cameras or phones can be used to take photos following the standard procedure. Where routine surveys are taking place, there should be little or no extra cost to collect footprints. Thus, FIT provides a cost-effective way to identify and sex individuals, monitor their population dynamics, and to carry out research and formulate effective conservation strategies.", "SDG": [15]}, "abnormal_behavior_recognition_for_intelligent_video_surveillance_systems_a_review": {"name": "Expert Systems With Applications", "abstract": " With the increasing number of surveillance cameras in both indoor and outdoor locations, there is a grown demand for an intelligent system that detects abnormal events. Although human action recognition is a highly reached topic in computer vision, abnormal behavior detection is lately attracting more research attention. Indeed, several systems are proposed in order to ensure human safety. In this paper, we are interested in the study of the two main steps composing a video surveillance system which are the behavior representation and the behavior modeling. Techniques related to feature extraction and description for behavior representation are reviewed. Classification methods and frameworks for behavior modeling are also provided. Moreover, available datasets and metrics for performance evaluation are presented. Finally, examples of existing video surveillance systems used in real world are described.", "keywords": "Computer,vision,Video,surveillance,system,Behavior,representation,Behavior,modeling", "introduction": "Human action recognition in videos is an active field in computer vision which is attracting more research attention in recent years. This topic becomes very important for many applications such as video surveillance, scene modeling and video content annotation and retrieval. Several previous surveys about human motion detection and analysis ( Aggarwal & Cai, 1999, 1997;, behavior analysis and understanding Ji & Liu, 2010 )( Pantic, Pentland, Nijholt, & Huang, 2007; and activity recognition Teddy, 2008 ) were published ( Table ( Shian-Ru et al., 2013 ) ). Recently, 1 and Dawn, Debapratim, Shaikh, and SoharabHossain (2015) reviewed several methods based on computer vision techniques to recognize simple activities performed by a single person such as running and walking. Hassan et al. (2014) reviewed techniques relative to the different phases of human activity recognition which are object segmentation, feature extraction and representation and activity classification. Particularly, Bux, Plamen, and Zulfiqar (2017) and Sarvesh and Anupam (2013) presented techniques for motion analysis and activity recognition in video surveillance applications. Indeed, the detection of an abnormal behavior in video surveillance is essential to ensure safety in both outdoor and indoor places such as train stations and airports. In fact, abnormal behavior detection is a particular problem of human action recognition. With the increasing number of surveillance cameras, the task of supervising multiple monitors by security agents becomes very difficult due to the human inattention and fatigue. Besides, abnormal events are relatively rare and don't occur frequently. This make the supervision task more complex and challenging. Therefore, there is a growing demand for an intelligent video surveillance system that detects, automatically, an abnormal behavior and rises an alarm.Mishra and Bhagat (2015)In fact, previous surveys of video surveillance systems were provided. For example,  presented an overview of automated surveillance systems for anomaly detection. Valera and Velastin (2005) provided a survey of abnormal human behavior methods in video surveillance applications within different contexts. More recently, Oluwatoyin and Kejun (2012) reviewed different techniques used by intelligent surveillance systems to monitor public spaces. Abnormal event detection methods in crowd surveillance videos were surveyed by de Campos (2014) and Zablocki, K., D., and R. (2014) . In this paper, we extensively review the existing methods that are used in video surveillance applications and we highlight the current advances in the field of abnormal behavior detection.Teng et al. (2015)The objective of an intelligent video surveillance system is to detect efficiently an interesting event from a large amount of videos in order to prevent dangerous situations. Generally, this task requires two video processing levels as shown in Fig.  . The first one consists of two steps. First, low level features, aiming to 1", "body": "Human action recognition in videos is an active field in computer vision which is attracting more research attention in recent years. This topic becomes very important for many applications such as video surveillance, scene modeling and video content annotation and retrieval. Several previous surveys about human motion detection and analysis In fact, previous surveys of video surveillance systems were provided. For example, The objective of an intelligent video surveillance system is to detect efficiently an interesting event from a large amount of videos in order to prevent dangerous situations. Generally, this task requires two video processing levels as shown in Fig. Main focus / Topic ReferenceThree phases of human activity recognition. ( de Campos, 2014 ) Human action recognition. ( Shian-Ru et al., 2013 ) Abnormal human behavior recognition. detect the interest region in the scene, are extracted. Then, primitives based on low level features are generated to describe the interest region. The second level provides semantic information about the human action and determines whether the behavior is normal or not. The rest of this paper is organized as follows. In Section 2 , the most important techniques for behavior representation including feature extraction and description are presented. Different frameworks and classification methods for behavior modeling both in crowded and uncrowded scenes are reviewed in Section 3 . In Section 4 , the most popular datasets used to evaluate a video surveillance system are first presented. Then, a performance evaluation of previous works are provided. Examples of existing video surveillance systems in real world are described in Section 5 .Finally, a conclusion and a discussion of this review are provided in Section 6 .Behavior representationBehavior representation is the low level processing step of behavior analysis. It aims to capture relevant features describing the target object in the video. It consists of two steps. First, the interest region in the scene is detected based on low level features. Then, a description for this region (target object) is provided. In fact, this level is difficult and challenging because it influences significantly the understanding of the interest object behavior. Indeed, the major challenge in behavior representation is to find suitable features that are robust to many transformations such as changes Feature types Description Comments ReferencesOptical flow based features (histograms, variation, acceleration, etc), motion information.Extraction of statistical proprieties from optical flow vector for motion characterization.-Global features. Interest points : STIP, CSTIP, MoSIFT, etc.Salient points detected in both space and time domains. Representation of significant motion variations corresponding to irregular actions.-Local features. -Sensitive to noise.-Not adapted to crowded scene.Spatio-temporal volume, cube, blob, etc.Obtaining the third (temporal) dimension by gathering consecutive frames.-Local features. -Sensitive to parameters (number of consecutive frames, size of the cube, etc) Shape (silhouette, HOG, rectangle, bounding box, etc).Describing the shape of the moving object in sequential frames. Abnormal behavior corresponds to shape change detection.-Widely used for falls detection (shape change). Texture (moments, GLCM, MDT, wavelets, etc).Extraction of local patterns (texture features) for each moving object included in a bounding box, rectangle, etc.Adapted for crowd monitoring (detecting changes of patterns). Tracking the moving object using trajectory (coordinates in each frame), optimization algorithm, etc.-Adapted for tracking a single person. in the background and on the object appearance. In Table To represent the target object, different aspects may be described such as the shape Local features are detected in a predefined region in the frame. This region may be represented by an interest points or a local area. For instance, Global features are used to describe motion in the entire frame. Optical flow features are commonly used to extract global motion information MHOF is combined with Edge Oriented Histogram (EOH) to obtain motion context for anomaly detection in crowded scenes by We note that optical flow is also widely used for violence detection which is a particular problem of abnormal behavior recognition. For instance, Motion information, for object tracking, can be extracted using optimization algorithms, e.g. Ant Colony Particularly, we note that the trajectory of the moving object is widely used to determine whether the behavior is normal or abnormal. Several previous works analyzed the behavior of the target object based on its trajectory Although local features represent accurately the local motion in the video, they may not produce significant information about the action when there is too much motion. On the other hand, global features provide holistic information of the whole scene but they generally give irrelevant information in case of cluttered background and noise. Based on both local and global features, Classifying abnormal behavior recognition methodsAbnormal behavior detection in video surveillance is a challenging task in computer vision and has seen lately important advances. The low level processing stages allow detecting and describing the moving object in the scene. However, those steps do not allow to understand the type of the action performed by the moving object or to determine if its behavior is normal or not. Since there are multiple works proposed that are related to abnormal behavior recognition in video surveillance, we aim in this review to group those papers according to :1. Modeling frameworks and classification methods. 2. Scene density and moving object interaction.Modeling frameworks and classification methodsRecognizing an abnormal behavior depends on the proposed framework and the method used to classify behaviors. Given the type of the samples required for the learning process (normal or abnormal), classification methods can be categorized into supervised, semi-supervised and unsupervised methods. In Table Supervised methods aim to model normal and abnormal behaviors via labeled data. They are generally designed to detect specific abnormal behaviors predefined in the training phase such as fighting detection Semi supervised methods need only normal video data for training and can be divided into rule based and model based approaches. The first category aims to develop a rule using normal patterns. Then, any sample that does not fit this rule is considered as an outlier (anomaly). For example, -Designed to detect specific behaviors, e.g. fighting, loitering and falling. -Easy to run and to understand. -Strongly depends on the training data.-Unknown anomalies cannot be detected.-Learning all abnormal behaviors is not practical in real world.Semi-supervised : rule based method -Rule construction using normal patterns.-Easy to perform and to interpret.High memory and computational complexities.-Any new sample that does not fit the rule is an anomaly.-Very expressive.-Close to human reasoning.Semi-supervised : model based method -Build a model representing the normal behaviors.-Model is easy to generate and to understand.-Sensitive to multiple parameters.-Any new sample that does not respect the model is an anomaly.-A new instance is rapidly classified.-Unknown normal data can be identified as abnormal (false alarm).UnsupervisedLearning using statistical proprieties extracted from unlabeled data.-Fast and easy to perform.-Time consuming for result interpretation.-No prior knowledge required.-Based on the assumption that abnormal behaviors are very rare compared to normal ones.within short execution time (150 frames per second), their result is highly affected by the threshold value. Other works rely on the construction of some rules in order to classify the behavior into normal and abnormal one. A fall detection system based on rules extracted using shape features is proposed by Besides, Tani, Lablack, Ghomari, and Bilasco (2015) used rules that where obtained through an ontology based-approach to detect abnormal events in video surveillance.  Unsupervised methods aim to learn normal and abnormal behaviors using statistical proprieties extracted from unlabelled data. For example, Alvar, Torsello, Miralles, and Armingol (2014) proposed an abnormal behavior method using an unsupervised learning framework based on Dominant Set. Scene density and moving object interactionThe density of the scene corresponds to the number of persons present within it. The choice of techniques to use in order to characterize the behavior is directly influenced by the scene density. Thus, the moving object in the scene can be a small number of persons (may be a single person) or a group of persons. Therefore, we distinguish two types of scenes. The first type, called uncrowded scene, is characterized by the presence of one or a few number of persons at the same time within the camera field. The second type is called crowded scene since it contains many persons. Table Uncrowded sceneIn this type of scene, we are interested on detecting an abnormal behavior performed by one or a few number of persons that are present within the camera field. When there is only one person in the scene, three major abnormal behaviors are generally considered which are falling detection, loitering and being in a wrong place ( Fig. Table 4Frameworks and classification methods for abnormal behavior detection.Modeling frameworks and classification methods ReferencesBag of words (BOW) approach Table 5Scene density and moving object interaction based grouping.Main topic Type of the interaction Type of the scene ReferencesNo Interaction One to one InteractionGroup InteractionCrowded Uncrowded People monitoring / Falling detection \u221a \u221a -Peds 2 : 30 videos (16 for training, 14 for testing), 360 \u00d7 240 pixels.-Abnormal events belong to no pedestrians in the walkways such as bikers and skaters.UMN -Normal scenarios contains people that are walking in different directions. -Resolution = 320 \u00d7 240 pixels.-Abnormal scenarios correspond to crowd dispersion.Violent Flows -Collected from Youtube. -Resolution = 320 \u00d7 240 pixels.-Designed for violence detection in crowded places.Action movies -Videos from action movies.-Abnormal behaviors correspond to one to one fight.Hockey Fight Web dataset -Normal scenarios correspond to normal pedestrians.-Different resolutions.-Abnormal scenarios are clash, escape panic, fight, etc. are context space model, data stream clustering algorithm and inference algorithm. Furthermore, many people are dying every day because they were in the wrong place such as a pedestrian in the road or a person crossing the railway of a train. An effort has been made in this topic to prevent such actions and capture the presence of a human being in a wrong place. For instance, On the other hand, when the scene contains a few number of persons, it is more interesting to detect violent actions such as fighting, kicking and punching between two persons ( Fig. Crowded sceneIn this type of scene, including a group of persons, it is not possible to track and analyze the behavior of each person individually. This is due to the occlusion and the small number of pixels representing every person in the frame. So, it is better to model the interaction between people in order to detect an abnormal crowd behavior. Several previous works proposed abnormal behavior detection methods in crowded scene based on the interaction between people. Fig. Performance evaluationThe performance evaluation step is essential not only to measure the efficiency of a proposed system, but also to compare it to other ones. There are multiple evaluation projects regarding video surveillance systems. For instance, TRECVID In the next subsections, we present first the widely used datasets for abnormal behavior recognition. Then, we define the popular metrics used to evaluate the performance of a video surveillance system.DatasetsTh high number of proposed methods for abnormal behavior recognition shows that it is widely studied area. There is a grown demand for public datasets to use for video surveillance system evaluation. In fact, those datasets can be categorized into crowded and uncrowded scenes. The first type consists of videos containing often violent actions such as punching and kicking. The second type of datasets contains videos describing the interaction between a group of persons acting abnormally such as panic escape. In Table Evaluation metricsSeveral metrics are provided to evaluate a video surveillance system. The two commonly used criteria are the Equal Error Rate (EER) and the Area Under Roc curve (AUC). Those two criteria are derived from the Receiver Operating Characteristic (ROC) Curve which is highly used for performance comparison. EER is the point on the ROC curve where the false positive rate (normal behavior is considered as abnormal) is equal to the false negative rate (abnormal behavior is identified as normal). For a good recognition algorithm, the EER should be as small as possible. Conversely, a system is considered having good performances if the value of the AUC is high. An other used recognition measure is the accuracy (A) which corresponds to the fraction of the correctly classified behaviors. In Table Existing video surveillance systemsTo ensure human safety, there is an immediate need for intelligent video surveillance systems that control private and public places and detect dangerous situations. For each system, there is a specific architecture depending on the environment (indoor/ outdoor). For example, Conclusions and discussions", "conclusions": "In this review, we studied the different levels of a video surveillance system which are behavior representation and behavior modeling. First, we surveyed the most popular methods used for features extraction and description. Then, we provided a comprehensive overview of different classification methods and frameworks for behavior modeling. Moreover, we presented the most challenging datasets and evaluation metrics used for video surveillance systems evaluation. Finally, we exhibited some existing intelligent video systems in real world.Despite the significant progress in the field of abnormal behavior recognition, there are some limitations that make it more diffi-cult and challenging. In fact, the choice of features that are used to characterize the moving object is a difficult task because it influences significantly the description and the analysis of the behavior. For example, describing the behavior when the background of the scene changes frequently or when new objects appear suddenly in the scene is a difficult task. Besides, the appearance of the moving target may change due to multiple factors, such as clothing  and scene place (outdoor/indoor, lift/stairs, etc). Therefore, choosing features that are robust to scene transformations (rotation, occlusion, cluttered backgrounds, etc) and less sensitive to the changes in the object appearance is essential in order to capture relevant and discriminative information about the moving object behavior. Furthermore, most abnormal behavior recognition algorithms suppose that the moving object is on the front of the camera. However, in reality the viewpoint is arbitrary. To overcome this limitation, there are works that use multiple cameras to capture different views for the moving object and, then, combine them. Despite the fact that those algorithms are effective and give good results, they are very sophisticated, time consuming and not suitable for real time applications. On the other hand, an observed behavior may have several interpretations depending on the context in which it is performed, the time and the place of the action. For example, running is a daily activity most people perform, but when a person is running on the road, it is considered as an abnormal behavior that has to be triggered. To overcome the aforementioned limitations, proposed systems used huge amount of training data including all possible scenarios. To deal with the important size of data, it is become a trend to use cloud computing which allows advanced algorithms, e.g. deep learning to work efficiently on larger datasets. In fact, due to their deep architectures, the use of deep learning algorithms has rapidly grown in order to obtain much larger capacity of learning.(short dress, coat, boot, sandal, etc)", "SDG": [16]}, "a_survey_on_visual_surveillance_of_object_motion_and_behaviors": {"name": "A Survey on Visual Surveillance of Object Motion and Behaviors", "abstract": " Visual surveillance in dynamic scenes, especially for humans and vehicles, is currently one of the most active research topics in computer vision. It has a wide spectrum of promising applications, including access control in special areas, human identification at a distance, crowd flux statistics and congestion analysis, detection of anomalous behaviors, and interactive surveillance using multiple cameras, etc. In general, the processing framework of visual surveillance in dynamic scenes includes the following stages: modeling of environments, detection of motion, classification of moving objects, tracking, understanding and description of behaviors, human identification, and fusion of data from multiple cameras. We review recent developments and general strategies of all these stages. Finally, we analyze possible research directions, e.g., occlusion handling, a combination of twoand three-dimensional tracking, a combination of motion analysis and biometrics, anomaly detection and behavior prediction, content-based retrieval of surveillance videos, behavior understanding and natural language description, fusion of information from multiple sensors, and remote surveillance.", "keywords": "Behavior understanding and description,fusion of data from multiple cameras,motion detection,personal identification,tracking,visual surveillance", "introduction": "A S AN ACTIVE research topic in computer vision, visual surveillance in dynamic scenes attempts to detect, recognize and track certain objects from image sequences, and more generally to understand and describe object behaviors. The aim is to develop intelligent visual surveillance to replace the traditional passive video surveillance that is proving ineffective as the number of cameras exceeds the capability of human operators to monitor them. In short, the goal of visual surveillance is not only to put cameras in the place of human eyes, but also to accomplish the entire surveillance task as automatically as possible.Visual surveillance in dynamic scenes has a wide range of potential applications, such as a security guard for communities and important buildings, traffic surveillance in cities and expressways, detection of military targets, etc. We focus in this paper on applications involving the surveillance of people or vehicles, as they are typical of surveillance applications in general, and include the full range of surveillance methods. Surveillance applications involving people or vehicles include the following.1) Access control in special areas. In some security-sensitive locations such as military bases and important governmental units, only people with a special identity are allowed to enter. A biometric feature database including legal visitors is built beforehand using biometric techniques. When somebody is about to enter, the system could automatically obtain the visitor's features, such as height, facial appearance and walking gait from images taken in real time, and then decide whether the visitor can be cleared for entry. 2) Person-specific identification in certain scenes. Personal identification at a distance by a smart surveillance system can help the police to catch suspects. The police may build a biometric feature database of suspects, and place visual surveillance systems at locations where the suspects usually appear, e.g., subway stations, casinos, etc. The systems automatically recognize and judge whether or not the people in view are suspects. If yes, alarms are given immediately. Such systems with face recognition have already been used at public sites, but the reliability is too low for police requirements. 3) Crowd flux statistics and congestion analysis. Using techniques for human detection, visual surveillance systems can automatically compute the flux of people at important public areas such as stores and travel sites, and then provide congestion analysis to assist in the management of the people. In the same way, visual surveillance systems can monitor expressways and junctions of the road network, and further analyze the traffic flow and the status of road congestion, which are of great importance for traffic management. 4) Anomaly detection and alarming. In some circumstances, it is necessary to analyze the behaviors of people and vehicles and determine whether these behaviors are normal or abnormal. For example, visual surveillance systems set in parking lots and supermarkets could analyze abnormal behaviors indicative of theft. Normally, there are two ways of giving an alarm. One way is to automatically make a recorded public announcement whenever any abnormal behavior is detected. The other is to contact the police automatically. 5) Interactive surveillance using multiple cameras.F o r social security, cooperative surveillance using multiple cameras could be used to ensure the security of an entire community, for example by tracking suspects over a wide area by using the cooperation of multiple cameras. For traffic management, interactive surveillance using multiple cameras can help the traffic police discover, track, and catch vehicles involved in traffic offences. It is the broad range of applications that motivates the interests of researchers worldwide. For example, the IEEE has sponsored the IEEE International Workshop on Visual Surveillance on three occasions, in India (1998), the U.S. , and Ireland (1999). In (2000) and [68], a special section on visual surveillance was published in June and August of 2000, respectively. In [1], a special issue on visual analysis of human motion was published in March 2001. In [78], a special issue on third-generation surveillance systems was published in October 2001. In [69], a special issue on understanding visual behavior was published in October 2002. Recent developments in human motion analysis are briefly introduced in our previous paper [130]. It is noticeable that, after the 9/11 event, visual surveillance has received more attention not only from the academic community, but also from industry and governments.[75]Visual surveillance has been investigated worldwide under several large research projects. For example, the Defense Advanced Research Projection Agency (DARPA) supported the Visual Surveillance and Monitoring (VSAM) project  in 1997, whose purpose was to develop automatic video understanding technologies that enable a single human operator to monitor behaviors over complex areas such as battlefields and civilian scenes. Furthermore, to enhance protection from terrorist attacks, the Human Identification at a Distance (HID) program sponsored by DARPA in 2000 aims to develop a full range of multimodal surveillance technologies for successfully detecting, classifying, and identifying humans at great distances. The European Union's Framework V Programme sponsored Advisor, a core project on visual surveillance in metrostations.[3]There have been a number of famous visual surveillance systems. The real-time visual surveillance system W4  employs a combination of shape analysis and tracking, and constructs models of people's appearances in order to detect and track groups of people as well as monitor their behaviors even in the presence of occlusion and in outdoor environments. This system uses the single camera and grayscale sensor. The VIEWS system [4] at the University of Reading is a three-dimensional (3-D) model based vehicle tracking system. The Pfinder system developed by Wren et al. [87] is used to recover a 3-D description of a person in a large room. It tracks a single nonoccluded person in complex scenes, and has been used in many applications. As a single-person tracking system, TI, developed by Olsen et al. [8], detects moving objects in indoor scenes using motion detection, tracks them using first-order prediction, and recognizes behaviors by applying predicates to a graph formed by linking corresponding objects in successive frames. This system cannot handle small motions of background objects. The system at CMU [9] can monitor activities over a large area using multiple cameras that are connected into a network. It can detect and track multiple persons and vehicles within cluttered scenes and monitor their activities over long periods of time. The above comments on [10]- [8] are derived from [10]. Please see [4] for more details.[4]As far as hardware is concerned, companies like Sony and Intel have designed equipment suitable for visual surveillance, e.g., active cameras, smart cameras , omni-directional cameras [76], [23], etc.[77]All of the above activities are evidence of a great and growing interest in visual surveillance in dynamic scenes. The primary purpose of this paper is to give a general review on the overall process of a visual surveillance system. Fig.  shows the general framework of visual surveillance in dynamic scenes. The prerequisites for effective automatic surveillance using a single camera include the following stages: modeling of environments, detection of motion, classification of moving objects, tracking, understanding and description of behaviors, and human identification. In order to extend the surveillance area and overcome occlusion, fusion of data from multiple cameras is needed. This fusion can involve all the above stages. In this paper we review recent developments and analyze future open directions in visual surveillance in dynamic scenes. The main contributions of this paper are as follows.1\u2022 Low-level vision, intermediate-level vision, and high-level vision are discussed in a clearly organized hierarchical manner according to the general framework of visual surveillance. This, we believe, can help readers, especially newcomers to this area, not only to obtain an understanding of the state-of-the-art in visual surveillance, but also to appreciate the major components of a visual surveillance system and their inter-component links. \u2022 Instead of detailed summaries of individual publications, our emphasis is on discussing various methods for different tasks involved in a general visual surveillance system. Each issue is accordingly divided into subprocesses or categories of various methods to examine the state of the art. Only the principles of each group of methods are described. The merits and demerits of a variety of different algorithms, especially for motion detection and tracking, are summarized. \u2022 We give a detailed review of the state of the art in personal identification at a distance and fusion of data from multiple cameras. \u2022 We provide detailed discussions on future research directions in visual surveillance, e.g., occlusion handling, combination of two-dimensional (2-D) tracking and 3-D tracking, combination of motion analysis and biometrics, anomaly detection and behavior prediction, behavior understanding and nature language description, content-based retrieval of surveillance videos, fusion of information from multiple sensors, and remote surveillance.The remainder of this paper is organized as follows. Section II reviews the work related to motion detection including modeling of environments, segmentation of motion, classification of moving objects. Section III discusses tracking of objects, and Section IV details understanding and description of behaviors. Sections V and VI cover, respectively, personal identification at a distance and fusion of data from multiple cameras. Section VII analyzes some possible directions for future research. The last section summarizes the paper.", "body": "A S AN ACTIVE research topic in computer vision, visual surveillance in dynamic scenes attempts to detect, recognize and track certain objects from image sequences, and more generally to understand and describe object behaviors. The aim is to develop intelligent visual surveillance to replace the traditional passive video surveillance that is proving ineffective as the number of cameras exceeds the capability of human operators to monitor them. In short, the goal of visual surveillance is not only to put cameras in the place of human eyes, but also to accomplish the entire surveillance task as automatically as possible.Visual surveillance in dynamic scenes has a wide range of potential applications, such as a security guard for communities and important buildings, traffic surveillance in cities and expressways, detection of military targets, etc. We focus in this paper on applications involving the surveillance of people or vehicles, as they are typical of surveillance applications in general, and include the full range of surveillance methods. Surveillance applications involving people or vehicles include the following.1) Access control in special areas. In some security-sensitive locations such as military bases and important governmental units, only people with a special identity are allowed to enter. A biometric feature database including legal visitors is built beforehand using biometric techniques. When somebody is about to enter, the system could automatically obtain the visitor's features, such as height, facial appearance and walking gait from images taken in real time, and then decide whether the visitor can be cleared for entry. 2) Person-specific identification in certain scenes. Personal identification at a distance by a smart surveillance system can help the police to catch suspects. The police may build a biometric feature database of suspects, and place visual surveillance systems at locations where the suspects usually appear, e.g., subway stations, casinos, etc. The systems automatically recognize and judge whether or not the people in view are suspects. If yes, alarms are given immediately. Such systems with face recognition have already been used at public sites, but the reliability is too low for police requirements. 3) Crowd flux statistics and congestion analysis. Using techniques for human detection, visual surveillance systems can automatically compute the flux of people at important public areas such as stores and travel sites, and then provide congestion analysis to assist in the management of the people. In the same way, visual surveillance systems can monitor expressways and junctions of the road network, and further analyze the traffic flow and the status of road congestion, which are of great importance for traffic management. 4) Anomaly detection and alarming. In some circumstances, it is necessary to analyze the behaviors of people and vehicles and determine whether these behaviors are normal or abnormal. For example, visual surveillance systems set in parking lots and supermarkets could analyze abnormal behaviors indicative of theft. Normally, there are two ways of giving an alarm. One way is to automatically make a recorded public announcement whenever any abnormal behavior is detected. The other is to contact the police automatically. 5) Interactive surveillance using multiple cameras.F o r social security, cooperative surveillance using multiple cameras could be used to ensure the security of an entire community, for example by tracking suspects over a wide area by using the cooperation of multiple cameras. For traffic management, interactive surveillance using multiple cameras can help the traffic police discover, track, and catch vehicles involved in traffic offences. It is the broad range of applications that motivates the interests of researchers worldwide. For example, the IEEE has sponsored the IEEE International Workshop on Visual Surveillance on three occasions, in India (1998), the U.S. Visual surveillance has been investigated worldwide under several large research projects. For example, the Defense Advanced Research Projection Agency (DARPA) supported the Visual Surveillance and Monitoring (VSAM) project There have been a number of famous visual surveillance systems. The real-time visual surveillance system W4 As far as hardware is concerned, companies like Sony and Intel have designed equipment suitable for visual surveillance, e.g., active cameras, smart cameras All of the above activities are evidence of a great and growing interest in visual surveillance in dynamic scenes. The primary purpose of this paper is to give a general review on the overall process of a visual surveillance system. Fig. \u2022 Low-level vision, intermediate-level vision, and high-level vision are discussed in a clearly organized hierarchical manner according to the general framework of visual surveillance. This, we believe, can help readers, especially newcomers to this area, not only to obtain an understanding of the state-of-the-art in visual surveillance, but also to appreciate the major components of a visual surveillance system and their inter-component links. \u2022 Instead of detailed summaries of individual publications, our emphasis is on discussing various methods for different tasks involved in a general visual surveillance system. Each issue is accordingly divided into subprocesses or categories of various methods to examine the state of the art. Only the principles of each group of methods are described. The merits and demerits of a variety of different algorithms, especially for motion detection and tracking, are summarized. \u2022 We give a detailed review of the state of the art in personal identification at a distance and fusion of data from multiple cameras. \u2022 We provide detailed discussions on future research directions in visual surveillance, e.g., occlusion handling, combination of two-dimensional (2-D) tracking and 3-D tracking, combination of motion analysis and biometrics, anomaly detection and behavior prediction, behavior understanding and nature language description, content-based retrieval of surveillance videos, fusion of information from multiple sensors, and remote surveillance.The remainder of this paper is organized as follows. Section II reviews the work related to motion detection including modeling of environments, segmentation of motion, classification of moving objects. Section III discusses tracking of objects, and Section IV details understanding and description of behaviors. Sections V and VI cover, respectively, personal identification at a distance and fusion of data from multiple cameras. Section VII analyzes some possible directions for future research. The last section summarizes the paper.II. MOTION DETECTIONNearly every visual surveillance system starts with motion detection. Motion detection aims at segmenting regions corresponding to moving objects from the rest of an image. Subsequent processes such as tracking and behavior recognition are greatly dependent on it. The process of motion detection usually involves environment modeling, motion segmentation, and object classification, which intersect each other during processing.A. Environment ModelingThe active construction and updating of environmental models are indispensable to visual surveillance. Environmental models can be classified into 2-D models in the image plane and 3-D models in real world coordinates. Due to their simplicity, 2-D models have more applications.\u2022 For fixed cameras, the key problem is to automatically recover and update background images from a dynamic sequence. Unfavorable factors, such as illumination variance, shadows and shaking branches, bring many difficulties to the acquirement and updating of background images. There are many algorithms for resolving these problems including temporal average of an image sequence Ridder et al. B. Motion SegmentationMotion segmentation in image sequences aims at detecting regions corresponding to moving objects such as vehicles and humans. Detecting moving regions provides a focus of attention for later processes such as tracking and behavior analysis because only these regions need be considered in the later processes. At present, most segmentation methods use either temporal or spatial information in the image sequence. Several conventional approaches for motion segmentation are outlined in the following.1) Background subtraction. Background subtraction is a popular method for motion segmentation, especially under those situations with a relatively static background. It detects moving regions in an image by taking the difference between the current image and the reference background image in a pixel-by-pixel fashion. It is simple, but extremely sensitive to changes in dynamic scenes derived from lighting and extraneous events etc. Therefore, it is highly dependent on a good background model to reduce the influence of these changes Of course, besides the basic methods described above, there are some other approaches for motion segmentation. Using the extended expectation maximization (EM) algorithm, Friedman et al. C. Object ClassificationDifferent moving regions may correspond to different moving targets in natural scenes. For instance, the image sequences captured by surveillance cameras mounted in road traffic scenes probably include humans, vehicles and other moving objects such as flying birds and moving clouds, etc. To further track objects and analyze their behaviors, it is essential to correctly classify moving objects. Object classification can be considered as a standard pattern recognition issue. At present, there are two main categories of approaches for classifying moving objects. By tracking an interesting moving object, its self-similarity is computed as it evolves over time. As we know, for periodic motion, its self-similarity measure is also periodic. Therefore time-frequency analysis is applied to detect and characterize the periodic motion, and tracking and classification of moving objects are implemented using periodicity. In Lipton's work Based on this useful cue, human motion is distinguished from motion of other objects, such as vehicles.The two common approaches mentioned above, namely shape-based and motion-based classification, can also be effectively combined for classification of moving objects. Furthermore, Stauffer III. OBJECT TRACKINGAfter motion detection, surveillance systems generally track moving objects from one frame to another in an image sequence. The tracking algorithms usually have considerable intersection with motion detection during processing. Tracking over time typically involves matching objects in consecutive frames using features such as points, lines or blobs. Useful mathematical tools for tracking include the Kalman filter, the Condensation algorithm, the dynamic Bayesian network, the geodesic method, etc. Tracking methods are divided into four major categories: region-based tracking, active-contour-based tracking, featurebased tracking, and model-based tracking. It should be pointed out that this classification is not absolute in that algorithms from different categories can be integrated together A. Region-Based TrackingRegion-based tracking algorithms track objects according to variations of the image regions corresponding to the moving objects. For these algorithms, the background image is maintained dynamically Although they work well in scenes containing only a few objects (such as highways), region-based tracking algorithms cannot reliably handle occlusion between objects. Furthermore, as these algorithms only obtain the tracking results at the region level and are essentially procedures for motion detection, the outline or 3-D pose of objects cannot be acquired. (The 3-D pose of an object consists of the position and orientation of the object). Accordingly, these algorithms cannot satisfy the requirements for surveillance against a cluttered background or with multiple moving objects.B. Active Contour-Based TrackingActive contour-based tracking algorithms track objects by representing their outlines as bounding contours and updating these contours dynamically in successive frames In contrast to region-based tracking algorithms, active contour-based algorithms describe objects more simply and more effectively and reduce computational complexity. Even under disturbance or partial occlusion, these algorithms may track objects continuously. However, the tracking precision is limited at the contour level. The recovery of the 3-D pose of an object from its contour on the image plane is a demanding problem. A further difficulty is that the active contour-based algorithms are highly sensitive to the initialization of tracking, making it difficult to start tracking automatically.C. Feature-Based TrackingFeature-based tracking algorithms perform recognition and tracking of objects by extracting elements, clustering them into higher level features and then matching the features between images. Feature-based tracking algorithms can further be classified into three subcategories according to the nature of selected features: global feature-based algorithms, local feature-based algorithms, and dependence-graph-based algorithms.\u2022 The features used in global feature-based algorithms include centroids, perimeters, areas, some orders of quadratures and colors \u2022 The features used in dependence-graph-based algorithms include a variety of distances and geometric relations between features In general, as they operate on 2-D image planes, feature-based tracking algorithms can adapt successfully and rapidly to allow real-time processing and tracking of multiple objects which are required in heavy thruway scenes, etc. However, dependencegraph-based algorithms cannot be used in real-time tracking because they need time-consuming searching and matching of graphs. Feature-based tracking algorithms can handle partial occlusion by using information on object motion, local features and dependence graphs. However, there are several serious deficiencies in feature-based tracking algorithms.\u2022 The recognition rate of objects based on 2-D image features is low, because of the nonlinear distortion during perspective projection and the image variations with the viewpoint's movement. \u2022 These algorithms are generally unable to recover 3-D pose of objects. \u2022 The stability of dealing effectively with occlusion, overlapping and interference of unrelated structures is generally poor.D. Model-Based TrackingModel-based tracking algorithms track objects by matching projected object models, produced with prior knowledge, to image data. The models are usually constructed off-line with manual measurement, CAD tools or computer vision techniques. As model-based rigid object tracking and model-based nonrigid object tracking are quite different, we review separately model-based human body tracking (nonrigid object tracking) and model-based vehicle tracking (rigid object tracking).1) Model-Based Human Body Tracking:The general approach for model-based human body tracing is known as analysis-by-synthesis, and it is used in a predict-match-update style. Firstly, the pose of the model for the next frame is predicted according to prior knowledge and tracking history. Then, the predicted model is synthesized and projected into the image plane for comparison with the image data. A specific pose evaluation function is needed to measure the similarity between the projected model and the image data. According to different search strategies, this is done either recursively or using sampling techniques until the correct pose is finally found and is used to update the model. Pose estimation in the first frame needs to be handled specially. Generally, model-based human body tracking involves three main issues:\u2022 construction of human body models; \u2022 representation of prior knowledge of motion models and motion constraints; \u2022 prediction and search strategies.Previous work on these three issues is briefly and respectively reviewed as follows.a) Human body models: Construction of human body models is the base of model-based human body tracking \u2022 Stick figure. The essence of human motion is typically contained in the movements of the torso, the head and the four limbs, so the stick-figure method is to represent the parts of a human body as sticks and link the sticks with joints. Karaulova et al. c) Search strategies: Pose estimation in a high-dimensional body configuration space is intrinsically difficult, so, search strategies are often carefully designed to reduce the solution space. Generally, there are four main classes of search strategies: dynamics, Taylor models, Kalman filtering, and stochastic sampling. Dynamical strategies use physical forces applied to each rigid part of the 3-D model of the tracked object. These forces, as heuristic information, guide the minimization of the difference between the pose of the 3-D model and the pose of the real object 2) Model-Based Vehicle Tracking: As to model-based vehicle tracking, 3-D wire-frame vehicle models are mainly used The research group at the University of Reading adopts 3-D wire-frame vehicle models. Tan et al. The NLPR group has extended the work of the research group at the University of Reading. Yang et al. The Karlsruhe group \u2022 As far as 3-D model-based tracking is concerned, after setting up the geometric correspondence between 2-D image coordinates and 3-D world coordinates by camera calibration, the algorithms naturally acquire the 3-D pose of objects. \u2022 The 3-D model-based tracking algorithms can be applied even when objects greatly change their orientations during the motion. Ineluctably, model-based tracking algorithms have some disadvantages such as the necessity of constructing the models, high computational cost, etc.IV. UNDERSTANDING AND DESCRIPTION OF BEHAVIORSAfter successfully tracking the moving objects from one frame to another in an image sequence, the problem of understanding-object behaviors from image sequences follows naturally. Behavior understanding involves the analysis and recognition of motion patterns, and the production of high-level description of actions and interactions.A. Behavior UnderstandingUnderstanding of behaviors may simply be thought as the classification of time varying feature data, i.e., matching an unknown test sequence with a group of labeled reference sequences representing typical behaviors. It is then obvious that a fundamental problem of behavior understanding is to learn the reference behavior sequences from training samples, and to devise both training and matching methods for coping effectively with small variations of the feature data within each class of motion patterns. Some efforts have been made in this direction a) Dynamic time warping (DTW): DTW is a template-based dynamic programming matching technique widely used in the algorithms for speech recognition. It has the advantage of conceptual simplicity and robust performance, and has been used recently in the matching of human movement patterns b) Finite-state machine (FSM):The most important feature of a FSM is its state-transition function. The states are used to decide which reference sequence matches with the test sequence. Wilson et al. d) Time-delay neural network (TDNN): TDNN is also an interesting approach to analyzing time-varying data. In TDNN, delay units are added to a general static network, and some of the preceding values in a time-varying sequence are used to predict the next value. As larger data sets become available, more emphasis is being placed on neural networks for representing temporal information. TDNN has been successfully applied to hand gesture recognition e) Syntactic techniques f) Non-deterministic finite automaton (NFA): Wada et al. g) Self-organizing neural network: The methods discussed in (a)-(f) all involve supervised learning. They are applicable for known scenes where the types of object motions are already known. The self-organizing neural networks are suited to behavior understanding when the object motions are unrestricted. Johnson et al. B. Natural Language Description of BehaviorsIn many applications it is important to describe object behaviors in natural language suitable for nonspecialist operator of visual surveillance a) Statistical models: A representative statistical model is the Bayesian network model b) Formalized reasoning: Formalized reasoning Although there is some progress in description of behaviors, some key problems remain open, for example how to properly represent semantic concepts, how to map motion characteristics to semantic concepts and how to choose efficient representations to interpret the scene meanings.V. P ERSONAL IDENTIFICATION FOR VISUAL SURVEILLANCEThe problem of \"who is now entering the area under surveillance\" is of increasing importance for visual surveillance. Such personal identification can be treated as a special behavior-understanding problem. Human face and gait are now regarded as the main biometric features that can be used for personal identification in visual surveillance systems A. Model-Based MethodsIn model-based methods, parameters, such as joint trajectories, limb lengths, and angular speeds, are measured Cunado et al. Ya m et al. Another recent paper Tracking and localizing the human body accurately in 3-D space is still difficult despite the recent work on structure-based methods. In theory, joint angles are sufficient for recognition of people by their gait. However, accurately recovering joint angles from a walking video is still an unsolved or not well-solved problem. In addition, the computational cost of the model-based approaches is quite high.B. Statistical MethodsStatistical recognition techniques usually characterize the statistical description of motion image sets, and have been well developed in automatic gait recognition Murase et al. Shutler et al. Statistical methods are relatively robust to noise and change of time interval in input image sequences. Compared with model-based approaches, the computational cost of statistical methods is low.C. Physical-Parameter-Based MethodsPhysical-parameter-based methods make use of geometric structural properties of a human body to characterize a person's gait pattern. The parameters used include height, weight, stride cadence and length, etc. For example, a gait recognition technique using specific behavior parameters is recently proposed by Bobick et al. Physical-parameter-based methods are intuitively understandable, and independent of viewing angles because these parameters usually are recovered in the 3-D space. However, they depend greatly on the vision techniques used to recover the required parameters, e.g., body-part labeling, depth compensation, camera calibration, shadow removal, etc. In addition, the parameters used for recognition may be not effective enough across a large population.D. Spatio-Temporal Motion-Based MethodsFor motion recognition based on spatio-temporal analysis, the action or motion is characterized via the entire 3-D spatio-temporal data volume spanned by the moving person in the image sequence. These methods generally consider motion as a whole to characterize its spatio-temporal distributions Perhaps the earliest approach to recognizing people is to obtain gait features from the spatio-temporal pattern of a walking figure Using the image self-similarity in XYT, BenAbdelkader et al. Kale et al. Spatio-temporal motion-based methods are able to better capture both spatial and temporal information of gait motion. Their advantage is low computational complexity and a simple implementation. However, they are susceptible to noise and to variations of the timings of movements.E. Fusion of Gait With Other BiometricsThe fusion of gait information with other biometrics can further increase recognition robustness and reliability. Shakhnarovich et al. Although many researchers have been working on gait recognition, current research of gait recognition is still in its infancy.First, most experiments are carried out under constrained circumstances, e.g., no occlusion happens while objects are usually moving, the background is simple, etc. Second, existing algorithms are evaluated on small databases. Future work on gait recognition will focus on handling these two problems.VI. FUSION OF DATA FROM MULTIPLE CAMERASMotion detection, tracking, behavior understanding, and personal identification at a distance discussed above can be realized by single camera-based visual surveillance systems. Multiple camera-based visual surveillance systems can be extremely helpful because the surveillance area is expanded and multiple view information can overcome occlusion. Tracking with a single camera easily generates ambiguity due to occlusion or depth. This ambiguity may be eliminated from another view. However, visual surveillance using multicameras also brings problems such as camera installation (how to cover the entire scene with the minimum number of cameras), camera calibration, object matching, automated camera switching, and data fusion.A. InstallationThe deployment of the cameras has a great influence on the real-time performance and the cost of the system. Cameras cannot be employed arbitrarily due to factors such as the topography of the area. Redundant cameras increase not only processing time and algorithmic complexity, but also the installation cost. In contrast, a lack of cameras may cause some blind angles, which reduce the reliability of a surveillance system. So the question of how to cover the entire scene with the minimum number of cameras is important. Pavlidis et al. B. CalibrationTraditional calibration methods use the 3-D coordinates and the image coordinates of some known points to compute the parameters of a camera. Calibration is more complex when multiple cameras are concerned. Current multiple camera self-calibration methods use temporal information. Stein and Lee et al. C. Object MatchingObject matching among multiple cameras involves finding the correspondences between the objects in different image sequences taken by different cameras. There are two popular methods: one is the geometry-based method that establishes correspondence according to geometric features transformed to the same space; and the other is the recognition-based method. As an example of the geometry-based method, Cai et al. D. SwitchingWhen an object moves out of the view field of an active camera, or the camera cannot give a good view of the moving object, then the system should switch to another camera that may give a better view of the object. The key problems are how to find the better camera and how to minimize the number of switches during tracking. Cai et al. E. Data FusionData fusion is important for occlusion handling and continuous tracking. Dockstader et al. F. Occlusion HandlingIn practice, self-occlusion, and occlusions between different moving objects or between moving objects and the background are inevitable. Multiple camera systems offer efficient and promising methods for coping with occlusion. Utsumi et al. VII. FUTURE DEVELOPMENTSIn Sections II-VI, we have reviewed the state-of-the-art of visual surveillance for humans and vehicles sorted by a general framework of visual surveillance systems. Although a large amount of work has been done in visual surveillance for humans and vehicles, many issues are still open and deserve further research, especially in the following areas.A. Occlusion HandlingOcclusion handing is a major problem in visual surveillance. Typically, during occlusion, only portions of each object are visible and often at very low resolution. This problem is generally intractable, and motion segmentation based on background subtraction may become unreliable. To reduce ambiguities due to occlusion, better models need be developed to cope with the correspondence between features and body parts, and thus eliminate correspondence errors that occur during tracking multiple objects. When objects are occluded by fixed objects such as buildings and street lamps, some resolution is possible through motion region analysis and partial matching. However, when multiple moving objects occlude each other, especially when their speeds, directions and shapes are very close, their motion regions coalesce, which makes the location and tracking of objects particularly difficult. The self-occlusion of a human body is also a significant and difficult problem. Interesting progress is being made using statistical methods to predict object pose, position, and so on, from available image information. Perhaps the most promising practical method for addressing occlusion is through the use of multiple cameras.B. Fusion of 2-D and 3-D TrackingTwo-dimensional tracking is simple and rapid, and it has shown some early successes in visual surveillance, especially for low-resolution application areas where the precise posture reconstruction is not needed, e.g., pedestrian and vehicle tracking in a traffic surveillance setting. However, the major drawback of the 2-D approach is its restriction of the camera angle.Compared with 2-D approaches, 3-D approaches are more effective for accurate estimation of position in space, more effective handling of occlusion, and high-level judgments about complex object movements such as wandering around, shaking hands, dancing, and vehicle overtaking. However, applying 3-D tracking requires more parameters and more computation during the matching process. Also, vision-based 3-D tracking brings a number of challenges such as the acquisition of object models, occlusion handling, parameterized object modeling, etc.In fact, the combination of 2-D tracking and 3-D tracking is a significant research direction that few researches have attempted. This combination is expected to fuse the merits of the 2-D tracking algorithms and those of the 3-D tracking algorithms. The main difficulties of this combination are:\u2022 deciding when 2-D tracking should be used and when 3-D tracking should be used;\u2022 how to initialize pose parameters for 3-D tracking according to the results from 2-D tracking, when the tracking algorithm is switched from 2-D to 3-D.C. Three-Dimensional Modeling of Humans and VehiclesWe think that it is feasible to build 3-D models for humans and vehicles. As far as vehicles are concerned, they can be treated as rigid objects, drawn from only a few classes and with invariable 3-D shapes during normal usage. It is possible to establish 3-D models of vehicles using CAD tools, etc. A generic and parametric model can be established for each class D. Combination of Visual Surveillance and Personal IdentificationAs mentioned in Section V, vision-based human identification at a distance has become increasingly important. Gait is a most attractive modality used for this purpose. Generally, future work on gait recognition will focus on the following directions.1) Establishing a large common database and a standard test protocol. The database with an independent subdatabase for the test just like the FERET protocol E. Behavior UnderstandingOne of the objectives of visual surveillance is to analyze and interpret individual behaviors and interactions between objects to decide for example whether people are carrying, depositing or exchanging objects, whether people are getting on or getting off a vehicle, or whether a vehicle is overtaking another vehicle, etc. Recently, related research has still focused on some basic problems like recognition of standard gestures and simple behaviors. Some progress has been made in building the statistical models of human behaviors using machine learning. Behavior recognition is complex, as the same behavior may have several different meanings depending upon the scene and task context in which it is performed. This ambiguity is exacerbated when several objects are present in a scene F. Anomaly Detection and Behavior PredictionAnomaly detection and behavior prediction are significant in practice. In applications of visual surveillance, not only should visual surveillance systems detect anomalies such as traffic accidents and car theft etc, according to requirements of functions, but also predict what will happen according to the current situation and raise an alarm for a predicted abnormal behavior. Implementations are usually based on one or other of the following two methods.1) Probability reasoning and prior rules combinedmethods. A behavior with small probability, or against the prior rules would be regarded as an anomaly. 2) Behavior-pattern-based methods. Based on learned patterns of behaviors, we can detect anomalies and predict object behaviors. When a detected behavior does not match the learned patterns, it is classed as an anomaly. We can predict an object behavior by matching the observed subbehavior of the object with the learned patterns. Generally, patterns of behaviors in a scene can be constructed by supervised or unsupervised learning of each object's velocities and trajectories, etc. Supervised learning is used for known scenes where objects move in pre-defined ways. For unknown scenes, patterns of behaviors should be constructed by self-organizing and self-learning of image sequences. Fernyhough et al. G. Content-Based Retrieval of Surveillance VideosThe task in content-based retrieval of surveillance videos is to retrieve video clips from surveillance video databases according to video contents, based on automatic image and video understanding. At present, research on video retrieval focuses on the low-level perceptively meaningful representations of pictorial data (such as color, texture, shape, etc) and simple motion information. These retrieval techniques cannot accurately and effectively search the videos for sequences related to specified behaviors. Semantic-based video retrieval (SBVR) aims to bridge the gap between low-level features and high-level semantic meanings. Based on automatic interpretation of contents in surveillance videos, SBVR may classify and further access the surveillance video clips that are related to specific behaviors, and supply a more high-level, more intuitive and more humanistic retrieval mode. Semantic-based retrieval of surveillance videos brings the following difficult problems: automatic extraction of semantic behavior features, combination between low-level visual features and behavior features, hierarchical organization of image and video features, semantic video indexing, inquire interface, etc.H. Natural Language Description of Object BehaviorsDescribing object behaviors by natural language in accord with human habits is a challenging research subject. The key task is to obtain the mapping relationships between object behaviors in image sequences and the natural language. These mapping relationships are related to the following two problems.1) Relationships between behaviors and semantic concepts. Each semantic concept of motion describes a class of behaviors, but each behavior may be related to multiple semantic concepts. After the mapping has been clearly defined, we could construct the relationship between the results of low-level image processing and object behaviors.The key problems include the modeling of semantic concepts of motions, and the automatic learning of semantic concepts of behaviors. 2) Semantic recognition and natural language description of object behaviors. People usually describe developments and transformations of objects with concepts at different levels. The higher level concepts require greater background knowledge. It is a key problem to analyze the behaviors of moving objects using the tracking results from low-level systems, and further recognize the more abstract semantic concepts at higher layers. We can use the corresponding relationships between semantic concepts and object behaviors, semantic networks with different layers and reasoning theory to explore this problem. Natural language is the most convenient and natural way for humans to communicate each other.Organizing recognized concepts and further representing object behaviors in brief and clear natural language is one of the ultimate goals of visual surveillance. In addition, the synchronous description, i.e., giving the description before a behavior finishes (during the behavior is progressing), is also a challenge. We should design an incremental description method which is able to predict object behaviors.I. Fusion of Data From Multiple SensorsIt is obvious that future visual surveillance systems will greatly benefit from the use of multiple cameras Besides video, sensors for surveillance include audio, infrared, ultrasonic, and radar, etc. Each of these sensors has its own characteristics. Surveillance using multiple different sensors seems to be a very interesting subject. The main problem is how to make use of their respective merits and fuse information from such kinds of sensors.J. Remote SurveillanceRemote surveillance becomes more and more important for many promising applications, e.g., military combat, prevention of forest fires, etc. Video data are acquired from distributed sensors and transmitted to a remote control center. The transmission process must satisfy the following requirements.\u2022 The upload bandwidth (from sensors to the control center) should be much wider than the download bandwidth (from the control center to sensors). \u2022 The security of transmission must be guaranteed. Because some surveillance data involve privacy, commercial secrets and even national security, and nevertheless are transmitted through public networks, information security becomes a key problem. This needs the developments of the techniques such as digital watermarking and encryption VIII. CONCLUSIONS", "conclusions": "Visual surveillance in dynamic scenes is an active and important research area, strongly driven by many potential and promising applications, such as access control in special areas, person-specific identification in certain scenes, crowd flux statistics and congestion analysis, and anomaly detection and alarming, etc.We have presented an overview of recent developments in visual surveillance within a general processing framework for visual surveillance systems. The state-of-the-art of existing methods in each key issue is described with the focus on the following tasks: detection, tracking, understanding and description of behaviors, personal identification for visual surveillance, and interactive surveillance using multiple cameras. As for the detection of moving objects, it involves environmental modeling, motion segmentation and object classification. Three techniques for motion segmentation are addressed: background subtraction, temporal differencing, and optical flow. We have discussed four intensively studied approaches to tracking: region based, active-contour based, feature based, and model based. We have reviewed several approaches to behavior understanding, including DTW, FSM, HMMs, and TDNN. In addition, we examine the state-of-the-art of behavior description. As to personal identification at a distance, we have divided gait recognition methods into four classes: mode based, statistics, physical-parameter based, and spatio-temporal motion based. As to fusion of data from multiple cameras, we have reviewed installation, object matching, switching, and data fusion.At the end of this survey, we have given some detailed discussions on future directions, such as occlusion handling, fusion of 2-D tracking and 3-D tracking, 3-D modeling of humans and vehicles, combination of visual surveillance and personal identification, anomaly detection and behavior prediction, content-based retrieval of surveillance videos, natural language description of object behaviors, fusion of data from multiple sensors, and remote surveillance. ", "SDG": [16]}, "beyond_simulations_serious_games_for_training_interpersonal_skills_in_law_enforcement": {"name": "Beyond Simulations: Serious Games for Training Interpersonal Skills in Law Enforcement", "abstract": " Serious games can be used to improve people's social awareness by letting them experience difficult social situations and learn from these experiences. However, we assert that, when moving beyond the strict realism that social simulations offer, techniques from role play may be used that offer more possibilities for feedback and reflection. We discuss the design of two such serious games for interpersonal skills training in the domain of law enforcement. These games feature intelligent virtual agents with which trainees have to interact across different scenarios to improve their social awareness. By interacting with the virtual agents, trainees experience how their behaviour influences the course of the intervention and its outcomes. We discuss how we intend to improve the learning experience in these serious games by including meta-techniques from role play. We close by describing the current and future implementations of our serious games.", "keywords": "Social simulation,serious games,role playing games,meta-techniques", "introduction": "In both interviews and street interventions, police officers strive to get witnesses, suspects and civilians to cooperate. Regrettably, people are not always open to this. Therefore, police officers are taught how to get them to assume a more receptive stance. Our long-term goal is the development of serious game prototypes in collaboration with the Dutch police so that these games assist in the training curriculum of police trainees by letting them practice with such interactions. The first of these is POINTER (POlice INTERview game), in which trainees train their interviewing skills with crime suspects; the second is LOITER (LOItering Teenagers, an Emergent Role-play), which lets trainees enact street interventions with loitering juveniles.In this paper, we describe the status quo of our research efforts toward these serious games. Of prime importance to the attainment of social awareness is insight into the thought processes that drive people. Therefore, we are building a cognitive model based on a corpus of police interviews to determine the factors underlying people's behaviour (described in Section III). We use this model to inform the behaviour of the virtual agents that enact the roles of suspects and juveniles in our games. In Section IV, we discuss the relations between social simulations and serious games. We explain how we sacrifice the realism usually found in social simulations in the design of POINTER and LOITER to provide more explicit feedback and moments of reflection. We elaborate on the work involved in implementing these games in more detail in Section V. We wrap up by discussing future research directions in Section VI.", "body": "In both interviews and street interventions, police officers strive to get witnesses, suspects and civilians to cooperate. Regrettably, people are not always open to this. Therefore, police officers are taught how to get them to assume a more receptive stance. Our long-term goal is the development of serious game prototypes in collaboration with the Dutch police so that these games assist in the training curriculum of police trainees by letting them practice with such interactions. The first of these is POINTER (POlice INTERview game), in which trainees train their interviewing skills with crime suspects; the second is LOITER (LOItering Teenagers, an Emergent Role-play), which lets trainees enact street interventions with loitering juveniles.In this paper, we describe the status quo of our research efforts toward these serious games. Of prime importance to the attainment of social awareness is insight into the thought processes that drive people. Therefore, we are building a cognitive model based on a corpus of police interviews to determine the factors underlying people's behaviour (described in Section III). We use this model to inform the behaviour of the virtual agents that enact the roles of suspects and juveniles in our games. In Section IV, we discuss the relations between social simulations and serious games. We explain how we sacrifice the realism usually found in social simulations in the design of POINTER and LOITER to provide more explicit feedback and moments of reflection. We elaborate on the work involved in implementing these games in more detail in Section V. We wrap up by discussing future research directions in Section VI.II. RELATED WORKThere are several research projects in which social interaction between human and virtual agents has been researched for educational purposes and serious games. The negotiation training systems of III. TOWARDS A COGNITIVE MODEL FOR SOCIAL INTERACTIONWe strive to use theories and concepts from social psychology to inform the behaviour of our agents in order to create a cognitive model that is both believable and explainable. Using a data-driven approach, we have investigated which theories and concepts are relevant to describe the interaction in a police interview Currently, we are building and evaluating a computational model that relates the mentioned theories to each other. We carried out an evaluation experiment in which we let participants interact with our model in an abstract way. We explained that they would interact with one of three virtual suspect personas of which we gave descriptions, and that their goal was to discover which of the three suspects communicated with them. They were able to interact with a virtual suspect in a turn-based fashion. First, participants indicated what kind of utterance they wished to perform by setting parameters related to the theories such as stance, rapport, and type of question. Then, we let the model interpret this combination of parameters by calculating how that utterance would influence the mental state of the persona. This persona subsequently responded with a set of parameters at the same level of abstraction as the input of the participants. Then, the participants had to interpret these parameters and create a new utterance based on this interpretation. This continued until the participants wished to guess which persona they were interacting with. Preliminary results indicate that the majority of participants correctly determined their interaction partners.IV. DESIGN POSSIBILITIES FOR SERIOUS GAMESSocial simulations try to offer strict representations of situations they are intended to model. In their current training curriculum, police trainees practice with professional actors to simulate and experience possible scenarios. We regard such simulations as a form of role play and see several possibilities for the design of serious games based on role playing games. We believe that the design of serious games only depends to certain degree on the domain for which they are intended. The learning goals are the most important factor when designing serious games and must first be determined. In our case, the overarching learning goal is that police officers should have social awareness: they should be able to explain how their behaviour influences that of others and vice versa. The computational model we described in the previous section should help trainees understand why people behave as they do. It is through our serious games that trainees then learn how this model functions. Ultimately, in order to secure and strengthen the knowledge they have gained from an experience with a simulation or a game, it is vital that trainees reflect on their behaviour A. Beyond Simulation, Towards LearningWe assert that the extent to which a serious game reflects the situations from the domain can be varied. In other words, the realisation of the role play can be more loose or imaginary than strict as in a simulation B. Techniques for Improving Learning in Serious GamesIn our serious games, we do not opt for maximum realism or fantasy, but for a balance between the two. To do so, we take inspiration from techniques used in improvisational theatre (improv) and live action role play (larp) We take the point of view that when a simulation or role play is carried out, the people involved in these events have two different roles: that of the actor, who has knowledge about the simulation, and that of the character, who is being simulated by the actor. Both in improv and role plays, this distinction between in-character (IC) and out-of-character (OOC) roles can be utilized by the players. For example, a player may know, OOC, that another character has deceived his character-but this player's character may not know. The player can then use this OOC knowledge to steer the play IC in a certain direction.In the design of our serious games, we expand the distinction between in-character and out-of-character by looking at so-called meta-techniques used in larp As explained above, reflection and feedback on their actions constitute a large part of the learning process for trainees. Therefore, when police trainees practice their skills with actors, their experience is evaluated during an after-action review. We choose a similar approach in our serious games by implementing meta-techniques that offer moments for reflection and feedback during gameplay. In our games, when players interact with virtual characters, we will enable these characters to express their thoughts to players in the form of comic-like 'thought bubbles', alike to the inner voice technique. Such information would assist players in determining the attitude and feelings of characters as a supplement to the signals they read from the nonverbal behaviour and utterances of characters. We are also exploring how act breaks can be implemented. For example, at set points during gameplay, the interaction can be paused to give players and characters the opportunity to ask each other questions. These questions include asking the reasons for certain actions or inquiring about the feelings of either the characters or players at specific points in the interaction.Key in implementing these techniques is the decision when they should be used. As in all games, there needs to be a balance between the challenge of the game and the skill level of the player. Therefore, we propose to monitor the progress of players during the game and provide them with help in the form of the above meta-techniques when they seem not to be up to the challenge. For example, when players keep acting aggressively in an interaction with the effect being that a virtual character does not cooperate, this character may use a thought bubble to give feedback on why the interaction is unsuccessful. Alternatively, an act break may be used to have a more in-depth discussion as to what went wrong. Both techniques may also be used together to reinforce each other. For example, if a character shows a thought bubble during gameplay to provide feedback, it can explain its thoughts in more detail during a subsequent act break.Aside from providing feedback and reflection, we are investigating ways to let the virtual characters adapt their behaviour to help players achieve their learning goals. This adaptation reflects the methods used in improv and role play as well: the virtual characters are able to adapt their behaviour to the learning goals of players. For example, if it turns out that a player has difficulties to negotiate with withdrawn people, the virtual characters can choose to behave more withdrawn, providing the player with the possibility to gain more experience with such interactions.V. IMPLEMENTATIONSAs said above, we are designing two serious game prototypes: POINTER for interview training and LOITER for street intervention training for police officers. Until now, we have largely focused on the conceptual and technical aspects underlying these systems. Currently, we are exploring how to implement our games. The balance between realism and fantasy plays an important role in visualising the interactions. We do not choose a highly realistic appearance for our games, as this will stand at odds with the meta-techniques we wish to use. Additionally, it may evoke false expectations when players expect behaviour from very realistic looking characters that may be more complex than our cognitive model supports. Therefore, we opt to keep things simple in terms of graphical quality, but we do investigate the effects of different types of visualisations. Using AGENT, the Awareness Game Environment for Natural Training To experiment with different game mechanics and concepts that we can incorporate in POINTER and LOITER, a board game called Sequacious was created, see Fig. VI. CONCLUSIONS AND FUTURE WORK", "conclusions": "The approach we take in designing POINTER and LOITER, our serious games for interpersonal skills training in the domain of law enforcement, expands the possibilities of social simulation by infusing it with techniques from the fields of improv and role play. Our next step is to implement and evaluate our ideas in more detail. The serious games will not replace the teachers of the Dutch police, but serve as addenda to the training curriculum. Through evaluation and further cooperation with the Dutch police, we seek to fit our games in their curriculum and find the correct balance between fantasy and realism so that they will be accepted by the trainees. Additionally, we wish to iteratively improve the cognitive model we have created by letting players provide feedback on the virtual characters in the system themselves. Lastly, we hope to encourage other researchers to look beyond simulations by investigating other ways to design educational systems, such as serious games and techniques from the arts.", "SDG": [16]}, "engineering_pro_sociality_with_autonomous_agents": {"name": "Engineering Pro-Sociality with Autonomous Agents", "abstract": " This paper envisions a future where autonomous agents are used to foster and support pro-social behavior in a hybrid society of humans and machines. Pro-social behavior occurs when people and agents perform costly actions that benefit others. Acts such as helping others voluntarily, donating to charity, providing informations or sharing resources, are all forms of pro-social behavior. We discuss two questions that challenge a purely utilitarian view of human decision making and contextualize its role in hybrid societies: i) What are the conditions and mechanisms that lead societies of agents and humans to be more pro-social? ii) How can we engineer autonomous entities (agents and robots) that lead to more altruistic and cooperative behaviors in a hybrid society? We propose using social simulations, game theory, population dynamics, and studies with people in virtual or real environments (with robots) where both agents and humans interact. This research will constitute the basis for establishing the foundations for the new field of Pro-social Computing, aiming at understanding, predicting and promoting pro-sociality among humans, through artificial agents and multiagent systems.", "keywords": "", "introduction": "Everyday we are inundated with reports of situations that challenge our belief in humanity. The aim of moving towards more humane and fair societies appears to have been forgotten, as anti-social behavior dominates the headlines. According to analysts, journalists and even some politicians, the world seems to be lacking empathy, compassion and caring 1 . When famous and influential people exhibit clear signs of not esteeming others, acting without conscience or guilt over the unearned privileges they often enjoy, we should indeed be worried. They are our society's role models. Similar concerns occur when established social norms  are unable to provide escape to Hardin's tragedy of the commons (Nyborg and others 2016)), resulting in undesirable situations such as antibiotic resistance, climate change, or overexploitation of natural resources (Hardin 1968(Levin 2006;.Nyborg and others 2016)Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.1 https://www.theguardian.com/science/2013/jan/04/barackobama-empathy-deficit One can question if the advent of autonomous technology is in itself contributing to the adverse situations that we are witnessing. It is undeniable that the rise of technological giants has promoted a society that is less equal, and more divided . Perhaps the perception of autonomy and intelligence in current systems is also a factor leading to a decrease in our sense of responsibility towards others and thus, make us, humans, less humane. It is hard to know what role increasingly autonomous technology will play in this new society. However, since we are on the brink of an \"autonomy\" revolution (the named fourth industrial revolution), with autonomous cars already in our streets and drones at our doorsteps, we must address these questions. Social psychology and behavioral economics have been researching how constructs, such as altruism or empathy, affect decision-making and cooperation. Findings in these areas have rarely been taken into consideration by computer scientists, engineers and technology developers in general. In fact, the dominant view of human decision making is based on the homo economicus principle of utility maximization, and this is already the backbone of several approaches to model behavior in autonomous machines.(Stockhammer 2015)Despite the negative examples and the predictions of mainstream economic models, humans often act in ways that benefit others: people behave pro-socially when giving money to charity, donating blood, sharing food, offering one's seat in the bus, helping a co-worker with some problem or informing an outsider about the direction to a city location. Beyond small gestures, cooperation is the building block of complex social behavior and it underlies the evolutionary origins of human societies. It is thereby fundamental to understand -and engineer -the contexts that prevent selfishness and conflict, while allowing pro-sociality to be sustained (or induced, when absent). It is not by chance that the evolution of cooperation has been identified by Science's invited panel of scientists as one the major scientific challenges of our century .(Pennisi 2005)In this context, we would like to ask how autonomous agents can be used to nurture or nudge  cooperation and pro-sociality in a society of humans and machines. How can we design autonomous agents which, immersed within humans, can promote collective action in situations where it may not naturally arise? How can we foster cooperation in organizations, help people to ad-dress cyber-bullying when they witness it, combat the bystander problem, make people engaged in social good, promote sustainable habits, fight climate change, and so on? Can autonomous systems play a role there? To address these problems, several mechanisms have been identified as supportive of cooperation, in situation ranging from two-person dilemmas to large-scale collective action problems (Thaler and Sunstein 2008)(Ostrom 2015;Nowak 2006;.Rand and Nowak 2013)Here we defend a complementarity between such mechanisms and autonomous machines in order to improve prosocial behaviour within human groups. This approach is particularly relevant in cooperation problems involving large populations, especially in situations where a minority of carefully engineered artificial agents may produce a regime shift towards pro-social behaviors. In fact, the introduction of artificial agents may offer the means to overcome largescale coordination barriers  and tipping points (Santos and Pacheco 2011)) towards a more pro-social environment. Similarly, it may create novel tipping points, initially absent from human social dynamics. This can be achieved by designing autonomous agents that could influence others to behave in a certain way, by increasing the visibility of actions, advertising reputations or collective risks, indirectly enforcing pre-defined social norms, introducing previously absent behaviors, or simply creating empathic relations with humans -among many other possibilities.(Scheffer 2009The recent interest in AI applications for the good of society is not new, and there has been a surge of new developments and events over the past few years. Competitions or workshops, like the AAAI'17 WS on \"AI and Operations Research for Social Good\", whose purpose is to explore and promote the application of artificial intelligence for social good, are among many examples that we can find nowadays. In fact, the United Nations 2 together with the XPRIZE Foundation organized the AI for Good Global Summit in Geneva in 2017. Among other topics, these events address technical AI approaches for creating more sustainable cities, deal with disaster response, address the impact of inequality, or improve public health. The work here proposed goes in that direction, having the potential to cause impact in some of these application areas.This paper, therefore, proposes a vision where autonomous systems pro-actively act, foster and promote prosociality, instead of passively allowing or supporting the delegation of responsibility into the technology. We believe that this new type of computing will be linked with aspects of transparency, accountability and participation, which are all timely and urgently needed in our society.To begin with, we define Pro-social Computing as \"computing directed at supporting and promoting actions that benefit the society and others at the cost of one's own\". This is a broad notion that may encompass different alternative views of how to engineer pro-social computing. To make it more concrete, we will start by proposing simple scenarios where pro-social computing can be used. Then we will give a glimpse of research agenda for engineering pro-social au-2 See http://www.itu.int/en/ITU-T/AI/Pages/201706default.aspx tonomous agents and discuss the future of this area.", "body": "Everyday we are inundated with reports of situations that challenge our belief in humanity. The aim of moving towards more humane and fair societies appears to have been forgotten, as anti-social behavior dominates the headlines. According to analysts, journalists and even some politicians, the world seems to be lacking empathy, compassion and caring 1 . When famous and influential people exhibit clear signs of not esteeming others, acting without conscience or guilt over the unearned privileges they often enjoy, we should indeed be worried. They are our society's role models. Similar concerns occur when established social norms Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.1 https://www.theguardian.com/science/2013/jan/04/barackobama-empathy-deficit One can question if the advent of autonomous technology is in itself contributing to the adverse situations that we are witnessing. It is undeniable that the rise of technological giants has promoted a society that is less equal, and more divided Despite the negative examples and the predictions of mainstream economic models, humans often act in ways that benefit others: people behave pro-socially when giving money to charity, donating blood, sharing food, offering one's seat in the bus, helping a co-worker with some problem or informing an outsider about the direction to a city location. Beyond small gestures, cooperation is the building block of complex social behavior and it underlies the evolutionary origins of human societies. It is thereby fundamental to understand -and engineer -the contexts that prevent selfishness and conflict, while allowing pro-sociality to be sustained (or induced, when absent). It is not by chance that the evolution of cooperation has been identified by Science's invited panel of scientists as one the major scientific challenges of our century In this context, we would like to ask how autonomous agents can be used to nurture or nudge Here we defend a complementarity between such mechanisms and autonomous machines in order to improve prosocial behaviour within human groups. This approach is particularly relevant in cooperation problems involving large populations, especially in situations where a minority of carefully engineered artificial agents may produce a regime shift towards pro-social behaviors. In fact, the introduction of artificial agents may offer the means to overcome largescale coordination barriers The recent interest in AI applications for the good of society is not new, and there has been a surge of new developments and events over the past few years. Competitions or workshops, like the AAAI'17 WS on \"AI and Operations Research for Social Good\", whose purpose is to explore and promote the application of artificial intelligence for social good, are among many examples that we can find nowadays. In fact, the United Nations 2 together with the XPRIZE Foundation organized the AI for Good Global Summit in Geneva in 2017. Among other topics, these events address technical AI approaches for creating more sustainable cities, deal with disaster response, address the impact of inequality, or improve public health. The work here proposed goes in that direction, having the potential to cause impact in some of these application areas.This paper, therefore, proposes a vision where autonomous systems pro-actively act, foster and promote prosociality, instead of passively allowing or supporting the delegation of responsibility into the technology. We believe that this new type of computing will be linked with aspects of transparency, accountability and participation, which are all timely and urgently needed in our society.To begin with, we define Pro-social Computing as \"computing directed at supporting and promoting actions that benefit the society and others at the cost of one's own\". This is a broad notion that may encompass different alternative views of how to engineer pro-social computing. To make it more concrete, we will start by proposing simple scenarios where pro-social computing can be used. Then we will give a glimpse of research agenda for engineering pro-social au-2 See http://www.itu.int/en/ITU-T/AI/Pages/201706default.aspx tonomous agents and discuss the future of this area.Application CasesJust to place this area into perspective, let us illustrate three simple situations where pro-social computing and, more specifically, pro-social agents, may play a role in changing the prevailing non-cooperative social dynamics, in a hybrid society of humans and robots.Fighting the bystander effectThe well-known case of Kitty Genovese's murder more than five decades ago is without a doubt the most publicized case of the infamous \"bystander effect\". In this horrific case, several witnesses were \"caught, fascinated, distressed, unwilling to help but unwilling to turn away\" (Darley and Latane 1968), while Kitty was attacked. Witnesses did not intervene, and Kitty Genovese was brutally murdered. The term bystander effect was actually coined after this event. In spite of controversies surrounding the role of the bystanders in that particular situation, many studies have been conducted over the years, where the bystander effect is repeatedly observed. This effect verifies that, as the number of people witnessing a distressing event increases, their willingness to help decreases (thus reducing pro-social behavior). In computer-mediated scenarios (e.g., social media) we can also observe the bystander effect, as it was shown that the amount of time for an intervention increases with number of people witnessing the situation (the virtual bystanders). In fact, the growth of cyber-bullying in social media can be clearly related to the bystander effect.Why do people witness, condemn, and yet do not help? According to the theory proposed by Darley and Latane, three processes may occur before there is an action by the bystander to aid the victim:\u2022 1) Audience inhibition, that is, individuals may not act as the risk of embarrassment arises if others are watching and it turns out that the situation did not require any help; \u2022 2) Social influence, whereby inaction becomes the established behavior as individuals are observing others and take their inaction as a guideline for their own behavior; \u2022 3) Diffusion of responsibility, that is, the costs of nonintervention are shared in the presence of other people.Finally, if there is partial observability and uncertainty about what the others are doing, any bystander can even assume that one of the observers is already acting and helping, therefore disregarding the need to offer any assistance. From a technological standpoint one can ask if this bystander effect may be addressed, and in particular if:\u2022 Can autonomous machines and agents (particularly if they are embodied in the physical world) be considered \"audience\" in this \"bystander\" effect? That is, would these autonomous machines increase the bystander effect? \u2022 In particular, does the diffusion of responsibility also occur when, instead of humans, we have \"autonomous machines\"?\u2022 And social influence? Can machines/agents exhibit behaviors (either by acting or non acting) that influence other's (and humans') behaviors? \u2022 If agents can have social influence on humans, would they be able to counter-act the bystander effect? If so, how can we build technology for that?Sustaining fairness and preventing inequalityHuman decision-making is often driven by fair and equalitarian motives In the context of UG, only the egalitarian division, in which both the Proposer and the Responder earn a similar reward, is considered a fair result. Multiple studies attest that people are fair when playing the UG ", "conclusions": "", "SDG": [16]}, "intelligent_financial_fraud_detection_a_comprehensive_review": {"name": "Author Biography", "abstract": " Jarrod completed his Bachelor degree in Computer Science from the University of Newcastle, Australia in 2013. As a researcher he is primarily interested in various aspects of information security. Jarrod's current research is focused on computational intelligence and data mining based financial fraud detection.", "keywords": "Financial fraud detection,Computational intelligence,Data mining,Anomaly detection,Classification", "introduction": "Financial fraud is an issue that has wide reaching consequences in both the finance industry and daily life. Fraud can reduce confidence in industry, destabilise economies, and affect people's cost of living. Traditional approaches relied on manual techniques such as auditing, which are inefficient and unreliable due to the difficulty of the problem. Data mining-based approaches have been shown to be useful because of their ability to identify small anomalies in large data sets . There are many different types of fraud, as well as a variety of data mining methods, and research is continually being undertaken to find the best approach for each case.[32]Financial fraud is a broad term with various potential meanings, but for our purposes it can be defined as the intentional use of illegal methods or practices for the purpose of obtaining financial gain . Fraud has a large negative impact on business and society: credit card fraud alone accounts for billions of dollars of lost revenue each year [59], and some figures suggest that the total yearly cost to the U.S. could be in excess of $400 billion [7], while a third study shows that UK insurers are out 1.6 billion pounds a year due to fraudulent claims [28]. Financial fraud also has broader ramifications on the industry, such as providing funding for illicit activities like drug trafficking and organised crime [32]. For credit card fraud the cost is typically worn by the merchants, who end up paying shipping, chargeback, and administrative costs as well as losing consumer confidence after being victim to a fraudulent transaction [7], [37]. In this way we can see the widespread consequences that fraud can have and the importance in minimising it.[41]Advancements in modern technologies such as the internet and mobile computing have led to an increase in financial fraud in recent years . Social factors such as the increased distribution of credit cards has increased spending but also resulted in an increase to fraud [54]. Fraudsters are continually refining their methods, and as such there is a requirement for detection methods to be able to evolve accordingly [41]. Data mining has already been shown to be useful in similar domains such as credit card approval, bankruptcy prediction, and analysis of share markets [7]. Fraud detection is a considered to be a similar classification problem but with a vast imbalance in fraudulent to legitimate transactions, and a sizable difference in cost for misclassifying them [35]. Data mining approaches are also applicable to fraud detection for their efficiency at processing large datasets and their ability to work without requiring knowledge of the input variables [16].[38]A useful framework for applying data mining to fraud detection is to use it as a method for classifying suspicious transactions or samples for further consideration. Studies show that reviewing 2% of credit card transactions could reduce fraud losses to 1% of the total cost of all purchases, with more assessments resulting in smaller loss but with an increase in auditing costs . A multi-layer pipeline approach can be used with each step applying a more rigorous method to detect fraud. Data mining can be utilised to efficiently filter out more obvious fraud cases in the initial levels and leave the more subtle ones to be reviewed manually [37].[37]In this article we will use a few broad terminologies that are defined here for clarity. Data mining refers to any method that processes large quantities of data to derive an underlying meaning. Within this classification we will consider two categories of data mining: statistical and computational. We define the statistical techniques as those that are based on traditional mathematical methods, such as logistic regression and Bayesian theory. Computational methods are those which use modern intelligence techniques, such as neural networks and support vector machines. Though these categories share many similarities, we will consider that the main difference between them is that computational methods are capable of learning from and adapt-ing to the problem domain, while statistical methods are more rigid. Both types of data mining will be researched in this article.The objective of this paper is to provide a review of existing literature in financial fraud detection and compare their findings (focusing primarily on literature published during 2004-2014 and research that reports empirical studies using CI-based techniques). A key focus of this review is on the reported performance of CI techniques for specific fraud types. None of the existing reviews (for example, , [32] and [62]) covers this aspect. This will provide an indication to future researchers on the areas that are currently available for future study. The remainder of the article is structured as follows. In the next section we will detail the history of intelligent fraud detection research. Section 3 will list the different types of financial fraud. Section 4 presents an overview of the assorted detection methods used. Section 5 details the specific efforts of previous researchers in detecting financial fraud. Section 6 offers an insight into what is missing from existing techniques and proposes areas for future research. Section 7 provides a conclusion to our research and a discussion of our findings.[63]", "body": "Financial fraud is an issue that has wide reaching consequences in both the finance industry and daily life. Fraud can reduce confidence in industry, destabilise economies, and affect people's cost of living. Traditional approaches relied on manual techniques such as auditing, which are inefficient and unreliable due to the difficulty of the problem. Data mining-based approaches have been shown to be useful because of their ability to identify small anomalies in large data sets Financial fraud is a broad term with various potential meanings, but for our purposes it can be defined as the intentional use of illegal methods or practices for the purpose of obtaining financial gain Advancements in modern technologies such as the internet and mobile computing have led to an increase in financial fraud in recent years A useful framework for applying data mining to fraud detection is to use it as a method for classifying suspicious transactions or samples for further consideration. Studies show that reviewing 2% of credit card transactions could reduce fraud losses to 1% of the total cost of all purchases, with more assessments resulting in smaller loss but with an increase in auditing costs In this article we will use a few broad terminologies that are defined here for clarity. Data mining refers to any method that processes large quantities of data to derive an underlying meaning. Within this classification we will consider two categories of data mining: statistical and computational. We define the statistical techniques as those that are based on traditional mathematical methods, such as logistic regression and Bayesian theory. Computational methods are those which use modern intelligence techniques, such as neural networks and support vector machines. Though these categories share many similarities, we will consider that the main difference between them is that computational methods are capable of learning from and adapt-ing to the problem domain, while statistical methods are more rigid. Both types of data mining will be researched in this article.The objective of this paper is to provide a review of existing literature in financial fraud detection and compare their findings (focusing primarily on literature published during 2004-2014 and research that reports empirical studies using CI-based techniques). A key focus of this review is on the reported performance of CI techniques for specific fraud types. None of the existing reviews (for example, Related WorkPrior investigation has already been performed on some aspects of intelligent financial fraud detection, and a brief history of the specific research methods undertaken is given here. Figure Recent fraud detection research has been far more varied in methods studied, although the former techniques are still popular. In 2004 Kou et al. reviewed the study of general fraud detection using analytic techniques including neural networks In  In 2009 Holton utilised a combination of text mining and Bayesian belief networks to identify disgruntled employees likely to commit corporate fraud In 2010 Cecchini et al. studied Accounting and Auditing Enforcement Releases (AAER) with their own text mining and support vector machine hybrid to predict financial statement fraud in US companies Humphrys et al. created text mining hybrids by utilising other common methods to act as the classifier. With a support vector machine, decision tree, and Bayesian belief network they managed to successfully identify fraud within the company's 10-K document filings Also in 2011 Ravisankar et al. compared a large range of methods to identify financial statement fraud within Chinese companies. In addition to support vector machines they looked at genetic programming, logistic regression, group method of data handling, and variety of neural networks In 2013 Huang investigated financial statement fraud in a series of Taiwanese companies using logistic regression and a support vector machine In 2014 Dong et al. used text mining to study AAERs for Chinese companies that were trading publicly in the US Types of Financial FraudThere are many different types of financial fraud, and a brief description of some of the major types will be listed here (also see Figure Credit Card Fraud. Credit card fraud refers to the unauthorised use of a person's credit card to perform fraudulent transactions without the user's knowledge Securities and Commodities Fraud. Securities fraud, also known as commodities fraud, refers to a variety of methods by which a person is deceived into investing into a company based on false information. It includes Pyramid Schemes, Ponzi Schemes, Hedge Fund Fraud, Foreign Exchange Fraud, and Embezzlement Financial Statement Fraud. Financial statements are the documents release by a company that explain details such as their expenses, loans, income and profits Reasons for committing financial statement fraud include improving stock performance, reducing tax obligations, or as an attempt to exaggerate performance due to managerial pressure Insurance Fraud. Insurance fraud is fraud that can be committed at any point during the insurance process, and by any people in the chain. Insurance claims fraud occurs when a customer submits a fraudulent insurance claim as a result of an exaggerated injury or loss of assets, or a completely fraudulent event. A common form of claims fraud is automobile insurance fraud, which is often committed by faking or intentionally committing accidents that result in excessive repair and injury costs. Larger scale claims fraud also occurs, such as crop insurance fraud where a consumer overstates their losses due to declining agricultural prices or the effects of natural disasters.Insurance fraud can also include excessive billing, duplicate claims, kickbacks to brokers and \"upcoding\" of items Mortgage Fraud. Mortgage fraud is a specific form of financial fraud that refers to manipulation of a property or mortgage documents. It is often committed to misrep-resent the value of a property for the purpose of influencing a lender to fund a loan for it Money Laundering. Money laundering is a method used by criminals to insert proceeds obtained from illicit ventures into valid businesses. This conceals the origin of the money, giving them the appearance of legitimate income and making it difficult to track their crimes. Money laundering is also undesirable as it enables the criminals to have economic influence Computational Intelligence and Data Mining Methods for Financial Fraud DetectionA large number of statistical and computational techniques exist that have been applied to data mining problems in recent years (see Figure Bayesian Belief Networks. Bayesian belief networks are a statistical classification technique that makes use of Bayes theorem, a method to determine the probability that a given hypothesis is true. The theorem states that for a hypothesis H (such as whether an object X can be classified within a given class), the probability P is given by:(A Bayesian belief network uses a classifier to calculate for all possible classes and inserts X into the class with the highest . In this way the network is shown to classify each sample into the class that it is most likely to belong to Graphically, a Bayesian belief network can be modelled as a directed acyclic graph, with nodes to represent the samples and edges to reflect a causal dependency between them (see Figure Logistic Regression or Logistic Model.Logistic regression is a statistical method of classifying binary data which uses a linear model, often referred to as a logistic or logit model, to perform regression on a set of variables (2)And the formula for calculating that a sample belongs in class is given by:(3)Where and w are the regression tuning parameters representing the intercept and coefficient vector respectively Neural Network. A neural network is a computational approximation of the human brain which uses a graph of vertices and edges to represent neurons and synapses Each node considers its input as a function of the vertices connected to it at the previous layer. For each neuron the signal received is given by: (4)Whereis the weight of the link between neurons and and is the input of . If the result is greater than a predetermined amount the current neuron \"fires\" and becomes an input for the next layer A back propagation neural network is trained by running samples from a set of training data through the network and comparing the results. For the first iteration the weights at each edge are normally determined randomly, and after the results have been calculated each weighting is adjusted slightly for the next sequence The hyperplane is constructed in such a way as to maximise the separation between both classes, which helps to reduce potential errors caused by overtraining (see Figure The classification for a support vector machine can therefore be defined as: Genetic Algorithms and Programming. Genetic algorithms use the concept of population evolution to iteratively improve solutions to the problem. It works by randomly creating a starting generation, then continuously reproducing each population using various techniques and choosing the survivors based on their fitness. Reproduction occurs by taking pairs of parents from the current generation and applying crossover on two points, then randomly mutating a single element of the resulting child. The ability of the children is measured using a fitness function, and the result of this determines which parents and children are chosen to represent the next generation.Measuring the fitness of the children can be as simple as measuring the percentage of samples they classify correctly. The algorithm terminates once a required fitness has been reached, but to avoid infinite looping a limit may be set on the number of iterations that are run (see Figure Decision Trees, Forests, and CART. Decision trees are a technique that classifies or predicts data using a tree with internal nodes representing binary choices on attributes and branches representing the outcome of that choice A decision forest, or random forest, is a collection of decision trees used to avoid the instability and risk of overtraining that can occur with a single tree Group Method of Data Handling. Group method of data handling is an inductive data mining algorithm that calculates optimal solutions through a series of increasingly accurate models. It begins with a simple model, typically a polynomial of the form: Text Mining. Text mining refers to a specific form of data mining which is performed on plain text. Given this broad description there are a variety of different approaches to text mining, though many work by using one or more common preprocessing steps to transform the data into more quantitative samples:\uf0b7 Filtering of stop words. Common joining words such as \"the\" or \"is\" are removed to simplify the dataset. \uf0b7 Stemming. This involves reducing derived words to their common base, such as removing plurals and tenses.\uf0b7 Identification of part-of-speech. This recognises the part of a sentence that a word occurs in and can be helpful in reducing error due to homophones and other language nuances. \uf0b7 Word frequency analysis. The frequency of word in document is calculated as \uf0b7 Word combination. Words are further combined into common concepts using a synonym list or more complicated measures. Once the text has been converted to a more quantitative form a traditional data mining method is applied to actually perform the classification Self-Organising Map. Self-organising maps are a form of artificial neural network which consists of a single matrix of neurons. A non-linear algorithm is used to map inputs from a high-dimensional space to the two-dimensional array of neurons. The mapping is designed to model similar input vectors as neurons that are closer together in the resulting matrix, providing a visualisation of the inputs. A distance or neighbourhood function is used to group the nodes, such as the Euclidean distance formula or Gaussian formula Process Mining. Process mining involves analysis of transactions and event logs to construct models representing the behaviour of a system. A specific process instance is created to represent the individual cases within the system, and assumptions are applied to the available data to determine whether it is suitable for observation with this particular process. Given a model of expected behaviour within the system, the typical operation steps are:\uf0b7 Log preparation and inspection. The logs are obtained and pre-processed to remove extraneous noise. If no existing model was available one can be created based on the provided logs. \uf0b7 Analysis. Models are observed to check various behaviours. Aspects such as control flow, performance, and user-roles can be analysed to determine the expected outcomes of the system.\uf0b7 Verification. Given the results of the above analysis, the process miner is applied to various samples and determines whether they represent typical system behaviour. In addition to classification, the ability to generate its own model makes process mining useful for discovering flows and practices within a complicated system Artificial Immune System. Artificial immune systems are a data mining approach that work by imitating the behaviour of a biological immune system to detect antigens One common form is clonal selection, which continually generates detector cells that only live a short time. If a cell detects an antigen it extends its life to fight the intruder, and may also mutate as a result of the conflict. The surviving cells at the end of the simulation are therefore the ones best suited to detecting the antigens. Another common implementation is negative selection, which works by randomly creating cells and determining which of these react with other non-invasive cells within the system. Any that do are discarded, resulting in the remainder being proficient at detecting intruders Hybrid Methods. Hybrid methods are a combination of multiple traditional methods by selecting beneficial attributes of each in an attempt to create a superior algorithm for a specific problem domain. Hybrid methods can be constructed in a variety of different ways: at the highest level methods can simply be applied linearly, with the outputs of the first providing the inputs of the second Classification of Existing Financial Fraud DetectionIn the following section we will classify the financial fraud detection techniques reviewed based on success rate, method chosen, and fraud type studied. This categorisation will enable us to demonstrate trends in current research methods, including which have been successful and any factors that have not been covered.Classification Based on PerformanceA variety of standards have been used to determine performance, but the three most commonly used are accuracy, sensitivity, and specificity. Accuracy measures the ratio of all successfully classified samples to unsuccessful ones. Sensitivity compares the amount of items correctly identified as fraud to the amount incorrectly listed as fraud, also known as the ratio of true positives to false positives. Specificity refers to the same concept with legitimate transactions, or the comparison of true negatives to false negatives Tables From the results we can see that CI methods typically had better success rate than statistical methods. Sensitivity was slightly better for random forests and support vector machines than logistic regression, with comparable specificity and accuracy Most of the research showed a large difference between each method's sensitivity and specificity results. For example, Bhattacharyya et al. showed that logistic regression, support vector machines and random forests all performed significantly better at detecting legitimate transactions correctly than fraudulent ones As explained previously, fraud detection is a problem with a large difference in misclassification costs: it is typically far more expensive to misdiagnose a fraudulent transaction as legitimate than the reverse. With that in mind it would be beneficial for a detection techniques to show a much higher sensitivity than specificity, meaning that these results are less than ideal. Contrary to this belief, Hoogs et al. hypothesised that financial statement fraud may carry higher costs for false positives, and their results reflect this with a much higher specificity Classification Based on Detection AlgorithmClassifying fraud detection practices by the detection algorithm is a useful way to identify the suitable techniques for this problem domain. It can also help us to determine why particular methods were chosen or successful. Additionally, we can identify any gaps in research by looking at algorithms which have not been explored sufficiently. Table Previously it was mentioned that early fraud detection research focussed on statistical models and neural networks; however, it may be noted that these methods still continue to be popular. Many used at least one form of neural network Several of the research focussed on a single form of fraud detection which they advocated above others, such as studying text mining with the singular validation decomposition vector A rising trend in fraud detection is the use of hybrid methods which utilise the strengths of multiple algorithms to classify samples. Duman and Ozcelik used a combination of scatter search and genetic algorithm, based on the latter but targeting attributes of scatter search such as the smaller populations and recombination as the reproduction method Classification Based on Fraud TypeGiven the varying nature of each type of fraud, the problem domain can differ significantly depending on the form that is being detected. By classifying the existing practices on the type of fraud investigated we can identify the techniques more suitable and more commonly used for a specific type of fraud. Additionally we can infer the varieties which are considered the most important for investigation depending on the scope and scale of their impact. Table As with each chosen algorithm, feature selection will differ depending on the problem domain. Specific financial statement fraud exists within individual companies, and as such attribute ratios are used instead of absolute values. Koh and Low provide a good example of the relevant ratios such as net income to total assets, interest payments to earnings before interest and tax, and market value of equity to total assets We can see that the existing research has been greatly unbalanced in fraud type studied. The vast majority of papers have focussed on two forms of financial fraud: credit card fraud and financial statement fraud. Only a handful of studies have looked at securities and commodities fraud, and many focussed on external forms of corporate fraud while neglecting the internal ones To determine which method was the most successful for each fraud type we looked at the accuracy scores reported in the experimental research. There are many metrices used to assess performance but accuracy was the most commonly used and therefore provided the best basis for comparison. Analysis of the individual problem domain is required to completely assess the usefulness of each method, but the results shown here should provide a useful starting point for future researchers to investigate new fraud detection algorithms. It should be noted that we have reported only the best accuracy result obtained by each method for specific fraud types; the test conditions for individual experiments are not necessarily comparable. Table All the solutions used for credit card fraud had a very high success rate, including regression, support vector machines, artificial immune systems, and random forests. Out of these the self organising map is recommended due to its perfect accuracy with a 10000 transaction sample Financial Fraud Detection Challenges and Future DirectionsFinancial fraud detection is an evolving field in which it is desirable to stay ahead of the perpetrators. Additionally, it is evident that there are still facets of intelligent fraud detection that have not been investigated. In this section we present some of the key issues associated with financial fraud detection and suggest areas for future research. Some of the identified issues and challenges are as follows:\uf0b7 Typical classification problems: CI and data mining-based financial fraud detection is subject to the same issues as other classification problems, such as feature selection, parameter tuning, and analysis of the problem domain. \uf0b7 Fraud types and detection methods: Financial fraud is a diverse field and there has been a large imbalance in both fraud types and detection methods studied: some have been studied extensively while others, such as hybrid methods, have only been looked at superficially. \uf0b7 Privacy considerations: Financial fraud is a sensitive topic and stakeholders are reluctant to share information on the subject. This has led to experimental issues such as undersampling. \uf0b7 Computational performance: As a high-cost problem it is desirable for financial fraud to be detected immediately. Very little research has been conducted on the computational performance of fraud detection methods for use in real-time situations. \uf0b7 Evolving problem: Fraudsters are continually modifying their techniques to remain undetected. As such detection methods are required to be able to constantly adapt to new fraud techniques.\uf0b7 Disproportionate misclassification costs: Fraud detection is primarily a classification problem with a vast difference in misclassification costs. Research on the performance of detection methods with respect to this factor is an area which needs further attention. \uf0b7 Generic framework: Given that there are many varieties of fraud, a generic framework which can be applied to multiple fraud categories would be valuable.As a classification problem, financial fraud detection suffers from the same issues as other similar problems. Feature selection has a high impact on the success of any classification method. While some researchers have mentioned feature selection for one type of fraud From the existing literature it is apparent that there are some forms of fraud that have not been investigated as extensively as others. Financial statement fraud has been considerably investigated, which is understandable given its high profile nature, but there are other forms of fraud that have a significant impact on consumers. Credit card fraud often has a direct impact on the public and the recent increase in online transactions has led to a majority of the U.S. public being concerned with identity theft The private nature of financial data has led to institutions being reluctant to share fraudulent information. This has had an affect both on the fraud types that have been investigated as well as the datasets used for the purpose. In the published literature many of the financial fraud simulations consisted of less than a few hundred samples, typically with comparable amounts of fraudulent and legitimate specimens. This is contrary to the realities of the problem domain, where fraud cases are far outweighed by legitimate transactions Some forms of financial fraud occur very rapidly, such as credit card fraud. If a fraudster obtains an individual's credit card information it's very likely that they will use it immediately until the card limit is reached. The ability to detect fraud in realtime would be highly beneficial as it may be able to prevent the fraudster from making subsequent transactions. Computational performance is therefore a key factor to consider in fraud detection. Though some researchers have noted the performance of their particular methods Unlike many classification problems, fraud detection solutions must be capable of handling active attempts to circumvent them. As detection methods become more intelligent, fraudsters are also constantly upgrading their techniques. For example, in the last few decades credit card fraud has moved from individuals stealing or forging single cards to large-scale phone and online fraud perpetrated by organised groups As explained previously fraud has a large cost to businesses. Additionally, fraud detection has associated costs: systems require maintenance and computational power, and auditors must be employed to monitor them and investigate when a potential fraud case is identified Given the diversity of common categories of fraud it would be useful to have some form of generic framework that could apply to more than one fraud category. Such a framework could be used to study the differences between various types of fraud, or even specific details such as differentiating between stolen and counterfeit credit cards Conclusion", "conclusions": "Fraud detection is an important part of the modern finance industry. This literature review studied research into intelligent approaches to fraud detection, both statistical and computational. Though their performance differed, each technique was shown to be reasonably capable at detecting various forms of financial fraud. In particular, the ability of the computational methods such as neural networks and support vector machines to learn and adapt to new techniques is highly effective to the evolving tactics of fraudsters.There are still many aspects of intelligent fraud detection that have not yet been the subject of research. Some types of fraud, as well as some data mining methods, have been superficially explored but require future study to be completely understood. There is also the opportunity to examine the performance of existing methods by adjusting their parameters, as well as the potential to study cost benefit analysis of computational fraud detection. Finally, further research into the differences between each type of financial fraud could lead to a general framework which would greatly improve the accuracy of intelligent detection methods.        ", "SDG": [16]}, "survey_on_computer_vision_for_uavs_current_developments_and_trends": {"name": "Survey on Computer Vision for UAVs: Current Developments and Trends", "abstract": " During last decade the scientific research on Unmanned Aerial Vehicless (UAVs) increased spectacularly and led to the design of multiple types of aerial platforms. The major challenge today is the development of autonomously operating aerial agents capable of completing missions independently of human interaction. To this extent, visual sensing techniques have been integrated in the control pipeline of the UAVs in order to enhance their navigation and guidance skills. The aim of this article is to present a comprehensive literature review on vision based applications for UAVs focusing mainly on current developments and trends. These applications are sorted in different categories according to the research topics among various research groups. More specifically vision based position-attitude control, pose estimation and mapping, obstacle detection as well as target tracking are the identified components towards autonomous agents. Aerial platforms could reach greater level of autonomy by integrating all these technologies onboard. Additionally, throughout this article the concept of fusion multiple sensors is highlighted, while an overview on the challenges C. Kanellakis ( ) \u2022 G.", "keywords": "UAVs,SLAM,Visual servoing,Obstacle avoidance,Target tracking", "introduction": "Unmanned Aerial Vehicles have become a major field of research in recent years. Nowadays, more and more UAVs are recruited for civilian applications in terms of surveillance and infrastructure inspection, thanks to their mechanical simplicity, which makes them quite powerful and agile. In general, aerial vehicles are distinguished for their ability to fly at various speeds, to stabilize their position, to hover over a target and to perform manoeuvres in close proximity to obstacles, while fixed or loitering over a point of interest, and performing flight indoors or outdoors. These features make them suitable to replace humans in operations where human intervention is dangerous, difficult, expensive or exhaustive.", "body": "Unmanned Aerial Vehicles have become a major field of research in recent years. Nowadays, more and more UAVs are recruited for civilian applications in terms of surveillance and infrastructure inspection, thanks to their mechanical simplicity, which makes them quite powerful and agile. In general, aerial vehicles are distinguished for their ability to fly at various speeds, to stabilize their position, to hover over a target and to perform manoeuvres in close proximity to obstacles, while fixed or loitering over a point of interest, and performing flight indoors or outdoors. These features make them suitable to replace humans in operations where human intervention is dangerous, difficult, expensive or exhaustive.Terminology DefinitionsThis article is reviewing the current State-of-the-Art on control, perception and guidance for UAVs and thus initially this section enlists some of the most used terms in the literature.Unmanned aerial vehicle -Aircraft without a human pilot onboard. Control is provided by an onboard computer, remote control or combination of both. Unmanned aircraft system -An unmanned aircraft system is an unmanned aircraft and the equipment necessary for the safe and efficient operation of that aircraft. An unmanned aircraft is a component of a UAS. It is defined by statute as an aircraft that is operated without the possibility of direct human intervention from within or on the aircraft UAV TypesThis massive interest for UAVs has led to the development of various aircraft types in many shapes and sizes to operate in different tasks Single rotor -This platform has the main rotor for navigation and a tail rotor for controlling the heading. Mostly they can vertically take-off and land and do not need airflow over the blades to move forward, but the blades themselves generate the required airflow. Piloted helicopters are popular in aviation but their unmanned versions are not so popular in UAV research community. A single-rotor helicopter can be operated by a gas motor for even longer endurance compared multi rotors. The main advantage is that it can carry heavy payloads (e.g sensors, manipulators) in either hovering tasks or long endurance flights in large areas outdoors. The disadvantages of such platforms are their mechanical complexity, danger from their generally large rotor, and cost. Multi rotor -This class of UAVs can be divided in subclasses depending on the number of rotor blades. The most common are considered quadrotor, hexarotor. Additionally tri-copters or octacopters have been developed. Mostly they can vertically take-off and land and do not need airflow over the blades to move forward, but the blades themselves generate the required airflow. Multi rotors can be operated both indoors and outdoors and are fast and agile platforms that perform demanding manouevres. They can also hover or move along a target in close quarters. The downsides of these types are the limited payload capacity and flight time. Additionally, the mechanical and electrical complexity is generally low with the complex parts being abstracted away inside the flight controller and the motors' electronic speed controllers.Fixed wing -The basic principle of these UAVs consist of a rigid wing with specific airfoil that can fly based on the lift generated by the forward airspeed (produced by a propeller). The navigation control is succeeded through specific control surfaces in the wings knowns as aileron (pitch), elevator (roll) and rudder (yaw). The simple structure of such vehicles is the greatest advantage from the other types. Their aerodynamics assist in longer flight ranges and loitering as well as high speed motion. Furthermore, they can carry heavier payloads compared to multi rotors, while the drawbacks of these platforms are the need for a runway to takeoff and landing and the fact that they need to move constantly preventing hovering tasks. The landing is also crucial for safe recovery of the vehicle. Hybrid -This class is an improved version of fixed wing aircrafts. Hybrid vehicles have the ability to hover and vertically takeoff and land. This type is still under developemnt.Overall, rotor crafts are more suitable for applications like infrastructure inspection and maintenance due to hover capabilities and their agile maneuvering. On the other hand, fixed wing vehicles fit better in aerial surveillance and mapping of large areas from greater heights. Table The aforementioned navigational equipment, questions the reliability and limit the best possible utilization of an UAV in real life applications. For this reason, new ways to estimate and track the position and orientation of the UAV were needed. An idealaccurate solution for the calculation of vehicle's pose would be the fusion of data from multiple collaborative sensors Nowadays, the evolution in embedded systems and the corresponding miniaturization has brought powerful yet low-cost camera modules and Inertial Measurement Unit (IMU)s that could be mounted on UAVs, extract useful information on board and feed back the necessary data, fused with measurements from inertial sensors. Different types of sensors can be employed depending on the task. Ultrasonic sensors (Fig. Motivation of this ReviewThe aim of this article is to provide an overview of the most important efforts in the field of computer vision for UAVs, while presenting a rich bibliography in the field that could support future reading in this emerging area. An additional goal is to gather a collection of pioneering studies that could act as a road-map for this broaden research area, towards autonomous aerial agents. Since the field of computer vision for UAVs is very generic, the depicted work will focus only in surveying the areas of: a) flight control or visual servoing, b) visual localization and mapping, and c) target tracking and obstacle detection.It should be highlighted that this article classified the aforementioned categories following the Navigation -Guidance -Control scheme. The big picture is to provide a significant insight for the entire autonomous system collecting all the pieces together. The concept of navigation monitors the motion of the UAV from one place to another processing sensor data. Through this procedure the UAV can extract essential information for it's state (kinematics and dynamics -state estimation), build a model of its surroundings (mapping and obstacle detection) and even track sequential objects of interest (target tracking) to enhance the perception capabilities. Thus, by combining localization and perception capabilities, the robotic platforms are enabled for Guidance tasks. In the Guidance system, the platform processes information from perception and localization parts to decide its next move according to specified task. In this category trajectory generation and path planning are included for motion planning, mission-wise decision making or unknown area exploration. Finally, the realization of actions derived from Navigation and Guidance tasks is performed within the Control section. The controller manipulates the inputs to provide the desired output enabling actuators for force and torque production to control the vehicle's motion. Generally, different controllers have been proposed to fulfill mission enabled requirements (position, attitude, velocity and acceleration control). In the following sections the major works that employ visual sensors for each defined category will be presented, while the Navigation, Guidance and Control The rest of this article is structured as follows. In Section 2 a complete overview of the most important approaches in the field of flight control will be presented. Furthermore, in Section 3 a survey on Perception (visual Simultaneous Localization and Mapping (SLAM), Obstacle detection and target tracking) Fig. Flight ControlIn this section different control schemes and algorithms are described that have been proposed throughout the years for UAvV position, attitude, velocity control. Innititally, Visual servoing schemes are described, followed by vision based UAV motion control.Visual ServoingThe main idea of Visual Servoing is to regulate the pose {C \u03be,T } (position and orientation) of a robotic platform relative to a target, using a set of visual features {f } extracted from the sensors. Visual features, in most of the cases, are considered as points but can also be parametrised in lines or geometrical shapes such as ellipses. More specifically, image processing methods are integrated in the control scheme so that either the 2D features or the 3D pose measurements along with IMU data {z I MU } are fed back in the closed loop system.In general, Visual Servoing can be divided into three techniques: a) Image Based Visual Servoing (IBVS), b) Position Based Visual Servoing (PBVS), and c) Hybrid Visual Servoing (IBVS + PBVS), depending on the type of the available information that the visual system provides to the control law. In the IBVS method, the 2D image features are used for the calculation of control values, while in the PBVS method the 3D pose of a target is utilized In In This method intended to be used for inspection tasks, where the UAV is tolerant to small change in its orientation so that it keeps the object inside the camera's field of view. The proposed controller was able to integrate the homography matrix from the vision system and also to decouple the translation and orientation dynamics of the UAV. Some previous and complimentary works in this area have been also presented in The collaboration of two quadrotors for visionbased lifting of a specific payload, with unknown position has been presented in In In The aforementioned studies consist of a big part in the ongoing research regarding Visual Servoing for UAVs'. A brief overview shows that since the control scheme of the aerial platforms considers Euclidean coordinates PBVS, it is able to produce smooth trajectories of the camera. However, it can not control directly the motion of the features and it may lead the target outside of the Field of View. On the other hand, IBVS controls directly the motion of the features in the image plane, while keeping the target inside the Field of View and ignoring the Euclidean pose of the platform and producing unpredicted trajectories for the UAV with high risks for collisions of the target. Thus IBVS is heavily depending on additional sensors such as IMUs to improve pose control of the UAV. Regarding computational aspects, IBVS outperforms PBVS and requires less processing power. The above comparison is summarized in Table Vision Based ControlIn this section research on UAV control using visual information is described.In NavigationIn this section major research in the fields of visual localization and mapping, obstacle detection and target tracking is presented.Visual Localization and MappingThe scope of localization and mapping for an agent is the method to localize itself locally, estimate its state and build a 3D model of its surroundings by employing among others vision sensors The combination of a visual graph-SLAM, with a multiplicative EKF for GPS-denied navigation, has been presented in In In In Additionally, in In In In In Furthermore, in In The Smartcopter, a low cost and low weight UAV for autonomous GPS denied indoor flights, using a smart phone for a processing unit was presented in Furthermore In A novel mosaic-based simultaneous localization using mosaics as environment representations has been presented in In In the sequel, this information was further processed in order to build a map of the surrounding area. In the presented evaluation scenario, it was assumed that the quadrotor was able to move vertically up and down, without rotating around its axis. Finally, in In In In Visual Simultaneous Localization and Mapping for UAVs is still facing various challenges towards a global and efficient solution for large scale and long term operations. The fast dynamics of the UAVs pose new challenges that should be addressed in order to reach stable autonomous flights. Some of the encountered challenges are shown in Table Obstacle DetectionObstacle detection and avoidance capabilities of UAVs are essential towards autonomous navigation. This capability is of paramount importance in classical mobile robots, however, this is transformed into a huge necessity in the special case of autonomous aerial vehicles in order to implement algorithms that generate collision free paths, while significantly increasing the UAV's autonomy, especially in missions where there is no line of sight. Figure In In In In In The VISual Threat Awareness (VISTA) system, for passive stereo image based obstacle detection, for UAVs was presented in In In In Aerial Target TrackingIn this section object tracking approaches for UAVs' are highlighted. In short, object tracking can be divided into object detection and object following strategies using image sequences. The visual sensor is used to estimate the relative position and translational velocity between the UAV and the object. Moreover, the visual information along with data from other sensors is used as an input to the designed controller of the UAV, in order to track the target. The interest for this area is augmenting as this technology can be used for airborne surveillance, search and rescue missions or even navigation tasks. In Fig. In In In Fig. Moreover, in Towards aerial surveillance, Regarding the people detection part, the thermal image was processed with various cascaded Haar classifiers whilst simultaneously contours were extracted from the optical image. In addition, in In In In In In In In In In LGF and LOF are implemented to remove outliers from global and local feature correspondences and provide a reliable detection of the object.GuidanceThis section presents a collection studies towards autonomous exploration for UAVs' combining methods mentioned in previous sections. Elaborate control laws employed to adjust the position and attitude of the vehicle combining information from computer vision, image processing, path planning or other research fields. This topic is broad and contains many strategies that approach the problem from various aspects. Coordinating sensors with controllers on UAVs' can be used as a basis for other sophisticated applications and determine their performance. The rest of this section provides a brief overview of the contributions in this field.In In In In In DiscussionChallengesThis article provided an overview of the advances in vision based navigation, perception and control for unmanned aerial systems, where the major contributions in each category were enlisted. It is obvious that integrating visual sensors in the UAV ecosystem is a research field that attracts huge resources, but still lacks of solid experimental evaluation. For various reasons aerial vehicles can be considered as a challenging testbed for computer vision applications compared to conventional robots. The dimensions of the aircraft's state is usually larger from the ones of a mobile robot, while the image processing algorithms have to provide visual information robustly in real time and should be able to compensate for difficulties like rough changes in the image sequence and 3D information changes in visual servoing applications. Despite the fact that the computer vision society has developed elaborate SLAM algorithms for visual applications, the majority of them, cannot be utilized for UAV's directly due to limitations posed by their architecture and their processing power. More specifically aircrafts have a maximum limit in generating thrust in order to remain airborne, which restricts the available payload for sensing and computing power. The fast dynamics of aerial platforms demand minimum delays and noise compensation in state computations in order to avoid instabilities. Furthermore, it should be noted that unlike the case of ground vehicles, UAVs cannot just stop operating when there is great uncertainty in the state estimation, a fact that could generate incoherent control commands to the aerial vehicle and make it unstable. In case that the computational power is not enough to update the velocity and attitude in time or there is a hardware-mechanical failure, the UAV could have unpredictable behaviour, increase/decrease speed, oscillate and eventually crash. Computer vision algorithms should be able to respond very quickly to scene changes (dynamic scenery), a consequence from UAVs native ability to operate in various altitudes and orientations, which results in sudden appearance and disappearance of obstacles and targets. An important assumption that the majority of the presented contributions consider, is the fact that the vehicles fly in low speeds in order to compensate the fast scene alterations. In other words, dynamic scenery poses a significant problem to overcome. Another challenge in SLAM frameworks that should be taken into account is the fact that comparing to ground vehicles, aerial platforms cover large areas, meaning that they build huge maps that contain more information. Object tracking methods should be robust occlusions, image noise, vehicle disturbances and illumination variations while pursuing the target. As long as the target remains inside the field of view but it is either occluded from another object or is not clearly visible from the sensor, is crucial for the tracker to keep operating, to estimate the target's trajectory, recover the process and function in harmony with the UAV controllers. Therefore the need for further, highly sophisticated and robust control schemes exists, to optimally close the loop using visual information.Nowadays, the integration of computer vision applications on UAVs has past it's infancy and without any doubt there have been made huge steps towards understanding and approaching autonomous aircrafts. The subject of UAVs' control is a well studied field, since various position, attitude, and rate controllers have been already proposed, while currently there is a significantly large focus of the research community on this topic. Thus, it is important to establish a reliable link between vision algorithms and control theory to reach greater levels of autonomy. The research work presented in this review, indicates that some techniques are experimentally proved but many of visual servoing, SLAM and object tracking strategies for autonomous UAVs are not yet fully integrated in their navigation controllers, since the presented approaches either work under some assumptions in simple experimental tests and system simplifications or remain in the simulation stage. In addition, their performance is constantly evaluated and improved so more and more approaches are introduced. Therefore, seminal engineering work is essential to take the current state of the art a step further and evaluate their performance in actual flight tests. Another finding from this survey is the fact that most experimental trials, reported in the presented literature, were performed on unmanned vehicles with an increased payload for sensory systems and onboard processing units. Nonetheless, it is clear that current research is focused on miniature aerial vehicles that can operate indoors, outdoors and target infrastructure inspection and maintenance using their agile maneuvering capabilities. Finally, it should be highlighted that it was not feasible to perform adequate comparison on the presented algorithms due to the lack of proper benchmarking tools and metrics for navigation and guidance topics Camera SensorsThis review article is focused on research work towards vision based autonomous aerial vehicles. Therefore an important factor that should be considered is the visual system used in individual papers. Throughout the review process 3 visual sensor types have mainly been distinguished. A BlueFox monocular camera from MatrixVision, the VI sensor stereo camera from Skybotix and Asus Xtion Pro a RGB-D sensor. The aforementioned sensors cover a great range of applications depending on the individual requirements. Regarding the utilized hardware, this survey will not provide more information, since in the most of the referenced articles, the results are being discussed in relation to the hardware utilized.Future TrendsUAVs possess some powerful characteristics, which in the near future potentially could turn them into the pioneering elements in many applications. Characteristics like the versatile movement, combined with special features, like the lightweight chassis and the onboard sensors could open a world of possibilities and these are the reasons why UAVs have gained so much attention in research. Nowadays, the scientific community is focused in finding more efficient schemes for using visual servoing techniques, develop SLAM algorithms for online -accurate localization and detailed dense 3D reconstruction, propose novel path planning methods for obstacle free navigation and integrate aerial trackers, for real scenario indoors and outdoors applications. Moreover, nowadays many resources are distributed in visual-inertial state estimation to combine advantages from both research areas. The evolution of processing power on board aerial agents will open new horizons in the field and define reliable visual-inertial state estimation as the standard procedure and the basic element of every agent. Additionally, elaborate schemes for online mapping will be studied and refined for dynamic environments. Moreover, there is ongoing research on equipping UAVs with robotic arms/tools in order to extend their capabilities in aerial manipulation for various tasks like maintenance. The upcoming trends will examine floating base manipulators towards task completion in either single or collaborative manner. Operating an aerial vehicle with a manipulator is not a straightforward process and many challenges exist, like the compensation for the varying Center Of Gravity and the external disturbances from the interaction, capabilities that are posing demanding vision based tasks and that are expected to revolutionize the current utilization of UAVs. Finally, there is also great interest in cooperative operation of multiple aerial platforms and mostly for distributed solutions were the agents act individually exchanging information among them to fulfill specific constraints. Aerial robotic swarms is the future for many applications such as inspection, search and rescue missions as well as farming, transportation and mining processes. List of AcronymsFig. 1", "conclusions": "", "SDG": [16]}, "\u201cspiders_in_the_sky\u201d_user_perceptions_of_drones_privacy_and_security": {"name": "\"Spiders in the Sky\": User Perceptions of Drones, Privacy, and Security", "abstract": " Drones are increasingly being used for various purposes from recording footage in inaccessible areas to delivering packages. A rise in drone usage introduces privacy and security concerns about flying boundaries, what data drones collect in public and private spaces, and how that data is stored and disseminated. However, commercial and personal drone regulations focusing on privacy and security have been fairly minimal in the United States. To inform privacy and security guidelines for drone design and regulation, we need to understand users' perceptions about drones, privacy, and security. In this paper, we describe a laboratory study with 20 participants who interacted with a real or model drone to elicit user perceptions of privacy and security issues around drones. We present our results, discuss the implications of our work, and make recommendations to improve drone design and regulations that enhance individual privacy and security.", "keywords": "H.5.m. Information Interfaces and Presentation (e.g. HCI): Miscellaneous Drones,quadcopter,privacy,usable security,users", "introduction": "Drones are fast becoming popular in the commercial and noncommercial sectors for a variety of purposes such as providing Internet access, capturing media footage of remote locations, and delivering packages [4,22,. In fact, the Federal Aviation Agency (FAA) in the United States (US) forecasts the sales of commercial drones will reach 2.7 million by 2020 29] and civil drones production is predicted to rise from 2.6 to 10.9 billion USD by 2025 [14].[35]Yet, regulation around drones has been slow to follow  although drones affect individual privacy and security because they can record or injure people [33][8,32,17,. Even with recently introduced rules governing drone operation, the FAA only provides unspecified \"privacy guidelines\" regarding Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. CHI'17, May 06-11, 2017, Denver, CO, USA \u00a92017 ACM. ISBN 978-1-4503-4655-9/17/05...$15.00 DOI: http://dx.doi.org/10.1145/3025453.3025632 drone usage 39]. To better inform privacy and security enhancing legislation to regulate where drones can go and what data they can collect, store, and disseminate, we first need to understand how users currently perceive drones, their purposes, and capabilities.[16]In our work, we build on a growing number of studies on understanding the privacy issues around drones; mostly conducted in countries outside of the US [22,10, with the exception of two closely related studies in the US 2][19,. Our goal is to help create privacy and security enhancing designs for drones and policies. To achieve this goal, we posed the following research question: how do users feel drones affect their personal privacy and security expectations?39]To answer this question, we conducted a study with 20 users in a laboratory setting at the University of Maryland, College Park (US). Each user interacted with a real or a model of a drone to better tease out how drones affect their privacy and security concerns and their attitudes towards drone regulations. We have two main findings. First, we confirm that concerns raised by prior studies such as drones invading privacy through watching and spying  still hold a year later in another part of the US and provide new evidence of negative perceptions around drones such as fear of damage or injury and unwillingness to disclose personal information under drone surveillance.[39]Second, we provide further evidence confirming findings from Wang et al.  that drone design, including the color, size, speed, and noise of drones shapes peoples' perceptions of privacy and security. We also provide new evidence that a drone's form factor, wind, guard, movements, camera location and quality, data recording capabilities, and feedback lights also affect privacy and security perceptions. Based on our findings, we make three recommendations for improving regulations and creating drone designs to enhance peoples' sense of privacy and security around drones.[39]", "body": "Drones are fast becoming popular in the commercial and noncommercial sectors for a variety of purposes such as providing Internet access, capturing media footage of remote locations, and delivering packages Yet, regulation around drones has been slow to follow In our work, we build on a growing number of studies on understanding the privacy issues around drones; mostly conducted in countries outside of the US To answer this question, we conducted a study with 20 users in a laboratory setting at the University of Maryland, College Park (US). Each user interacted with a real or a model of a drone to better tease out how drones affect their privacy and security concerns and their attitudes towards drone regulations. We have two main findings. First, we confirm that concerns raised by prior studies such as drones invading privacy through watching and spying Second, we provide further evidence confirming findings from Wang et al. BACKGROUND Drones, Usage, and CapabilitiesDrones are defined as unmanned aerial vehicles (UAVs), remotely piloted aircraft systems, quadcopters, multicopters Drones and UsersThere is a growing body of research examining human-centered issues around drones. For instance, Cauchard et al., have investigated natural human-drone interactions including understanding metaphors and relationships that occur during an interaction with a drone as well as how users encode emotion based on how drones move Drones, Privacy, and SecurityAn increasing number of studies tackle how users view privacy and security issues around drones. Lidynia et al. conducted a survey of 200 users in Germany to investigate perceptions of drone usage In Australia, researchers surveyed 500 people to understand public perceptions of drones In the US, there are at least two studies examining user perceptions of privacy and security issues around drones. In 2014, Herron et al. Our studies differ from these former studies because we systematically elicited feedback on mental models of drones and current regulations through sketching and annotation exercises. In addition, unlike the former studies, particularly Wang et al. Drone RegulationLegal scholars have been examining privacy laws governing drone usage In 2016, the FAA released the small unmanned aerial vehicle rule METHODOLOGYRecruitingWe conducted our research between March and May 2016 using a study design built on prior studies of privacy and security issues around other emerging technologies (i.e. self-driving cars) We recruited via mailing lists of various academic departments including English Literature, Psychology, Computer Science, and Information Studies and received 53 responses. In our advertisements, we did not use the words 'privacy' or 'security' to avoid bias in recruiting. We also asked for participants without motor, visual, or hearing impairments so they could provide feedback on the drone's features and maneuver away from our drone in the case of malfunction. We sent each respondent an online survey to gather demographic information and experiences of personal drone usage. This survey included two behavioral scales to measure what people do to protect their privacy (General Caution and Technical Protection Scales (rating from 1=never to 5=always)) and an attitude based scale on Privacy Concern (rated from 1=not concerned at all to 5=extremely concerned) We received 43 completed survey responses and selected 20 users for our experimental study to balance for age and gender. Participants were randomly assigned to one of two groups that we balanced for gender, one interacting with a real drone and the other group with a model drone. Splitting participants into two groups allowed us to clearly isolate the features of a real drone (such as sound, wind, and speed) that affect privacy and security concerns as opposed to a model drone. We used a model drone over no drone to ensure that all our participants had a realistic mental model of a drone since the model we created was the same size, shape, and color as the real drone along with features to resemble the camera. We also mimicked the real drone's movements with the model as well.Each participant took part in an in-person session at our institution lasting 60-90 minutes. The session consisted of an initial interview with two exercises, an experimental session, and an exit interview. Two researchers facilitated the session, one leading the participant through the tasks of the session, and the other taking notes and assisting with maneuvering the drone or model drone for the experimental session. The entire session was audio and video-taped. Participants were compensated with a $30 Amazon gift card.Initial InterviewWe conducted the initial interview in a room where a participant could not see our drone or model drone to assess their opinions beforehand. We asked questions about participants' privacy and security habits in general, their knowledge of drones, and discussed their reactions to a scenario where they encountered a drone hovering over their house. We also asked them about their understanding and preferences concerning FAA policies around drones at the time of the experiment.Next, participants completed two exercises. In the first exercise, we asked them to review the registration certificate that one of the researchers obtained after registering our drone. This certificate is now obsolete since the introduction of new FAA rules that came into effect after the completion of our study Experimental TasksAfter the initial interview, participants were brought to the experimental room. Ten participants interacted with a real Parrot AR.Drone 2.0, a four-rotor quadricopter [27] seen in Figure Experimental Task 1: Drone Design and SurveillanceIn this task, we elicited participants' initial reactions to a drone and its basic capabilities, such as unmanned aerial flight and the ability to record and transmit data. Participants in the drone group were asked to sit in front of a TV monitor. We programmed the drone to start up and hover at this time so that participants would see the live stream of the video that the drone was taking of them on the monitor. The researchers did not explicitly say anything about or point out the drone recording. Our non-drone group participants were shown a task card illustrating the scenario and the fake drone was maneuvered into a hovering position.Experimental Task 2: Physical Safety and SecurityIn Task 2, we wanted to examine reactions to the drone physically approaching a participant and determine if participants would answer personal questions while being recorded by a drone or model drone. In the drone group, after participants observed the drone for five minutes in Task 1, the drone would slowly move forward toward the participant. As the drone moved toward the participant, the image displayed on the monitor enlarged as well. In the non-drone group, a researcher moved the model drone toward the participant but there was no live feed of video. While the drone or model drone moved forward, we asked each participant three personal questions regarding their grade point average (GPA), income, and household size. We also asked how comfortable they felt answering these questions in the given situation.Experimental Task 3: Researcher Controls DroneIn Task 3, we elicited reactions to another person controlling a drone and the basic drone flight actions. In the drone group, a researcher controlled the real drone via the Parrot A.R. Free Flight application on an iPad. For the non-drone group, the researcher maneuvered the model drone to perform the actions instead. The actions for both groups were moving the drone left to right, up and down, and forward and back in the direction of the participant and back to its origin point. Each movement was performed continuously twice.Experimental Task 4: Participant Controls DroneIn the fourth task, we aimed to gather feedback about the experience of controlling a drone. We chose not to give the participants direct control of the drone due to safety. Drones can lose connectivity with the controller and could cause accidents. We also wanted to minimize the learning phase needed to control the drone, especially for those who had never flown remotely piloted devices.We created an iPad application with a web server coded with node JS, with a front-end application written in HTML, CSS and jQuery, also utilizing the \"node-ar-drone\" library The six actions were: moving the drone left to right, rushing forward and back toward the participants, moving vertically up and down, rushing vertically up and down, changing the drone's LED lights from green to red and continuously blinking red, and flipping the drone in a 360 degree on the vertical axes. Each movement was performed continuously except for the flips which were done twice. Participants in the non-drone group utilized the same iPad application as the drone group. When they pressed an action button, one of the researchers maneuvered the model drone to perform the relevant action.Experimental Task 5: Data, Storage, and TransmissionIn the final task, participants in the drone group were shown the footage taken of them throughout the experiment. We did not show the entire footage, just clips of the experiment until the participant decided to move on from viewing it. Participants in the non-drone group were shown prerecorded footage of the researchers because we did not want to record them using any other devices. Also, since our focus on this task was to see participants reactions to seeing recorded footage and the angle of the recording as opposed to the content of the recording, we felt these conditions were equivalent. We asked participants if they knew they were being recorded, how they felt about the drone's data transmission and storage capabilities, and who would own the data taken by the drone.Exit InterviewFollowing the experiment, we conducted an exit interview with each participant and asked them to describe what they were thinking in each of the experimental tasks, one by one. We also asked them about their attitudes towards privacy, security, and drones again to see if they had any differing views after encountering a drone or model drone. The end of the exit interview concluded the session and participants were thanked and given their compensation.AnalysisWe transcribed the audio files for each participant's session and one researcher performed a thematic inductive analysis FINDINGSWe first summarize participants' demographic information and attitudes towards privacy and security in general and around drones. Next, we describe our two main findings: negative concerns around drones, privacy, and security; and how drone design affects privacy and security perceptions.Participant DemographicsThe median age of our 20 participants was 23 years with a range of 19-56 years. There were 11 females and nine males, with an even gender split in the drone group, and females outnumbering males six to four in the non-drone group. The age range in the groups was similar. 19/20 participants had never operated a drone before with one being unsure. Nine participants had never seen a drone in person prior to the study.Privacy and Security Attitudes and BehaviorsOur participants scored a median of 3.3 on the Privacy Concern scale indicating they had about average concerns in terms of privacy attitudes. Participants also had a median score of 2.5 on the general caution scale and a median score of 3.3 on the technical protection scale indicating they were about below to average in terms of privacy related behaviors.On the SeBIS scale, our participants had a median score of 4.25 on device securement, a median score of 3 on password generation, a median score of 3 in proactive awareness, and a median score of 3.33 in software updating. Finally, participants had a median score of 3.49 on the entire SeBIS scale, indicating that they were about average for security behaviors with an above average rating for device securement.Negative Perceptions of DronesWe describe how participants reported more in-depth negative aspects of drones than positives, which stands in contrast to findings in prior works.Drones Still Seen as Privacy InvasiveParticipants told us what privacy means to them and that drones are privacy invasive. Similar to previous studies Privacy Definitions: Most of drone group participants (7/10) spoke about privacy having to do with the control over anyone interfering with private space. D1's quote exemplifies this concern: \"It's the knowledge that things that you would like to keep to yourself, like your backyard, knowing that nobody can fly over it. It would be kept private if you wanted it to be kept private. Unless you invited somebody over.\". The majority of non-drone group participants (6/10) spoke about privacy as having information transmitted only to the intended audience.Spying Drones: When asked about drones in relation to privacy and security, participants, more than half of all participants (11/20) mentioned the feeling of being watched around a drone, describing \"the window\" as \"where the invasion starts to affect your privacy\" (D7). Most participants (five drone group and six non-drone group) were also concerned about being spied on by a drone because of its recording capability. In a quote that illustrates this point, ND2 explained how the drone's recording capability could be used for spying: \"Maybe you want to record someone's private life, or some stars, some famous people\". For this reason, our participants felt that drones should be prevented from entering restricted areas such as government buildings or \"to spy on others\" (ND3).Recording Without Consent: 80% of our participants mentioned a concern for the potential to be recorded either in public or in their own homes through a window and not knowing that the recording was happening. D7 best reflects what participants told us: \"You can pull up a wall around your house if you want some privacy, but somebody can raise a drone higher, move the drone across the fence and the wall and still access that.\". Others also mentioned an issue of not knowing where the drone was looking. In one exemplifying quote, D3 said, \"People have privacy concerns because you can't really see where the drone's looking\".To mitigate these concerns, before seeing the drone or model drone, participants wondered if drones could indicate when they are taking pictures. For instance, a typical quote we heard was similar to that of D2: \"Maybe the drone could have a light or something that indicates that it's taking a picture to make it more safe and make sure that people's privacy is protected, maybe something like an indicator. Or maybe some technology that people could have that could tell them that, \"Oh, a drone is in their vicinity,\" and then taking pictures\". Overall, participants were concerned for their privacy and wanted feedback to know when they were being recorded.Fear of Damage or Injury From DronesUnlike in previous studies Injury, Weaponization, and Air Traffic: The majority of participants (15/20) worried about physical harm or death because of drones malfunctioning and falling out of the sky . Some heard stories on the news about drones causing injury when they fell. Others just surmised damage or injury from collisions could be an issue before they saw the drone or model for the first time. D3 explained the concern that most raised: \"I really don't want people to get hit with them because I don't even know what they look like. I assume they have blades maybe somewhere. I wouldn't want just random accidents and people getting hit all them time because people fly them too close.\".A large portion of participants (12/20) also worried about the drone's capability to carry items and raised the concern of not being able to tell if the \"payload\" was a camera, or weapon such as a bomb or gun. Some participants worried that drone owners might purposefully modify their drones to cause injury to others as D5 emphasized:\"The one thing I could think of would be just making sure you don't modify it to use anything that could hurt somebody, such as a weapon or something.\".Over half of our participants (11/20) were concerned with damage and interference with air traffic and physical safety to planes. D1 elaborated: \"My biggest concern is air traffic safety, air traffic control safety with drones. Until Amazon gets authorization to start delivering packages and things like they want to. I'm not too worried about security and privacy at this point. I'm more concerned about safety.\". Participants' concerns were exacerbated by the knowledge that the drone could venture out of human vision which would increase the potential for error.Harming Wildlife: In addition to injuring people, all of our participants' were concerned about drones disturbing nature. In an illustrative quote, D7 said: \"I think in urban areas, the impact to nature will be less, but I don't want to see drones in the Amazon, unless it's used for research purposes or something that will do good to the environment.\" In particular, participants did not want drones to harm endangered species or to become \"trash\" and impact the environment.Accountability For Drone Owners and Consent For RecordingSimilar to findings from prior work in New York However, participants wondered about consent for drone deliveries in residential areas. In a quote reflecting the participants' attitudes, D9 explained: \"As a consumer, I'd make my consent that my stuff can be delivered by a drone, but the drone still flies through a community and it occupies secondary space. Taking that consent from everyone is difficult. The government should intervene and they should have some laws regarding this.\".Additionally, after seeing the drone being controlled and controlling the drone, a fifth of our participants (three drone group and one non-drone group) raised concerns around the the distance between owners and those being recorded. The drone group participants mentioned that the distance created between the owner and the drone was likely to cause a lack of accountability on the part of the owner. Illustrating the sentiments we heard, D6 described this physical distance from the drone to the owner as creating a \"detachment\" which can make it easier for owners to \"project own sort of dysfunction onto\" the drone.The six non-drone group participants also mentioned concerns about the drone owner being anonymous to them, suggesting that this is a concern that emerges despite not experiencing the physicality of the drone. These participants told us that their privacy and security concerns escalated at the thought of not being able to see the drone owner, since that meant anyone could take a recording with a drone without physically being there. Participants were also concerned with not knowing when they were being recorded and where the data was being transmitting to. When asked to compare the drone recording to a cell phone, participants felt that with a cellphone they could still hide their face, or tell someone to stop or delete a recording. In contrast, they felt that could not easily provide consent to record or prevent a drone recording them.Distance between Drones and People: Participants felt the allowable distance between a drone and others depended on whether it was carrying a bomb or camera; if it was carrying weapons, the distance would have to be greater. Participants also mentioned that enforcing a strict distance between a drone and people for safety may not work if there are many drones in an area at once. In an extreme example, one of our participants (D2) re-imagined a scenario with Princess Diana being overwhelmed by paparazzi of a\"swarm of drones\", stating this would be \"creepy\" and potentially dangerous.To mitigate these concerns, several participants recommended a separation of space for drones and people. For instance, participants recommended commercial drones fly at a different altitude to better separate them out from people and to minimize crashes into other drones. In a typical example, D7 recommended, \"designated spaces\" like a \"road\" for drones, and suggested using highways so that even if drones fell, people would be protected in their cars. A separation of space, participants felt, could also prevent drone to drone collisions as captured by this quote from ND5: \"I think the biggest thing is going to be drones bumping into other drones, either deliberately or accidentally. If there were to be any legislation, I think that that is what should be addressed. Like the FAA has rules about how close things can be\". Because of concerns about physical security, participants also mentioned limiting the number of drones in a vicinity to minimize the risks of drones.Distance from Buildings: Unlike in previous work Distance from Wildlife: When asked in the exit interview, all participants quickly opposed drones being near wildlife. The main reasons were that the introduction of a mechanical object in nature could change animal behavior, or that drones could cause harm to, or be harmed by animals. ND9's quote summarized what we heard: \"I really am concerned about wildlife. From my experience, I don't really think that animals would respond that well to them. They'd probably either try to attack them and people would get angry.\". Many people were particularly concerned about birds and other animals who may be injured by drones and their \"blades\" flying around.Disclosing Personal Information Under Drone SurveillanceIn our experiment, it became clear that participants who interacted with a real drone were less comfortable disclosing information under of drone surveillance. For instance, half of our drone group participants felt answering questions in front of the drone while it was recording was \"unnerving\" (D5) and as expressed by D4:\"I think I'm a suspect. It is not a very threatening situation, but I feel like I need to pay attention\". The other half of our drone group participants were less concerned about answering private questions in front of the drone, because the drone was so loud that they felt it could not hear them. Four of these same participants felt that if the drone was quieter and they did not know who was operating the device, that they would be more cautious.In contrast, the majority of non-drone group (8/10) participants were comfortable in giving out personal information in front of the model drone. The two who were not comfortable said that they would be \"very uncomfortable if [he didn't] know who's controlling it\" (ND6) and because the recordings could \"be shown to anyone at any point of time. There is no control with me to actually stop that information.\" (ND10). Participants also compared a drone recording them and somebody taking a picture as ND5 explained:\"I don't really think of anybody having any ulterior motives with a random photo where this is close to me and hovering around me and listening to me.\" For this reason, our participants often reiterated that they would question the motive of the drone owner.Positive Perceptions of Drones Were Less Pronounced Interestingly, unlike in previous studies Drone Design Affects Privacy and Security PerceptionsOur second major finding is that there are many aspects of the drone itself, such as the color, size, and sound that affect privacy and security concerns supporting findings from previous studies Drone Attributes Make Drones Appear ThreateningForm Factor: Upon entering the experimental room, some participants immediately noticed and commented on the hovering drone whereas others observed until we asked more questions about the drone. A few participants who had never seen a drone in person before felt that the drone was \"intimidating\" (D4) depending on its proximity. All of the non-drone group felt discomfort because the drone was flying too close. Six of the drone group participants commented on the drone design saying the drone's form did not inspire trust. D7 elaborated: \"It doesn't make me trust it. It's black, maybe if it was a colorful coat thing. It doesn't come off as friendly right now.\".The non-drone group participants also felt that the drone design did not make them feel comfortable and that as one participant, ND4, put it, \"I think they look like spiders in the air and I think that it would just be a recipe for disaster\" when speaking about widespread drone usage. A few participants also felt that drones reminded them of attacking and military purposes. For example, D6 compared the drone to an object that reminded her of the movie \"Terminator\" and \"military things\". These findings stand in contrast to prior work Color: At least several participants (four drone group and three non-drone group) expressed similar concerns about the color of the drone evoking feelings of \"unfriendliness\". These participants were concerned about the drone being a monochrome black or dark color. To make the personal drone appear less likely to \"attack\" (ND6), participants suggested that drones should be painted in brighter colors, have logos, or writing on them. Moreover, participants suggested that drones should have hazard lights so that they can be easily distinguished from the background and allow people to react to them accordingly. At least three participants (two drone group and one non-drone group ) also mentioned that drones used by commercial entities should be friendly looking and have a \"logo of the company on it.\". Size, Stealth, and Safety: Participants in both groups said that the drone was bigger than what they had in mind which affected their perceptions of privacy and security. Four participants in the drone group and six non-drone group participants wondered if bigger drones might be concealing something inside them. Four participants from both groups mentioned that drones should also not be too small so as to become too stealthy. In one example given to us, a participant worried that small drones would be able to sneak into air vents and other restricted places or buildings \"that wouldn't normally be accessible to someone\" (ND9). All of the non-drone group participants also thought that drones smaller than the Parrot AR would be preferable for reasons such as being less likely to injure people.Sound: All of the drone group participants had a negative reaction to the sound that the drone made. Four participants felt the sound was not \"inviting\", \"loud\", \"noisy\", and \"scary\". These and other participants wanted the drone to sound less threatening so that it could approach others in a friendlier manner. Although non-drone group participants did not hear the drone, two mentioned that there would likely be sound from the drone and commented negatively on it.Wind: Seven drone group participants commented negatively on the breeze that they felt from the drone while in flight. The most concerned response was from D3, who described the sound of the wind as making the drone feel dangerous:\"All the air that it creates is weird. It feels powerful in a way because it's moving so much air that, it feels like if I stick my hand in that, it's going to chop it off.\". Our participants felt that the noise and wind combined could make the drone seem very \"threatening\" for some people. Only two drone group participants did not notice the wind from the drone at all. The participants who interacted with the model drone did not mention the drone producing wind.More than two thirds of the drone group participants felt more strongly about restrictions for personal drone usage after the experiment. Most were worried about limiting the sound, wind and speed of drones and wanted more stringent licensing requirements because drones could injure people when flying fast, scare or make people nervous if they fly too close to them, or if they fly in large numbers or in small spaces.Guard: Although most of our participants viewed the drone as threatening, after seeing a drone for the first time in the experiment, the majority of drone group participants (8/10) were surprised it was made of soft materials and that it had a guard around it. Many felt that the guard was an important design factor for preserving physical safety. These participants felt more secure about drones hitting people or buildings since they felt it would bounce off and not do too much damage. The majority of non-drone group participants (7/10) also thought that the drone was safer with the guard since the \"propellers may hurt people\" (ND2).Advantages of Drone Sound and WindEven though most participants commented on the sound and wind of drones as a negative, these attributes were also viewed as preserving privacy because they make drones less \"stealthy\" (D7). Six of drone group participants and one of the non-drone group appreciated the fact that the sound of the drone helped people to identify its presence as explained by ND5: \"I was at a wedding way out in the middle of nowhere, about two hours east of Seattle, so seriously in the mountains, seriously in the country. There was a drone flying over most of the time. I was trying to figure out whose it was, where it came from, and it was an annoying noise\". In another example, D4 said she would recognize a drone is nearby and recording because \"the drone is noisy so [she knows] that it is near.\".Drone Movements and Physical SecurityOur findings suggest that movements of a drone can significantly impact participants' concerns about their physical safety and about drones invading private space. Half of the drone group were concerned about the stability of the drone and commented that the way the drone moved exacerbated the feeling that it was an unfriendly and unsafe device: \"When it started, I think that it's a bit unstable and I was a bit worried about where it would go. Now, also, it's not quite stable and it feels also that it might do something.\" (D9). In another typical example, participant (D4) felt that when the drone was moving from left to right repeatedly in a very stable manner, it looked like a boxer performing a \"threatening action\".Participants also became more concerned when the drone sped up. In the experiment, when the drone rushed up and down, a few drone group participants worried that it would hit the ceiling or floor. During these movements, six drone group participants also became more aware of the drone's wind, which made them all feel as though they wanted more distance from the drone (D4). Participants also often felt that the drone moved in unusual ways that caused concern about it malfunctioning. For instance, when the drone flipped, half of the drone group participants reacted negatively with concerns that the flip movement was not a native function or ability, was scary, or was a malfunction. On the other hand, seven non-drone group participants found it \"amazing\" (ND1) or unsurprising. Similar to the participants interacting with the real drone, the three non-drone group participants who were concerned about the flip being a malfunction or in physical danger chose to abort or land the model drone.The direction and speed of the drone's movements also made participants wary for their physical safely. For instance, half the drone group participants expressed physical safety concerns when the drone rushed forward. Like others, D6 said that he felt:\"Concerned for our safety. All three of us. I was worried that it would hit us and then I was like, \"Oh, they look okay. They don't look alarmed so it's not going to come at us.\". For him and other participants, looking to the researchers reaction helped them stay calm despite the drone's fast movements towards them.Non-drone group participants reacted similarly. The majority (8/10) reacted to the drone rushing forward by pressing the \"abort\", \"land\", or \"stop\" buttons. Participant reactions were caused by concerns about personal space or physical safety. The general attitude toward the rush was that \"it was invading [my] personal space\" (ND3) and some \"took it as a sign of aggression\" (ND9). The concern then extended into not knowing \"when it will stop.\" (ND2, ND10).In both groups, participants who did not react negatively to the drone's rushing movements or flip were more familiar with the technology or felt that they were safe because they were in a laboratory setting. However, despite feeling safe the two drone group participants who did not react said they had backup plans to hit the drone down if needed and looked to the researchers reactions to decide how to react. For example, D5 said \"If the thing had gotten any closer, I probably would have swatted at it. But the fact that you didn't move made me think I'm okay with it.\"Clandestine Data Recording Without Proper FeedbackParticipants told us that how the drone is designed for recording, recording quality, data storage, and feedback on recording affects how they feel about privacy and security.Camera Location: As reported in previous studies Six drone group participants felt that the discomfort at being under drone surveillance was because in comparison to a video camera, the drone's camera was smaller and \"obscure\", and less obvious (D10). Other participants felt that the drone's larger size made it obvious when it was recording since the drone itself is hard to hide \"in such way that it's not going to draw attention to itself.\". (D5).In general, drone group participants felt that the drone recording was very clandestine, similar to a \"hidden camera\" (D4), since there was no indication of recording. Participants again expressed feelings of being watched despite its inability to be discrete due to its size and flying ability.The non-drone group also mentioned the invisibility of the camera, and the camera's limited view of its surroundings. For instance, some participants assumed the camera usually sits near the rotors but the camera on our drone was protruding out in the front. Others also talked about not knowing whether the camera was oriented in their direction since it is not clearly marked. ND10 explained how drones compared to his GoPro camera: \"I can know that [the GoPro] is pointing at me. I can know that it is actually looking the way the [operator] is looking. A drone, I'm not sure whether it is looking from the top, whether it is looking from the bottom, whether it is not even looking at me. If it is a drone, then I'm not sure where it is recording, at what angle it is recording.\" Participants also indicated a limited view of what the operator can see remotely meaning that the drone could have potential crashes.Camera Quality: Participants' privacy and security concerns were also related to the quality of the drone's camera. Four of our drone group participants were surprised by the camera quality. D3 \"thought it'd be grainy.\" At least two felt the camera and video quality were not as advanced as expected. Another concern raised by four drone group participants was about the drone having zooming capabilities that could compromise privacy as D7 explained :\"If it has those things, then it can be even more invasive. You can lift it up, and if you're in the National Mall, just zoom into the White House.\". On the other hand, at least four drone group participants felt less concerned about privacy after the experiment because they felt that the drones could not zoom in from high altitudes. Four out of ten non-drone group participants also brought up concerns regarding drone camera zoom capabilities. All participants wanted to limit the camera quality to mitigate privacy and security concerns around drones' recording data.Data Recording and Storage Vulnerability: We showed participants the footage captured throughout the experiment on our laptop. Participants had mixed emotions. Most of the drone group (8/10) and non-drone group (6/10) had concerns about either the way that the drone stored information or the recording process itself. At least two drone group participants who talked about the storage of the data mentioned concern about the ease of access of the data. In typical examples, D8 felt this could make distribution of the footage easier which could be a good or bad thing. D6 elaborated that it was scary when the data recorded by the drones was \"Not being clear in terms of how it's regulated, where it's being stored, who's using it, how vulnerable it is.\". Some liked that the recordings could be processed at a later time to when the data was recorded.Interestingly, participants noticed that the drone did not record sound in our study. The three drone group participants who noticed the lack of sound in the recording attributed it to the noise produced by the drone rendering a mic useless. Some felt that a sensitive mic could reconstruct the audio despite the noise. This elevated concerns about drones invading privacy.Non-drone group participants who were not concerned about the drone's data capture and storage were either familiar with the process, or thought it made sense since it happens with other recording devices. Four drone group and the majority of non-drone group (6/10) participants said the owner of the drone owns the footage, regardless of who was captured in media. Only one non-drone group participant said that that the owner of the drone should not have the rights to the data. Three drone group and only two non-drone group participants said that the subjects either own or should have a say in the ownership and use of the data. The rest of the drone group either did not know (2/10) or said that the media belongs to the registrant of the drone, not necessarily the owner.Drone Feedback Lights: Compounding privacy concerns, in our experiment, we noted that not all participants saw the drone's feedback lights at first or knew how to decipher them. Half of the drone group participants did not notice blinking lights in Task 5 and the other half noticed some or all of the lights changing colors from red to green. Participants who did see the lights in general did not know why the lights were changing. Three drone group participants thought that the lights indicated a coding error or malfunction. Others wondered if the lights indicated direction of travel. Participants felt that the lights were a good indicator if something was not right with the drone but agreed that the meaning of the lights were not easy to decipher without a manual.The non-drone group participants could not see the feedback lights blinking but were shown the task card illustrating the concept. These participants had similar concerns to those who interacted with the real drone. Most of the non-drone group participants (7/10 ) were not concerned with the blinking lights, and most thought that it was \"changing colors\" (ND2). Only three non-drone group participants hypothesized that blinking lights signaled an issue with the drone, that it was tracking something, or that there was a problem within its vicinity. Thus, the lack of visibility of drone feedback increased privacy and security concerns about knowing when a drone was recording, in violation of regulations, or malfunctioning.DISCUSSIONOur findings suggest that participants' perceptions of privacy and security issues around drones are similar to those indicated in previous studies We make three recommendations based on our findings: geo-fencing, creating designated spaces for drones, and enhancing the design of drones to mitigate privacy and security concerns. We also outline study limitations and future work.Geo-Fencing Using Existing InfrastructuresOur participants did not want drones to be near people, buildings, other drones, and wildlife to maintain their privacy and physical security. While current FAA regulations restrict drone flight to 5 miles away from airports, there is nothing preventing an operator from overstepping these rules aside from \"good faith\". To properly mitigate these privacy and security concerns, we recommend that geo-fencing Another route to make effective geo-fences against drones is to exploit existing infrastructures such as home networks to help create low-cost virtual drone barriers, given recent developments in Wi-Fi positioning Designated Spaces for DronesOur participants were also worried about widespread drone usage and having multiple drones in an area potentially causing harm from drone to drone collisions, and to protect their privacy from drone recordings. Participants also voiced concerns about not being able to easily identify an out of sight drone operator as raised in prior works Drone Design and Privacy and Security PerceptionsOur findings between the two study groups were similar but we found that the drone group perceived sound and wind as threatening drone attributes more than the non-drone group.The drone group was also more concerned about the uncertain location of the camera, especially when asked to disclose personal information. By comparison, non-drone participants were more comfortable disclosing their personal information around the model drone. Both groups also commented on sound and wind being privacy enhancers because of hearing a drone before seeing it. These findings suggest that future drone designs should explore the use of non-visual cues and recording feedback to enhance users' privacy and security.Towards \"Friendly\" DronesOur study also showed that participants perceived drones as threatening and unsafe and that participants were concerned about potential payload due to the drone's color, shape, and size. These findings suggest that a drone's appearance and features can be manipulated to enhance perceived privacy and security and to make people more wary of certain drones. We suggest that drone designers carefully consider the use of colors, logos, and decorations to make drones \"friendly\", or \"unfriendly\" in cases where it is necessary for people to keep their distance from certain drones. Drone sizes could be balanced to ensure that they are not perceived as too stealthy or concealing dangerous items. Some of our participants even suggested that circular drones would be less threatening, which aligns with previous work in human robot interaction suggesting that the shape of the robot can influence human perceptions Limitations and Future WorkOur study is limited to a small sample of the student population at our institution with about average privacy and security concerns and behavior intentions. Moreover, our study exposed users to drones indoors which may have amplified the effects of sound and wind, although participants mentioned these issues even when talking about drones they encountered outdoors. Future work should survey a larger more representative sample of the US population to see how our findings generalize. Also, we did not ask participants about or demonstrate an exhaustive set of scenarios of drone usage or drone types, or use a drone outdoors. These situations can be addressed in future studies.CONCLUSION", "conclusions": "We conducted a laboratory study with 20 university participants where users interacted with a drone or model drone to elicit privacy and security concerns around drones and drone regulation in the US. We found similar privacy and security concerns exist around drones to prior studies but that users also hold many negative perceptions around drones that were not covered in prior works. We also found that the drone design itself shapes privacy and security concerns and attitudes towards drone regulation. We recommend investigating how to make effective use of geo-fences, designated spaces for drones, and drone design to enhance positive drone perceptions and better protect users' privacy and security from drones. Future work will tackle these open questions.", "SDG": [16]}, "better_nicer_clearer_fairer_a_critical_assessment_of_the_movement_for_ethical_artificial_intelligence_and_machine_learning": {"name": "Better, Nicer, Clearer, Fairer: A Critical Assessment of the Movement for Ethical Artificial Intelligence and Machine Learning", "abstract": " the sociology of business ethics, we uncover the grounding assumptions and terms of debate that make some conversations about ethical design possible while forestalling alternative visions. Vision statements for ethical AI/ML co-opt the language of some critics, folding them into a limited, technologically deterministic, expert-driven view of what ethical AI/ML means and how it might work.", "keywords": "", "introduction": "Spurred in part by advances in machine learning, algorithmic processes and predictive analytics are being applied to domains from criminal justice  to consumer finance [1]. In response, computer scientists, engineers, and designers-as well as executives, philosophers, social scientists, regulators, lawyers, and activists-have proposed guidelines for the responsible development, deployment, and regulation of artificial intelligence and machine learning systems (AI/ML). All part of a broader debate over how, where, and why these technologies are integrated into political, economic, and social structures. Still 'in the making' [2], the ethics of these systems are 'up for grabs. [3] These debates present an opportunity to assess emergent approaches to incorporating ethics and values into AI/ML. In this paper, we examine highprofile \"values statements\" or manifestos ' that endorse principles of ethical design as a response to social anxieties surrounding AI/ML. Because widely applied AI/ML and their attendant ethical debates are relatively new, we are interested in how values statements work to construct a shared ethical framea seemingly common-sense yet hegemonic understanding of an 'ethics' of AI/ML, how those ethics should be adjudicated, and whose voices count in the process [4].[5]We proceed in four stages. First, we situate a number of high-profile values statements in the broader context of recent academic work on ethical AI/ML. Second, we review our theoretical background: joining the literature in values in design with the sociology of business ethics. Third, we describe our methods and our sample. Finally, we present our findings, identifying seven core themes: universal concerns, objectively measured; expert oversight; values-driven determinism; design as locus of ethical scrutiny; better building; stakeholder-driven legitimacy; and machine translation. Combined, these themes inform what Gabriel Abend  terms the 'moral background' of these values statements: the grounding assumptions and terms of debate that make conversations around ethics and AI/ML possible in the first place.[6]We draw two broad conclusions. First, these statements offer a deterministic vision of AI/ML, the ethics of which are best addressed through technical and design expertise. There is little sense from these documents that AI/ML can be limited or constrained (a feature perhaps stemming from the involvement of AI companies). Second, the ethical design parameters suggested by these statements share some of the processual elements and contextual framing of critical methodologies in science and technology studies (STS) and information science. However, this critical scholarship's explicit focus on normative ends devoted to social justice or equitable human flourishing is often missing from these vision statements. The \"moral background\" of ethical AI/ML discussions is closer to conventional business ethics than more radical traditions of social and political justice active today, such as prison abolitionism or workplace democracy.", "body": "Spurred in part by advances in machine learning, algorithmic processes and predictive analytics are being applied to domains from criminal justice We proceed in four stages. First, we situate a number of high-profile values statements in the broader context of recent academic work on ethical AI/ML. Second, we review our theoretical background: joining the literature in values in design with the sociology of business ethics. Third, we describe our methods and our sample. Finally, we present our findings, identifying seven core themes: universal concerns, objectively measured; expert oversight; values-driven determinism; design as locus of ethical scrutiny; better building; stakeholder-driven legitimacy; and machine translation. Combined, these themes inform what Gabriel Abend We draw two broad conclusions. First, these statements offer a deterministic vision of AI/ML, the ethics of which are best addressed through technical and design expertise. There is little sense from these documents that AI/ML can be limited or constrained (a feature perhaps stemming from the involvement of AI companies). Second, the ethical design parameters suggested by these statements share some of the processual elements and contextual framing of critical methodologies in science and technology studies (STS) and information science. However, this critical scholarship's explicit focus on normative ends devoted to social justice or equitable human flourishing is often missing from these vision statements. The \"moral background\" of ethical AI/ML discussions is closer to conventional business ethics than more radical traditions of social and political justice active today, such as prison abolitionism or workplace democracy.ContextFrom 2014 to 2016, President Obama and the White House Office of Science and Technology Policy identified \"big data\" as both a strategic priority and an area of legal and ethical concern. In a series of reports Subsequent scholarly work in AI/ML ethics has been more or less aligned with the vision outlined in a series of reports by the Obama White House. Popular and academic texts Though many of these efforts share common origins and aims, tensions have emerged. For example, debates centered on racial representation within AI/ML systems have surfaced unresolved issues around the meaning and scope of inclusion. Spurred in part by research demonstrating facial recognition technology's difficulty recognizing people of color (especially black people Other tensions speak to the fractured relationships between different research communities. Computer scientist Cathy O'Neil, for instance, publicly bemoans a lack of academic attention to fairness, accountability and transparency in algorithmic systems-and especially a perceived lack of academic efforts to inform policymakers and regulators Despite-or perhaps because of-these unresolved tensions, many high-profile companies, organizations, and communities have seized on public conversations as an opportunity to signal their commitment to ethics. Many of these efforts have involved the drafting of \"values statements\" articulating more or less robust visions for the ethical or responsible development of AI/ML. Such statements prompt more questions than answers. By setting the tone for conversations around ethics and AI/ML, these statements simultaneously erase existing tensions while producing new conflicts. They stake a claim for the territory of ethical AI/ML, define how we should debate them, and suggest how we should put them into practice-smoothing out otherwise fraught ethical terrain.Theoretical backgroundOur critical evaluation of high-profile values statements for ethical AI/ML explores the assumptions and the terms of debate making such statements possible in the first place-what Abend Values in technology designScholars have long paid attention to the relationship between human values and technology design (hereafter \"VID\" or \"values in design\"). These include Value-Sensitive Design (VSD) Identifying values is, of course, only the beginning of a principled design inquiry, a first step that opens the door to consideration of alternative courses of action and their potential outcomes Business and professional ethicsRecognizing values in design is one thing; tracing their origins is another. In addition to questions of values, there is a question of how specific modes of moral reasoning become embedded in specific ways of designing technologies. These are epistemological questions about different value systems and ontological questions of what ethics are and how they work. With assistance from the sociology of ethics, they can also become empirical questions for information science.The study of practical ethics as applied in businesses and professions is a long one Abend's insights resonate with the recent history of backlash against major Silicon Valley companies where cycles of malfeasance and apology from firms like Facebook are routine. The claims of business ethicists are familiar here-for example, that unethical behavior leads to business disasters. But popularizing and institutionalizing these claims, which are far from the only way of reasoning about business ethics, require what Abend calls a moral background-a specific arrangement of second-order social assumptions about what ethics mean and how they work, above first-order claims about ethical norms or behaviors. This is where the real social action is for Abend, and it is where our research focuses.MethodThe values attached to AI/ML work are still taking shape Frame analysisFrame analysis is ideally suited to tracing the implicit terms of this debate. Developed as a method in communications research, frame analysis investigates messages not just for their denotative content but also for processes whereby certain elements are selected for salience or erased We analyzed seven significant public statements meant to guide the development, implementation, and regulation of AI and ML. Within them, we looked for:\u00a7 common themes about the construction of ethical claims and their grounding assumptions; \u00a7 the logic undergirding these themes; and \u00a7 divergences among the sample (e.g., placeswhere first-order norms, or second-order assumptions differed). Using inductive coding DataOur sample focused on recent public statements of ethical principles issued by independent institutions. Most of these organizations are made up of technologists and firms active in the field of AI/ML. These \"envisioning bodies\" were usually convened for the express purpose of circulating their proposed principles and conducting further research, convenings, public education, and lobbying to support their application and dissemination.The oldest vision statement we examined was from December 2015 (OpenAI); the newest was from May 2018 (The Toronto Declaration). Though, as a practical matter, our sample ends in May 2018, additional high-profile statements have already been posted (including, notably, Google's AI principles). That these statements continue to emerge is a testament to their central role in debates over ethical AI/ML.Our sample includes the following institutions:The Partnership on AI to Benefit People and Society Membership: Nonprofit cooperative effort between Amazon, Apple, DeepMind, Google, Facebook, IBM and Microsoft, with second-tier partners from higher education, civil rights groups, and other industry partners.The FindingsDigital technologies are legitimate objects of ethical concern-and human values are embedded in their design. This argument may seem obvious to VID scholars, but it is no small concession in the face of powerful discourses of technological neutrality pervasive in Silicon Valley and elsewhere However, the fact the Montreal Declaration can assume its interlocutors will accept the argument developers should be creating \"moral machines\" signals an emerging acceptance of design as a legitimate site for ethical debate, rather than something that can be delegated to other domains (e.g., law). This initial assumption grounds the seven other core themes that join to form the moral background of 'ethical design': universal concerns, objectively measured; expert oversight; values-driven determinism; design as locus of ethical scrutiny; better building; stakeholder-driven legitimacy; and machine translation.Universal concerns, objectively measuredThe precise reasons why AI/ML are matters of ethical concern differ from organization to organization. Some lean on the language of distributive justice, arguing AI/ML's benefits and penalties will be unevenly distributed. For example, the Toronto Declaration argues that 'marginalized groups' will feel the brunt of discriminatory ML and so should be explicitly included in the development process. However, all the statements agree a) the positive and negative impacts of AI are a matter of universal concern, b) there is a shared language of ethical concern across the species, and c) those concerns can be addressed by objectively measuring those impacts. This is a universalist project that brooks little relativist interpretation.Often this ethical universalism is justified by reference to a hazy biological essentialism. CHT's core argument is that social media is hacking our attention, and that human brains cannot adequately cope with these addictive designs. Beneath a picture of a cartoon brain swarming with red notification icons on their website is the sentence \"There's an invisible problem that's affecting all of society.\" The Montreal Declaration follows a similar line: The community of concern is \"all human beings\" or even \"all sentient creatures.\" The Toronto Declaration departs from this conception of biological community, pursuing instead a legal universalism via human rights law as the grounds on which harms and remedies are understood.In order to address these shared concerns, ethical design advances a program of objective measurement of harms. FATML, for example, aims to support research addressing bias and discrimination through \"computationally rigorous methods.\" The third core value in its Principles for Accountable Algorithms is 'accuracy', which is meant to encourage the detailed logging of errors and uncertainty. Similarly, the Toronto Declaration endorses a program of impact assessment throughout the ML lifecycle.Expert oversightDespite assuming a universal community of ethical concern, these vision statements are not mass mobilization documents. Rather, they frame ethical design as a project of expert oversight, wherein primarily technical, and secondarily legal, experts come together to articulate concerns and implement primarily technical, and secondarily legal solutions. They draw a narrow circle of who can or should adjudicate ethical concerns around AI/ML.This assumption of expertise is clear from both the voices within these documents (e.g., ranging from major AI corporations to leading academics and legal minds) and from the substance of their proposals. The Partnership demarcates \"the public\", a body to be educated and surveyed, from \"stakeholders\", scientists, engineers and businesspersons who will educate and survey. Elsewhere, in describing Our Work, The Partnership separates out \"Engagement of Experts\" from \"Engagement of Other Stakeholders\". The former are leaders of scientific disciplines addressing or building AI, the latter range from individual product users to large corporations purchasing AI solutions or disrupted by AI in their sector. Experts make AI happen, Other Stakeholders have AI happen to them.Less frequently, statements acknowledge the importance of non-technical expertise. The Toronto Declaration's Preamble positions \"the universal, binding and actionable body of human rights law and standards\" as an invaluable supplement to technical debates. It argues not just for regulators to become experts in ML, but for ML procurers and developers to become experts in human rights and international due process standards-or at least employ such experts. Similarly, Axon's Ethics Board includes not only roboticists and computer vision experts but privacy researchers, former and current police officers, and criminologists. Over time, FATML's calls-for-papers embrace a broader community of experts. In 2014 and 2015 the \"machine learning community\" was the explicit audience. Later, this assumption broadens somewhat: the 2016 call has \"researchers and practitioners\" responding to the concerns of \"policymakers, regulators, and advocates\", while 2017's encourages submissions from \"practitioners in industry, government, and civil society.\"Values-driven determinismThe envisioning documents all offer deterministic framings of AI/ML as world-historical forces of change-inevitable seismic shifts to which humans can only react. Paradoxically, AI/ML are also at the same time described as values-driven, insofar as human beings create them. They are forces to which we must adapt and for which we are also responsible.The Montreal Declaration captures this tension well. While there is overriding hope that \"AI will make our societies better\", sections exploring individual values such as Justice veer between instrumental impact (e.g., \"What types of discrimination could AI create or exacerbate?\") and active human agency (e.g., \"Should the development of AI be neutral or should it seek to reduce social and economic inequalities?\"). Similarly, OpenAI's Charter is aimed at the mediumterm impact of inevitable \"highly autonomous systems that outperform humans at most economically valuable work\", by collaborating on \"value-aligned, safety-conscious project[s]\" in the present and nearterm. This tension between ethical conflict in design and instrumentalism in impact is perhaps resolved by reference to the expert oversight described above: Human agency is integral to ethical design, but it is largely a property of experts responsible for the design, implementation and, sometimes, oversight of AI/ML.In other places, this determinism manifests as teleology. It is taken as given that AI/ML technologies a) are coming and b) they will replace a broad swathe of human jobs and decisions. In its thematic pillar \"AI, Labor, and the Economy\", The Partnership on AI assures readers that AI will \"undoubtedly\" disrupt the labor market, \"as new kinds of work are created and other types of work become less needed due to automation.\" Consequently, ethical debate is largely limited to appropriate design and implementationnot whether these systems should be built in the first place.Crucially, edicts to do something new are framed as moral imperatives, while the possibility of not doing something is only a suggestion, if mentioned at all. This is true even for the more critical statements like The Toronto Declaration, which stresses that some groups deserve extra care when collecting and processing their data and that such care extends throughout a product's lifecycle. Here, attention is paid to the risks of harm from design through execution-yet it is still taken as a given that these data will be collected.Design as locus of ethical scrutinyFollowing from the previous theme, business practices which might affect AI/ML design and use (and which tend to overpower individual ethical concerns) remain a lacuna. Business decisions are never positioned as needing the same level of scrutiny as design decisions. In this way, the vision statements are reminiscent of many professional codes of ethics, which often detail the responsibilities of individual professionals without actively scrutinizing the nature of the profession or business in question The Montreal Declaration's \"Justice\" plank nods towards the problem of \"the concentration of power and wealth in the hands of a small number of AI companies\" but the principle that follows from this question returns to a focus on developing AI that promotes justice and reduces discrimination. The CHT appears on the surface to take a strong ethical stance against the business model of attention hacking, but the proposals that flow from its ethical stance are largely limited to design considerations-many of which have already have already been embraced by the industry verbatim (e.g., Facebook's emergent focus on 'time well spent' rather than raw engagement time, and Google's release of \"digital wellness\" tools in its new version Android Better buildingAn important consequence of business practices being discursively \"off the table\" is the implication that \"better building\" is the only ethical path forward. The overarching focus of the Montreal Declaration is the creation of 'responsible' AI, and while its questions ask stakeholders to consider whether autonomous agents should be able to run an abattoir or kill an animal, the proposed principles reframe the debate on the affirmative grounds of designing AI to fulfill social goods (i.e., eliminating discrimination, protecting humans from propaganda, etc.). Rhetorically, the corporate members of the Partnership commit themselves to \"better building\" by seeking to maximize the benefits of AI, minimize their disruptions and negative impacts, and educate the broader public on the role of AI in their lives. The only red line drawn by the Partnership is in its Tenets, which commit to \"opposing the development and use of AI technologies that would violate international conventions or human rights.\"Axon's Ethics Board announcement similarly frames their initiative as the responsible shepherding of innovations destined to improve policing. CEO Rick Smith says the Ethics Board was created \"to ensure any AI technology in public safety is developed responsibly.\" In a comment to the Washington Post, Smith echoed the values-driven determinism described earlier: \"It would be both naive and counterproductive to say law enforcement shouldn't have these new technologies\" It is clear that \"better building\" is the only way forward because no statement offers \"not building\" as an alternative. Across the statements, the Toronto Declaration contained the only gesture toward not building, but ultimately demurs: \"Where the risk of discrimination or other rights violations has been assessed to be too high or impossible to mitigate the private sector should consider not deploying a machine learning application.\" There are no other red lines which should not be crossed, for state or corporate actors.Stakeholder-driven legitimacyProponents of ethical design often articulate a desire to open or sustain conversations by engaging as many stakeholders-largely experts-as possible. This positions ethical design as ethical, in part, because it is given a thorough vetting. Vetting legitimates decisions through an appeal to transparency, but without specifying any subsequent substantive commitments.This legitimacy appears to hold even if, as in Axon's case, consulted stakeholders are limited in their capacity to impact design. Indeed, it appears part of the mission of Axon's Ethics Board is to simply release reports that \"demonstrate a commitment to public transparency.\" The Partnership's explicit mission is to bring disparate others into discussions by extending the discussion out those disparate others. They break down \"How We're Doing It\" into four prongs: engaging domain experts \"to discuss and provide guidance\"; hearing the concerns of non-expert stakeholders in industries affected by AI and bringing those concerns back into research and development; producing third-party studies, supporting moonshot ideas, and \"the identification and celebration of important work\"; and developing \"informational materials\" for the broader public.FATML and the Toronto Declaration evidence a similar commitment. Both recognize that a conversation among technical experts is ongoing, and other (expert) voices need to be brought to the table. FATML seems to have come to this conclusion over time, as evidenced in its CFPs, while the Declaration's Preamble highlights the importance of bringing human rights literacy to a conversation heretofore dominated by engineers. Decisions made about AI/ML need to be made with a wide community of experts, and the wideranging impact of AI/ML demands a wide-ranging group of experts to research those impacts.Machine TranslationThe broad circle of (expert) consultation is also extended to AI and ML technologies themselves. Vision statements often position 'explicable' and 'transparent' (as opposed to \"black-boxed\") systems as both a foundation of moral AI/ML and a means by which moral questions are pursued. Under the Montreal Declaration's value of Knowledge, there are questions posed not just about what AI means for human knowledge (e.g., \"Does the development of AI put critical thinking at risk?\") but what knowledge humans should have about AI (e.g., \"Is it acceptable not to be informed that medical or legal advice has been given by a chatbot?\"). This dual emphasis makes clear that the proposed principle that follows-\"The development of AI should promote critical thinking and protect us from propaganda and manipulation\"is a two-way street: Moral machines not only shield us from fake news, they make their inner workings clear enough to ensure no fake news lies within.FATML further grounds this background element in technical specifics with its Principles for Accountable Algorithms, originally developed at a Dagstuhl Seminar entitled \"Data, Responsibly\", with an explicit audience of \"developers and product managers.\" The explicit goal is to make AI and ML \"publicly accountable\" via \"an obligation to report, explain or justify algorithmic decision-making as well as mitigate any negative social impacts or potential harms.\" This is grounded in five core values, including explainability and auditability, each linked with steps to take and questions to take in building social impact statements for algorithms. Developers should have a plan to explain algorithmic decisions and should \"consider whether a directly interpretable or explainable model can be used.\"Summary", "conclusions": "We have identified seven core elements of ethical design's moral background: Universal concerns, objectively measured; expert oversight; values-driven determinism; design as locus of ethical scrutiny; better building; stakeholder-driven legitimacy; and machine translation. What unites them? Two underlying themes stand out.First, conversations around the design and deployment of ethical AI/ML are taking place among experts well aware they are under public scrutiny-the prospect of massive job losses and rigged elections has raised the public profile of their work. Building a moral background for ethical design is partly about shaping public perception, providing the concepts through which AI/ML can be understood. One goal for these envisioning statements is thus to generate the moral consensus Popper knew already existed within the scientific community on nuclear and biological weapons: acknowledgment of a specific set of threats, and a specific set of people, tools, and ideas ready to respond. Yet the problems remain, in this view, fundamentally technical, shielded from democratic intervention. Other forms of expertise appear in these statements, but the problems themselves are to be solved by experts in the technical features of AI/ML systems.Second, and perhaps to the surprise of critical researchers engaged in this work for decades, ethical design seems to share many conceptual similarities with Values@Play, Values Sensitive Design, and neighboring fields. FATML's Principle for Accountable Algorithms in particular makes it clear that, \"Algorithms and the data that drive them are designed and created by people-There is always a human ultimately responsible for decisions made or informed by an algorithm. 'The algorithm did it' is not an acceptable excuse if algorithmic systems make mistakes or have undesired consequences, including from machine-learning processes.\" This language, common in these values statements, paints a clear picture of moral causation: Poor ethics lead to bad designs, which produce harmful outcomes. This is far from an obvious, let alone the only possible, conclusion-but it is, nonetheless, the causation narrative on offer here.", "SDG": [17]}}