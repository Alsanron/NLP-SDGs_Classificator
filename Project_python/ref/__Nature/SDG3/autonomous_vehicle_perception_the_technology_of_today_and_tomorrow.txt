Transportation Research Part C xxx (xxxx) xxxxxx

Contents lists available at ScienceDirect

Transportation Research Part C
journal homepage: www.elsevier.com/locate/trc

Review

Autonomous vehicle perception: The technology of today and
tomorrow
Jessica Van Brummelena, Marie OBriena, Dominique Gruyerb, Homayoun Najjarana,



a

Advanced Control and Intelligent Systems Laboratory, University of British Columbia, Kelowna, British Columbia, Canada
Laboratoire sur les Interactions Vehicules, Infrastructure, Conducteurs (LIVIC), IFSTTAR-CoSys-LIVIC, 25 alle des Marronniers, 78000 Versailles,
France
b

AR TI CLE I NF O

AB S T R A CT

Keywords:
Automotive sensors
Autonomous vehicles
Intelligent vehicles
Localization and mapping
Machine vision
Sensor fusion

Perception system design is a vital step in the development of an autonomous vehicle (AV). With
the vast selection of available o-the-shelf schemes and seemingly endless options of sensor
systems implemented in research and commercial vehicles, it can be dicult to identify the
optimal system for ones AV application. This article presents a comprehensive review of the
state-of-the-art AV perception technology available today. It provides up-to-date information
about the advantages, disadvantages, limits, and ideal applications of specic AV sensors; the
most prevalent sensors in current research and commercial AVs; autonomous features currently
on the market; and localization and mapping methods currently implemented in AV research.
This information is useful for newcomers to the AV eld to gain a greater understanding of the
current AV solution landscape and to guide experienced researchers towards research areas requiring further development. Furthermore, this paper highlights future research areas and draws
conclusions about the most eective methods for AV perception and its eect on localization and
mapping. Topics discussed in the Perception and Automotive Sensors section focus on the sensors
themselves, whereas topics discussed in the Localization and Mapping section focus on how the
vehicle perceives where it is on the road, providing context for the use of the automotive sensors.
By improving on current state-of-the-art perception systems, AVs will become more robust, reliable, safe, and accessible, ultimately providing greater eciency, mobility, and safety benets
to the public.

1. Introduction
Autonomous vehicle (AV) technology is making a prominent appearance in our society in the form of advanced driver assistance
systems (ADAS) in both research and commercial vehicles. These technologies aim to reduce the amount and severity of accidents,
increase mobility for people with disabilities and the elderly, reduce emissions, and use infrastructure more eciently (Fagnant and
Kockelman, 2015). One of the major motivations accelerating the advancement of AV technologies is their insusceptibility to humanrelated errors, such as distraction, fatigue, and emotional driving, which currently cause approximately 94% of accidents according to
a statistical survey completed by the National Highway Trac Safety Administration (NHTSA) (Singh, 2015).
As research, testing, and deployment of vehicles with AV technology is escalating around the world, the development of standardized guidelines and regulations has become a major focus to ensure safe integration into society. The U.S. Department of



Corresponding author.
E-mail addresses: jess.vanbrummelen@gmail.com (J. Van Brummelen), marie.lynn.obrien@gmail.com (M. OBrien), dominique.gruyer@ifsttar.fr (D. Gruyer),
homayoun.najjaran@ubc.ca (H. Najjaran).
https://doi.org/10.1016/j.trc.2018.02.012
Received 14 July 2017; Received in revised form 17 January 2018; Accepted 16 February 2018
0968-090X/  2018 Elsevier Ltd. All rights reserved.

Please cite this article as: Van Brummelen, J., Transportation Research Part C (2018), https://doi.org/10.1016/j.trc.2018.02.012

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

Fig. 1. Overview of the autonomous navigation process.

Transportation and the NHTSA have recently adopted the Society of Automotive Engineers international standard for automation
levels which dene autonomous vehicles from Level 0 (the human driver has full control) to Level 5 (the vehicle completely drives
itself) (Transportation, 2016). Currently, due to limitations and high costs of available sensors, most commercial vehicles only include
Level 1 to Level 2 autonomy, which require constant driver attention and control. The autonomous features in these vehicles generally consist of emergency braking, blind spot detection, and/or lane keeping. Nonetheless, Level 3 autonomous features are
available in the Tesla Model S and Model X. However, recent accidents have initiated concerns regarding the drivers understanding
and capability of using the technology safely (Krisher and Durbin, 2016).
Presently, a major concern is the occurrence of new, unsafe driving practices as a result of drivers who do not understand or are
not aware of how the AV technologies work (Kyriakidis et al., 2017; Lu et al., 2016). Furthermore, in order for autonomous features
and vehicles to have signicant, far-reaching eects in improving safety, mobility and eciency, the public must understand the
capabilities of the technology (Kyriakidis et al., 2015, 2017; Lu et al., 2016). This includes important factors such as the limitations of
the technology, the application for the technology, and the appropriate scenarios to use and/or rely on the technology.
As a brief overview, autonomous vehicle navigation can be visualized as ve main components (Fig. 1): Perception, Localization
and Mapping, Path Planning, Decision Making, and Vehicle Control (Cheng, 2011). Perception uses sensors to continuously scan and
monitor the environment, similar to human vision and other senses (Maurer et al., 2016). Localization and mapping algorithms
calculate the global and local location of the ego-vehicle and map the environment from sensor data and other perception outputs
(Maurer et al., 2016). Path planning determines possible safe routes for the ego-vehicle based on perception, and localization and
mapping information (Katrakazas et al., 2015). The decision-making component is responsible for calculating the optimal route based
on the possible paths, the current vehicle state, and the environment information (e.g., road attributes, weather conditions, road
signs, etc.) (Maurer et al., 2016). The vehicle control module will then calculate the appropriate vehicle command (torque, acceleration, steering wheel angle, etc.) in order to follow the optimal route decision, such as a lane change, a right turn, or another
maneuver (Gruyer et al., 2016b). It is important to note that the autonomous navigation process is a high frequency recursive process.
This allows AVs to eectively handle high-speed motion and dynamic objects, such as pedestrians, motorcycles, and cars (Julier and
Durrant-Whyte, 2003). An overview of the autonomous navigation process is shown in Fig. 1. This paper will focus on the Perception and Localization and Mapping stages.
This paper aims to provide a comprehensive review of the state-of-the-art AV perception technology to address a lack of synthesized information about sensor, hardware, and algorithm requirements for eective AV perception. In general, robust and reliable
perception, and localization and mapping are required in order to make accurate and reliable decisions for vehicle control. This paper
provides a review of the current sensor technology used for perception, as well as an overview of the methods used for localization
and mapping.
In essence, this paper aims to answer the following questions:

 Which sensors are currently used in prominent research and commercial vehicles?
 What are the current advantages and shortcomings of the sensors?
 Which localization and mapping techniques are being used in research and commercial vehicles with respect to sensing the egovehicles environment?
 What are the shortcomings of these localization and mapping methods and how can they be improved?
 What are the current areas of research that need to be addressed?
 How will this technology evolve in the future?
The paper is organized as follows. Section 2 introduces the background of AV research. Section 3 discusses the capabilities and
2

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

shortcomings of automotive sensors commonly used in research and commercial vehicles. Section 4 provides an overview of localization and mapping techniques. Section 5 discusses the future of research in AV perception, providing useful insight for experienced
AV researchers and engineers, and Section 6 concludes the review paper. Note that, although obstacle detection and tracking, road
detection and tracking, ego-localization, and environment estimation are subcomponents of AV perception, these topics are considered outside of the scope of this paper and will not be discussed in detail. Further information about these topics may be found in
Zhu et al. (2017).
It is worth to note that this papers Localization and Mapping section was included to provide the context for how the ego-vehicle
senses its surroundings and to illustrate how perception and sensing are closely related to localization and mapping. For example, an
AV must know where it is on the road (through localization) to better estimate which objects are in its surroundings. A more practical
example might be if an AV has sensed (through localization and mapping) that it is nearing a crosswalk, the vehicle should prioritize
sensing for pedestrians around the crosswalk (which can only be done if the vehicle has localized itself). The link between perception
and sensing, and localization and mapping works in the opposite direction as well. For example, if a vehicle senses road markings to
the left and right of the vehicle, then the vehicle is likely to be in the center of the road, and thus, localization algorithms should be
updated accordingly. Furthermore, the sensor technology (e.g., GPS, IMU, LIDAR, etc.) discussed in the Perception and Automotive
Sensing section informs how localization and mapping algorithms are implemented, as discussed in the Localization and Mapping
section.
Finally, note that the purpose of this paper is to provide an overview of current AV perception research and to identify areas of AV
perception that require further work. This paper is meant to be used as a guide for future research, as well as provide essential
information about perception for newcomers to the AV eld. Thus, detailed analyses or rankings of perception algorithms and current
sensors (which change regularly with the rapid advancement of research in this eld) are outside of the scope of this paper.
Nonetheless, the reader is strongly encouraged to investigate such analyses further.
This article can also benet researchers through providing tables and gures that synthesize AV research in the following areas:

 Table 1: challenges in AV research and the extent to which the challenges have been addressed
 Table 2: advantages and disadvantages of common passive and active sensors
 Table 3: sensor arrangements in prominent research and commercial vehicles
 Table 4: autonomous features available in commercial vehicles
 Table 5: localization and mapping methods used by prominent research vehicles
 Fig. 1, 4 and 5: various autonomous navigation processes
 Figs. 2 and 3: AV history and milestones
Researchers may refer to these gures and tables when developing presentations, deciding on future research areas, choosing AV
sensors, or developing new autonomous features.
2. History and background information
Since before the twenty-rst century, researchers and industrial leaders have been competing to develop the rst fully autonomous vehicle that is robust, reliable and safe enough for real-world and high-speed driving environments. Major contributors to early
AV research can be attributed to AV tests and competitions held around the world. These competitions provided opportunities for
industry and researchers to assess the capabilities and boundaries of AVs in various driving environments. However, more importantly, they identied major diculties and shortcomings in AV software and hardware, some of which remain unresolved today.
One of the rst long-distance AV road tests, No Hands Across America, was introduced in 1995 (Bertozzi et al., 2000; Jochem
and Pomerleau, 1995). This event pushed the boundaries of AV technology requiring the AV to steer across the United States while
the human drivers controlled the vehicle s acceleration and braking. Around the same time, an AV drove from Germany to Denmark
in the Munich to Odense UBM Test (Bertozzi et al., 2000; Maurer et al., 1996). In 1998, an AV journeyed through the rolling hills
and unpredictable weather conditions of Italy in the ARGO Project (Bertozzi et al., 1998, 2000; Broggi et al., 2000). In each of these
tests, the AVs drove autonomously for 9098% of the journey using primitive lane departure warning systems, lane keeping systems,
and inter-distance/speed regulation systems (Bertozzi et al., 1998).
Moreover, through these tests, vehicle developers noted many areas in AV technology requiring signicant improvement. These
areas included image processing and other perception techniques; driving in complex, urban scenarios and poor weather conditions;
and improving erroneous obstacle and road marking detection (Behringer and Maurer, 1996; Bertozzi et al., 1998, 2000). For
example, ARGO s lane detection algorithm required ideal road conditions in order to drive autonomously, including at and weakly
curved (or straight) roads, and roads with excellent road markings (Broggi et al., 2000). Thus, much of the diculty and unpredictability of real-world autonomous driving was eliminated in this test. Furthermore, in both No Hands Across America and
Munich to Odense UBM Test, the AVs struggled to drive in unfavorable lighting conditions, such as dark tunnels, or when sunlight
pointed directly in the camera lens (Bertozzi et al., 2000). The diculties encountered related to lighting conditions have begun to be
addressed with the improved vision technology and algorithms (such as using the HSL instead of RGB color space), the fusion of two
camera data streams, and the fusion of active and passive sensors (Cacciola, 2007; Chan et al., 2007). However, work still needs to be
completed in this area and will be further discussed in Section 3.
In 2003, the next major competition was initiated by the Defense Advanced Research Projects Agency (DARPA) which required
vehicles to drive without the aid of road markings through an o-road desert course (Rou and Hinchey, 2011). The rst DARPA
3

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

Table 1
Summary of Problems Exposed and Addressed by AV Projects.
Project(s)/Competition(s)

Problems exposed/addressed by project

Current state of problem: Largely addressed
(LA), relatively addressed (RA) or largely
unaddressed (UA)

PROMETHEUS (19871995) (Oagana, 2016; Eureka, 1995; Amin
et al., 1995; Eleter and Rombaut, 1996)

 Autonomous lane keeping
 Adaptive cruise control
 Automatic emergency calling systems

 LA
 LA
 LA

No Hands Across America (1995), Munich to Odense UBM Test
(1995), ARGO(1998) (Bertozzi et al., 2000; Jochem and
Pomerleau, 1995; Maurer et al., 1996; Bertozzi et al., 2000;
Broggi et al., 2000; Behringer and Maurer, 1996)

 Vision-based object detection/tracking
 Perception in unfavorable lighting
conditions
 Improvement of obstacle and road
marking detection
 Complexities of urban driving
 Perception in dicult weather
conditions

 LA
 RA

DARPA Grand Challenge (2004), Second DARPA Grand Challenge
(2006), (Rou and Hinchey, 2011; Walton, 2004; Buehler
et al., 2007)

 O-road navigation
 Obstacle avoidance

 LA
 RA

DARPA Urban Challenge(2007) (Rou and Hinchey, 2011;
Campbell et al., 2010; Buehler et al., 2009; Montemerlo et al.,
2008)

 Trac light and sign detection
 Ability to test in real trac situations
 Obstacle detection - especially
pedestrian and cyclist detection
 High-speed autonomous driving;
eciency of detection algorithms
 Complex urban driving (dense trac,
intersections, etc.)

 LA
 RA
 RA

 RA
 UA
 UA

 RA
 UA

 Temporary autonomous driving
systems
 V2V to increase redundancy in data
 Safety software architecture of AVs;
detection of hardware/software/sensor
failure

 RA

Safe Road Trains for the Environment (SARTRE) (20092012)
(Larburu et al., 2010; Davila et al., 2013; Bergenhem et al.,
2010)

 Vehicle platooning and relevant
environmental and safety benets

 RA

VisLab Intercontinental Autonomous Challenge(VIAC) (2010)
(Broggi et al., 1999; Broggi et al., 2012; Boudette, 2017;
Sorokanich, 2014)

 Vehicle platooning in real trac
situations
 Vehicle platooning without a priori
information
 Autonomous driving without a priori
information

 LA

Grand Cooperative Driving Challenge (2011) (van Nunen et al.,
2012; Geiger et al., 2012a)

 Ecient cooperative driving in
intersections

 UA

eFuture (2013) (eFuture, 2011)

 Energy-ecient AV technology
 Standardized Advanced Driver
Assistance Systems (ADAS)
 Data fusion for increased perception
accuracy
 Human acceptance of AVs

 RA
 RA

 Real-world platooning using V2V
communication

 RA

Highly Automated Vehicles for Intelligent Transportation (HAVEit)
(20082011) (HAVEit, 2008; Vanholme et al., 2013)

European Truck Platooning Challenge (2016) (Fudge, 2016; van
Nunen et al., 2016)

 RA
 UA

 LA
 UA

 RA
 UA

Grand Challenge was held in 2004. Unfortunately, no vehicle that entered could complete the course. However, in 2005, a second
Grand Challenge was held and ve vehicles successfully completed the course, providing a great victory for AV research (Buehler
et al., 2007).
After the DARPA Grand Challenges, AV research steadily increased (see Fig. 2) (Reuters, 2017) and researchers began to address
the challenges of driving in complex and urban environments with dense trac, complex intersections, and overtaking and lane
change maneuvers. Since AV testing on public roads was not yet permitted in many areas, these challenges were dicult to address,
as well as test, in real-world situations. The DARPA Urban Challenge, held in 2007, attempted to address this. It consisted of several
urban driving scenarios, including an intersection, simulated highway on-ramp, and o-road to on-road route. Moreover, to further
simulate a real-world urban environment, the vehicles were required to obey trac laws (Rou and Hinchey, 2011). Four vehicles
successfully completed the DARPA Urban Challenge, including teams from Carnegie Mellon University, Stanford University, Virginia
Polytechnic Institute and State University, and the Massachusetts Institute of Technology. Each of these vehicles used unique sensing,
4

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

Table 2
Typical uses, advantages, and disadvantages of common AV sensors.
Sensors and Uses
Single Camera
 Obstacle detection and
classication
 Lane detection

Advantages

Disadvantages

 Computationally inexpensive relative to stereovision

Computationally expensive relative to active sensors
(Sivaraman and Trivedi, 2013)
 Dicult to measure distances (Rasshofer and Gresser, 2005)
(may use optical ow or camera/LIDAR/radar fusion methods
(Gruyer et al., 2013))
 Velocity information must be calculated

 Good for classication

 Wide FOV while maintaining good resolution
(Rasshofer and Gresser, 2005)
 Provides additional information about the
environment (color, texture, etc.) (Sivaraman and
Trivedi, 2013)
 Long range (with high resolution cameras)
Stereovision
 Obstacle detection and
classication
 Lane detection
 3D mapping

 Depth perception similar to human eyes (eective at
close range) (Rasshofer and Gresser, 2005)
 3D construction
 Good for classication
 Provides additional information about the
environment (color, texture, etc.) (Sivaraman and
Trivedi, 2013)
 Long range (with high resolution cameras)
 Better detection than regular vision

LIDAR
 Obstacle detection
 3D mapping (with multi-layered
LIDAR)
 Lane detection (via intensity
measurements)

 Direct distance measurements
 Large FOV with high resolution and medium range
(Rasshofer and Gresser, 2005)
 Multi-layer LIDAR allows robust 3D construction
 May have internal mechanism to limit the impact of
poor weather conditions (Stu, 2017)

Radar
 Obstacle detection

 Direct distance measurements
 Direct velocity measurements (Leonard et al., 2008)
 Long-range, mid-range and short-range options
available (depending on company) (Henawy and
Schneider, 2011; Leonard et al., 2008)
 Does well in poor weather conditions (Rasshofer and
Gresser, 2005)
 High accuracy (Rasshofer and Gresser, 2005)

Sonar
 Near obstacle detection (e.g.,
parking assistance systems)

 Poor performance in poor weather conditions (Rasshofer and
Gresser, 2005)
 Sensitive to lighting conditions (Sivaraman and Trivedi, 2013)
 Long range applications require more computation
 Computationally expensive (Huber et al., 2011; Maddern and
Newman, 2016)
 Velocity and distance information must be calculated
 Poor performance in poor weather conditions (Rasshofer and
Gresser, 2005)
 Long range applications require more computation

 Sensitive to lighting conditions (Sivaraman and Trivedi, 2013)

 Poor classication compared to vision (Bacha et al., 2008)
 Velocity information must be calculated
 Diculty detecting highly reective objects (Leonard et al.,
2008)
 Typically, poor detection in rain, fog and snow (Rasshofer and
Gresser, 2005) (although some LIDAR have countermeasures,
such as measuring multiple times per laser pulse (Stu, 2017))
 Poor very near (< 2 m) measurement (Rasshofer and Gresser,
2005)
 Long range radar have small FOVs (Rasshofer and Gresser,
2005)
 Poor classication (Leonard et al., 2008)
 Poor very near (< 2 m) measurement (Rasshofer and Gresser,
2005)
 Poor pedestrian detection
 Poor static object detection
 Interference of multiple reections can cause false alarms

 Direct distance measurements

 Poor angular resolution (Rasshofer and Gresser, 2005)

 Very near range (< 2 m) (Rasshofer and Gresser,
2005)
 Can operate in fog and snow (Rasshofer and Gresser,
2005)

 Poor detection beyond 2 m (Rasshofer and Gresser, 2005)

perception, and localization techniques to localize the vehicle in the environment and detect, classify and track obstacles (Campbell
et al., 2010).
Although the DARPA Urban Challenge tested the AVs in scenarios much closer to everyday driving, these challenges still lacked
important and common roadway obstacles, such as pedestrians and cyclists. Additionally, the AVs were not required to detect trac
lights or signs, as these were omitted from the challenge or provided by RNDF les (Buehler et al., 2009; Rou and Hinchey, 2011).
Furthermore, the vehicles drove slowly throughout the challenge (30 mph or less) (Montemerlo et al., 2008).
To address a lack of real-world AV testing, in 2010, the VisLab Intercontinental Autonomous Challenge (VIAC) was initiated. In
this challenge, two AVs (a leader and a follower) drove across multiple countries, encountering urban, o-road, and highway scenarios, as well as a myriad of weather conditions. Impressively, the vehicles autonomously drove through these situations with no a
priori maps or prior knowledge about standard road shapes and sizes for guidance. Instead, the follower vehicle detected the leader
5

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

Table 3
Prominent research and commercial vehicle sensors.
Vehicle

Research Vehicles
Audi s Research Vehicle (Gitlin, 2016b; Pachal, 2016; Souppouris, 2014)
AutoNOMOS s MadeInGermany (Volkswagen Passat) (Ghring et al., 2013)
Carnegie Mellon s Urban Challenge entry, Boss (2007 Chevy Tahoe; 1st place) (Grisleri
and Fedriga, 2010; Urmson et al., 2008)
Ford s Hybrid Fusion research vehicle (Gitlin, 2016a)
Google s research vehicles (Toyota Prius; Lexus CT; Custom Google vehicle) (Google, 2017;
Guizzo, 2011; Vanderbilt, 2012)
LIVIC s CARLLA (Vanholme et al., 2013) (information also gathered by personal
interviews with LIVIC labs)
MIT s Urban Challenge entry, Talos (Land Rover LR3; 4th place) (Leonard et al., 2008)
Nagoya and Nagasaki University s Open ZMP Robocar HV (Toyota Prius) (Kato et al., 2015)
Stanford s Urban Challenge Entry, Junior (2006 Volkswagen Passat; 2nd place) (Levinson
et al., 2011; Montemerlo et al., 2008)
Virginia Tech s Urban Challenge entry, Odin (2005 Hybrid Ford Escape; 3rd place) (Bacha
et al., 2008)
VisLab s BRAiVE (Grisleri and Fedriga, 2010; VisLab, 2010)
Volvo s research vehicle (Stoklosa, 2015; Cars, 2016)
Commercial Vehicles
2015 Inniti Q50S (Sherman, 2016)
2016 Lexus RX (Lexus, 2017; Vandezande, 2013)
2016 Volvo XC90 (Volvo, 2016)
BMW750i xDrive (BMW, 2017; Sherman, 2016)
Ford (high-end production vehicles) (Company, 2014)
Mercedes-Benz E and S-Class (Sherman, 2016; Tingwall, 2013; Ulrich, 2014; Mercedes-Benz,
2013; Vanderbilt, 2012)
Otto Semi-Trucks (Stewart, 2016)
Renault GT Nav (Renault, 2017)
Tesla Model S (Golson, 2016; Sherman, 2016)

Vision

Stereovision

Infrared
Camera

LIDAR

Radar

Sonar















































































-


























 denotes unspecied/unidentied information.
Note: Not all vehicles were included in this table due to the fast growth and vastness of the industry and research in this area.
Table 4
Available autonomous features in high-end commercial vehicles.
Vehicles

BMW750i xDrive (BMW, 2017; Sherman, 2016)
Ford (high-end production vehicles) (Company, 2014)
2015 Inniti Q50S (Sherman, 2016)
Lexus RX (Lexus, 2017)
Mercedes-Benz E and S-class (Sherman, 2016; MercedesBenz, 2013; Ulrich, 2014; Tingwall, 2013;
Vanderbilt, 2012)
Otto Semi-Trucks (Stewart, 2016)
Renault GT Nav (Renault, 2017)
Tesla Model S (Golson, 2016; Sherman, 2016)
Volvo XC90 (Volvo, 2016)

Autonomous Freeway
Driving

Autonomous Lane
Change






Semi-Autonomous
Parking

Semi-Autonomous
Braking



























vehicle and applied platooning techniques to drive across several European and Asian countries (Broggi et al., 2010, 2012).
Unlike the vehicles in many past road tests (e.g., in the Munich to Odense UBM Test and the No Hands Across America test
(Maurer et al., 1996; Pomerleau, 1995)), many AVs today, such as Google s AVs, are heavily reliant on a priori information, including
a priori maps (Boudette, 2017; Sorokanich, 2014). A priori maps are detailed, static records of the surrounding environment, which
alleviate the high computational load of mapping the environment in real-time. For reference, to build an a priori map a human will
pre-drive the desired route, and following this, the vehicle will autonomously drive this route using the previously collected data. The
AV system can then focus primarily on providing accurate localization, as well as detecting and reacting to obstacles. However, a
priori information can limit the AVs ability to adapt and react safely to new situations such as new construction zones, potholes, and
stoplights (Sorokanich, 2014). Since common environmental changes can drastically aect a vehicle s ability to rely on these premade maps, other localization and mapping methods in real-time are being researched. This topic is discussed in detail in Section 4.
Connected vehicle technology, such as vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication, may be able
6

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

Table 5
Prominent research vehicles localization and mapping methods.
Research Vehicles

A priori method



Audi s research vehicle (Gitlin, 2016b; Pachal, 2016; Souppouris, 2014)
AutoNOMOS Labs MadeInGermany (Wang et al., 2011a)
Braunschweig University of Technology s Leonie (Saust et al., 2011)
Bundeswehr University of Munich s VaMP (Maurer et al., 1996)
Carnegie Mellon s NavLab in No Hands Across America (Thorpe et al., 1988)
Carnegie Mellon s Urban Challenge entry, Boss (1st place) (Grisleri and Fedriga, 2010; Urmson et al., 2008)
Ford s Hybrid Fusion research vehicle (Gitlin, 2016a)
General Motors research vehicles (Davies, 2016)
Google s research vehicles (Google, 2017; Guizzo, 2011; Vanderbilt, 2012)
Honda s research vehicle (Betters, 2015)
Karlsruhe Institute of Technology s Bertha Benz (Ziegler et al., 2014)
LIVIC s CARLLA (Vanholme et al., 2013) (information also gathered by personal interviews with LIVIC labs)
MIT s Urban Challenge entry, Talos (4th place) (Leonard et al., 2008)
Nagoya and Nagasaki University s Open ZMP Robocar HV (Kato et al., 2015)
nuTonomy s vehicles (Hodson, 2016)
Oxford University s Wildcat (Hawkins et al., 2011)
Stanford s Shelley (Broggi et al., 2014)
Stanford s Urban Challenge entry, Junior (2nd place) (Levinson et al., 2011; Montemerlo et al., 2008)
Toyota s research vehicle (Betters, 2015)
Uber s vehicles (Insider, 2017)
University of Parma s ARGO (Broggi et al., 1999)
Virginia Tech s Urban Challenge entry, Odin (3rd place) (Bacha et al., 2008)
VisLab s BRAiVE (Broggi et al., 2013; Grisleri and Fedriga, 2010; VisLab, 2010)
Volvo s research vehicle (Stoklosa, 2015; Cars, 2016)
a

SLAM-based method







a

a












a




a







a



Heavily reliant on certain a priori information, but not highly detailed, pre-created maps.

Autonomous Vehicle Publications
2500

Number of articles

2000

1500
1000
500
0
1991

1993

1995

1997

1999

2001

2003

2005

2007

2009

2011

2013

2015

Year
Fig. 2. Number of articles in the Web of Science database (Reuters, 2017) containing the keywords autonomous and vehicle from 1991 to 2016. Note that as the Web
of Science collects more articles, the data for recent years (2016, 2015, 2014, ) may increase. Era I, Era II, and Era III refer to the dierent stages of autonomous
vehicle events and challenges as described in Fig. 3.

to resolve some of the existing problems resulting from heavy reliance on a priori information. V2I communication could provide a
network for intersections, road signs and construction signs to transfer important infrastructure information, such as road layout
changes, speed limits, and trac light information to AVs (Barrachina et al., 2013; Ilgin Guler et al., 2014). Similarly, V2V communication allows vehicles to share data, such as vehicle state, positioning, or intention to change lanes, with other vehicles (Dang
et al., 2014; Dey et al., 2016). By integrating V2V and V2I communication with autonomous vehicle technology, an eective cooperative driving network can be established (Barrachina et al., 2013; Kaviani et al., 2016).
In 2011, a challenge in the Netherlands called the Grand Cooperative Driving Challenge (GCDC) aimed to accelerate cooperative driving technology. In this event, teams of AVs cooperatively drove along a contained highway and intersection, transmitting signals to each other to indicate their intentions. The event s objective was to complete the challenges quickly (in terms of
throughput), fuel-eciently, and stably through employing V2V and V2I communication (Geiger et al., 2012a; van Nunen et al.,
2012). A more recent challenge, the 2016 European Truck Platooning Challenge, also explored V2V technology, aiming to encourage
the development self-driving, connected vehicles. In this challenge, autonomous vehicles platooned behind human-driven vehicles
from cities in Sweden, Germany and Belgium to Rotterdam in the Netherlands. This challenge allowed V2V communication to be
employed and tested in a real-world, large-scale environment (van Nunen et al., 2016; Fudge, 2016).
7

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

Fig. 3. Signicant road tests and AV challenges from 1995 to 2016. Era I is characterized by long distance highway road tests, Era II is characterized by DARPA
challenges towards complex, unaided autonomous driving, and Era III is characterized by connected vehicle and platooning road tests.

Other inuential projects include the Programme for European Trac with Highest Eciency and Unprecedented Safety
(PROMETHEUS) (Eureka, 1995), Safe Road Trains for the Environment (SARTRE) (Larburu et al., 2010), Highly Automated Vehicles
for Intelligent Transport (HAVEit) (Vanholme et al., 2013), and eFuture (eFuture, 2011) projects. The PROMETHEUS project, which
ran from 1987 to 1995, combined the technical knowledge and skills of universities, electronics manufacturers, and car companies
from across Europe to develop early AVs. The project included a range of topics from collision avoidance systems to cooperative
driving to vehicle environmental sustainability (Eureka, 1995; Oagana, 2016). A signicant prototype developed in this project was
the ProLab2 vehicle, which used a perception and copilot system to monitor the environment and develop warnings signals for a
human driver. This project was quite advanced for its time, incorporating static and dynamic environments, data fusion, decisionmaking, and human-machine interface research (Amin et al., 1995; Eleter and Rombaut, 1996).
The SARTRE project, which ran from 2009 to 2012, was a large-scale connected-vehicle project. In this project, aspects of vehicle
platooning, from the strategies and safety benets of vehicle platooning to the eciency and aerodynamics of vehicle platooning,
were investigated. A major outcome of this project was the development and implementation of safe connected, AV platoons, which
drove in various European countries including Sweden and Spain Bergenhem et al. (2010), Davila et al. (2013) and Larburu et al.
(2010). Furthermore, the HAVEit project, which ran from 2008 to 2011, aimed to improve the safety, eciency and comfort of
driving. This project produced safety architecture software to manage smart actuators, cooperative driving communication and
redundancy data; a co-system to increase human driving safety by assessing the driver s state (e.g., drowsy, alert, stressed, etc.); and
highly automated driving systems, which included automated queue and congestion assistance, temporary auto-pilot, and active
green driving systems (HAVEit, 2008). Most recently, the eFuture project, which ran from 2010 to 2013, aimed to develop energy
ecient advanced driver assistance system (ADAS) technology with standardized interface and signal denitions (eFuture, 2011).
These projects focused on the development of important areas of autonomous and semi-autonomous vehicle advancement including
cooperative driving, driver monitoring, and methods to improve the energy eciency of driving.
Although each of these challenges and projects (see Fig. 3) developed promising advances in AV technology, industrial leaders,
such as Google and Tesla, acknowledge that AVs are not yet robust enough to drive without human supervision (Eddy, 2016; Tesla,
2016). Nonetheless, the human aspect of driving is the root cause of most accidents (Lu et al., 2016). As mentioned previously, in
2015, an estimated ninety-four percent of motor vehicle accidents in the United States were due to human-related causes (such as
distraction, drowsiness, inattention and emotion) (Singh, 2015). Thus, in order to reduce motor vehicle accidents, the human aspect
of driving should be improved through monitoring or co-pilot systems (as in the HAVEit and eFuture projects), or entirely removed
through fully autonomous systems. In conclusion, to attain the full potential of such autonomous systems, the chief remaining
perception challenges include:

 AV perception in poor weather and lighting conditions
 AV perception in complex urban environments
 Autonomous driving without heavy reliance on a priori perception data
 Utilization of connected vehicle technology to improve accuracy, certainty and reliability of perception
 Development of safety measures in case of faulty sensors/perception
A summary of the challenges exposed by the AV projects and competitions is provided in Table 1. This table also provides
information about the extent to which each problem has been addressed (as of the time of publication of this paper), which may be
used to guide future research.

8

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

3. Perception and automotive sensors
Similar to human drivers, AVs must be able to continuously observe their surroundings and accurately calculate their location on
both a local (relative to the curb, obstacles, intersections, etc.) and global (which street, neighborhood, city, etc.) scale. To provide
AVs with these abilities, sensors are installed in and around the vehicle.
3.1. Automotive sensor technology overview
As an overview, automotive sensing falls into three main categories: self-sensing, localization, and surrounding-sensing (Maurer
et al., 2016; Vanholme, 2012). Self-sensing uses proprioceptive sensors to measure the current state of the ego-vehicle, including the
vehicle s velocity, acceleration, yaw, and steering angle. Proprioceptive information is commonly determined using pre-installed
measurement units, such as odometers, inertial measurement units (IMUs), gyroscopes, and information from the controller area
network (CAN) bus. Localization, using external sensors such as GPS or dead reckoning by IMU readings, determines the vehicle s
global and local position. Lastly, surrounding-sensing uses exteroceptive sensors to perceive road markings, road slope, trac signs,
weather conditions, the state (position, velocity, acceleration, etc.) of obstacles including other vehicles, and even the state of the
driver (vigilance, drowsiness, fatigue, boredom due to monotony, etc.) (Vanholme, 2012).
Proprioceptive and exteroceptive sensors can be categorized as either active or passive sensors (Vanholme, 2012). Active sensors
emit energy in the form of electromagnetic waves and measure the return time to determine parameters such as distance. Examples
include sonar, radar, and LIght Detection And Ranging (LIDAR) sensors. Passive sensors do not emit signals, but rather perceive
electromagnetic waves already in the environment (e.g., light-based and infrared cameras). Determining which sensors or combination of sensors that are best suited for AV applications can be dicult. Therefore, before choosing a specic sensor or a combination of sensors, it is important to consider design factors such as: (1) the proprioceptive or exteroceptive sensor information
required, (2) whether a passive or active sensor should be used, (3) budget, and (4) whether to use a single sensor or multiple sensors.
Localization typically uses a combination of sensors such as GPSs, IMUs, odometers, and cameras (by matching between primitives and a map, i.e., SLAM) for high precision results (Levinson and Thrun, 2010; Marais et al., 2014; Vanholme, 2012; Wolf and
Sukhatme, 2004). Data fusion from multiple sensors can minimize shortcomings of individual sensors and increase the reliability and
robustness of the system (Bresson et al., 2016; Gruyer et al., 2015). Depending on budget, sensors used for localization can range from
a very expensive, highly accurate and precise GPS (RTK) and IMU (with ber optics) system or a combination of less expensive GPS
and IMU sensors coupled with other technologies, such as 3D obstacle detection systems (Obst et al., 2012). For production vehicles,
developers commonly aim to minimize cost and maximize safety; thus, the second method is often used.
The combination of low-cost IMUs, which can localize reliably for short periods of time (such as through tunnels), and GPSs,
which can localize reliably for long periods of time, but may lose connection when in remote areas or through tunnels, can eectively
reduce positioning errors and provide localization information during GPS outages (Sasiadek and Wang, 2003). This is a classic
example of multisensor fusion (Kaviani et al., 2016), which will be discussed further below. One example of such multisensor fusion
in the literature includes (Gruyer et al., 2016a), in which researchers fuse GPS, INS, road marking detection, and road marking map
information to achieve sub-decimeter positioning accuracy. Another example includes (Bresson et al., 2016), in which researchers
fuse various levels of data (e.g., direct sensor information, algorithm output, high-level object information such as obstacle dynamics
and road markings, etc.) to provide improved positioning accuracy.
In general, AV developers use dierent combinations of light-, sound-, and/or vision-based sensors for detection of the vehicle s
surroundings. These sensors can be either passive or active with most current passive sensors being less expensive than active sensors,
such as the active sensor, LIDAR (Ros et al., 2015; Sun et al., 2006). However, active sensor (specically LIDAR) prices may be
dramatically decreasing with the advent of start-up companies, such as Innoviz (Ackerman, 2016a), which aims to develop a low cost
LIDAR (less than $100.00 USD), and large investments into the LIDAR company, Velodyne (Vincent, 2016). Additionally, as the
development of AV technology using active sensors increases, active signals may begin to interfere with each other (Bertozzi et al.,
2000; Sun et al., 2006), making these sensors impractical. For instance, in high-density trac conditions, radar systems may pick up
other vehicles radar signals, causing false detections, interference and additional uncertainty (Hischke, 1995; Schipper et al., 2015).
Vision-based systems do not face this challenge, which is a key factor leading some researchers to focus on vision as a primary
perception mode for future AV systems (Bertozzi et al., 2000; Ranft and Stiller, 2016; Sivaraman and Trivedi, 2013).
Nevertheless, vision-based systems have their own shortcomings. A key concern with vision-based systems is that computing
distance information requires complex algorithms, whereas active sensors can directly determine this information with high accuracy. Stereovision (which consists of two cameras mounted at a horizontal distance from one another) can provide 3D information
such as depth; however, when the point of interest is far away (e.g., on the horizon), the images collected by each camera become
eectively the same, and are no longer able to provide the required information for 3D perception (Gu, 2014). As well, the computational complexity of creating a 3D space using stereovision (computation of a disparity map) is much larger than creating 3D
space using 3D LIDAR systems, such as the Velodyne HDL or Ibeo LUX (Huber et al., 2011; Maddern and Newman, 2016).
For more information about the advantages and disadvantages of common passive and active sensors, please refer to Table 2. The
purpose of this table is to provide a general overview of common AV sensors for newcomers to AV research. It also may be useful as a
quick reference point for presentations. Research and development in the area of AV sensors is intense. New products with higher
accuracies and better performances are introduced on almost a daily basis. The reader is encouraged to use this guide as a starting
point and check the state-of-the-art. For example, new generation LIDAR and radar, such as the solid-state (or phased array) LIDAR
(Duy, 2017; Ackerman, 2016b), may not generally be used in AVs today, but the reader is encouraged to investigate such technology
9

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

further.
To compensate for individual shortcomings, sensors can be coupled via multisensor fusion. Multisensor fusions benets, including improving perception accuracy, reliability, and robustness, are widely known and used in AV research (Dasarathy, 1997;
Kaviani et al., 2016; Sasiadek and Wang, 2003; Gruyer et al., 2016a; Bresson et al., 2016). As mentioned previously, a classic
multisensor fusion example is GPS/INS fusion (Kaviani et al., 2016). Another example of multisensor fusion includes combining
camera and LIDAR sensor data (Ashraf et al., 2017). In Daraei et al. (2017), LIDAR and camera data is fused to decrease uncertainty
and increase detection accuracy. Combining sensor data from multiple dierent sensors with an identical eld of view is an example
of competitive fusion (Durrant-Whyte, 1990). LIDAR and camera data can also be fused purely for calibration purposes (Zhou, 2014).
For example, the depth information from the LIDAR can be used to eciently nd a projection matrix between the vision and LIDAR
sensors (Park et al., 2014).
Sensor fusion is also not restricted to fusing data from multiple sensors. Sensor fusion can also be performed by fusing data from
multiple readings of a single sensor to obtain a more reliable output (Elmenreich, 2002). However, in general and in a more traditional sense, one may choose two identical sensors with an identical scope of work to make the system more reliable through system
redundancy. This is called competitive fusion (Durrant-Whyte, 1990). One can also use two complementary sensors to increase
coverage. For example, using complementary LIDAR on either side of a vehicle to cover a wider angle ahead of the vehicle (DurrantWhyte, 1990).
Sensor fusion is widely used for a number of reasons including more reliable data outcomes, more coverage, applicability, and
above all, lower initial investment (equipment cost) though at a higher computation cost. The lattermost reason is often why sensor
fusion is preferred over using single, highly accurate sensors. The initial monetary cost of a highly accurate sensor is generally
signicantly higher than the cost of two low-cost sensors, which can generally achieve similar or better results to single sensor
algorithms via sensor fusion (Leonard et al., 2008; Suhr et al., 2017; Sanchez-Lopez et al., 2016; Park et al., 2017). It can be
mathematically proven that the covariance of measurements of two sensors is less than that of both of them regardless of their
individual variances (Maybeck, 1979). Nonetheless, the use of highly accurate sensors is generally a good thing regardless of them
being fused to other sensors, provided the cost is still aordable for automotive applications.
To further illustrate the usefulness of sensors based on their accuracy and cost, consider the IMU. An cost eective IMU
($10$100) alone is generally not accurate enough for any robotics or AV applications, but can be used in a sensor fusion system to
increase accuracy to a useful level. A mid-level IMU ($1000$2000) may be sucient for certain navigation problems (short-term
motion estimation), but it is neither accurate enough to be used as a sole sensor (i.e., dead reckoning) nor aordable for widespread
AV use. Finally, high-end IMUs (>$10000), which are often used in aircrafts, are quite accurate, but remain beyond the reach of AV
applications due to high initial costs (Douxchamps, 2017).
The sensor arrangements chosen for AV research and commercial vehicles are noticeably linked to the vehicles specic autonomous application and desired maneuvers. For example, the vehicles in the DARPA Urban Challenge were generally outtted with
multiple, expensive LIDAR and radar sensors, but lacked sonar sensors, since the challenges did not focus on low-speed, precise
maneuvers (such as parallel parking). Conversely, many production vehicles, such as the Tesla Model S and the Mercedes-Benz S
class, include ultrasonic (sonar) sensors for parking maneuvers, but do not use LIDAR, due to current high costs. Infrared cameras,
which are used to detect pedestrians and other obstacles at night, are predominantly found on production vehicles, possibly since
research vehicles may primarily drive during the day. Other commercial vehicles, such as Renault, Volvo, and Audi, use Mobileye
technology. This technology consists of multiple mono-cameras with dierent focal points. It is aordable and ecient for motorways, albeit less eective in suburban and urban situations (Gitlin, 2016c). For more information on sensor arrangements in prominent research and commercial vehicles, see Table 3.
Furthermore, Table 4 provides an overview of the autonomous features available in commercial vehicles. (Note that since research vehicles are continually evolving as dierent research topics are investigated, it is dicult to know which features a vehicle
may have at any point in time; thus, a table for autonomous research vehicle features was omitted.) Currently, many research vehicles
are being tested with level three autonomy (limited self-driving automation (NHTSA, 2013)) whereas the high-end commercial
vehicles are generally available with level one or two autonomy (function-specic automation or combined function automation
(NHTSA, 2013)) and contain dierent autonomous and semi-autonomous features.
3.2. Areas for automotive sensor and perception improvement
Even though sensor technology has rapidly advanced, there are still many areas that require improvement before level ve
autonomy can be achieved. Three major challenges related to sensor technology that remain unresolved are: (1) perception in poor
weather conditions, (2) perception in changing and unfavorable lighting conditions and (3) human perception of automotive sensors.
3.2.1. Perception in poor weather conditions
Perception in poor weather conditions such as snow, heavy rain, and fog is an important AV research topic as these scenarios
continuously prove to be problematic even for human drivers. In snowy conditions, it has been found that both vision-based and
LIDAR-based systems have extreme diculty (Rasshofer and Gresser, 2005). Many vision-based AVs rely on observing the road
markings to navigate the roads. However, a thin layer of snow can cause these markings to completely disappear, making navigation
dicult. Furthermore, even obscured, dirty, worn or painted-over road markings (without snowy conditions) can cause diculty for
the AV s perception system (Rebut et al., 2004). In addition, the heaviness or density of the snow has been found to aect the
LIDAR beams causing reections o snowakes producing phantom obstacles (Radecki et al., 2016). These phantom obstacles
10

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

can inhibit the AV s ability to correctly judge the environment and may cause the vehicle to mistakenly stop. In rainy or foggy
conditions, similar diculties arise. LIDAR has been found to detect rainwater splashing upwards from behind vehicles as phantom
obstacles, and fog can obscure the cameras views, inhibiting the ability to reliably perceive the vehicle s surroundings (Rasshofer
and Gresser, 2005).
As noted in Table 2, radar generally performs well in poor weather conditions. However, autonomous driving cannot rely purely
on radar for perception due to the radar s inability to perform robust classication and detect road markings. A potential solution to
the challenges posed by driving in poor weather conditions may instead lie in vision algorithm improvement. Since humans can drive
safely in rain and snow using only their eyes for perception, algorithms that mimic biological vision have the potential to allow for
safe autonomous driving in heavy rain or snow. An introduction to such biomimetic vision can be found in Denuelle and Srinivasan
(2015), Lowry et al. (2016), Safwan et al. (2013) and Wang et al. (2011b); however, current research (to the best of the authors
knowledge) does not yet address biomimetic vision for navigation in poor weather conditions.
In Radecki et al. (2016), sensor fusion of camera, LIDAR, and radar sensors was used to detect pedestrians and vehicles in cloudy,
sunny, snowy, rainy, and dark conditions. However, according to the authors, the system still requires improvement to robustly and
consistently detect obstacles under these conditions. This may be completed by using additional sensors or utilizing connected vehicle
technology to further validate sensor data. Other techniques, such as using stereovision and scene priors (Gehrig et al., 2013) or
relying on taillight recognition (Cui et al., 2015), may address this problem.
3.2.2. Perception in changing and unfavorable lighting conditions
Lens-ares, large shadows and other unfavorable lighting conditions also present diculties for perception. For example, vision
systems may confuse large shadows to be parts of other objects (Balan et al., 2007). Furthermore, dierent visual cues (such as
taillights, reective road markings, etc.), and/or thermal imaging (far-infrared) cameras may need to be added to current perception
systems to enhance performance in low-light conditions or at night (Borkar et al., 2009; Hurney et al., 2015; Schamm et al., 2010).
Nonetheless, even systems using such technology can have diculty detecting and tracking obstacles. According to Hurney et al.
(2015), a signicant amount of far-infrared camera detection and tracking algorithms are not computationally ecient enough to be
used in real-time.
Other perception systems attempt to address light condition problems by relying on a priori information about the environment.
Stanford updated their research vehicle, Junior, to be able to detect trac lights in diering lighting conditions using an a priori list of
trac light locations (Levinson et al., 2011). Nevertheless, relying on a priori information can cause problems when there are changes
to the environment. For example, if a new trac light was installed without Junior s a priori list being updated, the vehicle would not
attempt to detect it (Levinson et al., 2011).
Other methods may instead rely on active sensors, such as LIDAR, to overcome the lighting condition problem. Such sensors do
not require external light for perception, allowing them to detect obstacles in poor light and at night (Jo et al., 2017). Nonetheless,
LIDAR data often contain noise when observing complicated or deeply textured objects, such as bushes, hindering the system s
perception (Jo et al., 2017). Thus, to better address the lighting condition problem, data from multiple sensors, such as cameras and
LIDAR sensors, can be combined, contributing dierent advantages in poor lighting conditions to achieve better results (Jo et al.,
2017; Radecki et al., 2016). Although researchers have attempted to address driving in poor lighting and weather conditions, it is
evident that extensive research is still required to resolve the remaining challenges before vehicles can reliably drive in such conditions.
3.2.3. Human perception of automotive sensors
In order for autonomous features to eectively increase safety, there must be a certain level of public acceptance and understanding of AV technology, including an understanding of the capability of AV sensors. As discussed previously, automotive sensors
are integrated into vehicles to assist human drivers with tasks such as detecting obstacles, maintaining a set speed, and emergency
braking. When humans do not understand the capability of sensors or the sensors are insucient for the driving task, the human
driver may either rely too much on the sensor or disregard the sensors readings entirely. Thus, it is important for sensors to be
sucient for the driving task and as capable and robust as possible. Furthermore, human drivers must be informed of the capability of
sensors to prevent misunderstandings that could potentially cause accidents.
To further illustrate the need for improved sensors as well as increased public understanding, the following example is provided.
Long-range automotive radar, which typically has the longest range of all AV active sensors, only has a range of 4.57.5 s at highway
speeds (150250 m range (Henawy and Schneider, 2011; Rasshofer and Gresser, 2005) at a speed of 120 km/h). According to ICBC
standards, human drivers are instructed to look at least twelve seconds ahead to ensure safe driving (British Columbia (ICBC), 2015).
When solely comparing these numbers, the sensors seem insucient. However, since computers have the ability to react much faster
than humans do, the current state-of-the-art sensor ranges (especially when supplemented with vision, which can detect obstacles
further down the road) are generally sucient for safe driving, as shown by Google and Tesla s excellent driving records (Levin and
Harris, 2017).
Thus, even though sensor ranges may in fact be sucient, it is important to ensure the public understands the technology s
capacity (e.g., its ability to adequately detect obstacles in typical conditions) as well as its limitations (e.g., its inability to detect
obstacles past a certain distance or in poor weather conditions). According to Kyriakidis et al. (2017), the public may not be able to
benet from intermediate stages of autonomous driving (e.g., Level 2 and 3 autonomy, in which the driver is still required to take
control of the vehicle) since drivers may not understand the autonomous features extent and limitations and therefore may not safely
regain control of the driving tasks when necessary. Furthermore, studies have found that many drivers prefer to have full control of
11

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

the driving task (Kyriakidis et al., 2015, 2017); thus, consumers may not utilize autonomous features thereby also declining to utilize
the safety benets of such technology. The technology behind AVs may continue to evolve rapidly, but human perception of the
technology may limit its reach and benets. An example illustrating the importance of the public s perception of AV sensors and the
robustness of these sensors includes the fatal crash on May 7, 2016 involving Tesla s autopilot. In this accident, the autopilot s vision
system failed to see a semi-truck trailer against the white sky. Furthermore, the driver, who likely assumed the autopilot system could
detect all obstacles on the road, failed to take control of the vehicle, resulting in a fatal collision (Greenemeier, 2016).
Evidently, it is important to ensure that the sensors being used are as capable and robust as possible, and that the public understands the benets of AV technology and how to use this technology eectively. Nonetheless, since the vast topic of human factors
(HF) in automation is outside the scope of this paper, additional articles of interest that address this topic include (Bansal et al., 2016;
Folsom, 2011; Schoettle and Sivak, 2014; Vagia et al., 2016), which investigate the public s opinion and/or social ramications of
autonomous driving features, and (Banks and Stanton, 2016b,a; Heikoop et al., 2016; Kim and Yang, 2017; Lu et al., 2016; Mok et al.,
2015; Ohn-Bar and Trivedi, 2016; Payre et al., 2016; Saito et al., 2016; Stanton et al., 2007), which analyze the safety and eectiveness of drivers interactions with autonomous features.
Although many commercial and research vehicles contain varied sensor combinations to increase perception reliability and
robustness, much work still needs to be done before level ve autonomy is completely achieved. Sensor reliability in poor weather
conditions, sensor cost (especially active sensor cost), and the public s understanding of autonomous sensors and systems must
improve before fully autonomous commercial vehicles should be available for public use. Furthermore, monitoring systems must be
developed to detect and identify system failures (e.g., faulty sensors/actuators, algorithmic errors) and degraded operating conditions
to ensure the AV s sensors are working correctly while on the road. Although this topic is not the focus of this paper, it will be briey
discussed in the next section, as it is important to consider when developing any AV technology.
3.3. Sensor failure and operating condition monitoring
Once AVs can robustly perceive the environment, they will also need to be able to detect and identify sensor and perception
failures and/or degradations. An analogy can be made between a human driver perceiving an illuminated engine light and deciding
to safely pull over, and an AV determining that one of its sensors is compromised and safely maneuvering the vehicle o the road.
To address this diculty in aerospace systems, often multiple identical sensors are incorporated to increase redundancy, validate
each sensor s correct operation through comparison, and thus determine whether any of the sensors are faulty. However, duplicating
sensors can be costly and inecient (Simani, 2003). Thus, other fault detection and isolation (FDI) methods have been proposed, such
as using an observer (or system model) to determine whether a drone s current behavior is signicantly dierent from its fault-free
behavior (Heredia et al., 2008; Simani, 2003).
In autonomous road vehicle studies, researchers have found some success using similar model-based methods (Huang and Su,
2015; Jaradat et al., 2013; Shrivastava and Rajamani, 2001); however, these methods require near-perfect system models in order
to be eective (Pous et al., 2017). Moreover, nding a near-perfect AV system model is extremely dicult due to the nonlinearity of
vehicle maneuvers and unpredictability of the vehicle s surrounding environment (Pous et al., 2017). Thus, in real-world situations,
model-based FDI methods may not be eective.
An alternative proposed in Pous et al. (2017) uses analytical redundancy and nonlinear transformation methods to compare
sensor metrics in order to detect and identify faulty or deviant sensors. In this application, the false alarm rate for sensor faults is low;
however, the missed detection rate is high. Thus, continued research into FDI methods that do not rely on near-perfect system models,
yet correctly detect, and identify faults is encouraged.
3.4. Perception Algorithms
This section provides a brief, high-level overview of algorithms currently used in AV perception. Note that this paper does not aim
to review or survey perception algorithms in detail (other papers, including (Sivaraman and Trivedi, 2013; Viswanathan and Hussein,
2017; Javaheri et al., 2017; Nguyen and Le, 2013; Geiger et al., 2012b; Sun et al., 2006) do so already), but to provide an overview
for newcomers to the AV eld. At the highest level, perception algorithms can be separated into three categories: mediated perception, behavior reex perception, and direct perception. In mediated perception, algorithms develop detailed maps of the AVs
surroundings through analyzing distances to vehicles, pedestrians, trees, road markings, etc. This is the most common AV perception
technique used in research today. Conversely, behavior reex perception algorithms use articial intelligence techniques to map
sensor data (e.g., an image of the vehicles environment) directly to driving maneuvers. The third category, direct perception,
combines the articial intelligence of the behavior reex approach with the metric collection of the mediated perception approach.
This approach is a newer concept and discussed in more detail in Section 4.4 (Chen et al., 2015). In addition to these three paradigms,
perception algorithms may be categorized as vision-based algorithms, point-cloud based algorithms, or a mixture of the two (through
using fusion techniques, as discussed in Section 3.1). Vision-based perception relies predominantly on camera data. Thus, these
algorithms dissect pixel-based video to detect vehicles, pedestrians, and other obstacles in the environment. Algorithms may use
geometry, optical ow, color, or other image characteristics for detection (Sivaraman and Trivedi, 2013). Point-cloud based perception relies predominantly on data from points (or distances measured to objects) in 3D space collected by active sensors. The
algorithms may involve deriving structure from masses of points through the density, geometry or pattern of the points in order to
detect objects (Nguyen and Le, 2013).
12

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

4. Localization and mapping
Localization and mapping has been a popular research topic for many years. It has evolved from stationary, indoor mapping for
mobile robot applications, to outdoor, dynamic, high-speed localization and mapping for AVs. The initial problem researchers faced
was determining how to measure the environment while moving to create reliable and accurate maps for successful navigation
through indoor areas. This required the robot to determine where it was relative to obstacles and walls while simultaneously creating
(and storing) an accurate map. This process became known as Simultaneous Localization and Mapping (SLAM) (Durrant-Whyte and
Bailey, 2006).
4.1. Initial algorithms
Initial SLAM algorithms were generally variants of occupancy grid maps or topological/feature-based maps (Leonard et al., 1992;
Lowry et al., 2016). Occupancy grid maps provide the likelihood of the presence of an obstacle in each gridline of space. The
following steps are generally completed in such grid map algorithms: (1) the robot senses for obstacles, (2) the likelihood of obstacles
presence is increased or decreased on the grid depending on whether an obstacle was detected, (3) the robot localizes itself on the
grid using odometry and sensed distance-to-landmark information, (4) the robot moves somewhere else in the space, and (5) the
process is repeated until a complete map is created or the robot s goal is reached (Biswas et al., 2002; Leonard et al., 1992; Stepan
et al., 2005). A similar process is implemented for feature-based maps; however, the amount of information collected is selective and
data storage is much more ecient. Instead of storing a large grid containing everything in the space, only features or landmarks and
the relationship between these landmarks (such as distance) are stored (i.e., object states) (Leonard et al., 1992).
After the initial SLAM problem was solved, the next diculty was determining whether obstacles were dynamic or stationary. In
initial SLAM algorithms, once a robot recognized an obstacle, the robot would always avoid that area on the map, regardless if it was
a stationary or moving object. To address this challenge, algorithms needed to determine not only the likelihood of an object actually
being there (depending on the reliability and accuracy of the robot s sensors), but also the likelihood of an obstacle being there in the
future. One early method used a feature-based map that determined where features were presently, and predicted where objects
would move in the next algorithmic cycle (Leonard et al., 1992).
To move towards AV localization and mapping, researchers began developing road-based algorithms. These were initially based
on robotic SLAM algorithms; however, important dierences between robotic and road vehicle mapping (such as the environment
surrounding the vehicle and the speed of the vehicle) became apparent. Specically, the majority of the robotic algorithms were
designed to map indoor, highly structured, well-lit environments, rather than outdoor, variably-lit, road-based environments (in
which AVs need to localize themselves globally (on the earth) as well as locally (on the road)). Furthermore, AVs operate at highwayspeeds, rather than indoor walking speeds, which require faster and more ecient mapping algorithms.
4.2. Global and local localization
Unlike indoor robots, AVs must be able to: (1) globally localize themselves in order to drive from one part of the world to another
and (2) calculate their local location relative to other obstacles in order to follow safe driving rules, such as remaining centered and in
the correct lane. Local localization algorithms are similar to indoor robotic algorithms, with the addition of road marking and road
shape detection (Urmson et al., 2008), whereas global localization generally requires the use of GPS and IMU technology (and may
even use infrastructure beacons or patterns) (Kaviani et al., 2016).
In (Levinson et al., 2007), global and local AV localization and mapping is established using GPS, IMU, wheel odometer, and
LIDAR information to create an urban map with a resolution of ve centimeters. The algorithm distinguishes between static and
dynamic elements of the environment (for example, between the road and vehicles) and removes the dynamic elements from the
map. This map can then be used for autonomous driving and localization and is an example of an a priori mapping strategy. This
method was further improved by using additional probabilistic modelling in Levinson and Thrun (2010). Although both of these
techniques are quite accurate, they require human-driven vehicles to pre-map the road and provide this information to the AVs prior
to driving.
In other techniques, AVs map and localize while driving. This is known as Simultaneous Localization and Mapping (SLAM)
(Dissanayake et al., 2001) and will be discussed further in Section 4.3.2. Stanford s entry in the DARPA Urban Challenge used local
road curvature, the reectivity of the road and the vehicle s inertial movement to increase the accuracy of GPS information, which
was provided to the vehicle s path-planning algorithm in real time (Montemerlo et al., 2008). Team MIT s entry relied less on GPS
information (using it only when absolutely necessary) and more heavily on local perception to provide data to the path-planning
algorithm (Leonard et al., 2008).
4.3. Increasing the eciency of localization and mapping
Since vehicles drive at much higher speeds than indoor robots, AVs require faster and more ecient algorithms. This is typically
addressed through one of two methods: (1) by using pre-created, detailed and/or HD dynamic maps (leaving only localization to be
completed during the autonomous driving) and (2) by selectively mapping certain areas of the road (similarly to how humans focus
on the road in front of them).
13

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

4.3.1. A Priori and HD Mapping and Localization
Many state-of-the-art vehicles, such as the Google, Uber and Navya Arma vehicles, use a priori mapping methods (Boudette, 2017;
Muoio, 2016; Navya, 2017). These methods consist of pre-driving specic roads and collecting detailed sensor data, such as 3D
images and highly accurate GPS information. Large databases store the created detailed maps for vehicles to drive autonomously on
those specic roads. Local localization is performed by observing similarities between the a priori maps and the current sensor data,
whereas obstacle detection is achieved through observing discrepancies between the a priori maps and the current sensor data. This
method is extremely eective for driving along roads that do not change often. However, as discussed previously, if drastic changes
occur, the method may be unable to localize correctly or may detect harmless discrepancies, such as lighting or weather changes, as
obstacles that need to be avoided.
In Ros et al. (2015), a priori mapping and localization is accomplished by combining the accuracy of oine perception with the
real-time computation of online perception. The novelty of this method is that visual data is used for a priori mapping, whereas in
most systems, active sensor data, such as LIDAR data, is used. This is advantageous, since with the expected increase in number of
AVs on the road, active sensors may become unreliable due to the increased density of active sensor signals (Hischke, 1995). Alternatively, in Obst et al. (2012), localization was achieved by pre-creating 3D maps of tall surrounding buildings for raytracing
purposes. This addresses the problem that vehicles cannot always obtain strong, reliable GPS signals in dense urban areas.
Google s vehicles, which also use a priori mapping and localization methods, have autonomously driven over 2.4 million kilometers, including driving through complex situations such as intersections (Google, 2017). Despite these outstanding numbers, a
priori methods such as these cannot handle many common situations. For instance, due to the limited number of distinct landmarks on
bridges, Uber s vehicles cannot localize themselves, and thus are unable to drive over bridges without human intervention (Muoio,
2016). Weather conditions, such as snow or fog, also create problems for this technique due to the drastic change in the environment
s appearance, making map matching dicult. Changes to the road, such as new construction zones, speed limits or trac lights, also
pose problems, as the vehicle assumes that major components of the environment will be identical to the original a priori data. Thus, a
vehicle with an a priori-based localization and mapping may speed through construction zones or new trac lights without noticing
the change in trac rules (Sorokanich, 2014). Furthermore, in order for this technique to be viable on a large scale, all roads around
the world would have to be initially pre-mapped and then continually re-mapped in order for vehicles to adapt to new situations.
An emerging technique under the umbrella of a priori mapping and localization includes HD mapping. HD maps are based on a
priori data; however, they are dynamic, updated in the cloud, and utilize V2I and V2V communication to improve accuracy.
Essentially, HD maps provide centimeter-accurate a priori map information to AVs, and AVs provide the cloud-based HD maps with
updated map information based on collected sensor data. The maps are continually updated in this manner oine (Seif and Hu,
2016). Although HD maps are not widely utilized today, initial prototypes have been implemented by Audi and BMW (Abuelsamid,
2017; Rabel, 2016; Seif and Hu, 2016). The main challenges with HD map technology regard the large size of the HD map data,
latency in transmissions between the cloud and the AV, and the computational eciency of the oine data processing (Seif and Hu,
2016). With improvements in these areas, HD maps could become the standard a priori mapping and localization technique for AVs.
4.3.2. Simultaneous localization and mapping (SLAM)
Vehicle localization performed while on the road (i.e., the localization portion of Simultaneous Localization and Mapping
(SLAM)) does not rely heavily on a priori information. This allows AVs to continuously observe the environment and readily adapt to
new situations. Consequently, localization through SLAM requires more computationally intensive algorithms and may be subject to
more uncertainty depending on the sensors used and surroundings. These algorithms must dierentiate between static objects in the
environment (which can help with localization) and dynamic objects (which should not be used for localization, since they move)
without any prior knowledge. Many of these algorithms currently rely on road marking detection for local localization, which pose
problems when the road markings have faded, are partially occluded (by weather or other objects), or are non-existent (Gruyer et al.,
2016a; Rebut et al., 2004; Wu and Ranganathan, 2013).
Various probabilistic techniques have been researched for discriminating between static and dynamic objects in the environment
using SLAM-based methods. Early techniques include vehicles from the DARPA Urban Challenge. Carnegie Mellon University s
vehicle, Boss, dierentiated between static and dynamic obstacles by agging them as either moving, not-moving, or observed
moving, and then transferred these obstacles from a static map to a dynamic map and vice versa, allowing the vehicle to plan
dierent routes based on obstacles predicted motions (Urmson et al., 2008). An improvement on this method can be found in Cho
et al. (2014), which includes detection and classication of vehicles, pedestrians, and cyclists.
Although Carnegie Mellon s entry did not pre-map the Urban Challenge course (and thus can be considered to be using online,
SLAM-based techniques), it relied heavily on a priori information, such as road shape and accurate road position (Urmson et al.,
2007). Team MIT s submission took a dierent approach, relying almost solely on currently perceived obstacles and road markings
for localization. In their opinion, since human drivers do not require detailed, a priori maps or even GPS information, vehicles should
not necessarily require this information either. Thus, similarly to a human, their vehicle gathered information about the environment
while on the road to produce a local map, which was sent to a motion planner module (Leonard et al., 2008).
Other vehicles have used similar SLAM-based techniques. VisLab developed multiple AVs that relied on perception (such as
camera vision) for local navigation and used sparsely detailed maps for global localization (Broggi et al., 2013). The vehicles semiautonomously drove 13,000 km from Italy to China through various weather, road, and trac conditions. In this challenge the
vehicles did not plan global paths, but instead followed waypoints or platooned behind a human-driven vehicle in order to move from
one point to another point on a map (Broggi et al., 2012), revealing a need for further research into real time SLAM-based methods. In
Marais et al. (2014), image processing was completed while on the road to improve GPS accuracy. Instead of creating
14

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

computationally heavy 3D maps, GPS accuracy was improved by determining where satellites were located (relative to buildings and
the ego-vehicle) through image processing and excluding data from satellites that were hidden from view. However, the process was
not as accurate as desired. Table 5 provides an overview of the localization and mapping methods used by prominent research
vehicles.
When developing SLAM-based autonomous driving algorithms, it is important to consider current human driver behavior and
ensure AVs can safely react to this behavior (Schnelle et al., 2016). The AV must be able to perceive and understand human drivers
intentions even when they are not correct or lawful (e.g., accidental lane departure (Wu et al., 2012) versus the intention to change
lanes without signaling, which may be shown by a vehicle inching towards another lane) (Windridge et al., 2013). To address this
diculty, naturalistic driving studies (NDSs, in which driving behavior is observed and analyzed) have been performed, such as the
100-Car NDS (Neale et al., 2005). Although NDSs and human driving behavior are not the topic of this paper, the data collected
through these studies may be used in multiple ways to address real time SLAM challenges and other autonomous driving challenges.
In Satzoda and Trivedi (2015), NDSs were analyzed to extract semantic and quantitative information from video, vehicle dynamics, global positioning, map, and orientation data. The data extracted included the vehicle s position within the lane, which lane
the vehicle was in, the vehicle s speed, the road curvature, and the density of trac. Although the intention of such extraction was to
begin the automation of semantic analysis for crash prediction, the techniques used to extract such vehicle data may be used for
SLAM-based vehicle localization.
In Janardhanan et al. (2015), researchers utilized similar lane marking detection techniques to localize while on the road and
navigate through an experimental emergency scenario. In this article, the ego-vehicle determined its lane position as well as distances
to various objects to plan and execute a safe maneuver around a vehicle parked partly in the ego-vehicle s lane. Although the
techniques used were eective, they were only used in a single scenario, in which there was only a single vehicle to avoid. In Wu and
Ranganathan (2013), however, real time road marking detection was tested in more complicated scenarios. By determining the
position and orientation of road markings in images collected from the ego-vehicle s camera, the relative pose and position of the
ego-vehicle within a lane could be determined. Nonetheless, the global position of this vehicle could not be determined without a
highly detailed map of road marking locations (i.e., a priori information). Furthermore, the absolute error of the pose estimation was
quite large, at 0.99 m (Wu and Ranganathan, 2013). A more recent method, described in Gruyer et al. (2016a), localized using road
marking detection, road marking map, GPS, and INS/odometer sensor information and obtained highly accurate results; however,
this approach also required a priori information.
In situations with multiple vehicles, V2V technology can optimize SLAM-based, lane-level localization. A Markov-based information sharing approach is used to communicate GPS data between vehicles, resulting in improved lane-level localization over
single-GPS localization in Dao et al. (2006). However, it is noted in this article that there is still signicant noise in the localization
and further research in this area should continue.
The Tesla Model S is a well-known example of a semi-autonomous commercial vehicle that primarily uses real time SLAM-based
techniques. The Tesla s autopilot feature allows the vehicle to drive itself along freeways and highways, performing lane changes and
autonomously adjusting its speed (under constant human supervision). However, the Tesla Model S cannot drive by itself through
complex situations, such as intersections (Canada, 2017a) and still requires the human driver to take back control of the vehicle in
unknown or unpredictable situations. This is a common theme throughout SLAM-based techniques. With further research into efcient perception data processing and situation-based high level decision-making, SLAM-based methods have the potential to be able
to drive in all situations (not only pre-mapped situations), and furthermore, to overcome the challenges that a priori-based methods
have been unable to address.

4.4. Recent emerging autonomous navigation approaches
Up to this point, it has been assumed that the autonomous navigation process includes the following stages: environmental
perception, localization and mapping, path planning, decision-making, and vehicle control. However, some researchers are developing AV algorithms that skip the localization and mapping and path planning stages1 and rely purely on environmental perception
to make driving decisions. Thus, instead of the process depicted in Fig. 1, the process resembles Fig. 4 or Fig. 5. According to Chen
et al. (2015), the navigation process could be streamlined by reducing the number of metrics (e.g., angle of the ego-vehicle, distance
to obstacles, distance to adjacent vehicles, etc.) analyzed, allowing more computation time for environmental perception. Furthermore, without a local localization and mapping stage, the perception stage does not require the high level of detail and thus is less
computationally expensive (Chen et al., 2015). To illustrate, instead of continuously detecting and classifying objects, the AV directly
links a particular image or scenario with a driving action through machine learning techniques. This process is called the behavior
reex approach and is diametrically opposed to the mediated perception approach, which is the standard method of AV perception
that has been discussed in this paper (which includes perceiving and analyzing the environment, localizing the ego-vehicle, and
planning detailed trajectories and maneuvers) (Chen et al., 2015). If the mediated perception approach falls out of favor, the requirements for the perception technology and as well as localization and mapping may change drastically. For example, the demand
for vision-based perception may increase, and the demand for highly detailed a priori information may decrease.
1
Note that the algorithms skip the local mapping and localization and path planning stages (i.e., creating detailed maps of the vehicle s surroundings and planning
detailed local paths to follow within this map). The global mapping and localization and path planning stages are still necessary for the vehicle to get from point A to
point B; however, they can be computed prior to driving and are thus excluded from the navigation cycle.

15

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

Fig. 4. Overview of the behavior reex autonomous navigation process. Note that in the behavior reex approach s perception stage, no specic metrics (e.g., angle of
the ego-vehicle, distance to obstacles, distance to adjacent vehicles, etc.) are analyzed.

Fig. 5. Overview of the direct perception autonomous navigation process. Note that in the direct perception approach s perception stage, specic metrics (e.g., angle
of the ego-vehicle, distance to obstacles, distance to adjacent vehicles, etc.) are analyzed.

One of the rst examples of an AV using the behavior reex approach is the Autonomous Land Vehicle in a Neural Network
(ALVINN) (Pomerleau, 1989). In this perception system, a neural network determines the vehicle s trajectory based on images and
laser range nder data. In a more recent example, a TORCS-simulated vehicle navigates a racetrack using the behavior reex approach, demonstrating the approach s ability to optimize acceleration and braking using only images as the input (Jan et al., 2013).
Although the behavior reex approach in these articles is much less computationally expensive than the widely-used mediated
perception approach, the authors point out that it is not robust enough to drive through highly complex situations or complete
complex maneuvers (Chen et al., 2015). As its name suggests, this approach is more of a reex and is likely most useful when
reacting to sudden events (e.g., collision avoidance or mitigation), but not for the development of high-level strategies or the prediction of hazardous situations (Chen et al., 2015). Thus, the authors propose the direct perception approach, which combines datagathering techniques found in the mediated approach with the computational eciencies of the behavior reex approach (Chen
et al., 2015). Due to this increased computational eciency, this approach could potentially address the problem of time and
computation restraints of typical SLAM-based approaches in industry today.
In the direct perception approach, the vehicle completes sections of the mapping-related computation (for example, determining
the vehicle s current angle on the road and the distance to surrounding vehicles and lane markings) but does not create a complete
local map or any detailed trajectory plans. Thus, this approach skips the majority of the localization and mapping stage and the
entirety of the path planning stage. This process is depicted in Fig. 5. Using image data, relevant, specied local localization metrics
(i.e., ego-vehicle angle, distances to road markings, and distances to nearby vehicles) and basic driving logic, the vehicle can directly
make driving decisions.
In Chen et al. (2015), a TORCS-simulated AV successfully navigates a busy racetrack using the direct perception approach. This
study uses deep neural networks and TORCS image data to determine the AV s next steering angle and velocity. Furthermore, the
same perception system was tested using video and images from the KITTI database, showing that the system could recognize lane
congurations and transitions in the real world (Chen et al., 2015). Although this method shows promising initial results in terms of
computational eciency (i.e., does not parse entire driving scenes) and controlling the vehicle comparably to the mediated perception approach (Chen et al., 2015), it is unclear whether this method will be able to control the vehicle safely in real-world driving
situations (e.g., less-structured or emergency situations).
Nonetheless, the direct perception approach developed in Chen et al. (2015) seems to have sparked great interest within the AV
eld. The direct perception approach is examined in Zhou (2016) to determine whether historical data has an eect on the approach
s future performance. By applying statistical methods, the researchers nd that historical data in direct perception is, in fact,
16

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

predictively useful. This provides grounds for testing more complex machine learning techniques with the direct perception approach
(Zhou, 2016).
Furthermore, (Xiong et al., 2016) looks at combining a deep learning approach with a safety-based control approach to autonomous driving. This approach aims to develop a deep-learning driving technique with increased safety measures. As in Chen et al.
(2015), the driving results are simulation-based and do not yet provide solid evidence for deep-learning-based approaches to be safe,
ecient, and eective methods for real-world autonomous driving.
The direct perception, deep learning approach to autonomous driving provides an interesting alternative to classical AV techniques. Depending on the results of future research, this approach may change how AV perception is managed and achieved.
5. Future research areas and AV technology advancements
The previous sections in this article provided information about AV history, AV perception sensors, and AV localization and
mapping while alluding to areas of research that need improvement. This section provides an in-depth summary of areas of AV
perception requiring further improvement as well as information about recent related advancements in technology.
5.1. Automotive sensor research areas and advancements
This section provides a summary of future areas of research to improve AV sensors and information about recent advancements in
AV sensors. The following list summarizes areas related to AV sensors needing further development:

 Improving detection and reducing uncertainty in poor lighting and weather conditions
 Improving detection and reducing uncertainty in complex environments
 Reducing uncertainty in sensor data by cross-verifying obstacle locations and signals through:






 Further development of sensor fusion algorithms
 Use of more sensors and sources for sensor fusion to build multiple layers of environment modeling
 V2V and V2I communication
Ensuring the public understands sensor capacity and limitations
Using more passive sensors (versus active sensors) or developing eective algorithms to counteract the increased density and
therefore interference of active sensor signals
Decreasing the overall cost of AV sensor systems by:
 Further developing sensor fusion algorithms using low-cost sensors
 Utilizing new low-cost, highly-eective sensors that may be introduced in the near future (e.g., new LIDAR from Innoviz
(Ackerman, 2016a))
Developing fault detection and isolation systems for automotive sensors and algorithms
Using multisensor data fusion in order to limit the impact of sensors drawbacks and exploit the advantages of each sensor through
using the sensors complementarity and redundancy in order to improve accuracy, certainty, and reliability

As several key challenges remain unresolved for AV perception, many companies are competing for a piece of the AV technology
market. Osram Opto Semiconductors just recently announced the development of a four channel LIDAR with a mass production price
predicted to be less than fty dollars (Howard, 2016). Another development in LIDAR technology includes low cost solid-state
sensors, which are more resilient and smaller than conventional LIDAR (Ackerman, 2016b; Ross, 2017). Improvement to LIDAR
technology such as this will help to reduce the price of autonomous features.
Additionally, Seegrid, a manufacturing robotics company, uses stereovision as the primary sensor for autonomous navigation
(Pettitt, 2016). Even though vision is currently not the primary sensor used by companies such as Google and Ford (Amadeo, 2017;
Center, 2016), many car manufacturers, including Audi, Mercedes-Benz, and Volvo, use the Mobileye sensor for their semi-autonomous features (Gitlin, 2016c). The Mobileye sensor primarily uses mono-cameras for obstacle detection and tracking, and road
marking detection (Mobileye, 2017). Furthermore, a recent project involving primarily vision-based sensors includes the Transit
IDEA Program. This system uses the Rosco/Mobileye Shield+ system to provide collision avoidance warnings to transit bus drivers
(Spears et al., 2017). This system, as well as a second vision-primary system was tested in over 30 h of real-world driving with
positive results, further contributing to vision-based AV perception research (Ke et al., 2017).
Tesla uses a similar concept (and used to utilize Mobileye technology (Lambert, 2017)) by integrating eight cameras, ultrasonic
sensors and radar for their autonomous features (Canada, 2017b). As discussed, combining these sensor technologies will reduce
shortcomings and provide necessary redundancy for autonomous driving. Taking a dierent approach, the company AutonomouStu
provides complete AV research and development platforms such as the Ford Fusion and Lincoln MKZ (AutonomousStu, 2017). These
platforms allow researchers to focus on the development of new algorithms to improve localization and mapping, path planning,
high-level decision making and vehicle control instead of focusing on the sensor fusion framework required for these algorithms.
5.2. Localization and mapping research areas and advancements
In terms of localization and mapping, continued research to increase the reliability, eciency and robustness of the algorithms is
required before vehicles can reliably localize themselves in all driving situations. This may be completed by improving a priori based
17

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

and/or SLAM-based localization and mapping methods. Additionally, it is expected that localization and mapping techniques will
continually be updated as the improved automotive sensing techniques become available. Further research focused on improving a
priori mapping and localization methods may include:

 Developing a system to compensate for the lack of large, interconnected, highly detailed databases that will be required for
autonomous driving based on a priori maps, potentially through HD map technology
 Addressing poor localization in snow, rain and fog; in areas with fewer landmarks (such as on bridges or along long straight



sections of roads); and along roads that have undergone large changes (such as construction sites) through:
 Improved a priori information about such situations and landmarks, and/or
 Improved SLAM through better sensing techniques and sensors
Incorporating cooperative driving techniques (V2V and V2I) into localization and mapping to reduce uncertainty and improve
vehicle safety (OBrien et al., 2016)
Developing a framework to ensure vehicles do not attempt to drive using outdated maps, and to address questions such as:
 How old is too old for an a priori map?
 Should vehicles legally be allowed to drive using an old map?
 How will vehicles become aware of trac pattern changes, such as construction zones?

SLAM-based methods also must be improved in terms of eciency, accuracy, reliability, and robustness for complex situations.
Further research to improve localization and mapping in real time may include:

 Improving the eciency of local localization and mapping algorithms to be used in real time (and not analyzed after the fact, as in
Satzoda and Trivedi (2015))
 Improving the robustness of SLAM in poor weather conditions
 Enhancing GPS reliability and accuracy, or reducing the cost of DGPS technology
 Utilizing further V2V and V2I communication to reduce localization uncertainty
Although the DARPA Urban Challenge attempted to address problems with a priori-based localization by preventing the entrants
from pre-mapping road sections, the vehicles that placed rst and second still relied heavily on a priori information, such as generic
road structure and road markings. Furthermore, the entries that did not rely heavily on a priori information also did not compete in
complex situations, such as urban areas with pedestrians and cyclists. Other vehicles that localize through SLAM have partially
addressed such situations, including the Tesla, but still lack the ability to autonomously drive through any situation other than
straightforward highway/freeway driving.
Determining a proper balance between relying on a priori information to increase algorithm eciency and current perception data
to increase the vehicle s ability to adapt to new situations remains an unresolved challenge in AV research. Certain aspects of a priori
data have proven to be necessary, such as address coordinates, whereas other a priori data is useful, but unnecessary, such as road
shape or detailed, pre-mapped 3D images. Future AVs will need to rely on a combination of current perception data and a priori
information; however, it is clear that further research and development is required to provide reliable, accurate, and robust enough
perception algorithms for full autonomy. In fact, perception stages remain essential to guaranteeing the development of ecient,
fully automated applications.
Furthermore, in recent works the direct perception approach and other deep learning driving techniques have been proposed to
improve performance (without relying on a priori information) by streamlining the autonomous navigation process by eliminating the
localization and mapping and path planning stages or using end-to-end deep learning techniques (Chen et al., 2015; Bojarski et al.,
2016). However, further research must be completed concerning how the direct perception approach handles real-world emergencies. This is especially pertinent since the primary purpose of AVs is to increase safety by decreasing human-related error.
Although sensor technologies allow AVs to outperform human perception in many areas, AVs are not yet ready to provide level
ve autonomy. With the aforementioned improvements, however, AVs will be much closer to providing the eciency, mobility, and
safety benets predicted.

6. Conclusions
In this paper, an overview of AV sensor technology, localization and mapping techniques, and future perception research areas
were presented. Although current perception systems implemented in Level 1 to Level 3 autonomous systems have been shown to
increase vehicle safety, there is still much to improve upon before fully autonomous vehicles will be available to the public. The three
main areas requiring improvement presented in this paper include: (1) reduction of uncertainty in perception, (2) reduction in cost of
perception systems, and (3) operating safety for sensors and algorithms. A fourth area not discussed in this paper, but that is of equal
importance, includes the eciency of computational methods and algorithms for AV perception. These methods comprise a large
amount of research conducted in the AV eld. Researchers are continually aiming to improve the eciency of detection and classication, localization and mapping, and other AV perception related algorithms (Huang et al., 2017; Jagannathan et al., 2017). All in
all, with further research and development of AV perception systems, AVs will likely be driving on public roads while increasing
driving safety, sustainability and mobility in the near future.
18

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

Acknowledgments
This work was supported by the Natural Sciences and Engineering Research Council of Canada [Grant No. 341887].
References
Abuelsamid, S., Feb. 2017. BMW, HERE And Mobileye Team Up To Crowd-Source HD Maps For Self-Driving. Forbes. <https://www.forbes.com/sites/
samabuelsamid/2017/02/21/bmw-here-and-mobileye-team-up-to-crowd-source-hd-maps-for-self-driving/>.
Ackerman, E., Aug 2016a. Israeli startup innoviz promises $100 solid-state automotive lidar by 2018. <http://spectrum.ieee.org/cars-that-think/transportation/
sensors/israeli-stealth-startup-innoviz-promises-100-solidstate-automotive-lidar-by-2018>.
Ackerman, E., Jan 2016b. Quanergy announces $250 solid-state lidar for cars, robots, and more. <http://spectrum.ieee.org/cars-that-think/transportation/sensors/
quanergy-solid-state-lidar>.
Amadeo, R., Jan 2017. Googles waymo invests in lidar technology, cuts costs by 90 percent. <https://arstechnica.com/cars/2017/01/googles-waymo-invests-inlidar-technology-cuts-costs-by-90-percent/>.
Amin, S.M., Garca-Ortiz, A., Wootton, J.R., 1995. Network, control, communication and computing technologies for intelligent transportation systems overview of the
special issue. Math. Comput. Modell. 22 (4), 110.
Ashraf, I., Hur, S., Park, Y., 2017. An investigation of interpolation techniques to generate 2d intensity image from LIDAR data. IEEE Access 5, 82508260.
AutonomousStu, 2017. <http://www.autonomoustu.com/platform/>.
Bacha, A., Bauman, C., Faruque, R., Fleming, M., Terwelp, C., Reinholtz, C., Hong, D., Wicks, A., Alberi, T., Anderson, D., et al., 2008. Odin: Team victortangos entry
in the darpa urban challenge. J. Field Rob. 25 (8), 467492.
Balan, A.O., Black, M.J., Haussecker, H., Sigal, L., 2007. Shining a light on human pose: on shadows, shading and the estimation of pose and shape. In: 2007 IEEE 11th
Int. Conf. Computer Vision. IEEE, pp. 18. http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4409005 .
Banks, V.A., Stanton, N.A., 2016a. Driver-centred vehicle automation: using network analysis for agent-based modelling of the driver in highly automated driving
systems. Ergonomics 59 (11), 14421452.
Banks, V.A., Stanton, N.A., 2016b. Keep the driver in control: automating automobiles of the future. Appl. Ergon. 53, 389395.
Bansal, P., Kockelman, K.M., Singh, A., 2016. Assessing public opinions of and interest in new vehicle technologies: an austin perspective. Transp. Res. Part C: Emerg.
Technol. 67, 114.
Barrachina, J., Sanguesa, J.A., Fogue, M., Garrido, P., Martinez, F.J., Cano, J.-C., Calafate, C.T., Manzoni, P., 2013. V2X-d: A Vehicular Density Estimation System That
Combines V2V and V2I Communications. IEEE. http://ieeexplore.ieee.org/abstract/document/6686518/ .
Behringer, R., Maurer, R.B.M., Sep 1996. Results on visual road recognition for road vehicle guidance. In: Proc. Conf. Intelligent Vehicles. pp. 415420.
Bergenhem, C., Huang, Q., Benmimoun, A., Robinson, T., 2010. Challenges of platooning on public motorways. In: 17th World Congr. Intelligent Transport Systems.
pp. 112. <http://www.sartre-project.eu/en/publications/Documents/ITS%20WC%20challenges%20of%20platooning%20concept%20and%20modelling
%2010%20b.pdf>.
Bertozzi, M., Broggi, A., Conte, G., Fascioli, A., Fascioli, R., 1998. Vision-based automated vehicle guidance: the experience of the argo vehicle. Tecniche di Intelligenza
Articiale e Pattern Recognit. per la Visione Articiale 3540.
Bertozzi, M., Broggi, A., Fascioli, A., 2000. Vision-based intelligent vehicles: State of the art and perspectives. Rob. Auton. Syst. 32 (1), 116.
Betters, E., Dec 2015. Self-driving cars: 14 automakers betting on driverless vehicles. <http://www.pocket-lint.com/news/136208-self-driving-cars-14-automakersbetting-on-driverless-vehicles>.
Biswas, R., Limketkai, B., Sanner, S., Thrun, S., 2002. Towards object mapping in non-stationary environments with mobile robots. In: 2002 IEEE/RSJ Int. Conf.
Intelligent Robots and Systems, vol. 1. IEEE, pp. 10141019. http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1041523 .
BMW, 2017. Bmw connecteddrive: Intelligent driving. <http://www.bmw.com/com/en/insights/technology/connecteddrive/2013/driver_assistance/intelligent_
driving.html>.
Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L.D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., Zieba, K., Apr. 2016.
End to End Learning for Self-Driving Cars. arXiv:1604.07316 [cs]ArXiv: 1604.07316. http://arxiv.org/abs/1604.07316.
Borkar, A., Hayes, M., Smith, M.T., Pankanti, S., 2009. A layered approach to robust lane detection at night. In: 2009 IEEE Workshop Comput. Intell. Vehicles and
Vehicular Systems (CIVVS09). IEEE, pp. 5157. http://ieeexplore.ieee.org/abstract/document/4938723/ .
Boudette, N.E., Mar 2017. Building a road map for the self-driving car. The N.Y. Times. URL <https://www.nytimes.com/2017/03/02/automobiles/wheels/selfdriving-cars-gps-maps.html>.
Bresson, G., Rahal, M.-C., Gruyer, D., Revilloud, M., Alsayed, Z., 2016. A cooperative fusion architecture for robust localization: application to autonomous driving. In:
2016 IEEE 19th Int. Conf. Intelligent Transportation Systems (ITSC). IEEE, pp. 859866. http://ieeexplore.ieee.org/abstract/document/7795656/ .
Broggi, A., Bertozzi, M., Fascioli, A., 2000. Architectural issues on vision-based automatic vehicle guidance: the experience of the argo project. Real-Time Imag. 6 (4),
313324.
Broggi, A., Bertozzi, M., Fascioli, A., Conte, G., 1999. Automatic Vehicle Guidance: The Experience of the ARGO Autonomous Vehicle. World Scientic google-BooksID: K1e55e8wiEUC.
Broggi, A., Bombini, L., Cattani, S., Cerri, P., Fedriga, R., Jun 2010. Sensing requirements for a 13,000 km intercontinental autonomous drive. In: 2010 IEEE Intelligent
Vehicles Symp. pp. 500505. <http://www.ce.unipr.it/people/cattani/publications-pdf/iv2010-porter.pdf>.
Broggi, A., Buzzoni, M., Debattisti, S., Grisleri, P., Laghi, M.C., Medici, P., Versari, P., 2013. Extensive tests of autonomous driving technologies. IEEE Trans. Intell.
Transport. Syst. 14 (3), 14031415.
Broggi, A., Cerri, P., Debattisti, S., Laghi, M.C., Medici, P., Panciroli, M., Prioletti, A., Jun 2014. Proud-public road urban driverless test: Architecture and results. In:
2014 IEEE Intelligent Vehicles Symp. Proc. pp. 648654.
Broggi, A., Cerri, P., Felisa, M., Laghi, M.C., Mazzei, L., Porta, P.P., 2012. The vislab intercontinental autonomous challenge: an extensive test for a platoon of
intelligent vehicles. Int. J. (Wash.) Vehic. Auton. Syst. 10 (3), 147164.
Buehler, M., Iagnemma, K., Singh, S., 2007. The 2005 DARPA Grand Challenge: The Great Robot Race. Springer Science & Business Media.
Buehler, M., Iagnemma, K., Singh, S., 2009. The DARPA Urban Challenge: Autonomous Vehicles in City Trac. Springer.
Cacciola, S.J., 2007. Fusion of laser range-nding and computer vision data for trac detection by autonomous vehicles. Ph.D. thesis, Virginia Polytechnic Institute
and State University. <http://theses.lib.vt.edu/theses/available/etd-12142007-105238/>.
Campbell, M., Egerstedt, M., How, J.P., Murray, R.M., 2010. Autonomous driving in urban environments: approaches, lessons and challenges. Philos. Transa. Roy. Soc.
A: Math., Phys. Eng. Sci. 368 (1928), 46494672.
Canada, T., 2017a. Model s software version 7.0. <https://www.tesla.com/en_CA/presskit/autopilot?redirect=no>.
Canada, T., 2017b. Tesla autopilot. URL https://www.tesla.com/autopilot.
Cars, V., 2016. Self-driving car technology  intellisafe. <http://www.volvocars.com/intl/about/our-innovation-brands/intellisafe/intellisafe-autopilot/this-isautopilot/the-tech>.
Center, F.M., 2016. No lights? no problem! ford fusion autonomous research vehicles use lidar sensor technology to see in the dark. <https://media.ford.com/
content/fordmedia/fna/us/en/news/2016/04/11/no-lights-no-problem-ford-fusion-autonomous-research-vehicles-.html>.
Chan, Y.M., Huang, S.S., Fu, L.C., Hsiao, P.Y., Sep 2007. Vehicle detection under various lighting conditions by incorporating particle lter. In: 2007 IEEE Intelligent
Transportation Systems Conf. pp. 534539.
Chen, C., Se, A., Kornhauser, A., Xiao, J., 2015. Deepdriving: Learning aordance for direct perception in autonomous driving. In: Proc. IEEE Int. Conf. Computer

19

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

Vision. pp. 27222730. <http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Chen_DeepDriving_Learning_Aordance_ICCV_2015_paper.html>.
Cheng, H., 2011. Autonomous Intelligent Vehicles: Theory, Algorithms, and Implementation. Springer Science & Business Media.
Cho, H., Seo, Y.W., Kumar, B.V.K.V., Rajkumar, R.R., May 2014. A multi-sensor fusion system for moving object detection and tracking in urban driving environments.
In: 2014 IEEE Int. Conf. Robotics and Automation (ICRA). pp. 18361843.
Company, F.M., 2014. Accident avoidance and driver assist technologies. <http://corporate.ford.com/microsites/sustainability-report-2013-14/vehicle-avoidance.
html>.
Cui, Z., Yang, S.W., Tsai, H.M., Sep 2015. A vision-based hierarchical framework for autonomous front-vehicle taillights detection and signal recognition. In: 2015 IEEE
18th Int. Conf. Intelligent Transportation Systems. pp. 931937.
Dang, R., Ding, J., Su, B., Yao, Q., Tian, Y., Li, K., Oct 2014. A lane change warning system based on v2v communication. In: 17th Int. IEEE Conf. Intelligent
Transportation Systems (ITSC). pp. 19231928.
Dao, T.-S., Leung, K.K., Clark, C.M., Huissoon, J.P., 2006. Co-operative lane-level positioning using markov localization. In: 2006 IEEE Intelligent Transportation
Systems Conf. IEEE, pp. 10061011. http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1707353 .
Daraei, M.H., Vu, A., Manduchi, R., 2017. Velocity and Shape from Tightly-Coupled LiDAR and Camera. In: Proc. 2017 IEEE Intelligent Vehicles Symposium.
Dasarathy, B.V., 1997. Sensor fusion potential exploitation-innovative architectures and illustrative applications. Proc. IEEE 85 (1), 2438.
Davies, A., May 2016. Gms using cameras on customer cars to build self-driving car maps. <https://www.wired.com/2016/01/gms-building-self-driving-car-mapswith-cameras-on-customer-cars/>.
Davila, A., Aramburu, E., Freixas, A., Apr 2013. Making the best out of aerodynamics: Platoons. In: SAE Technical Paper. SAE International, pp. 16. http://dx.doi.org/
10.4271/2013-01-0767.
Denuelle, A., Srinivasan, M.V., Dec 2015. Bio-inspired visual guidance: From insect homing to uas navigation. In: 2015 IEEE Int. Conf. Robotics and Biomimetics
(ROBIO). pp. 326332.
Dey, K.C., Rayamajhi, A., Chowdhury, M., Bhavsar, P., Martin, J., 2016. Vehicle-to-vehicle (v2v) and vehicle-to-infrastructure (v2i) communication in a heterogeneous
wireless network performance evaluation. Transp. Res. Part C: Emerg. Technolo. 68, 168184.
Dissanayake, M.W.M.G., Newman, P., Clark, S., Durrant-Whyte, H.F., Csorba, M., 2001. A solution to the simultaneous localization and map building (SLAM) problem.
IEEE Tran. Robot. Autom. 17 (3), 229241.
Douxchamps, D., 2017. A small list of IMU/INS/INU. https://damien.douxchamps.net/research/imu/.
Duy, L., Apr. 2017. Next-Generation LiDAR Better than Ever. Point of Beginning. <http://www.pobonline.com/articles/100871-next-generation-lidar-better-thanever>.
Durrant-Whyte, H., Bailey, T., 2006. Simultaneous localization and mapping: Part i. IEEE Robot. Autom. Magaz. 13 (2), 99110.
Durrant-Whyte, H.F., 1990. Sensor models and multisensor integration. In: Autonomous Robot Vehicles. Springer, New York, NY, pp. 7389. http://dx.doi.org/10.
1007/978-1-4613-8997-2_7.
Eddy, N., Jan 2016. Google details self-driving cars problems in dmv report. <http://www.informationweek.com/it-life/google-details-self-driving-cars-problems-indmv-report/d/d-id/1323905>.
eFuture, 2011. efuture eceint mobility: Expected results. <http://www.efuture-eu.org/about/expected-results/>.
Eleter, B., Rombaut, M., 1996. Intelligent mapping of the prolab2 vehicle dynamic environment. Math. Comput. Simul 41 (34), 329336.
Elmenreich, W., 2002. An introduction to sensor fusion. Tech. Rep. 47/2001, Vienna University of Technology, Austria.
Eureka, 1995. Programme for a european trac system with highest eciency and unprecedented safety. <http://www.eurekanetwork.org/project/id/45>.
Fagnant, D.J., Kockelman, K., 2015. Preparing a nation for autonomous vehicles: opportunities, barriers and policy recommendations. Transp. Res. Part A: Policy Pract.
77, 167181.
Folsom, T.C., 2011. Social ramications of autonomous urban land vehicles. In: 2011 IEEE Int. Symp. Technology and Society (ISTAS). IEEE, pp. 16. http://
ieeexplore.ieee.org/abstract/document/7160596/ .
for Intelligent Transport (HAVEit), H.A.V., 2008. Haveit project website. <http://www.haveit-eu.org/>.
Fudge, A. (Ed.), Apr 2016. European Truck Platooning Challenge 2016. Challenge Network and Rijkswaterstaat.
Gehrig, S., Reznitskii, M., Schneider, N., Franke, U., Weickert, J., Dec 2013. Priors for stereo vision under adverse weather conditions. In: 2013 IEEE Int. Conf.
Computer Vision Workshops (ICCVW). pp. 238245.
Geiger, A., Lauer, M., Moosmann, F., Ranft, B., Rapp, H., Stiller, C., Ziegler, J., 2012a. Team annieways entry to the 2011 grand cooperative driving challenge. IEEE
Trans. Intell. Transport. Syst. 10081017. URL http://w.cvlibs.net/publications/Geiger2012TITS.pdf .
Geiger, A., Lenz, P., Urtasun, R., Jun. 2012b. Are we ready for autonomous driving? The KITTI vision benchmark suite. In: 2012 IEEE Conference on Computer Vision
and Pattern Recognition. pp. 33543361.
Ghring, D., Latotzky, D., Wang, M., Rojas, R., 2013. Semi-autonomous car control using brain computer interfaces. Springer, Ch. Semi-autonomous car control using
brain computer interfaces, pp. 393408. <http://link.springer.com/chapter/10.1007/978-3-642-33932-5_37>.
Gitlin, J.M., Jan 2016a. Ars talks self-driving car technology with ford at ces. URL <http://arstechnica.com/cars/2016/01/ars-talks-self-driving-technology-withford-at-ces/>.
Gitlin, J.M., Jan 2016b. Assists, autopilot, and more: Ars talks about autonomous driving with audi. URL <http://arstechnica.com/cars/2016/01/assists-autopilotand-more-ars-talks-about-autonomous-driving-with-audi/>.
Gitlin, J.M., May 2016c. From audi to volvo, most self-driving cars use the same hardware. URL <https://arstechnica.com/cars/2016/05/from-audi-to-volvo-mostself-driving-cars-use-the-same-hardware/>.
Golson, J., Aug 2016. Teslas autopilot system is reportedly getting more sensors. URL <http://www.theverge.com/2016/8/11/12443310/tesla-autopilot-nextgeneration-radar-triple-camera>.
Google, 2017. Google self-driving car project. <http://www.google.com/selfdrivingcar>.
Greenemeier, L., Jul 2016. Deadly tesla crash exposes confusion over automated driving. <https://www.scienticamerican.com/article/deadly-tesla-crash-exposesconfusion-over-automated-driving/>.
Grisleri, P., Fedriga, I., 2010. The braive autonomous ground vehicle platform. IFAC Proc. Vol. 43 (16), 497502.
Gruyer, D., Belaroussi, R., Li, X., Lusetti, B., Revilloud, M., Glaser, S., 2015. Persee: a central sensors fusion electronic control unit for the development of perceptionbased adas. In: 2015 14th IAPR Int. Conf. Machine Vision Applications (MVA). IEEE, pp. 250254. http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=
7153178 .
Gruyer, D., Belaroussi, R., Revilloud, M., 2016a. Accurate lateral positioning from map data and road marking detection. Exp. Syst. Appl. 43, 18.
Gruyer, D., Cord, A., Belaroussi, R., 2013. Target-to-track collaborative association combining a laser scanner and a camera. In: 2013 16th Int. IEEE Conf. Intelligent
Transportation Systems (ITSC). IEEE, pp. 11251130. URL http://ieeexplore.ieee.org/abstract/document/6728383/ .
Gruyer, D., Demmel, S., Magnier, V., Belaroussi, R., 2016b. Multi-hypotheses tracking using the dempstershafer theory, application to ambiguous road context. Inform.
Fusion 29, 4056.
Gu, T., 2014. Real time obstacle depth perception using stereo vision. Ph.D. thesis, University of Florida.
Guizzo, E., 2011. How googles self-driving car works. IEEE Spectrum Online, October 18. <http://spectrum.ieee.org/automaton/robotics/articial-intelligence/howgoogle-self-driving-car-works>.
Hawkins, R., Clegg, K., Alexander, R., Kelly, T., 2011. Using a software safety argument pattern catalogue: two case studies. In: Int. Conf. Computer Safety, Reliability,
and Security. Springer, pp. 185198. http://link.springer.com/10.1007%2F978-3-642-24270-0_14 .
Heikoop, D.D., de Winter, J.C., van Arem, B., Stanton, N.A., 2016. Psychological constructs in driving automation: a consensus model and critical comment on
construct proliferation. Theoret. Issues Ergon. Sci. 17 (3), 284303.
Henawy, M.A., Schneider, M., Oct 2011. Integrated antennas in ewlb packages for 77 ghz and 79 ghz automotive radar sensors. In: 2011 41st European Microwave
Conf. pp. 13121315.

20

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

Heredia, G., Ollero, A., Bejar, M., Mahtani, R., 2008. Sensor and actuator fault detection in small autonomous helicopters. Mechatronics 18 (2), 9099.
Hischke, M., Sep 1995. Collision warning radar interference. In: Proc. Intelligent Vehicles 95 Symp. pp. 1318.
Hodson, H., Sep 2016. Uber and google race against car rms to map the worlds cities. <https://www.newscientist.com/article/mg23130932-900-uber-and-googlerace-against-car-rms-to-map-the-worlds-cities/>.
Howard, B., Nov 2016. Big boost for self-driving cars: Osram cuts lidar cost to less than $50. <https://www.extremetech.com/extreme/239359-big-boost-self-drivingcars-osram-cuts-lidar-cost-less-50>.
Huang, T.-W., Hsu, C.-C., Wang, W.-Y., Baltes, J., 2017. ROSLAMa faster algorithm for simultaneous localization and mapping (SLAM). In: Robot Intelligence
Technology and Applications 4. Advances in Intelligent Systems and Computing. Springer, Cham, pp. 6574. http://dx.doi.org/10.1007/978-3-319-31293-4_6.
Huang, W., Su, X., 2015. Design of a fault detection and isolation system for intelligent vehicle navigation system. Int. J. (Wash.) Navigat.Observat. 2015, 119.
Huber, D., Kanade, T., Badino, H., 2011. Integrating lidar into stereo for fast and improved disparity computation. In: 2011 Int. Conf. 3D Imaging, Modeling,
Processing, Visualization and Transmission (3DIMPVT). IEEE, pp. 405412. http://ieeexplore.ieee.org/abstract/document/5955388/ .
Hurney, P., Waldron, P., Morgan, F., Jones, E., Glavin, M., 2015. Review of pedestrian detection techniques in automotive far-infrared video. IET Intel. Transport Syst.
9 (8), 824832.
Ilgin Guler, S., Menendez, M., Meier, L., 2014. Using connected vehicle technology to improve the eciency of intersections. Transp. Res. Part C: Emerg. Technol. 46,
121131.
Insider, B., Feb 2017. Uber builds out mapping data for autonomous cars. <http://www.businessinsider.com/uber-builds-out-mapping-data-for-autonomous-cars2017-2>.
Jagannathan, S., Desappan, K., Swami, P., Mathew, M., Nagori, S., Chitnis, K., Marathe, Y., Poddar, D., Narayanan, S., Jan. 2017. Ecient object detection and
classication on low power embedded systems. In: 2017 IEEE International Conference on Consumer Electronics (ICCE). pp. 233234.
Jan, K., Cuccu, G., Schmidhuber, J., Gomez, F., 2013. Evolving large-scale neural networks for vision-based torcs. In: Proc. 15th Annu. Conf. Genetic and Evol. Comput.
pp. 206212.
Janardhanan, S., Keshavarz, M., Laine, L., Sep 2015. Introduction of trac situation management for a rigid truck, tests conducted on object avoidance by steering
within ego lane. In: 2015 IEEE 18th Int. Conf. Intelligent Transportation Systems. pp. 15271532.
Jaradat, M.A., Abdel-Hafez, M.F., Saadeddin, K., Jarrah, M.A., Apr 2013. Intelligent fault detection and fusion for ins/gps navigation system. In: 2013 9th Int. Symp.
Mechatronics and Applications (ISMA). pp. 15.
Javaheri, A., Brites, C., Pereira, F., Ascenso, J., Jul. 2017. Subjective and objective quality evaluation of 3d point cloud denoising algorithms. In: 2017 IEEE
International Conference on Multimedia Expo Workshops (ICMEW). pp. 16.
Jo, J., Tsunoda, Y., Stantic, B., Liew, A.W.-C., 2017. A Likelihood-Based Data Fusion Model for the Integration of Multiple Sensor Data: A Case Study with Vision and
Lidar Sensors. Vol. 447. Springer International Publishing, Ch. A Likelihood-Based Data Fusion Model for the Integration of Multiple Sensor Data: A Case Study
with Vision and Lidar Sensors, pp. 489500, dOI: http://dx.doi.org/10.1007/978-3-319-31293-4_39. <http://link.springer.com/10.1007/978-3-319-312934_39>.
Jochem, T., Pomerleau, D., 1995. No hands across america ocial press release.
Julier, S., Durrant-Whyte, H., 2003. On the role of process models in autonomous land vehicle navigation systems. IEEE Trans. Robot. Autom. 19 (1), 114.
Kato, S., Takeuchi, E., Ishiguro, Y., Ninomiya, Y., Takeda, K., Hamada, T., 2015. An open approach to autonomous vehicles. IEEE Micro 35 (6), 6068.
Katrakazas, C., Quddus, M., Chen, W.-H., Deka, L., 2015. Real-time motion planning methods for autonomous on-road driving: State-of-the-art and future research
directions. Transp. Res. Part C: Emerg. Technol. 60, 416442.
Kaviani, S., OBrien, M., Van Brummelen, J., Michelson, D., Najjaran, H., 2016. Ins/gps localization for reliable cooperative driving. In: 2016 IEEE Canadian Conf.
Electrical and Computer Engineering (CCECE). Vancouver, Canada, pp. 14.
Ke, R., Lutin, J., Spears, J., Wang, Y., Jul. 2017. A Cost-Eective Framework for Automated Vehicle-Pedestrian Near-Miss Detection Through Onboard Monocular
Vision. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). pp. 898905.
Kim, H.J., Yang, J.H., 2017. Takeover requests in simulated partially autonomous vehicles considering hum. factors. IEEE Trans. Human-Mach. Syst. PP (99), 16.
Krisher, T., Durbin, D.-A., Sep 2016. Tesla update halts automatic steering if driver inattentive. <http://phys.org/news/2016-09-tesla-halts-automatic-driverinattentive.html>.
Kyriakidis, M., de Winter, J.C.F., Stanton, N., Bellet, T., van Arem, B., Brookhuis, K., Martens, M.H., Bengler, K., Andersson, J., Merat, N., et al., 2017. A hum. factors
perspective on automated driving. Theoret. Issues Ergon. Sci. 127.
Kyriakidis, M., Happee, R., de Winter, J., 2015. Public opinion on automated driving: Results of an international questionnaire among 5000 respondents. Transp. Res.
Part F: Trac Psychol. Behav. 32, 127140.
Lambert, F., Mar 2017. Teslas vision and autopilot chip eorts validated by intels $15 billion acquisition of mobileye. <https://electrek.co/2017/03/13/teslavision-autopilot-chip-intel-mobileye/>.
Larburu, M., Sanchez, J., Rodriguez, D.J., Oct 2010. Safe road trains for environment: Hum. factors aspects in dual mode transport systems. In: ITS World Congr. pp.
112.
Leonard, J., How, J., Teller, S., Berger, M., Campbell, S., Fiore, G., Fletcher, L., Frazzoli, E., Huang, A., Karaman, S., et al., 2008. A perception-driven autonomous
urban vehicle. J. Field Rob. 25 (10), 727774.
Leonard, J.J., Durrant-Whyte, H.F., Cox, I.J., 1992. Dynamic map building for an autonomous mobile robot. Int. J. (Wash.) Robot. Res. 11 (4), 286298.
Levin, S., Harris, M., Mar 2017. The road ahead: self-driving cars on the brink of a revolution in california. The Guardian. <https://www.theguardian.com/
technology/2017/mar/17/self-driving-cars-california-regulation-google-uber-tesla>.
Levinson, J., Askeland, J., Becker, J., Dolson, J., Held, D., Kammel, S., Kolter, J.Z., Langer, D., Pink, O., Pratt, V., et al., Jun 2011. Towards fully autonomous driving:
Systems and algorithms. In: 2011 IEEE Intelligent Vehicles Symp. (IV). pp. 163168.
Levinson, J., Montemerlo, M., Thrun, S., 2007. Map-based precision vehicle localization in urban environments. Robot.: Sci. Syst. 4, 1.
Levinson, J., Thrun, S., May 2010. Robust vehicle localization in urban environments using probabilistic maps. In: 2010 IEEE Int. Conf. Robotics and Automation
(ICRA). pp. 43724378.
Lexus, 2017. Lexus rx - safety. <http://www.lexus.com/models/RX/safety>.
Lowry, S., Sunderhauf, N., Newman, P., Leonard, J.J., Cox, D., Corke, P., Milford, M.J., 2016. Visual place recognition: a survey. IEEE Trans. Robot. 32 (1), 119.
Lu, Z., Happee, R., Cabrall, C.D., Kyriakidis, M., de Winter, J.C., 2016. Visual place recognition: a survey. Transp. Res. Part F: Tra. Psychol. Behav. 43, 183198.
Maddern, W., Newman, P., 2016. Real-time probabilistic fusion of sparse 3d lidar and dense stereo. In: 2016 IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS).
IEEE, pp. 21812188. http://ieeexplore.ieee.org/abstract/document/7759342/ .
Marais, J., Meurie, C., Attia, D., Ruichek, Y., Flancquart, A., 2014. Toward accurate localization in guided transport: combining gnss data and imaging information.
Transp. Res. Part C: Emerg. Technol. 43, 188197.
Maurer, M., Behringer, R., Furst, S., Thomanek, F., Dickmanns, E.D., Aug 1996. A compact vision system for road vehicle guidance. In: Proc. 13th Int. Conf. Pattern
Recognit. Vol. 3. pp. 313317 vol.3.
Maurer, M., Gerdes, J.C., Lenz, B., Winner, H. (Eds.), 2016. Autonomous Driving. Springer Berlin. Springer, Berlin, Heidelberg. http://dx.doi.org/10.1007/978-3-66248847-8.
Maybeck, P.S., 1979. Chapter 1: Introduction. In: Stochastic Models, Estimation, and Control, vol. 1. Academic Press, New York, pp. 1012.
Mercedes-Benz, Aug 2013. <https://www.mercedes-benz.com/en/mercedes-benz/innovation/mercedes-benz-intelligent-drive/>.
Mobileye, 2017. About us. <http://www.mobileye.com/about/>.
Mok, B.K.-J., Sirkin, D., Sibi, S., Miller, D.B., Ju, W., 2015. Understanding driver-automated vehicle interactions through wizard of oz design improvisation. In: Proc.
Int. Driving Symp. Hum. Factors in Driver Assessment, Training and Vehicle Design. pp. 386392. <http://www.wendyju.com/publications/058.pdf>.
Montemerlo, M., Becker, J., Bhat, S., Dahlkamp, H., Dolgov, D., Ettinger, S., Haehnel, D., Hilden, T., Homann, G., Huhnke, B., et al., 2008. Junior: the stanford entry
in the urban challenge. J. Field Rob. 25 (9), 569597.

21

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

Muoio, D., Aug 2016. Heres why self-driving cars cant handle bridges. <http://www.businessinsider.com/autonomous-cars-bridges-2016-8>.
Navya, 2017. Navya -100% autonomous, driverless and electric. URL http://navya.tech/.
Neale, V.L., Dingus, T.A., Klauer, S.G., Sudweeks, J., Goodman, M., 2005. An overview of the 100-car naturalistic study and ndings. National Highway Trac Safety
Administration, Paper. <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.2366&rep=rep1&type=pdf>.
Nguyen, A., Le, B., Nov. 2013. 3d point cloud segmentation: A survey. In: 2013 6th IEEE Conference on Robotics, Automation and Mechatronics (RAM). pp. 225230.
(NHTSA), N.H.T.S.A., May 2013. U.s. department of transportation releases policy on automated vehicle development. <http://www.nhtsa.gov/About+NHTSA/
Press+Releases/U.S.+Department+of+Transportation+Releases+Policy+on+Automated+Vehicle+Development>.
Oagana, A., Jan 2016. A short history of mercedes-benz autonomous driving technology. <http://www.autoevolution.com/news/a-short-history-of-mercedes-benzautonomous-driving-technology-68148.html>.
OBrien, M., Kaviani, S., Van Brummelen, J., Michelson, D., Najjaran, H., 2016. Localization Estimation Filtering Techniques for Reliable Cooperative Driving. In: 2016
Canadian Society Mechanical Engineering Int. Congr. Kelowna, Canada, pp. 15.
Obst, M., Bauer, S., Wanielik, G., Apr 2012. Urban multipath detection and mitigation with dynamic 3d maps for reliable land vehicle localization. In: 2012 IEEE/ION
Position Location and Navigation Symp. (PLANS). pp. 685691.
of British Columbia (ICBC), I.C., 2015. Learn to drive smart: Your guide to driving safely. Insurance Corporation of British Columbia (ICBC). <http://www.icbc.com/
driver-licensing/Documents/driver-full.pdf>.
of Transportation, U.D., Sep 2016. Federal automated vehicles policy.
Ohn-Bar, E., Trivedi, M.M., 2016. Looking at humans in the age of self-driving and highly automated vehicles. IEEE Trans. Intell. Vehic. 1 (1), 90104.
Pachal, P., Jul 2016. Despite teslas setbacks, audi is racing fast toward self-driving cars. <http://mashable.com/2016/07/16/audi-self-driving-a7/>.
Park, G., Hwang, Y., Choi, S.B., 2017. Vehicle positioning based on velocity and heading angle observer using low-cost sensor fusion. J. Dynam. Syst., Measur., Control
139 (12), 13.
Park, Y., Yun, S., Won, C.S., Cho, K., Um, K., Sim, S., 2014. Calibration between color camera and 3d LIDAR instruments with a polygonal planar board. Sensors (Basel)
14 (3), 53335353.
Payre, W., Cestac, J., Delhomme, P., 2016. Fully automated driving impact of trust and practice on manual control recovery. Hum. Fact.: J. Hum. Fact. Ergon. Soc. 58
(2), 229241.
Pettitt, J., Dec 2016. This company has developed stereo cameras for driverless cars. <http://www.cnbc.com/2016/12/02/seegrid-sensors-for-driverless-cars.html>.
Pomerleau, D., Sep 1995. Ralph: Rapidly adapting lateral position handler. In: Proc. IEEE Symp. Intelligent vehicle. pp. 15. <http://www.cs.cmu.edu/tjochem/nhaa/
ralph.html>.
Pomerleau, D.A., 1989. Alvinn, an autonomous land vehicle in a neural network. Advances in neural information processing systems. <https://papers.nips.cc/paper/
95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf>.
Pous, N., Gingras, D., Gruyer, D., 2017. Intelligent vehicle embedded sensors fault detection and isolation using analytical redundancy and nonlinear transformations.
J. Control Sci. Eng. 2017, 110.
Rabel, D., Dec. 2016. How HERE HD Live Map paved the way for autonomous cars in 2016. <http://360.here.com/2016/12/13/how-here-hd-live-map-paved-theway-for-autonomous-cars-in-2016/>.
Radecki, P., Campbell, M., Matzen, K., 2016. All weather perception: Joint data association, tracking, and classication for autonomous ground vehicles. arXiv preprint
arXiv:1605.02196. http://arxiv.org/abs/1605.02196.
Ranft, B., Stiller, C., 2016. The role of machine vision for intelligent vehicles. IEEE Trans. Intell. Vehic. 1 (1), 819.
Rasshofer, R.H., Gresser, K., 2005. Automotive radar and lidar systems for next generation driver assistance functions. Adv. Radio Sci. 3 (B.4), 205209.
Rebut, J., Bensrhair, A., Toulminet, G., 2004. Image segmentation and pattern recognit. for road marking analysis. In: 2004 IEEE Int. Symp. Industrial Electronics, vol.
1. IEEE, pp. 727732. http://ieeexplore.ieee.org/abstract/document/1571896/ .
Renault, 2017. Adas: a range of technologies promoting safety and easier driving experience. <https://group.renault.com/en/passion-2/innovation/renault-a-borninnovator/adas-a-range-of-technologies-promoting-safety-and-easier-driving-experience/>.
Reuters, T., 2017. Thomson reuters - ip & science - web of science. <http://ipscience.thomsonreuters.com/product/web-of-science/>.
Ros, G., Ramos, S., Granados, M., Bakhtiary, A., Vazquez, D., Lopez, A.M., Jan 2015. Vision-based oine-online perception paradigm for autonomous driving. In: 2015
IEEE Winter Conf. Applications Computer Vision. pp. 231238.
Ross, P.E., Apr 2017. Velodyne announces a solid-state lidar. <http://spectrum.ieee.org/cars-that-think/transportation/sensors/velodyne-announces-a-solidstatelidar>.
Rou, C., Hinchey, M., 2011. Experience from the DARPA Urban Challenge. Springer Science & Business Media.
Safwan, M., Zaheen, M.Y., Ahmed, M.A., Kamal, M.S., Kumar, R., 2013. Bio-mimetic vision system for autonomous mobile robot navigation. Sir Syed Univ. Res. J. Eng.
Technol. 3. http://sirsyeduniversity.edu.pk/ssurj/rj/le/article/5_Bio-Mimetic%20Vision%20System%20-%20Final.pdf .
Saito, Y., Itoh, M., Inagaki, T., 2016. Driver assistance system with a dual control scheme: Eectiveness of identifying driver drowsiness and preventing lane departure
accidents. IEEE Trans. Human-Mach. Syst. 46 (5), 660671.
Sanchez-Lopez, J.L., Fu, C., Campoy, P., 2016. FuSeOn: a low-cost portable multi sensor fusion research testbed for robotics. In: Robot 2015: Second Iberian Robotics
Conference. Advances in Intelligent Systems and Computing. Springer, Cham, pp. 5768. http://dx.doi.org/10.1007/978-3-319-27146-0_5.
Sasiadek, J.Z., Wang, Q., 2003. Low cost automation using ins/gps data fusion for accurate positioning. Robotica 21 (3), 255260.
Satzoda, R.K., Trivedi, M.M., 2015. Drive analysis using vehicle dynamics and vision-based lane semantics. IEEE Trans. Intell. Transport. Syst. 16 (1), 918.
Saust, F., Wille, J.M., Lichte, B., Maurer, M., Jun 2011. Autonomous vehicle guidance on braunschweigs inner ring road within the stadtpilot project. In: 2011 IEEE
Intelligent Vehicles Symp. (IV). pp. 169174.
Schamm, T., von Carlowitz, C., Zllner, J.M., 2010. On-road vehicle detection during dusk and at night. In: 2010 IEEE Intelligent Vehicles Symp. (IV). IEEE, pp.
418423. http://ieeexplore.ieee.org/abstract/document/5548013/ .
Schipper, T., Prophet, S., Harter, M., Zwirello, L., Zwick, T., 2015. Simulative prediction of the interference potential between radars in common road scenarios. IEEE
Trans. Electromagn. Compat. 57 (3), 322328.
Schnelle, S., Wang, J., Su, H.-J., Jagacinski, R., 2016. A personalizable driver steering model capable of predicting driver behaviors in vehicle collision avoidance
maneuvers. IEEE Trans. Human-Mach. Syst. 1.
Schoettle, B., Sivak, M., 2014. A survey of public opinion about autonomous and self-driving vehicles in the US, the UK, and Australia. No. 21 in UMTRI-2014. The
University of Michigan Sustainable Worldwide Transportation. <https://deepblue.lib.umich.edu/handle/2027.42/108384>.
Seif, H.G., Hu, X., Jun. 2016. Autonomous Driving in the iCityHD Maps as a Key Challenge of the Automotive Industry. Engineering 2 (2), 159162. <http://www.
sciencedirect.com/science/article/pii/S2095809916309432>.
Sherman, D., Feb 2016. Semi-autonomous cars compared! tesla model s vs. bmw 750i, inniti q50s, and mercedes-benz s65 amg. URL http://www.caranddriver.com/
features/semi-autonomous-cars-compared-tesla-vs-bmw-mercedes-and-inniti-feature.
Shrivastava, A., Rajamani, R., 2001. Fault diagnostics for gps-based lateral vehicle control. In: Proc. 2001 American Control Conf. Vol. 1. pp. 3136 vol.1.
Simani, S., 2003. Model-based fault diagnosis in dynamic systems using identication techniques. Ph.D. thesis, University of Modena and Reggio Emilia. <http://
silviosimani.it/thesis.pdf>.
Singh, S., Feb 2015. Critical reasons for crashes investigated in the national motor vehicle crash causation survey. <https://crashstats.nhtsa.dot.gov/Api/Public/
ViewPublication/812115>.
Sivaraman, S., Trivedi, M.M., 2013. Looking at vehicles on the road: a survey of vision-based vehicle detection, tracking, and behavior analysis. IEEE Trans. Intell.
Transport. Syst. 14 (4), 17731795.
Sorokanich, R., 2014. 6 simple things googles self-driving car still cant handle. <http://gizmodo.com/6-simple-things-googles-self-driving-car-still-cant-han1628040470>.
Souppouris, A., Dec 2014. Riding in audis 150mph self-driving rs 7, the anti-google car. <https://www.engadget.com/2014/12/18/audi-self-driving-rs-7-concept-

22

Transportation Research Part C xxx (xxxx) xxxxxx

J. Van Brummelen et al.

test-drive/>.
Spears, J., Lutin, J.M., Wang, Y., Ke, R., Clancy, S.M., May 2017. Active Safety-Collision Warning Pilot in Washington State. Tech. Rep. Transit IDEA Project 82,
Transportation Research Board. <https://trid.trb.org/view.aspx?id=1480393>.
Stanton, N.A., Young, M.S., Walker, G.H., 2007. The psychology of driving automation: a discussion with professor don norman. Int. J. (Wash.) Vehic. Des. 45 (3), 289.
Stepan, P., Kulich, M., Preucil, L., 2005. Robust data fusion with occupancy grid. IEEE Trans. Syst., Man, Cybernet., Part C (Appl. Rev.) 35 (1), 106115.
Stewart, J., May 2016. $30k retrot turns dumb semis into self-driving robots. <https://www.wired.com/2016/05/otto-retrot-autonomous-self-driving-trucks/>.
Stoklosa, A., Feb 2015. Volvo has a production-viable autonomous car, will put it on the road by 2017. <http://blog.caranddriver.com/volvo-has-a-productionviable-autonomous-car-will-put-it-on-the-road-by-2017/>.
Stu, A., 2017. ibeo lux fusion system  lidar  product. <https://autonomoustu.com/product/ibeo-lux-fusion-system/>.
Suhr, J.K., Jang, J., Min, D., Jung, H.G., 2017. Sensor fusion-based low-cost vehicle localization system for complex urban environments. IEEE Trans. Intell. Transport.
Syst. 18 (5), 10781086.
Sun, Z., Bebis, G., Miller, R., 2006. On-road vehicle detection: a review. IEEE Trans. Pattern Anal. Mach. Intell. 28 (5), 694711.
Tesla, Jun 2016. A tragic loss. <https://www.tesla.com/en_CA/blog/tragic-loss>.
Thorpe, C., Hebert, M.H., Kanade, T., Shafer, S.A., 1988. Vision and navigation for the carnegie-mellon navlab. IEEE Trans. Pattern Anal. Mach. Intell. 10, 362373.
Tingwall, E., 2013. Sensory overload: How the new mercedes s-class sees all. Car and Driver. <http://blog.caranddriver.com/sensory-overload-how-the-newmercedes-s-class-sees-all/>.
Ulrich, L., 2014. Top ten tech cars. IEEE Spectr. 51 (4), 3847.
Urmson, C., Anhalt, J., Bagnell, D., Baker, C., Bittner, R., Clark, M.N., Dolan, J., Duggins, D., Galatali, T., Geyer, C., et al., 2008. Autonomous driving in urban
environments: boss and the urban challenge. J. Field Rob. 25 (8), 425466.
Urmson, C., Bagnell, J.A., Baker, C.R., Hebert, M., Kelly, A., Rajkumar, R., Rybski, P.E., Scherer, S., Simmons, R., Singh, S., Apr 2007. Tartan racing: A multi-modal
approach to the DARPA Urban Challenge. Carnegie Mellon University. <http://repository.cmu.edu/robotics/967/>.
Vagia, M., Transeth, A.A., Fjerdingen, S.A., 2016. A literature review on the levels of automation during the years. What are the dierent taxonomies that have been
proposed? Appl. Ergon. 53, 190202.
van Nunen, E., Koch, R., Elshof, L., Krosse, B., Oct 2016. Sensor safety for the european truck platooning challenge. In: Proc. 23rd ITS World Congr. pp. 112.
<https://www.researchgate.net/prole/Ellen_Nunen/publication/311716444_Sensor_Safety_for_the_European_Truck_Platooning_Challenge/links/
58579b6008ae086bfd0faa/Sensor-Safety-for-the-European-Truck-Platooning-Challenge.pdf>.
van Nunen, E., Kwakkernaat, M.R.J.A.E., Ploeg, J., Netten, B.D., 2012. Cooperative competition for future mobility. IEEE Trans. Intell. Transport. Syst. 13 (3),
10181025.
Vanderbilt, T., Jan 2012. Let the robot drive: The autonomous car of the future is here. WIRED. <http://www.wired.com/2012/01/_autonomouscars/>.
Vandezande, L., Sep 2013. Lexus drops night vision tech. <http://www.autoguide.com/auto-news/2013/09/lexus-drops-night-vision-tech.html>.
Vanholme, B., Jun 2012. Highly automated driving on highways based on legal safety. PhD thesis, University of Evry-Val-dEssonne.
Vanholme, B., Gruyer, D., Lusetti, B., Glaser, S., Mammar, S., 2013. Highly automated driving on highways based on legal safety. IEEE Trans. Intell. Transport. Syst. 14
(1), 333347.
Vincent, J., Aug 2016. Ford and baidu invest $150 million in lidar technology for autonomous cars. <http://www.theverge.com/2016/8/16/12499622/ford-baiduvelodyne-investment>.
VisLab, 2010. Braive external and internal equipment. <http://www.braive.vislab.it/equipment.php>.
Viswanathan, V., Hussein, R., 2017. Applications of image processing and real-time embedded systems in autonomous cars: a short review. Int. J. Image Process. (IJIP)
11 (2), 35. http://www.cscjournals.org/manuscript/Journals/IJIP/Volume11/Issue2/IJIP-1114.pdf .
Volvo, 2016. Pilot assist. <http://support.volvocars.com/uk/cars/Pages/owners-manual.aspx?mc=v526&my=2016&sw=15w46&article=
548956727ac6edfbc0a80151522a4edc>.
Walton, M., May 2004. Robots fail to complete Grand Challenge. CNN. <http://www.cnn.com/2004/TECH/ptech/03/14/darpa.race/index.html>.
Wang, M., Ganjineh, T., Rojas, R., 2011a. Action annotated trajectory generation for autonomous maneuvers on structured road networks. In: 2011 5th Int. Conf.
Automation, Robotics and Applications (ICARA 2011). pp. 16.
Wang, T., Xin, J., Zheng, N., Aug 2011b. A method integrating human visual attention and consciousness of radar and vision fusion for autonomous vehicle navigation.
In: 2011 IEEE 4th Int. Conf. Space Mission Challenges Information Technology (SMC-IT). pp. 192197.
Windridge, D., Shaukat, A., Hollnagel, E., 2013. Characterizing driver intention via hierarchical perception-action modeling. IEEE Trans. Human-Mach. Syst. 43 (1),
1731.
Wolf, D., Sukhatme, G.S., Apr 2004. Online simultaneous localization and mapping in dynamic environments. In: Proc. 2004 IEEE Int. Conf. Robotics and Automation.
Vol. 2. pp. 13011307 Vol.2.
Wu, C.F., Lin, C.J., Lee, C.Y., 2012. Applying a functional neurofuzzy network to real-time lane detection and front-vehicle distance measurement. IEEE Trans. Syst.,
Man, Cybernet., Part C (Appl. Rev.) 42 (4), 577589.
Wu, T., Ranganathan, A., 2013. Vehicle localization using road markings. In: 2013 IEEE Intelligent Vehicles Symp. (IV). IEEE, pp. 11851190. http://ieeexplore.ieee.
org/document/6629627/ .
Xiong, X., Wang, J., Zhang, F., Li, K., 2016. Combining deep reinforcement learning and safety based control for autonomous driving. arXiv preprint arXiv:1612.00147.
<https://arxiv.org/abs/1612.00147>.
Zhou, E., 2016. Nested models and nonparametric lstms in vision-based autonomous driving and developing an r package for bayesian-optimized deep learning. Ph.D.
thesis, Princeton University. <http://csml.princeton.edu/sites/csml/les/resource-links/zhou_eddie_nal_thesis.pdf>.
Zhou, L., 2014. A new minimal solution for the extrinsic calibration of a 2d LIDAR and a Camera using three plane-line correspondences. IEEE Sens. J. 14 (2), 442454.
Zhu, H., Yuen, K.-V., Mihaylova, L., Leung, H., 2017. Overview of environment perception for intelligent vehicles. IEEE Trans. Intell. Transport. Syst. 118.
Ziegler, J., Bender, P., Schreiber, M., Lategahn, H., Strauss, T., Stiller, C., Dang, T., Franke, U., Appenrodt, N., Keller, C.G., et al., 2014. Making bertha drive - an
autonomous journey on a historic route. IEEE Intell. Transport. Syst. Magaz. 6 (2), 820.

23

