The bias detectives
As machine learning infiltrates society, scientists grapple with how to make algorithms fair.

I

n 2015, a worried father asked Rhema Vaithianathan a question that
still weighs on her mind. A small crowd had gathered in a basement
room in Pittsburgh, Pennsylvania, to hear her explain how software
might tackle child abuse. Each day, the areas hotline receives dozens of
calls from people who suspect that a child is in danger; some of these are
then flagged by call-centre staff for investigation. But the system does not
catch all cases of abuse. Vaithianathan and her colleagues had just won a
half-million-dollar contract to build an algorithm to help.
Vaithianathan, a health economist who co-directs the Centre for
Social Data Analytics at the Auckland University of Technology in New
Zealand, told the crowd how the algorithm might work. For example,
a tool trained on reams of data  including family backgrounds and
criminal records  could generate risk scores when calls come in. That

could help call screeners to flag which families to investigate.
After Vaithianathan invited questions from her audience, the father
stood up to speak. He had struggled with drug addiction, he said, and
social workers had removed a child from his home in the past. But he
had been clean for some time. With a computer assessing his records,
would the effort hed made to turn his life around count for nothing? In
other words: would algorithms judge him unfairly?
Vaithianathan assured him that a human would always be in the loop,
so his efforts would not be overlooked. But now that the automated tool
has been deployed, she still thinks about his question. Computer calculations are increasingly being used to steer potentially life-changing
decisions, including which people to detain after they have been charged
with a crime; which families to investigate for potential child abuse,

.
d
e
v
r
e
s
e
r
s
t
h
g
i
r
l
l
A
.
e
r
u
t
a
N
r
e
g
n
i
r
p
S
f
o
t
r
a
p
,
d
e
t
i
m
i
L
s
r
e
h
s
i
l
b
u
P
n
a
l
l
i
m
c
a
M
8
1
0
2


2 1 J U N E 2 0 1 8 | VO L 5 5 8 | NAT U R E | 3 5 7

ILLUSTRATION BY MARIO WAGNER

BY RACHEL COURTLAND

and  in a trend called predictive policing
 which neighbourhoods police should focus
on. These tools promise to make decisions
more consistent, accurate and rigorous. But
oversight is limited: no one knows how many
are in use. And their potential for unfairness is
raising alarm. In 2016, for instance, US journalists argued that a system used to assess the
risk of future criminal activity discriminates
against black defendants.
What concerns me most is the idea that
were coming up with systems that are supposed to ameliorate problems [but] that might
end up exacerbating them, says Kate Crawford,
co-founder of the AI Now Institute, a research
centre at New York University that studies the
social implications of artificial intelligence.
With Crawford and others waving red flags,
governments are trying to make software more
accountable. Last December, the New York
City Council passed a bill to set up a task force
that will recommend how to publicly share
information about algorithms and investigate Police in Camden, New Jersey, use automated tools to help determine which areas need patrolling.
them for bias. This year, Frances president,
Emmanuel Macron, has said that the country will make all algorithms University in California, is still assessing the tool. But Dalton says
used by its government open. And in guidance issued this month, the preliminary results suggest that it is helping. The cases that call-centre
UK government called for those working with data in the public sector staff refer to investigators seem to include more instances of legitimate
to be transparent and accountable. Europes General Data Protection concern, she says. Call screeners also seem to be making more consistRegulation (GDPR), which came into force at the end of May, is also ent decisions about cases that have similar profiles. Still, their decisions
expected to promote algorithmic accountability.
dont necessarily agree with the algorithms risk scores; the county is
In the midst of such activity, scientists are confronting complex hoping to bring the two into closer alignment.
questions about what it means to make an algorithm fair. Researchers
As the AFST was being deployed, Dalton wanted more help working
such as Vaithianathan, who work with public agencies to try to build out whether it might be biased. In 2016, she enlisted Alexandra
responsible and effective software, must grapple with how automated Chouldechova, a statistician at Carnegie Mellon University in Pittsburgh,
tools might introduce bias or entrench existing inequity  especially to analyse whether the software was discriminating against particular
if they are being inserted into an already discriminatory social system. groups. Chouldechova had already been thinking about bias in algorithms
The questions that automated decision-making tools raise are not  and was about to weigh in on a case that has triggered substantial
entirely new, notes Suresh Venkatasubramanian, a theoretical computer debate over the issue. In May that year, journalists at the news website
scientist at the University of Utah in Salt Lake City. Actuarial tools for ProPublica reported on commercial software used by judges in Broward
assessing criminality or credit risk have been around for decades. But County, Florida, that helps to decide whether a person charged with a
as large data sets and more-complex models become widespread, it is crime should be released from jail before their trial. The journalists said
becoming harder to ignore their ethical implications, he says. Computer scientists have no choice but to be engaged now. We can no longer
just throw the algorithms over the fence and see what happens.

FAIRNESS TRADE-OFFS

When officials at the Department of Human Services in Allegheny
County, where Pittsburgh is located, called in 2014 for proposals for an
automated tool, they hadnt yet decided how to use it. But they knew they
wanted to be open about the new system. Im very against using government money for black-box solutions where I cant tell my community
what were doing, says Erin Dalton, deputy director of the departments
Office of Data Analysis, Research and Evaluation. The department has a
centralized data warehouse, built in 1999, that contains a wealth of information about individuals  including on housing, mental health and
criminal records. Vaithianathans team put in an impressive bid to focus
on child welfare, Dalton says.
The Allegheny Family Screening Tool (AFST) launched in August
2016. For each phone call to the hotline, call-centre employees see
a score between 1 and 20 that is generated by the automated riskassessment system, with 20 corresponding to a case designated as highest risk. These are families for which the AFST predicts that children
are most likely to be removed from their homes within two years, or
to be referred to the county again because a caller has suspected abuse
(the county is in the process of dropping this second metric, which does
not seem to closely reflect the cases that require further investigation).
An independent researcher, Jeremy Goldhaber-Fiebert at Stanford

that the software was biased against black defendants. The tool, called
COMPAS, generated scores designed to gauge the chance of a person
committing another crime within two years if released.
The ProPublica team investigated COMPAS scores for thousands
of defendants, which it had obtained through public-records requests.
Comparing black and white defendants, the journalists found that a
disproportionate number of black defendants were false positives: they
were classified by COMPAS as high risk but subsequently not charged
with another crime.
The developer of the algorithm, a Michigan-based company called
Northpointe (now Equivant, of Canton, Ohio), argued that the tool was
not biased. It said that COMPAS was equally good at predicting whether
a white or black defendant classified as high risk would reoffend (an
example of a concept called predictive parity). Chouldechova soon
showed that there was tension between Northpointes and ProPublicas
measures of fairness1. Predictive parity, equal false-positive error rates,
and equal false-negative error rates are all ways of being fair, but are

.
d
e
v
r
e
s
e
r
s
t
h
g
i
r
l
l
A
.
e
r
u
t
a
N
r
e
g
n
i
r
p
S
f
o
t
r
a
p
,
d
e
t
i
m
i
L
s
r
e
h
s
i
l
b
u
P
n
a
l
l
i
m
c
a
M
8
1
0
2


3 5 8 | NAT U R E | VO L 5 5 8 | 2 1 J U N E 2 0 1 8

If you want to be fair in one
way, you might necessarily
be unfair in another.

TIMOTHY CLARY/AFP/GETTY

NEWS FEATURE

statistically impossible to reconcile if there are differences across two
groups  such as the rates at which white and black people are being
rearrested (see How to define fair). You cant have it all. If you want
to be fair in one way, you might necessarily be unfair in another definition that also sounds reasonable, says Michael Veale, a researcher in
responsible machine learning at University College London.
In fact, there are even more ways of defining fairness, mathematically speaking: at a conference this February, computer scientist Arvind
Narayanan gave a talk entitled 21 fairness definitions and their politics
 and he noted that there were still others. Some researchers who have
examined the ProPublica case, including Chouldechova, note that its
not clear that unequal error rates are indicative of bias. They instead
reflect the fact that one group is more difficult to make predictions
about than another, says Sharad Goel, a computer scientist at Stanford.
It turns out that thats more or less a statistical artefact.
For some, the ProPublica case highlights the fact that many agencies lack resources to ask for and properly assess algorithmic tools. If
anything, what its showing us is that the government agency who hired
Northpointe did not give them a well-defined definition to work with,
says Rayid Ghani, who directs the Center for Data Science and Public Policy at the University of Chicago, Illinois. I think that governments need
to learn and get trained in how to ask for these systems, how to define the
metrics they should be measuring and to make sure that the systems they
are being given by vendors, consultants and researchers are actually fair.
Allegheny Countys experience shows how difficult it is to navigate
these questions. When Chouldechova, as requested, began digging
through the Allegheny data in early 2017, she found that its tool also suffered similar statistical imbalances. The model had some pretty undesirable properties, she says. The difference in error rates was much higher
than expected across race and ethnicity groups. And, for reasons that
are still not clear, white children that the algorithm scored as at highest
risk of maltreatment were less likely to be removed from their homes
than were black children given the highest risk scores2. Allegheny and
Vaithianathans team are currently considering switching to a different
model. That could help to reduce inequities, says Chouldechova.
Although statistical imbalances are a problem, a deeper dimension of
unfairness lurks within algorithms  that they might reinforce societal
injustices. For example, an algorithm such as COMPAS might purport
to predict the chance of future criminal activity, but it can only rely on
measurable proxies, such as being arrested. And variations in policing
practices could mean that some communities are disproportionately
targeted, with people being arrested for crimes that might be ignored
in other communities. Even if we are accurately predicting something,
the thing we are accurately predicting might be the imposition of injustice, says David Robinson, a managing director at Upturn, a non-profit
social-justice organization in Washington DC. Much would depend on
the extent to which judges rely on such algorithms to make their decisions  about which little is known.
Alleghenys tool has come under criticism along similar lines. Writer
and political scientist Virginia Eubanks has argued that, irrespective of
whether the algorithm is accurate, it is acting on biased inputs, because
black and biracial families are more likely to be reported to hotlines.
Furthermore, because the model relies on public-services information in
the Allegheny system  and because the families who used such services
are generally poor  the algorithm unfairly penalizes poorer families by
subjecting them to more scrutiny. Dalton acknowledges that the available
data are a limitation, but she thinks the tool is needed. The unfortunate
societal issue of poverty does not negate our responsibility to improve our
decision-making capacity for those children coming to our attention,
the county said in a response to Eubanks, posted on the AFST website
earlier this year.

TRANSPARENCY AND ITS LIMITS

Although some agencies build their own tools or use commercial
software, academics are finding themselves in demand for work on
public-sector algorithms. At the University of Chicago, Ghani has been
working with a range of agencies, including the public-health department

How to define fair
Researchers studying bias in algorithms say there are many ways
of defining fairness, which are sometimes contradictory.
Imagine that an algorithm for use in the criminal-justice system
assigns scores to two groups (blue and purple) for their risk of
being rearrested. Historical data indicate that the purple group has
a higher rate of arrest, so the model would classify more people
in the purple group as high risk (see figure, top). This could occur
even if the models developers try to avoid bias by not directly
telling their model whether a person is blue or purple. That is
because other data used as training inputs might correlate with
being blue or purple.

Classified
high risk

A high-risk status cannot perfectly predict rearrest, but the
algorithms developers try to make the prediction equitable: for
both groups, high risk corresponds to a two-thirds chance of
being rearrested within two years. (This kind of fairness is termed
predictive parity.) Rates of future arrests might not follow past
patterns. But in this simple example, assume that they do: as
predicted, 3 out of 10 in the blue group and 6 out of 10 in the purple
group (and two-thirds of those labelled high risk in each group)
are indeed rearrested (indicated in grey bars in figure, bottom).

False
positives

This algorithm has predictive parity. But theres a problem. In the
blue group, 1 person out of 7 (14%) was misidentified as high
risk; in the purple group, it was 2 people out of 4 (50%). So purple
individuals are more likely to be false positives: misidentified as
high risk.
As long as blue and purple group members are rearrested at
different rates, then it will be difficult to achieve predictive parity
and equal false-positive rates. And it is mathematically impossible
to achieve this while also satisfying a third measure of fairness:
equal false-negative rates (individuals who are identified as low risk
but subsequently rearrested; in the example above, this happens
to be equal, at 33%, for both purple and blue groups).
Some would see the higher false-positive rates for the purple
group as discrimination. But other researchers argue that this
is not necessarily clear evidence of bias in the algorithm. And
there could be a deeper source for the imbalance: the purple
group might have been unfairly targeted for arrest in the first
place. In accurately predicting from past data that more people
in the purple group will be rearrested, the algorithm could be
recapitulating  and perhaps entrenching  a pre-existing
societal bias. R.C.

.
d
e
v
r
e
s
e
r
s
t
h
g
i
r
l
l
A
.
e
r
u
t
a
N
r
e
g
n
i
r
p
S
f
o
t
r
a
p
,
d
e
t
i
m
i
L
s
r
e
h
s
i
l
b
u
P
n
a
l
l
i
m
c
a
M
8
1
0
2


2 1 J U N E 2 0 1 8 | VO L 5 5 8 | NAT U R E | 3 5 9

SOURCE: A. CHOULDECHOVA

FEATURE NEWS

of Chicago on a tool to predict which homes might harbour hazardous the information. It seems like that will be a limitation on our ability to
lead. In the United Kingdom, researchers at the University of Cambridge assess fairness, he says. The scope of GDPR provisions that might give
have worked with police in County Durham on a model that helps to the public insight into algorithms and the ability to appeal is also in quesidentify who to refer to intervention programmes, as an alternative to tion. As written, some GDPR rules apply only to systems that are fully
prosecution. And Goel and his colleagues this year launched the Stanford automated, which could exclude situations in which an algorithm affects
Computational Policy Lab, which is conducting collaborations with gov- a decision but a human is supposed to make the final call. The details,
ernment agencies, including the San Francisco District Attorneys office. Mittelstadt says, should eventually be clarified in the courts.
Partnerships with outside researchers are crucial, says Maria McKee,
an analyst at the district attorneys office. We all have a sense of what is AUDITING ALGORITHMS
right and what is fair, she says. But we often dont have the tools or the Meanwhile, researchers are pushing ahead on strategies for detecting
research to tell us exactly, mechanically, how to get there.
bias in algorithms that havent been opened up for public scrutiny. Firms
There is a large appetite for more transparency, along the lines adopted might be unwilling to discuss how they are working to address fairness,
by Allegheny, which has engaged with stakeholders and opened its doors says Barocas, because it would mean admitting that there was a problem
to journalists. Algorithms generally exacerbate problems when they are in the first place. Even if they do, their actions might ameliorate bias
closed loops that are not open for algorithmic auditing, for review, but not eliminate it, he says. So any public statement about this will
or for public debate, says Crawford at the AI
also inevitably be an acknowledgment that the
Now Institute. But it is not clear how best to
problem persists. But in recent months, Micromake algorithms more open. Simply releasing
soft and Facebook have both announced the
all the parameters of a model wont provide
development of tools to detect bias.
much insight into how it works, says Ghani.
Some researchers, such as Christo Wilson, a
Transparency can also conflict with efforts to
computer scientist at Northeastern University
protect privacy. And in some cases, disclosing
in Boston, try to uncover bias in commercial
too much information about how an algorithm
algorithms from the outside. Wilson has creworks might allow people to game the system.
ated mock passengers who purport to be in
One big obstacle to accountability is that
search of Uber taxi rides, for example, and has
agencies often do not collect data on how the
uploaded dummy CVs to a jobs website to test
tools are used or their performance, says Goel.
for gender bias. Others are building software
A lot of times theres no transparency because
that they hope could be of general use in selftheres nothing to share. The California legisassessments. In May, Ghani and his colleagues
lature, for instance, has a draft bill that calls for
released open-source software called Aequitas
risk-assessment tools to help reduce how often
to help engineers, policymakers and analysts
defendants must pay bail  a practice that has
to audit machine-learning models for bias.
been criticized for penalizing lower-income
And mathematician Cathy ONeil, who has
defendants. Goel wants the bill to mandate that
been vocal about the dangers of algorithmic
data are collected on instances when judges
decision-making, has launched a firm that is
disagree with the tool and on specific details,
working privately with companies to audit
including outcomes, of every case. The goal is
their algorithms.
fundamentally to decrease incarceration while
Some researchers are already calling for a
maintaining public safety, he says, so we have
step back, in criminal-justice applications and
to know  is that working?
Rhema Vaithianathan builds algorithms to help
other areas, from a narrow focus on building
Crawford says that a range of due process flag potential cases of child abuse.
algorithms that make forecasts. A tool might
infrastructure will be needed to ensure that
be good at predicting who will fail to appear
algorithms are made accountable. In April, the AI Now Institute in court, for example. But it might be better to ask why people dont
outlined a framework3 for public agencies interested in responsible appear and, perhaps, to devise interventions, such as text reminders or
adoption of algorithmic decision-making tools; among other things, transportation assistance, that might improve appearance rates. What
it called for soliciting community input and giving people the ability to these tools often do is help us tinker around the edges, but what we need
appeal decisions made about them.
is wholesale change, says Vincent Southerland, a civil-rights lawyer and
Many are hoping that laws could enforce such goals. There is some racial-justice advocate at New York Universitys law school. That said,
precedent, says Solon Barocas, a researcher who studies ethics and policy the robust debate around algorithms, he says, forces us all to ask and
issues around artificial intelligence at Cornell University in Ithaca, New answer these really tough fundamental questions about the systems that
York. In the United States, some consumer-protection rules grant citi- were working with and the ways in which they operate.
zens an explanation when an unfavourable decision is made about their
Vaithianathan, who is now in the process of extending her childcredit4. And in France, legislation that gives a right to explanation and abuse prediction model to Douglas and Larimer counties in Colorado,
the ability to dispute automated decisions can be found as early as the sees value in building better algorithms, even if the overarching
1970s, says Veale.
system they are embedded in is flawed. That said, algorithms cant be
The big test will be Europes GDPR, which entered into force on helicopter-dropped into these complex systems, she says: they must be
25 May. Some provisions  such as a right to meaningful information implemented with the help of people who understand the wider context.
about the logic involved in cases of automated decision-making  seem But even the best efforts will face challenges, so in the absence of straight
to promote algorithmic accountability. But Brent Mittelstadt, a data ethi- answers and perfect solutions, she says, transparency is the best policy.
cist at the Oxford Internet Institute, UK, says the GDPR might actually I always say: if you cant be right, be honest. 
hamper it by creating a legal minefield for those who want to assess
fairness. The best way to test whether an algorithm is biased along certain Rachel Courtland is a science journalist based in New York City.
lines  for example, whether it favours one ethnicity over another  1. Chouldechova, A. Preprint at https://arxiv.org/abs/1703.00056 (2017).
requires knowing the relevant attributes about the people who go into 2. Chouldechova, A., Putnam-Hornstein, E., Benavides-Prado, D., Fialko, O. &
Vaithianathan, R. Proc. Machine Learn. Res. 81, 134148 (2018).
the system. But the GDPRs restrictions on the use of such sensitive data
3. Reisman, D., Schultz, J., Crawford, K. & Whittaker, M. Algorithmic Impact
are so severe and the penalties so high, Mittelstadt says, that companies
Assessments: A Practical Framework for Public Agency Accountability (AI Now, 2018).
in a position to evaluate algorithms might have little incentive to handle 4. Wachter, S., Mittelstadt, B. & Floridi, L. Sci. Robotics 2, eaan6080 (2017).
.
d
e
v
r
e
s
e
r
s
t
h
g
i
r
l
l
A
.
e
r
u
t
a
N
r
e
g
n
i
r
p
S
f
o
t
r
a
p
,
d
e
t
i
m
i
L
s
r
e
h
s
i
l
b
u
P
n
a
l
l
i
m
c
a
M
8
1
0
2


3 6 0 | NAT U R E | VO L 5 5 8 | 2 1 J U N E 2 0 1 8

AUT

NEWS FEATURE

