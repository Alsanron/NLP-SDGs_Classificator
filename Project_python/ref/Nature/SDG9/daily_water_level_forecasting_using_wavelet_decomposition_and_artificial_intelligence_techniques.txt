Accepted Manuscript
Daily Water Level Forecasting Using Wavelet Decomposition and Artificial
Intelligence Techniques
Youngmin Seo, Sungwon Kim, Ozgur Kisi, Vijay P. Singh
PII:
DOI:
Reference:

S0022-1694(14)00971-8
http://dx.doi.org/10.1016/j.jhydrol.2014.11.050
HYDROL 20065

To appear in:

Journal of Hydrology

Received Date:
Revised Date:
Accepted Date:

25 July 2014
15 October 2014
14 November 2014

Please cite this article as: Seo, Y., Kim, S., Kisi, O., Singh, V.P., Daily Water Level Forecasting Using Wavelet
Decomposition and Artificial Intelligence Techniques, Journal of Hydrology (2014), doi: http://dx.doi.org/10.1016/
j.jhydrol.2014.11.050

This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers
we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and
review of the resulting proof before it is published in its final form. Please note that during the production process
errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.

1

Daily Water Level Forecasting Using Wavelet Decomposition and Artificial

2

Intelligence Techniques

3

Youngmin Seo1 Sungwon Kim2* Ozgur Kisi3 and Vijay P. Singh4

4
5
6

1

7
8

Adjunct Professor, Department of Constructional Disaster Prevention Engineering, Kyungpook
National University, Sangju, South Korea, 742-711 (E-mail: ymseo@knu.ac.kr)

2

9

Associate Professor, Department of Railroad and Civil Engineering, Dongyang University, Yeongju,
South Korea, 750-711 (E-mail: swkim1968@dyu.ac.kr)

10

3

11

Samsun, Turkey (E-mail: okisi@basari.edu.tr)

12

4

Department of Civil Engineering, Architecture and Engineering Faculty, Canik Basari University,

Caroline & William N. Lehrer Distinguished Chair and Distinguished Professor, Department of

13

Biological and Agricultural Engineering & Zachry Department of Civil Engineering, Texas A &

14

M University, College Station, Texas, 77843-2117 (E-mail: vsingh@tamu.edu)

15
16
17
18

* Corresponding Author; Phone: 82-54-630-1241; Fax: 82-54-637-8027; Email: swkim1968@dyu.ac.kr

19

Abstract

20

Reliable water level forecasting for reservoir inflow is essential for reservoir operation.

21

The objective of this paper is to develop and apply two hybrid models for daily water level

22

forecasting and investigate their accuracy. These two hybrid models are wavelet-based

23

artificial neural network (WANN) and wavelet-based adaptive neuro-fuzzy inference system

24

(WANFIS).

25

Wavelet decomposition is employed to decompose an input time series into

26

approximation and detail components. The decomposed time series are used as inputs to

27

artificial neural networks (ANN) and adaptive neuro-fuzzy inference system (ANFIS) for

28

WANN and WANFIS models, respectively. Based on statistical performance indexes, the

29

WANN and WANFIS models are found to produce better efficiency than the ANN and

30

ANFIS models. WANFIS7-sym10 yields the best performance among all other models. It is

31

found that wavelet decomposition improves the accuracy of ANN and ANFIS.

32

This study evaluates the accuracy of the WANN and WANFIS models for different

33

mother wavelets, including Daubechies, Symmlet and Coiflet wavelets. It is found that the

34

model performance is dependent on input sets and mother wavelets, and the wavelet

35

decomposition using mother wavelet, db10, can further improve the efficiency of ANN and

36

ANFIS models. Results obtained from this study indicate that the conjunction of wavelet

37

decomposition and artificial intelligence models can be a useful tool for accurate forecasting

38

daily water level and can yield better efficiency than the conventional forecasting models.

39
40
41
42
43

Key Words: Water level forecasting, wavelet decomposition, artificial neural network,
adaptive neuro-fuzzy inference system

44

1. Introduction

45

The temporal and spatial variability of precipitation is very high in South Korea. About

46

70% of annual precipitation occurs in the summer season between June and September (Bae

47

et al., 2007). In addition to precipitation characteristics, overpopulation, urbanization,

48

industrialization and change of farming practices further complicate operational hydrology.

49

The control and management of reservoir systems are essential in South Korea, since they

50

play an important role in water supply and flood prevention. Accurate forecasting of reservoir

51

inflow is critical for enhancing reservoir operation, water supply, flood prevention,

52

hydropower generation, water resources management and decision support system.

53

Conventionally, reservoir inflow forecasting has been done using statistical models

54

based on time series analysis, including AR (autoregressive), ARMA (autoregressive moving

55

average), ARIMA (autoregressive integrated moving average), FGN (fractional Gaussian

56

noise), BL (broken line), TF (transfer function), TFN (transfer function noise) and ARMAX

57

(autoregressive moving average with exogenous terms). Since most current models can be

58

classified as linear models, they have limited capability to forecast the inflow pattern that is

59

highly nonlinear and non-stationary.

60

Over the past years, artificial intelligence (AI) techniques have been successfully

61

developed for modeling non-linear hydrologic systems. In particular, artificial neural

62

networks (ANNs) and adaptive neuro-fuzzy inference system (ANFIS) have been accepted as

63

effective tools for modeling complex hydrologic systems (Bae et al., 2007; Cheng et al.,

64

2005b; Coulibaly et al., 2000; El-Shafie et al., 2007; Figueiredo et al., 2007; Jain et al., 1999;

65

Jeong and Kim, 2005; Jothiprakash and Magar, 2012; Karimi-Googhari and Lee, 2011; Kim

66

et al., 2013b; Othman and Naseri, 2011; Razavi and Araghinejad, 2009; Seo et al., 2013a,

67

2013b, 2013c; Wu et al., 2009).

68

ANNs are parallel computational models that resemble biological neural network and

69

have better generalization capabilities. ANFIS, on the other hand, combines the advantages of

70

both ANN and fuzzy inference system (Okkan, 2012) and is becoming more popular in

71

hydrological applications. It is reported to perform better than ANNs for flood forecasting

72

and long-term discharge prediction (Chau et al., 2005; Cheng et al., 2005a, 2005b). Although

73

ANN and ANFIS have been extensively used for prediction of hydrological variables, they

74

have also some problems when dealing with non-stationary data.

75

Since hydrological time series includes several frequency components and have

76

nonlinear relationships, hybrid model approaches have been used to improve the performance

77

of models forecasting (Okkan, 2012). These approaches include chaotic neural networks

78

(Karunasinghe and Liong, 2006), neural networks based on set pair analysis (SPA) and

79

principle component analysis (PCA) (Wang et al., 2006a; Wang et al., 2006b; Wu et al.,

80

2009), threshold neural networks (Wang et al., 2006b), cluster-based hybrid neural networks

81

(Cigizoglu and Kisi, 2005), and bootstrapped artificial neural networks (Han et al., 2007;

82

Jeong and Kim, 2005; Jia and Culver, 2006; Kim et al., 2013a; Seo et al., 2013a, 2013c;

83

Sharma and Tiwari, 2009; Srivastav et al., 2007; Tibshirani, 1996; Tiwari and Chatterjee,

84

2010a, 2010b; Twomey and Smith, 1998; Zio, 2006).

85

In the last years, the conjunction of wavelet transform and AI techniques has been

86

successfully implemented in hydrological applications (Abiyev, 2011; Adamowski and Chan,

87

2011; Adamowski and Prasher, 2012; Adamowski and Sun, 2010; Anctil and Tape, 2004;

88

Belayneh and Adamowski, 2012; Cannas et al., 2006; Khanghah et al., 2012; Kisi, 2008,

89

2011; Kisi et al., 2011; Nejad and Nourani, 2012; Nourani et al., 2012; Okkan, 2012; Okkan

90

and Serbes, 2013; Rajaee, 2010; Rajaee et al., 2011; Tiwari and Chatterjee, 2010b; Wang and

91

Ding, 2003; Wang et al., 2009; Wei et al., 2012). The wavelet transform is another data-

92

preprocessing technique which can analyze a signal in both time and frequency so that it can

93

overcome the drawbacks of conventional Fourier transform. The wavelet transform provides

94

an effective decomposition of time series so that the decomposed data can increase the

95

performance of hydrological forecasting models by capturing the useful information on

96

different resolution levels (Nourani et al., 2009, 2011).

97

Adamowski and Sun (2010) proposed a method based on coupling discrete wavelet

98

transforms and ANN for streamflow forecasting for non-perennial rivers in semi-arid

99

watersheds. The performance of the coupled wavelet-neural network models (WA-ANN) was

100

compared with the ANN models for streamflow forecasting. They found that the WA-ANN

101

models provided more accurate streamflow forecasting than the ANN models.

102

Tiwari and Chatterjee (2010b) developed a hybrid wavelet-bootstrap-ANN (WBANN)

103

model to investigate the potential of wavelet and bootstrapping techniques for developing an

104

accurate and reliable ANN model for hourly flood forecasting. They compared the

105

performance of WBANN model with three different ANN models including traditional ANN,

106

wavelet-based ANN (WANN) and bootstrap-based ANN (BANN). They found that the

107

overall WBANN model was more accurate and reliable as compared to the other three

108

models. They also found that the WBANN model improved the reliability for flood

109

forecasting with greater confidence.

110

Adamowski and Chan (2011) proposed a method coupling the discrete wavelet

111

transform and ANN for monthly groundwater level forecasting. Comparing the proposed

112

coupled wavelet-neural network models (WA-ANN) with ANN and ARIMA models for

113

groundwater level forecasting, they found that the WA-ANN models provided more accurate

114

average groundwater level forecasts than the ANN and ARIMA models.

115

Kisi (2011) proposed a simple wavelet regression (WR) approach for modeling daily

116

reference evapotranspiration. The accuracy of the WR models was compared with that of the

117

single linear regression (LR) and the empirical models including CIMIS Penman, Hargreaves,

118

Ritchie and Turc. Comparison of these models showed that the WR models performed better

119

than the LR and empirical models for modeling daily reference evapotranspiration.

120

Kisi et al. (2011) proposed a new method combining wavelet and gene expression

121

programming (WGEP) methods for forecasting long-term air temperature. This method

122

combines the discrete wavelet and genetic programming methods. Comparing the accuracy of

123

single GEP and WGEP models, they found that the WGEP model performed much better than

124

the single GEP model. They also found that the WGEP model significantly increased the

125

accuracy of single GEP model especially for forecasting long-term air temperatures.

126

Adamowski and Prasher (2012) compared support vector regression (SVR) and wavelet

127

networks (WN) for daily runoff forecasting in a mountainous watershed. They found that the

128

best WN model performed slightly better than the best SVR model.

129

Nejad and Nourani (2012) applied wavelet-based global soft thresholding method to

130

denoise daily time series of streamflow. The denoised time series was imposed on an ANN

131

model to forecast streamflow. They found that the results of the ANN model for streamflow

132

forecasting could be improved by 11% when the wavelet-based denosing approach, as a pre-

133

processing method, was applied to the data.

134

Okkan (2012) developed a hybrid model using discrete wavelet transform (DWT) and

135

feed forward neural networks (FFNNs) for monthly runoff prediction. It was found that the

136

hybrid models successfully predicted monthly runoff series and gave good prediction

137

performance as compared with conventional methods, including FFNN, multiple linear

138

regression (MLR), wavelet-MLR combined model, and PCA-based neural networks.

139

Okkan and Serbes (2013) developed different hybrid models combining DWT and

140

different black box techniques, including multiple linear regression (MLR), feed forward

141

neural networks (FFNN), and least square-support vector machines (LS-SVM) for reservoir

142

inflow modeling using weather data. They found that the DWT-FFNN model performed

143

better than other models in terms of mean square errors (MSE) and determination coefficients

144

(R2). They also found that DWT increased the accuracy of MLR and LS-SVM.

145

This study develops and applies two different hybrid models, wavelet-based artificial

146

neural network (WANN) and wavelet-based adaptive neuro-fuzzy inference system

147

(WANFIS), for daily water level forecasting in the Andong dam watershed, South Korea. The

148

statistical performance measures are employed to evaluate the developed models. The

149

performance of WANN and WANFIS models is compared with that of ANN and ANFIS

150

models, respectively. The effect of different mother wavelets such as Daubechies (db1, db2,

151

db4, db6, db8 and db10), Symmlet (sym2, sym4, sym8 and sym10) and Coiflet (coif6, coif12

152

and coif18) wavelets on accuracy of the WANN and WANFIS models is also investigated in

153

this study.

154
155

2. Methodology

156

2.1 Wavelet decomposition

157

Wavelet analysis is a multi-resolution analysis in time and frequency domains. The

158

wavelet transform decomposes a time series signal into different resolutions by controlling

159

scaling and shifting. It provides a good localization properties in both time and frequency

160

domains (Nejad and Nourani, 2012). It also has an advantage that it has flexibility in

161

choosing the mother wavelet, which is the transform function, according to the characteristics

162

of time series. The continuous wavelet transform (CWT) of a signal x(t ) is defined as

163

(Adamowski and Sun, 2010):

164

CWTx ( , s ) =

1


|s|

+



 t 
x(t )* 
 s


 dt ,


(1)

165

where s = the scale parameter,  = the translation parameter, * = the complex conjugate,

166

and (t ) = mother wavelet. CWT necessitates a large amount of computation time and

167

resources, while DWT requires less computation time and is simpler to implement than CWT.

168

DWT involves choosing scales and positions, which are called dyadic scales and positions,

169

based on powers of two. This is achieved by modifying the wavelet representation as

170

(Adamowski and Sun, 2010):

171

 j ,k (t ) =

 t  k 0 s0j 

,
j
| s0j |  s0

1

(2)

172

where j and k = integers that control the wavelet dilation and translation, respectively.

173

s0 > 1 is a fixed dilation step, and  0 = the location parameter. The most common and

174

simplest choice for parameters are s0 = 2 and  0 = 1 (Nourani et al., 2009). Using the

175

wavelet discretization, the time-scale space can be sampled at discrete levels.

176

A fast DWT algorithm developed by Mallat (1989) is based on four filters, including

177

decomposition

178

reconstruction high-pass filters. For the practical implementation of Mallats algorithm, low-

179

pass and high-pass filters are used instead of father and mother wavelets, which are also

180

called scaling and wavelet functions, respectively. The low-pass filter, associated with the

181

scaling function, allows the analysis of low frequency components, while the high-pass filter,

182

associated with the wavelet function, allows the analysis of high frequency components.

183

These filters used in Mallats algorithm are determined according to the selection of mother

184

wavelets (Gonzlez-Audcana et al., 2005). The multiresolution analysis by Mallats

185

algorithm is a procedure to obtain approximations and details for a given time series

186

signal. An approximation holds the general trend of the original signal, while a detail depicts

187

high-frequency components of it. A multilevel decomposition process (Figure 1) can be

188

achieved, where the original signal is broken down into lower resolution components

low-pass,

decomposition

high-pass,

reconstruction

low-pass

and

189

(Catalo et al., 2011). Detailed information for Mallats algorithm can be found in Nason

190

(2010).

191

There are many types of wavelets, including Daubechies, Symmlet, Coiflet, Meyer,

192

Gaussian, Mexican hat, Morlet, and Shannon wavelets, and so on, which can be used for

193

wavelet-based time series analysis (Minu et al., 2010). For discrete wavelet analysis,

194

orthogonal wavelets (Daubechies extremal-phase and least-asymmetric wavelets) and B-

195

spline biorthogonal wavelets have been commonly used. For continuous wavelet analysis,

196

Morlet, Meyer, Gaussian and Paul wavelets have been used (Mathworks, 2014b). The

197

extremal-phase wavelets including Haar and db N wavelets, where N refers to the number of

198

vanishing moments (Constantine and Percival, 2013), are also called Daublets or Daubechies

199

wavelets. Daubechies wavelets are one of the most widely used wavelets. They represent a

200

collection of orthogonal mother wavelets with compact support, characterized by a maximal

201

number of vanishing moments for some given length of the support. For example,

202

Daubechies wavelet of order 1 (db1), which also called the Haar wavelet, has one vanishing

203

moment, db2 has two vanishing moments and so on. The Haar wavelet has the shortest

204

support among all orthogonal wavelets. The Haar wavelet generates orthogonal wavelets by

205

translations and dilations. It is the single symmetric orthogonal wavelet (Stolojescu, 2012).

206

The least-asymmetric wavelets are also known as Symmlets. The Symmlets are proposed to

207

improve symmetry as a modification to Daubechies wavelets. The Symmlets are compact

208

supported, orthogonal, continuous, but only nearly symmetric mother wavelets. Their

209

construction is very similar to the construction of Daubechies wavelets, but the symmetry of

210

Symmlets is stronger than that of Daubechies wavelets. Coifman wavelets, which also called

211

Coiflets, are discrete wavelets which are compactly supported wavelets and designed to be

212

more symmetrical than Daubechies wavelets (Stolojescu, 2012).

213

214

2.2 Artificial neural network

215

ANN is parallel computing systems that were developed originally, based on the

216

structure and functional aspects of biological neural networks. A feed-forward ANN

217

comprises a system of units, analogous to neurons, that are arranged in layers (Imrie et al.,

218

2000). A multilayer perceptron (MLP) is the most popular neural network architecture. MLP

219

is typically composed of several layers of nodes (neurons). An input layer, which is the first

220

layer, receives external information. The problem solution is obtained in an output layer

221

which is the last layer. One or more intermediate layers, which are called hidden layers,

222

separate the input and output layers. The nodes in adjacent layers are usually fully connected

223

by acyclic arcs, which are called synapses, from the input layer to the output layer (Zhang et

224

al., 1998). Figure 2 shows the general architecture of ANN.

225
226

The simplest MLP consists of an input layer with n covariates and an output layer with
one output neuron. It calculates the function (Gnther and Fritsch, 2010):
n


o (x) = f  w0 +  wi xi  = f ( w0 + w  x) ,


i =1

227

(3)

228

where w0 = the intercept, w = (w1, , wn ) is the vector consisting of all synaptic weights

229

without the intercept, and x = ( x1 , , xn ) is the vector of all covariates.

230

Hidden layers can be included to increase the model flexibility. However, Hornik et al.

231

(1989) showed that one hidden layer is sufficient to model any piecewise continuous function.

232

MLP with a hidden layer and J hidden neurons calculates the following function (Gnther

233

and Fritsch, 2010):

234

J
n



o (x) = f  w0 +  w j  f  w0 j +  wij xi 



j =1
i=1
J


= f  w0 +  w j  f ( w0 j + w j x) ,


j =1

(4)

235

where w0 = the intercept of the output neuron, w0 j = the intercept of the jth hidden neuron,

236

wj = the synaptic weight corresponding to the synapse starting at the jth hidden neuron and

237

leading to the output neuron, w j = (w1 j ,, wnj ) is the vector of all synaptic weights

238

corresponding to the synapses leading to the jth hidden neuron, and x = ( x1 ,, xn ) is the

239

vector of all covariants.

240

All hidden neurons and output neurons calculate an output of f ( g ( z0 , z1,, zk ))

241

= f ( g (z )) from the outputs of all preceding neurons, z0 , z1, , zk , where g = the

242

integration function, f = the activation function, and the neuron z0  1 is the intercept.

243

The integration function is often defined as g (z ) = w0 z0 +  i =1 wi zi = w0 + w  z . The

244

activation function is usually a bounded non-decreasing nonlinear and differentiable function,

245

including the logistic function or the hyperbolic tangent (Gnther and Fritsch, 2010).

k

246

The training of a neural network model is a process of adjusting the connection weights

247

and biases so that its output can match the desired output best. Specifically, at each setting of

248

the connection weights, it is possible to calculate the error committed by the networks simply

249

by taking the difference between the desired and actual responses (Simpson, 1990; Specht,

250

1991; Gallant, 1993; Bishop, 1994; Tsoukalas and Uhrig, 1997; Vallet-Coulomb et al., 2001;

251

Haykin, 2009). In this study, ANN was trained with the Vanilla backpropagation algorithm,

252

which is also called the standard backpropagation algorithm, using training data.

253
254

2.3 Adaptive neuro-fuzzy inference system

255

ANFIS is a combination of an adaptive neural network and a fuzzy inference system

256

(FIS). Parameters of FIS are determined by neural network learning algorithms. Since this

257

system is based on FIS, reflecting vague knowledge, an important aspect is that the system

258

should always be interpretable in terms of fuzzy IF-THEN rules. ANFIS can approximate any

259

real continuous function on a compact set to any degree of accuracy (Jang et al., 1997).

260

ANFIS identifies a set of parameters through a hybrid learning rule combining the

261

backpropagation gradient descent method and least-square method. There are two approaches

262

for FIS, namely the approaches of Mamdani (Mamdani and Assilian, 1975) and Sugeno

263

(Takagi and Sugeno, 1985). The differences between the two approaches arise from the

264

consequent part. Mamdanis approach uses fuzzy membership functions (MFs), whereas

265

Sugenos approach uses linear or constant functions. In this study, Sugenos approach was

266

used for forecasting daily water level. Figure 3 shows the general architecture of ANFIS.

267

The procedure described by Jang et al. (1997) was adopted for forecasting daily water

268

level. As a simple example of the procedure adopted, a FIS with two inputs and one output is

269

considered. ANFIS has five layers comprising different node functions as shown in figure 3.

270

The output of the ith node in layer 1 is denoted as O1,i . Every node i in layer 1 is an adaptive

271

node, with O1,i =  Ai ( x ) for i=1, 2 or O1,i =  Bi2 ( y ) for i=3, 4, where x (or y) is the input

272

to node i, and Ai (or Bi  2 ) is a linguistic label (e.g., LOW or HIGH) associated with this

273

node. The Gaussian MFs for A and B can be written as:

274

 ( x  c ) 2 
 ( y  c ) 2  ,
and

(
y
)
=
exp
Bi 2

 2 2 
2
 2




 A ( x) = exp 
i

(5)

275

where [ , c ] = the parameter set. Any continuous and piecewise differential functions, such

276

as commonly used triangular-shaped or bell-shaped MFs, are also qualified candidates for

277

node function in layer 1 (Jang, 1993). Parameters in layer 1 are called premise parameters. In

278

this study, Gaussian MFs were used for ANFIS models to determine the best results for input

279

nodes.

280

Layer 2 consists of nodes labeled  , that multiply the incoming signals and sends the

281

product out. The output of layer 2 comprises the membership values of the premise part and

282

can be written as:

O2,i = wi =  Ai ( x ) Bi ( y ) , i = 1, 2 .

283

(6)

284

The nodes labeled N calculate the ratio of the ith rules firing strength to the sum of all

285

rules firing strengths in layer 3. Each node output represents the firing strength of a rule and

286

can be written as:

287

O3,i = wi =

wi
, i = 1, 2 ,
w1 + w2

(7)

288

where wi is the output of layer 3. The outputs of layer 3 are called normalized firing

289

strengths. The nodes of layer 4 are adaptive with node functions and can be written as:
O4,i = wi f i = wi ( pi x + qi y + ri ) , i = 1, 2 ,

290

[ pi , qi , ri ]

(8)

291

where

292

parameters. The single fixed node of layer 5 labeled  computes the final output as the

293

summation of all incoming signals that can be written as:

is the parameter set. Parameters of layer 4 are referred to as consequent

2

2

O5,i =  wi fi =

294

i =1

w f

i i

i =1
2

w

.

(9)

i

i =1

295

ANFIS is trained using a hybrid learning algorithm, combining the least-square method

296

and the backpropagation gradient descent method, to adjust the premise and consequent

297

parameters. In forward pass, the least-square method is used to identify consequent

298

parameters. In backward pass, the gradient descent method is used to propagate the errors

299

backward and adjust premise parameters. Detailed information for ANFIS can be found in

300

Jang (1993).

301
302

2.4 WANN and WANFIS

303

WANN is the conjunction model of wavelet decomposition and ANN. WANFIS is the

304

conjunction model of wavelet decomposition and ANFIS. The wavelet decomposition is

305

employed to decompose an input time series into approximation and detail components. The

306

decomposed time series are used as inputs to ANN and ANFIS for WANN and WANFIS

307

models, respectively.

308

WANN and WANFIS consist of a two-step algorithm. The first step corresponds to

309

multilevel wavelet decomposition. The input time series of ANN and ANFIS are decomposed

310

using wavelet transform. In other words, details and approximations obtained by wavelet

311

decomposition are used as input to the ANN and ANFIS. In this study, DWT using Mallats

312

algorithm was used for decomposing the time series signals. As described in section 2.1, the

313

multiresolution analysis by Mallats algorithm generates approximations and details for a

314

given time series signal. An approximation holds the general trend of the original signal,

315

whereas a detail depicts high-frequency components of it. Therefore, the original signal is

316

broken down into lower resolution components. For example, three-level DWT decomposes a

317

signal x(t) into D1, D2, D3 and A3, where D1, D2 and D3 are details and A3 is an

318

approximation. D1, D2, D3 and A3 are used as input to the ANN and ANFIS. The second

319

step corresponds to training and testing phases using the ANN and ANFIS, respectively. For

320

the general description of ANN and ANFIS, refer to in Section 2.2 and 2.3, respectively. In

321

this step, the general strategies for modeling the ANN and ANFIS can be used including the

322

selection of lag time for input variables, the number of hidden nodes, learning algorithms,

323

type and number of membership function and generation of FIS structure, etc. The algorithm

324

of WANN and WANFIS is summarized as follows:

325
326
327
328
329

Step 1. Multilevel wavelet analysis using DWT decomposes a signal into details (D1,
D2, , Dk) and approximation (Ak), where k is decomposition level.
Step 2. ANN and ANFIS are trained and tested using the details and approximation as
input and the model performance is evaluated.
Figure 4 shows the flowchart for forecasting daily water level using WANN and WANFIS.

330
331

2.5 Performance evaluation

332

The performance of water level forecasting models (ANN, ANFIS, WANN, and

333

WANFIS) was evaluated using seven performance indexes including the coefficient of

334

efficiency (CE), the index of agreement (d), the coefficient of determination (r2), the root

335

mean squared error (RMSE), the mean absolute error (MAE), the mean squared relative error

336

(MSRE), and the mean higher order error (MS4E). These performance indexes (CE, d, r2,

337

RMSE, MAE, MSRE and MS4E) can be written as (Dawson and Wilby, 2001):
N

* 2
i

 (WL WL )
i

338

CE = 1

i =1
N

2

,

(10)

 (WL WL)
i

i=1
N

* 2
i

 (WL WL )
i

339

d = 1

i=1

N

2

,

(11)

 ( WL WL + WL WL )
*
i

i

i =1

340

341




r 2 = 




2




i =1
 ,

N
2
2 N
*

WL

WL
WL

WL
( i
) ( i
) 

i=1
i=1

N

)
(WLi WL)(WL WL
*
i

 1
RMSE = 

 N

0.5

WL  WLi   ,


 
i =1

N

*
i

2

(12)

(13)

342

MAE =

1
N

1
MSRE =
N

343

N

 WL WL
*
i

,

i

(14)

i=1

N



2

(WLi WL*i )

,

WL2i

i =1

N

(15)

 (WL  WL )
i

344

MS 4 E =

i =1

N

* 4
i

,

(16)

345

where WL*i = the forecasted values, WLi = the observed values, WL = the mean of

346

 = the mean of forecasted values, and N = the number of observed
observed values, WL

347

values.

348

349

3. Case Study

350

The Andong dam watershed (Figure 5) in South Korea was chosen for developing and

351

applying water level forecasting models in this study. The watershed is located in Nadong

352

River watershed which is in the south-eastern region of South Korea. Andong dam is an 83

353

m-high and 612 m-long rock fill dam. The reservoir area is 51.5 square kilometer and the

354

total reservoir capacity is 1,248 million cubic meter. The Andong dam was mainly

355

constructed for flood control, water supply, and energy generation. The watershed area of the

356

dam is 1,628.7 square kilometer and the stream length is 170.1 kilometer. The watershed has

357

a mean elevation of 548.1 m and a mean slope of 49.2 %. The annual mean precipitation of

358

the watershed is 1,070.6 mm.

359

Daily water level data from two streamflow gauging stations, Socheon and Dosan, were

360

obtained from the observation archives of Water Management Information System (WAMIS),

361

which is operated by the Ministry of Land, Infrastructure and Transport (MOLIT), South

362

Korea. Figure 5 shows the locations of streamflow gauging stations. The collected data were

363

prepared for the period between 2002 and 2013 years. The data were divided into two parts,

364

the first nine years (2002-2010) data for model training and the remaining three years (2011-

365

2013) data for model testing.

366

367

4. Application and Results

368

In this study, we used various computer programs such as MATLAB (Mathworks,

369

2014a), R language (R Core Team, 2014), R packages including wmtsa (Constantine and

370

Percival, 2013) and RSNNS (Bergmeir and Ben
tez, 2012) for water level forecasting using

371

wavelet decomposition.

372
373

4.1 Analysis

374

Appropriate input variables must be selected in advance for developing and applying

375

ANN, ANFIS, WANN, and WANFIS models for forecasting daily water level. In this study,

376

since daily water level data from two gauging stations are strongly auto-correlated and the

377

influence of rainfall is relatively marginal, simple models with only the daily water level as

378

inputs were selected. In addition to the current values of daily water level, daily water level

379

values at various lag times were also considered. This study used a statistical approach

380

suggested by Sudheer et al. (2002) to identify the optimal lag time for the inputs. The method

381

is based on the heuristic that the potential influencing variables corresponding to different

382

time lags can be identified through the statistical analysis of data series, such as

383

autocorrelation function (ACF), partial autocorrelation function (PACF), cross correlation

384

function (CCF) and average mutual information (AMI). This study determined the optimal

385

lag time of the inputs using ACF and CCF. For ACF > 0.6 and CCF > 0.6, maximum three-

386

lag time was determined.

387

To obtain the appropriate set of input variables, several models were trained with

388

different set of input variables. Table 1 shows seven sets of input and output variables for

389

model configuration. These sets were used as inputs for ANN, ANFIS, WANN, and

390

WANFIS models, respectively.

391

The input data were decomposed by DWT to develop and apply WANN and WANFIS

392

models. The optimal decomposition level must be selected in advance to determine the

393

performance of the model in the wavelet domain. Chou and Wang (2004) found that only one

394

decomposition level could not represent the process of streamflow time series. Kisi and

395

Cimen (2011) used three decomposition levels for forecasting monthly streamflow. Several

396

researchers have used an empirical equation to determine the decomposition level (Nourani et

397

al., 2009; Tiwari and Chatterjee, 2010b; Adamowski and Chan, 2011; Nejad and Nourani,

398

2012; Belayneh and Adamowski, 2012). In this study, the decomposition level was

399

determined using the following empirical equation (Nourani et al., 2009):

L = int [ log( N )]

400

(17)

401

where L = the decomposition level, N = the number of time series data, and int[] = the

402

integer-part function. In this study, three decomposition levels were obtained. Thus, input

403

times series were decomposed using different mother wavelets, and sub-time series of 2-day

404

mode (D1), 4-day mode (D2), 8-day mode (D3) and approximation mode (A3) for each input

405

set were obtained for the training and testing periods. In case of Set 2 (Table 1), for example,

406

input variables ( WLSC (t ) and WLDS (t  1) ) were decomposed into four sub-time series

407

components (D1, D2, D3 and A3) so that eight sub-time series were used as inputs for

408

WANN and WANFIS models.

409

This study also aims at examining the effects of different mother wavelets on the

410

efficiency of developed models. For this purpose, the performance of applied models were

411

investigated for different mother wavelets, including Haar wavelet (db1), Daubechies-2 (db2),

412

Daubechies-4 (db4), Daubechies-6 (db6), Daubechies-8 (db8), Daubechies-10 (db10),

413

Symmlet-2 (sym2), Symmlet-4 (sym4), Symmlet-8 (sym8), Symmlet-10 (sym10), Coiflet-6

414

(coif6), Coiflet-12 (coif12), and Coiflet-18 (coif18). For discrete wavelet analysis,

415

Daubechies wavelets have been commonly used mother wavelets, and Symmlets and Coiflets

416

have been also applied in hydrologic wavelet-based studies (Abdullah, 2000; Cannas et al.,

417

2005; Alikhani, 2009; Adamowski and Sun, 2010; Tiwari and Chatterjee, 2010b; Deka et al.,

418

2012; Nejad and Nourani, 2012; Nourani et al., 2012; Evrendilek, 2014; Santos et al., 2014).

419

Daubechies, Symmlet and Coiflet wavelets provide compact support (Vonesch et al., 2007;

420

Mathworks, 2014b), indicating that the wavelets have non-zero basis functions over a finite

421

interval, as well as full scaling and translational orthonormality properties (Popivanov and

422

Miller, 2002; de Artigas et al., 2006). These features are very important for localizing events

423

in the time-dependent signals (Popivanov and Miller, 2002). Based on these features,

424

Daubechies, Symmlet and Coiflet wavelets were selected as mother wavelets in this study.

425

Figure 6 shows an example of the original times series and sub-time series (D1, D2, D3 and

426

A3) decomposed using db10 wavelet for the training period.

427

The selection of effective wavelet components is important for the performance of

428

developed models. Previous studies selected effective wavelet components using the

429

correlation coefficients between wavelet components and observed values (Alikhani, 2009;

430

Tiwari and Chatterjee, 2010b; Kisi et al., 2011). The effective wavelet components have also

431

been selected using other methods, including Mallows Cp (Okkan, 2012; Okkan and Serbes,

432

2013), correlation coefficient, mutual information and Shannon entropy (Khanghah et al.,

433

2012), and self-organizing map (Nourani et al., 2012). Several researchers also used all

434

decomposed components as the effective wavelet components (Adamowski and Sun, 2010;

435

Adamowski and Chan, 2011; Kisi, 2011; Adamowski and Prasher, 2012; Belayneh and

436

Adamowski, 2012).

437

To construct new input time series from the wavelet components, several methods have

438

been used, including summing the effective components (Alikhani, 2009; Partal and

439

Cigizoglu, 2009; Kisi, 2010; Kisi and Cimen, 2011; Kisi et al., 2011), summing the

440

components for different levels (Adamowski and Chan, 2011; Adamowski and Prasher, 2012;

441

Belayneh and Adamowski, 2012), using all components for different levels without summing

442

the components (Cannas et al., 2005; Adamowski and Sun, 2010; Kisi, 2011), and using only

443

effective components without summing them (Okkan, 2012). Adamowski and Sun (2010)

444

used all sub-series as inputs to ANN models, since an averaging or optimizing selection of

445

only certain sub-series would have been a diminutive approach, and all sub-series coefficients

446

are equally important and contain information about the original time series. Belayneh and

447

Adamowski (2012) showed that using the sum of all the sub-series as an input provided more

448

accurate results than using certain sub-series that exhibited the highest correlations with the

449

original time series. Okkan (2012) used components of sub-time series as individual separate

450

model inputs determined with the regression method. In this study, new input time series was

451

set by using all components for different levels without summing the components.

452

MLP neural network model was used, which has an input layer, an output layer and a

453

hidden layer between the input and output layers, to forecast daily water level using ANN and

454

WANN models. The number of nodes in the hidden layer was determined using a trial-and-

455

error approach following the previous studies (Kisi, 2007; Kim et al., 2012; Kim et al.,

456

2013b). The optimal size of hidden nodes was determined by systematically increasing the

457

number of nodes from 1 to 30 until the network performance no longer was significantly

458

improved. For example, the optimal structure of ANN for input set, Set 6, was determined as

459

5-10-1. Here, the numbers indicate, respectively, an MLP neural network model comprising 5

460

input, 10 hidden, and 1 output nodes. In this study, ANN and WANN models were trained

461

with the Vanilla backpropagation algorithm, also called standard backpropagation, which is

462

the most common learning algorithm. Learning algorithms such as Levenberg-Marquardt and

463

conjugate gradient algorithms can used to improve the performance of ANN and WANN

464

models. Application and comparison of different learning algorithms for ANN modeling are

465

beyond the scope of this study. In this study, we selected common modeling strategies for

466

focusing on the effects of wavelet decomposition for model performance. The logistic

467

sigmoid activation function was selected for computing the output of each neuron. According

468

to the selected algorithm, the data of input and output nodes were scaled into the range of [0,

469

1].

470

In this study, the FIS of Sugeno type was used to forecast daily water level using ANFIS

471

and WANFIS models. We generated the FIS structure using the subtractive clustering

472

algorithm to overcome the drawback of the grid partition (GP) based ANFIS model. The

473

main drawback of the GP based ANFIS is that the number of control rules rapidly increases

474

and the running time also exponentially increases as the number of input variables and MFs

475

increases. The generated FIS was trained using a hybrid learning algorithm that applies a

476

combination of the least-square method and the backpropagation gradient descent method.

477

The ability of ANFIS model to achieve the performance goal depends on the predefined

478

internal ANFIS parameters, including the number and shape of MFs and the step size (El-

479

Shafie et al., 2007). Two approaches, including optimization algorithm (Hassanain et al.,

480

2004) and trial-and-error approach (Kim et al., 2002), have been used for determining the

481

optimal learning parameters of ANFIS and WANFIS models. In this study, we selected three

482

Gaussian MFs for each input node and a linear function as output MFs using the trial-and-

483

error approach. Since the different values of step size did not significantly affect the

484

performance of models, we used default values, initial step size of 0.01, step size decrease

485

rate of 0.9, and step size increase rate of 1.1, suggested by Fuzzy Logic Toolbox in

486

MATLAB (Mathworks, 2014a).

487

Based on the modeling strategies, a total of 196 models was developed in this study.

488

Seven ANN and ANFIS models were developed by using seven input sets (Table 1). 91

489

WANN and WANFIS models were developed by using the combination of 13 mother

490

wavelets and seven input sets (Table 1). Among total 196 models, 12 models with higher

491

performance were selected for performance comparison. Table 2 shows different

492

configurations of the models selected for performance comparison.

493
494

4.2 Evaluation of model performance

495

The performance indexes used in this study for evaluation of the applied models

496

accuracy in forecasting daily water level are: CE, d, r2, RMSE, MAE, MSRE, and MS4E.

497

Larger values of CE, d and r2 and smaller values of RMSE, MAE, MSRE and MS4E indicate

498

higher model efficiency.

499

Figure 7 shows model performance rankings of WANN and WANFIS models for

500

different input sets and mother wavelets when considering all input sets. In this study, model

501

performance rankings were employed to identify forecasting models with higher efficiency.

502

Models with higher values for CE, d and r2 have higher ranking, while models with higher

503

values for RMSE, MAE, MSRE and MS4E have lower ranking. Overall rankings were

504

determined by summing rankings for performance indexes and sorting the added ranking

505

values. The numbers (Figure 7) indicate model performance rankings. Model performance

506

was found to be dependent on input sets and mother wavelets. It can be seen from figure 7

507

that Set 4, Set 5, Set 6 and Set 7 yielded higher model efficiency than those for Set 1, Set 2

508

and Set 3 for the WANN and WANFIS models. This indicates that WANN and WANFIS

509

models with a larger number of lagged input variables for Socheon and Dosan stations can

510

yield higher efficiency. WANN models with db6, db8 and db10, and WANFIS models with

511

db8, db10 and sym10 were found to yield higher efficiency than other models when

512

considering all input sets.

513

Model performance for different mother wavelets from each input set was also evaluated.

514

Figure 8 shows model performance rankings of WANN and WANFIS for different mother

515

wavelets from each input set. The numbers (Figure 8) indicate model performance rankings

516

from each input set. It can be seen from figure 8(a) that WANN models with db6, coif12 and

517

coif18 yielded higher efficiency than the other mother wavelets for Set 1.The first three best

518

mother wavelets for the other sets were obtained as: db6, db8 and db10 for Set 2; db10,

519

sym10 and coif18 for Set 3; db6, db10 and sym4 for Set 4; db6, db8 and db10 for Set 5; db4,

520

db10 and coif18 for Set 6; db4, db8 and db10 for Set 7. In particular, WANN models with

521

db10 yielded the highest efficiency for Set 2, Set 5, Set 6 and Set 7, and the second highest

522

efficiency for Set 4. This indicates that wavelet decomposition using mother wavelet, db10,

523

can further improve the efficiency of ANN models as compared with the other mother

524

wavelets.

525

It can be seen from figure 8(b) that WANFIS models with db10, sym8 and coif6 yielded

526

higher efficiency than the other mother wavelet for Set 1. The first three best mother wavelets

527

for the other sets were obtained as: db10, sym8 and coif12 for Set 2; db10, sym8 and sym10

528

for Set 3; db8, db10 and sym8 for Set 4; db8, db10 and sym10 for Set 5; db8, db10 and

529

coif18 for Set 6; db10, sym10 and coif18 for Set 7. In particular, WANFIS models with db10

530

yielded the highest efficiency for Set 2, Set 4 and Set 6, and the second highest efficiency for

531

Set 3, Set 5 and Set 7. This indicates that wavelet decomposition using mother wavelet, db10,

532

can further improve the efficiency of ANFIS models as compared with the other mother

533

wavelets.

534

Table 3 summarizes the values of performance measures for the optimal ANN, ANFIS,

535

WANN and WANFIS models. It is clear from the table 3 that WANN models such as

536

WANN5-db10, WANN6-db10, WANN7-db4, WANN7-db10 and WANN7-coif12 have higher

537

CE, r2 and d values and lower RMSE, MAE, MSRE and MS4E values than those of the

538

optimal ANN7 model. WANFIS models such as WANFIS7-db8, WANFIS7-db10, WANFIS7-

539

sym8, WANFIS7-sym10 and WANFIS7-coif18 also have higher CE, r2 and d values and

540

lower RMSE, MAE, MSRE and MS4E values than those of the optimal ANFIS5 model. This

541

indicates that the WANN and WANFIS models are superior to the ANN and ANFIS models in

542

terms of various model efficiencies. It should be noted that the WANN and WANFIS models

543

which use wavelet components by DWT as inputs could yield higher efficiency than the ANN

544

and ANFIS models which use original input data without the wavelet decomposition.

545

Comparison of ANN and ANFIS models indicates that ANFIS5 model performs better

546

than ANN7 model with respect to CE, r2, d, RMSE, MAE, MSRE and MS4E statistics. It can

547

be said that the ANFIS model is superior to the ANN model in forecasting daily water level.

548

Comparison of wavelet-based ANN and ANFIS models reveals that the WANN models have

549

lower CE, r2 and d values and higher RMSE, MAE, MSRE and MS4E values than those of

550

the WANFIS models. Among the wavelet-based models, the WANFIS7-sym10 model yielded

551

the best performance with respect to various model efficiencies.

552

Figure 9 and 10 show scatter plots of the optimal models given in Table 3 for the testing

553

period. Standard deviations around the y = x line (red line) for WANN and WANFIS

554

models are lower than those for ANN and ANFIS models. In other words, when y = ax + b

555

lines (blue lines) fitted in the scatter graph were examined, for the WANN and WANFIS

556

models, a gets closer to the value 1 and b gets closer the value 0. Figure 11 and 12 show

557

water level hydrographs of the optimal models for the testing period. Each upper figure

558

shows the water level hydrographs of total testing period, whereas each lower figure shows

559

the zoom part of the rectangle box for the upper figure. It is clear from the figures that the

560

estimates of the WANN and WANFIS models are closer to the corresponding observed water

561

level values than those of the ANN and ANFIS models. From all these graphs, it can be said

562

that the wavelet decomposition can significantly improve the efficiency of the ANN and

563

ANFIS models in forecasting daily water levels. It was also observed from the scatter plots

564

that, for WANFIS models, two straight lines were almost consistent and standard deviations

565

were lower compared with other models. Especially, the WANFIS models produced better

566

forecasting for high water level values, while other models were slightly overestimated or

567

underestimated. This indicates that the WANFIS models are more accurate than the other

568

models.

569
570

5. Conclusions

571

This paper investigates the accuracy of two different hybrid models, wavelet-based

572

artificial neural networks and wavelet-based neuro-fuzzy inference system, for forecasting

573

daily water level in the Andong dam watershed, South Korea. The specific objectives are to

574

develop and evaluate the methods for improving daily water level forecasting, comparing

575

with the single ANN and ANFIS models, and evaluating the performance of these models

576

based on various performance indexes, including the coefficient of efficiency (CE), the index

577

of agreement (d), the coefficient of determination (r2), the root mean squared error (RMSE),

578

the mean absolute error (MAE), the mean squared relative error (MSRE), and the mean

579

higher order error (MS4E).

580

The WANN and WANFIS models perform better results than the ANN and ANFIS

581

models, respectively. Comparison wavelet-based models indicate that the WANFIS models

582

are superior to the WANN models, and WANFIS7-sym10 yields the best performance among

583

all other models in terms of various model efficiencies. It is found that the conjunction of

584

wavelet decomposition and ANN and ANFIS models can significantly improve the models

585

accuracy in forecasting daily water level. The effect of different mother wavelets such as

586

Daubechies (db1, db2, db4, db6, db8 and db10), Symmlet (sym2, sym4, sym8 and sym10)

587

and Coiflet (coif6, coif12 and coif18) wavelets on the accuracy of WANN and WANFIS

588

models were also investigated in this study. It is found that the model performance is

589

dependent on input sets and mother wavelets, and the wavelet decomposition using db10 can

590

further improve the efficiency of the ANN and ANFIS models. Results obtained in this study

591

indicate that the conjunction with wavelet decomposition and artificial intelligence models

592

can be a successful tool for forecasting daily water level.

593

The developed models can be applied to forecast hydrological variables for other

594

watersheds having different climate, geographical and hydrological conditions for improving

595

the applicability of models. It can be suggested to develop hybrid models combining wavelet

596

decomposition with other artificial intelligence models and evolutionary algorithms for

597

forecasting hydrological variables with non-stationary and non-linear relationships. A

598

comparative study on different learning algorithms such as Levenberg-Marquardt and

599

conjugate gradient algorithms can be also suggested to improve the performance of hybrid

600

models. It can be also suggested as further studies to investigate the model performance for

601

different input series constructed from effective or all wavelet components.

602
603

References

604

Abdullah, A., 2000. Some aspects of wavelet analysis in time series. Research Report 2000:4,

605
606
607
608
609
610

Department of Statistics, Gteborg University, Sweden.
Abiyev, R.H., 2011. Fuzzy wavelet neural network based on fuzzy clustering and gradient
techniques for time series prediction. Neural Comput. Appl. 20 (2), 249-259.
Adamowski, J., Chan, H.F., 2011. A wavelet neural network conjunction model for
groundwater level forecasting. J. Hydrol. 407 (1), 28-40.
Adamowski, J., Prasher, S.O., 2012. Comparison of machine learning methods for runoff

611

forecasting in mountainous watersheds with limited data. J. Water Land Dev. 17, 89-97.

612

Adamowski, J., Sun, K., 2010. Development of a coupled wavelet transform and neural

613

network method for flow forecasting of non-perennial rivers in semi-arid watersheds. J.

614

Hydrol. 390 (1), 85-91.

615
616
617
618
619
620

Alikhani, A., 2009. Combination of neuro fuzzy and wavelet model usage in river
engineering. Int. J. Energy Environ. 3 (3), 122-134.
Anctil, F., Tape, D.G., 2004. An exploration of artificial neural network rainfall-runoff
forecasting combined with wavelet decomposition. J. Environ. Eng. Sci. 3, 121-128.
Bae, D.H., Jeong, D.M., Kim, G., 2007. Monthly dam inflow forecasts using weather
forecasting information and neuro-fuzzy technique. Hydrol. Sci. J. 52 (1), 99-113.

621

Belayneh, A., Adamowski, J., 2012. Standard precipitation index drought forecasting using

622

neural networks, wavelet neural networks, and support vector regression. Appl. Comput.

623

Intell. Soft Comput. 2012 (6), doi:10.1155/2012/794061.

624
625
626
627

Bergmeir, C., Bentez, J.M., 2012. Neural networks in R using the Stuttgart neural network
simulator: RSNNS. J. Stat. Softw. 46 (7), 1-26.
Bishop, C.M., 1994. Neural networks and their applications. Rev. Sci. Instrum., 65 (6), 18031832.

628

Cannas, B., Fanni, A., Sias, G., Tronci, S., Zedda, M.K., 2005. River flow forecasting using

629

neural networks and wavelet analysis. In: Proceedings of the European Geosciences

630

Union, Vienna, Austria, 7, 24-29.

631

Cannas, B., Fanni, A., See, L., Sias, G., 2006. Data preprocessing for river flow forecasting

632

using neural networks: Wavelet transforms and data partitioning. Phys. Chem. Earth. 31

633

(18), 1164-1171.

634

Catalo, J.P.S., Pousinho, H.M.I., Mendes, V.M.F., 2011. Hybrid wavelet-PSO-ANFIS

635

approach for short-term electricity prices forecasting. IEEE Trans. Power Syst. 26 (1),

636
637
638

137-144.
Chau, K., Wu, C., Li, Y., 2005. Comparison of several flood forecasting models in Yangtze
River. J. Hydrol. Eng. 10 (6), 485-491.

639

Cheng, C., Chau, K., Sun, Y., Lin, J., 2005a. Long-term prediction of discharges in Manwan

640

Reservoir using artificial neural network models. Lect. Notes Comput. Sci. 3498, 1040-

641

1045.

642

Cheng, C.T., Lin, J.Y., Sun, Y.G., Chau, K., 2005b. Long-term prediction of discharges in

643

Manwan hydropower using adaptive-network-based fuzzy inference systems models.

644

Lect. Notes Comput. Sci. 3612, 1152-1161.

645
646
647
648

Chou, C.M., Wang, R.Y., 2004. Application of wavelet based multi-model Kalman filters to
real-time flood forecasting. Hydrol. Process. 18 (5), 987-1008.
Cigizoglu, H.K., Kisi, O., 2005. Flow prediction by three back propagation techniques using
k-fold partitioning of neural network training data. Nordic Hydrol. 36 (1), 49-64.

649

Constantine, W., Percival, D., 2013. wmtsa: Wavelet methods for time series analysis. R

650

package version 2.0-0. http://CRAN.R-project.org/package=wmtsa. Accessed 6 July 2014.

651

Coulibaly, P., Anctil, F., Bobee, B., 2000. Daily reservoir inflow forecasting using artificial

652
653
654
655
656

neural networks with stopped training approach. J. Hydrol. 230 (3-4), 244-257.
Dawson, C.W., Wilby, R.L., 2001. Hydrological modelling using artificial neural networks.
Prog. phys. Geog. 25 (1), 80-108.
de Artigas, M.Z., Elias, A.G., de Campra, P.F., 2006. Discrete wavelet analysis to assess
long-term trends in geomagnetic activity. Phys. Chem. Earth, 31 (1-3), 77-80.

657

Deka, P.C., Haque, L., Banhatti, A.G., 2012. Discrete wavelet-ANN approach in time series

658

flow forecasting  A case study of Brahmaputra River. Int. J. Earth Sci. Eng., 5 (4), 673-

659

685.

660

El-Shafie, A., Taha, M.R., Noureldin, A., 2007. A neuro-fuzzy model for inflow forecasting

661

of the Nile river at Aswan high dam. Water Resour. Manag. 21 (3), 533-556.

662

Evrendilek, F., 2014. Assessing neural networks with wavelet denoising and regression

663

models in predicting diel dynamics of eddy covariance-measured latent and sensible heat

664

fluxes and evapotranspiration. Neural Comput. Appl., 24 (2), 327-337.

665

Figueiredo, K., Barbosa, C.R.H., Da Cruz, A., Vellasco, M., Pacheco, M., Conteras, R., 2007.

666

Neural networks for inflow forecasting using precipitation information. Lect. Notes

667

Comput. Sci. 4570, 552-561.

668
669

Gallant, S.I., 1993. Neural network learning and expert system, MIT Press, Cambridge, MA,
USA.

670

Gonzlez-Audcana, M., Otazu, X., Fors, O., Seco, A., 2005. Comparison between Mallats

671

and the  trous discrete wavelet transform based algorithms for the fusion of

672

multispectral and panchromatic images. Int. J. Remote Sens., 26 (3), 595-614.

673

Gnther, F., Fritsch, S., 2010. Neuralnet: Training of neural networks. R J. 2 (1), 30-38.

674

Han, D., Kwong, T., Li, S., 2007. Uncertainties in real-time flood forecasting with neural

675

networks. Hydrol. Process. 21 (2), 223-228.

676

Hassanain, M.A., Taha, M.R., Noureldin, A., El-Sheimy, N., 2004. Automization of INS/GPS

677

integration system using genetic optimization. In: Proceedings of the 5th International

678

Symposium of Soft Computing for Industry, Seville, Spain.

679

Haykin, S., 2009. Neural networks and learning machines, 3rd ed., Prentice Hall, NJ, USA.

680

Hornik, K., Stichcombe, M., White, H., 1989. Multilayer feedforward networks are universal

681

approximators. Neural Netw. 2 (5), 359-366.

682

Imrie, C.E., Durucan, S., Korre, A., 2000. River flow prediction using artificial neural

683

networks: Generalisation beyond the calibration range. J. Hydrol. 233 (1-4), 138-153.

684

Jain, S.K., Das, A., Srivastava, D.K., 1999. Application of ANN for reservoir inflow

685

prediction and operation. J. Water Resour. Plan. Manag. 125 (5), 263-271.

686
687
688
689
690
691
692
693

Jang, J.S.R., Sun, C.T., Mizutani, E., 1997. Neuro-fuzzy and soft computing: A computational
approach to learning and machine intelligence. Prentice-Hall, New Jersey.
Jang, J.S.R., 1993. ANFIS: Adaptive-network-based fuzzy inference system. IEEE Trans.
Syst. Man Cybern. 23 (3), 665-685.
Jeong, D., Kim, Y.O., 2005. Rainfall-runoff models using artificial neural networks for
ensemble streamflow prediction. Hydrol. Process. 19 (19), 3819-3835.
Jia, Y., Culver, T.B., 2006. Bootstrapped artificial neural networks for synthetic flow
generation with a small data sample. J. Hydrol. 331 (3-4), 580-590.

694

Jothiprakash, V., Magar, R.B., 2012. Multi-time-step ahead daily and hourly intermittent

695

reservoir inflow prediction by artificial intelligent techniques using lumped and

696

distributed data. J. Hydrol. 450-451, 293-307.

697
698
699
700

Karimi-Googhari, S.H., Lee, T.S., 2011. Applicability of adaptive neuro-fuzzy inference
systems in daily reservoir inflow forecasting. Int. J. Soft Comput. 6 (3), 75-84.
Karunasinghe, D.S.K., Liong, S.Y., 2006. Chaotic time series prediction with a global model:
Artificial neural network. J. Hydrol. 323 (1-4), 92-105.

701

Khanghah, T.R., Nourani, V., Parhizkar, M., Sharghi, E., 2012. Application of information

702

content to extract wavelet-based feature of rainfall-runoff process. In: Proceedings of the

703

12th WSEAS International Conference on Applied Computer Science, WSEAS, Greece,

704

148-153.

705
706
707
708
709
710

Kim, B., Park, J.H., Kim, B.S., 2002. Fuzzy logic model of Langmuir probe discharge data.
Comput. Chem. 26 (6), 573-581.
Kim, S., Park, K.B., Seo, Y., 2012. Estimation of pan evaporation using neural networks and
climate-based models. Disaster Adv., 5(3), 34-43.
Kim, S., Shiri, J., Kisi, O., 2012. Pan evaporation modeling using neural computing approach
for different climatic zones. Water Resour. Manag. 26 (11), 3231-3249.

711

Kim, S., Seo, Y., Singh, V.P., 2013a. Assessment of pan evaporation modeling using

712

bootstrap

713

doi:10.1061/(ASCE)CP. 19435487.0000367.

resampling

and

soft

computing

methods.

J.

Comput.

Civ.

Eng.

714

Kim, S., Shiri, J., Kisi, O., Singh, V.P., 2013b. Estimating daily pan evaporation using

715

different data-driven methods and lag-time patterns. Water Resour. Manag. 27 (7), 2267-

716

2286.

717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735

Kisi, O., 2007. Streamflow forecasting using different artificial neural network algorithms. J.
Hydrol. Eng. 12 (5), 532-539.
Kisi, O., 2008. River flow forecasting and estimation using different artificial neural network
techniques. Hydrol. Res. 39 (1), 27-40.
Kisi, O., 2010. Wavelet regression model for short-term streamflow forecasting. J. Hydrol.
389 (3-4), 344-353.
Kisi, O., 2011. Evapotranspiration modeling using a wavelet regression model. Irrig. Sci. 29
(3), 241-252.
Kisi, O., Cimen, M., 2011. A wavelet-support vector machine conjunction model for monthly
streamflow forecasting. J. Hydrol. 399 (1-2), 132-140.
Kisi, O. Shiri, J. Nazemi, A.H., 2011. A wavelet-genetic programming model for predicting
short-term and long-term air temperatures. J. Civ. Eng. Urban. 1 (1), 25-37.
Mallat, S.G., 1989. A theory for multiresolution signal decomposition: The wavelet
representation. IEEE Trans. Pattern. Anal. Mach. Intell. 11 (7), 674-693.
Mamdani, E.H., Assilian, S., 1975. An experiment in linguistic synthesis with a fuzzy logic
controller. Int. J. Man Mach. Stud. 7 (1), 1-13.
Mathworks, 2014a. Fuzzy logic toolbox user's guide. The Mathworks, Inc. http://www.
mathworks.com/help/pdf_doc/fuzzy/fuzzy.pdf. Accessed 6 July 2014.
Mathworks, 2014b. Wavelet toolbox users guide. The Mathworks, Inc. http://www.

736
737
738

mathworks.com/help/pdf_doc/wavelet/wavelet_ug.pdf. Accessed 6 July 2014.
Minu, K.K., Lineesh, M.C., John, C.J., 2010. Wavelet neural networks for nonlinear time
series analysis. Appl. Math. Sci., 4 (50), 2485-2495.

739

Nason, G., 2010. Wavelet methods in statistics with R. Springer, New York.

740

Nejad, F.H., Nourani, V., 2012. Elevation of wavelet denoising performance via an ANN-

741

based streamflow forecasting model. Int. J. Comput. Sci. Manag. Res. 1 (4), 764-770.

742

Nourani, V., Alami, M.T., Aminfar, M.H., 2009. A combined neural-wavelet model for

743

prediction of Ligvanchai watershed precipitation. Eng. Appl. Artif. Intell. 22 (3), 466-472.

744

Nourani, V., Kisi, O., Komasi, M., 2011. Two hybrid artificial intelligence approaches for

745

modeling rainfall-runoff process. J. Hydrol. 402 (1-2), 41-59.

746

Nourani, V., Parhizkar, M., Khanghah, T.R., Baghanam, A.H., Sharghi, E., 2012. Wavelet-

747

based feature extraction of rainfall-runoff process via self-organizing map. In:

748

Proceedings of the 12th WSEAS International Conference on Applied Computer Science,

749

WSEAS, Greece, 101-106.

750

Okkan, U., 2012. Using wavelet transform to improve generalization capability of feed

751

forward neural networks in monthly runoff prediction. Sci. Res. Essays 7 (17), 1690-1703.

752

Okkan, U., Serbes, Z.A., 2013. The combined use of wavelet transform and black box models

753
754
755
756
757

in reservoir inflow modeling. J. Hydrol. Hydromech. 61 (2), 112-119.
Othman, F., Naseri, M., 2011. Reservoir inflow forecasting using artificial neural network. Int.
J. Phys. Sci. 6 (3), 434-440.
Partal, T., Cigizoglu, H.K., 2009. Estimation and forecasting of daily suspended sediment
data using wavelet-neural networks. J. Hydrol. 358 (3-4), 317-331.

758

Popivanov, I., Miller, R.J., 2002. Similarity search over time-series data using wavelets. In:

759

Proceedings of 18th International Conference on Data Engineering, San Jose, CA, 212-

760

221.

761

R Core Team, 2014. R: A language and environment for statistical computing. R Foundation

762

for Statistical Computing, Vienna, Austria. http://www.R-project.org. Accessed 6 July

763

2014.

764
765

Rajaee, T., 2010. Wavelet and neuro-fuzzy conjunction approach for suspended sediment
prediction. Clean Soil Air Water 38 (3), 275-288.

766

Rajaee, T., Nourani, V., Mohammad, Z.K., Kisi, O., 2011. River suspended sediment load

767

prediction: Application of ANN and wavelet conjunction model. J. Hydrol. Eng. 16 (8),

768

613-627.

769
770

Razavi, S., Araghinejad, S., 2009. Reservoir inflow modeling using temporal neural networks
with forgetting factor approach. Water Res. Manag. 23 (1), 39-55.

771

Santos, C.A.G., Freire, P.K.M.M., Silva, G.B.L., Silva, R.M., 2014. Discrete wavelet

772

transform coupled with ANN for daily discharge forecasting into Trs Marias reservoir.

773

In: Proceedings of the International Association of Hydrological Sciences, Bologna, Italy,

774

100-105.

775

Seo, Y., Kim, S., Singh, V.P., 2013a. Flood forecasting and uncertainty assessment using

776

bootstrapped ANFIS. In: Proceedings of 6th Conference of Asia Pacific Association of

777

Hydrology and Water Resources, Seoul, South Korea, 1-8.

778

Seo, Y., Park, K.B., Kim, S., 2013b. Comparative study on fuzzy rule-based systems for flood

779

level forecasting. In: Proceedings of Korea Water Resources Association, South Korea,

780

421-425 (In Korean).

781

Seo, Y., Park, K.B., Kim, S., Singh, V.P., 2013c. Application of bootstrap-based artificial

782

neural networks to flood forecasting and uncertainty assessment. In: Proceedings of 6th

783

International Perspective on Water Resources and the Environment, EWRI-ASCE, Izmir,

784

Turkey.

785

Sharma, S.K., Tiwari, K.N., 2009. Bootstrap based artificial neural network (BANN) analysis

786

for hierarchical prediction of monthly runoff in Upper Damodar Valley Catchment. J.

787

Hydrol. 374 (3-4), 209-222.

788
789
790
791

Simpson, P.K., 1990. Artificial neural system: foundations, paradigms, applications and
implementations, Elsevier, NY, USA.
Specht, D.F., 1991. A general regression neural network. IEEE Trans. Neural Netw., 2 (6),
568-576.

792

Srivastav, R.K., Sudheer, K.P., Chaubey, I., 2007. A simplified approach to quantifying

793

predictive and parametric uncertainty in artificial neural network hydrologic models.

794

Water Resour. Res. 43 (10), W10407.

795
796
797
798
799
800
801
802

Stolojescu, C.L., 2012. A wavelets based approach for time series mining, Ph.D. dissertation,
Telecom Bretagne, France.
Sudheer, K.P., Gosain, A.K., Ramasastri, K.S., 2002. A data-driven algorithm for constructing
artificial neural network rainfall-runoff models. Hydrol. Process., 16 (6), 1325-1330.
Takagi, T., Sugeno, M., 1985. Fuzzy identification of systems and its application to modeling
and control. IEEE Trans. Syst. Man Cybern. 15 (1), 116-132.
Tibshirani, R., 1996. A comparison of some error estimates for neural network models.
Neural Comput. 8 (1), 152-163.

803

Tiwari, M.K., Chatterjee, C., 2010a. Uncertainty assessment and ensemble flood forecasting

804

using bootstrap based artificial neural networks (BANNs). J. Hydrol. 382 (1-4), 20-33.

805

Tiwari, M.K., Chatterjee, C., 2010b. Development of an accurate and reliable hourly flood

806

forecasting model using wavelet-bootstrap-ANN (WBANN) hybrid approach. J. Hydrol.

807

394 (3-4), 458-470.

808
809
810

Tsoukalas, L.H., Uhrig, R.E., 1997. Fuzzy and neural approaches in engineering, John Wiley
& Sons Inc., NY, USA.
Twomey, J.M., Smith, A.E., 1998. Bias and variance of validation methods for function

811

approximation neural networks under conditions of sparse data. IEEE Trans. Syst. Man

812

Cybern. 28 (3), 417-430.

813
814
815
816
817
818
819
820

Vallet-Coulomb, C., Legesse, D., Gasse, F., Travi, Y., Chernet, T., 2001. Lake evaporation
estimates in tropical Africa (Lake Ziway, Ethiopia). J. Hydrol., 245 (1-4), 1-18.
Vonesch, C., Blu, T., Unser, M., 2007. Generalized Daubechies wavelet families. IEEE Trans.
Signal Process., 55 (9), 4415-4429.
Wang, W., Ding, J., 2003. Wavelet network model and its application to the prediction of
hydrology. Nat. Sci. 1 (1), 67-71.
Wang, W., Van Gelder, P., Vrijling, J.K., Ma, J., 2006a. Forecasting daily streamflow using
hybrid ANN models. J. Hydrol. 324 (1-4), 383-399.

821

Wang, H.F., Huang, W.J., Wang, W.S., 2006b. Cuntan station of the Yangtze River annual

822

runoff forecasting with set pair analysis method. J. Heilongjiang Hydraul. Eng. Coll. 33

823

(4), 3-5.

824
825

Wang, W., Jin, J., Li, Y., 2009. Prediction of inflow at Three Gorges Dam in Yangtze River
with wavelet network model. Water Res. Manag. 23 (13), 2791-2803.

826

Wei, S., Song, J., Khan, N.I., 2012. Simulating and predicting river discharge time series

827

using a waveletneural network hybrid modelling approach. Hydro. Process. 26 (2), 281-

828

296.

829
830
831
832

Wu, C.L., Chau, K.W., Li, Y.S., 2009. Predicting monthly streamflow using data-driven
models coupled with data-preprocessing techniques. Water Resour. Res. 45 (8), W08432.
Zhang, G., Patuwo, B.E., Hu, M.Y., 1998. Forecasting with artificial neural networks: The
state of the art. Int. J. Forecast. 14 (1), 35-62.

833

Zio, E., 2006. A study of the bootstrap method for estimating the accuracy of artificial neural

834

networks in predicting nuclear transient processes. IEEE Trans. Nucl. Sci. 53 (3), 1460-

835

1478.

837

List of figures

838
839

Figure 1 Mallats algorithm for three-level decomposition of a signal

840

Figure 2 General architecture of ANN

841

Figure 3 General architecture of ANFIS

842

Figure 4 Flowchart for WANN and WANFIS

843

Figure 5 Study region and locations of observations

844

Figure 6 Original and decomposed time series (D1, D2, D3 and A3) using db10 wavelet for

845
846
847
848
849

training period
Figure 7 Model performance rankings of WANN and WANFIS for different input sets and
mother wavelets
Figure 8 Model performance rankings of WANN and WANFIS for different mother wavelets
for each input set

850

Figure 9 Scatter plot for ANN and WANN models

851

Figure 10 Scatter plot for ANFIS and WANFIS models

852

Figure 11 Daily water level hydrograph for ANN and WANN models

853

Figure 12 Daily water level hydrograph for ANFIS and WANFIS models

854
855

856

List of tables

857
858

Table 1 Different sets of input and output variables for model configuration

859

Table 2 Different configurations of models with higher performance

860

Table 3 Comparison of model performance

861

862
863

Table 1 Different sets of input and output variables for model configuration

864

Input set
Input variables
Set 1
WLSC(t)
Set 2
WLSC(t), WLDS(t-1)
Set 3
WLSC(t), WLDS(t-2), WLDS(t-1)
Set 4
WLSC(t-1), WLSC(t), WLDS(t-1)
Set 5
WLSC(t-1), WLSC(t), WLDS(t-2), WLDS(t-1)
Set 6
WLSC(t-1), WLSC(t), WLDS(t-3), WLDS(t-2), WLDS(t-1)
Set 7
WLSC(t-2), WLSC(t-1), WLSC(t), WLDS(t-3), WLDS(t-2), WLDS(t-1)
WL: daily water level, SC: Socheon station, DS: Dosan station

865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884

Output variable
WLDS(t)
WLDS(t)
WLDS(t)
WLDS(t)
WLDS(t)
WLDS(t)
WLDS(t)

885
886
887
888
889

Table 2 Different configurations of models with higher performance
Models
ANN7
WANN5-db10
WANN6-db10
WANN7-db4
WAMM7-db10
WANN7-coif12
ANFIS5
WANFIS7-db8
WANFIS7-db10
WANFIS7-sym8
WANFIS7-sym10
WANFIS7-coif18

890
891
892
893
894
895
896
897
898
899
900
901
902
903
904

Input sets
Set 7
Set 5
Set 6
Set 7
Set 7
Set 7
Set 5
Set 7
Set 7
Set 7
Set 7
Set 7

Mother wavelets
Daubechies-10 (db10)
Daubechies-10 (db10)
Daubechies-4 (db4)
Daubechies-10 (db10)
Coiflet-12 (coif12)
Daubechies-8 (db8)
Daubechies-10 (db10)
Symmlet-8 (sym8)
Symmlet-10 (sym10)
Coiflet-18 (coif18)

The digits after ANN, ANFIS, WANN and WANFIS represent input sets from table 1. For example, WANN5db10 means WANN model for Set 5 and mother wavelet, db10.

905
906
907
908
909
910
911

Table 3 Comparison of model performance
Models
ANN7
WANN5-db10
WANN6-db10
WANN7-db4
WANN7-db10
WANN7-coif12
ANFIS5
WANFIS7-db8
WANFIS7-db10
WANFIS7-sym8
WANFIS7-sym10
WANFIS7-coif18

912
913

CE
0.965
0.979
0.979
0.979
0.980
0.979
0.980
0.996
0.996
0.995
0.996
0.996

d

r2

0.991
0.994
0.995
0.994
0.995
0.994
0.995
0.999
0.999
0.999
0.999
0.999

0.967
0.980
0.981
0.981
0.981
0.979
0.982
0.995
0.996
0.995
0.996
0.996

RMSE
(m)
0.0787
0.0615
0.0616
0.0610
0.0597
0.0617
0.0595
0.0282
0.0269
0.0293
0.0267
0.0273

MAE
(m)
0.0525
0.0422
0.0419
0.0440
0.0411
0.0433
0.0361
0.0171
0.0168
0.0177
0.0162
0.0166

MSRE
(10-6)
0.2154
0.1319
0.1327
0.1299
0.1244
0.1329
0.1233
0.0278
0.0253
0.0298
0.0248
0.0259

MS4E
(10-4m4)
11.909
1.747
1.577
0.944
1.425
1.286
2.138
0.140
0.093
0.166
0.116
0.104

LEVEL 1

LEVEL 2

LEVEL 3

High-pass Downsample

Hi

2

s

D1
Hi

Lo

2

Low-pass Downsample

Lo

Hi

Convolve with high-pass filter (Hi)

Lo

Convolve with low-pass filter (Lo)

2

2

Keep the even indexed elements
(downsampling)

D2
Hi

2

D3

Lo

2

A3

2

s: signal
D1, D2 and D3: Details
A3: Approximation

Figure 1 Mallats algorithm for three-level decomposition of a signal

Bias 1

Bias 2

HIDDEN
LAYER

INPUT
LAYER
Input 1

Input 2

OUTPUT
LAYER
Output

Input 3
Input 4

Input 5

Figure 2 General architecture of ANN

LAYER 1
A1

LAYER 3

LAYER 2
w1

x

N

LAYER 4
y
x

LAYER 5

w1
w1f1

A2

f
B1
y
B2

w2

w2f2

N
w2

x

y

Figure 3 General architecture of ANFIS

Input Data
(WLSC(t-2), WLSC (t-1), WLSC(t), WLDS(t-3), WLDS(t-2), WLDS(t-1))

Discrete Wavelet Decomposition
(db1, db2, db4, db6, db8, db10, sym2, sym4, sym8, sym10, coif6, coif12, coif18)

Details

Approximation

(D1, D2, D3)

(A3)

Training ANN and ANFIS
ANN

ANFIS

Setting ANN Structure

Generating FIS

- Input, hidden and output layers
- Logistic activation function
- Identity output function

- Subtractive clustering
- Input MFs: Gaussian type
- Output MF: Linear type

Training ANN

Training FIS

- Vanilla backpropagation

- Hybrid learning algorithm

Performance Evaluation
- CE, d, r2, RMSE, MAE, MSRE, MS4E

Comparative Analysis
Figure 4 Flowchart for WANN and WANFIS

South
Korea
Andong Dam
Watershed

Nakdong
River
Basin

Figure 5 Study region and locations of observations

(a) Original time series

(b) D1

(c) D2
Figure 6 Original and decomposed time series (D1, D2, D3 and A3) using db10 wavelet for
training period

(d) D3

(e) A3
Figure 6 Original and decomposed time series (D1, D2, D3 and A3) using db10 wavelet for
training period (Contd)

coif18

80

70

50

37

15

10

27

coif12

79

68

59

46

26

42

5

coif6

88

74

65

53

52

55

49

sym10

85

64

45

41

16

29

13

sym8

84

76

56

54

19

35

23

sym4

91

71

66

36

17

27

31

sym2

82

78

67

47

32

43

34

db10

83

58

51

25

4

2

1

db8

89

61

57

39

9

18

5

db6

81

63

60

22

14

11

7

db4

89

72

62

40

20

8

2

db2

86

77

69

48

44

21

29

db1

87

73

75

37

33

23

11

Wavelets

80

60

40

20

0

Set 1 Set 2 Set 3 Set 4 Set 5 Set 6 Set 7

Input sets
(a) WANN
Figure 7 Model performance rankings of WANN and WANFIS for different input sets and
mother wavelets

coif18

83

68

48

22

28

21

3

coif12

86

60

54

43

27

26

6

coif6

80

75

71

53

44

42

12

sym10

91

63

29

25

8

23

1

sym8

79

60

45

18

41

35

5

sym4

85

69

66

39

33

30

9

sym2

87

76

73

56

51

46

36

db10

81

60

34

16

14

11

2

db8

90

64

58

17

15

13

4

db6

82

67

59

19

20

24

7

db4

83

69

65

39

32

30

9

db2

88

78

72

55

51

49

38

db1

88

77

73

57

50

47

37

Wavelets

80

60

40

20

0

Set 1 Set 2 Set 3 Set 4 Set 5 Set 6 Set 7

Input sets
(b) WANFIS
Figure 7 Model performance rankings of WANN and WANFIS for different input sets and
mother wavelets (Contd)

Wavelets

coif18

2

6

2

4

4

3

9

coif12

1

5

6

9

9

11

3

coif6

10

10

9

12

13

13

13

sym10

7

4

1

8

5

9

7

sym8

6

11

4

13

7

10

8

sym4

13

7

10

3

6

8

11

sym2

4

13

11

10

10

12

12

db10

5

1

3

2

1

1

1

db8

11

2

5

6

2

5

3

db6

3

3

7

1

3

4

5

db4

11

8

8

7

8

2

2

db2

8

12

12

11

12

6

10

db1

9

9

13

4

11

7

6

12

10

8

6

4

2

Set 1 Set 2 Set 3 Set 4 Set 5 Set 6 Set 7

Input sets
(a) WANN
Figure 8 Model performance rankings of WANN and WANFIS for different mother
wavelets for each input set

Wavelets

coif18

5

7

4

5

6

3

3

coif12

8

1

5

9

5

6

6

coif6

2

10

10

10

10

10

10

sym10

13

4

1

6

1

4

1

sym8

1

1

3

3

9

9

5

sym4

7

8

9

7

8

7

8

sym2

9

11

12

12

12

11

11

db10

3

1

2

1

2

1

2

db8

12

5

6

2

3

2

4

db6

4

6

7

4

4

5

7

db4

5

8

8

7

7

7

8

db2

10

13

11

11

12

13

13

db1

10

12

12

13

11

12

12

12

10

8

6

4

2

Set 1 Set 2 Set 3 Set 4 Set 5 Set 6 Set 7

Input sets
(b) WANFIS
Figure 8 Model performance rankings of WANN and WANFIS for different mother
wavelets in each input set (Contd)

(a) ANN7

(b) WANN5-db10
Figure 9 Scatter plot for ANN and WANN models

(c) WANN6-db10

(d) WANN7-db4
Figure 9 Scatter plot for ANN and WANN models (Contd)

(e) WANN7-db10

(f) WANN7-coif12
Figure 9 Scatter plot for ANN and WANN models (Contd)

(a) ANFIS5

(b) WANFIS7-db8
Figure 10 Scatter plot for ANFIS and WANFIS models

(c) WANFIS7-db10

(d) WANFIS7-sym8
Figure 10 Scatter plot for ANFIS and WANFIS models (Contd)

(e) WANFIS7-sym10

(f) WANFIS7-coif18
Figure 10 Scatter plot for ANFIS and WANFIS models (Contd)

(a) ANN7
Figure 11 Daily water level hydrograph for ANN and WANN models

(b) WANN5-db10
Figure 11 Daily water level hydrograph for ANN and WANN models (Contd)

(c) WANN6-db10
Figure 11 Daily water level hydrograph for ANN and WANN models (Contd)

(d) WANN7-db4
Figure 11 Daily water level hydrograph for ANN and WANN models (Contd)

(e) WANN7-db10
Figure 11 Daily water level hydrograph for ANN and WANN models (Contd)

(f) WANN7-coif12
Figure 11 Daily water level hydrograph for ANN and WANN models (Contd)

(a) ANFIS5
Figure 12 Daily water level hydrograph for ANFIS and WANFIS models

(b) WANFIS7-db8
Figure 12 Daily water level hydrograph for ANFIS and WANFIS models (Contd)

(c) WANFIS7-db10
Figure 12 Daily water level hydrograph for ANFIS and WANFIS models (Contd)

(d) WANFIS7-sym8
Figure 12 Daily water level hydrograph for ANFIS and WANFIS models (Contd)

(e) WANFIS7-sym10
Figure 12 Daily water level hydrograph for ANFIS and WANFIS models (Contd)

(f) WANFIS7-coif18
Figure 12 Daily water level hydrograph for ANFIS and WANFIS models (Contd)

914
915

-

916

We applied two hybrid models for daily water level forecasting and investigate their
accuracy

917

-

We applied wavelet decomposition theory to ANN and ANFIS.

918

-

WANN and WANFIS models produce better efficiency than ANN and ANFIS models.

919

-

Wavelet decomposition improves the accuracy of ANN and ANFIS.

920

-

The accuracy of the WANN and WANFIS models for different mother wavelets was also

921
922

evaluated

